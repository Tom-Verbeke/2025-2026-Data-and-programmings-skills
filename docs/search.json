[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Introduction\nIn this introduction we will first show how you can install R and R Studio. After the installation is complete, this chapter continues with an overview of the most important panes in the R Studio IDE. In the third section, you’ll learn how to start a new project and how to organize the files in that project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#installing-r-and-r-studio",
    "href": "introduction.html#installing-r-and-r-studio",
    "title": "1  Introduction",
    "section": "2.1 Installing R and R Studio",
    "text": "2.1 Installing R and R Studio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data and programming skills",
    "section": "",
    "text": "Preface\nThis is the syllabus for the B-KUL-HBA12H Data and programming skills 2025-2026. In this class, You’ll learn how to use R and R Studio to import, tidy, transform and visualize data. R is a programming language for statistical computations. As with any language, learning R includes learning vocabulary and grammar. Using this language, you can tell the computer what it needs to do.\nR was developed in the 1990’s by Ross Ihaka and Robert Gentleman while both were at the Department of Statistics of the University of Auckland in Auckland, New Zealand. Initially, their plan was to write a language that would allow them to teach statistics at Auckland. The name “R” was coined as a joke and refers to the R’s in Robert and Ross. Other departments started to use the R language and it became free software by 1994. As the user base grew, the project needed more structure: the R core team was founded in 1997 and continues to develop the R language, in the same year, the Comprehensive R Archive Network (CRAN) was founded and hosts R’s source code, user created packages, the R foundation was set up in 2003 to offer financial support to the development of R, the R Journal published its first issue in 2009, … . As the time of writing, R version 4.4.2 “Pile of Leaves” was the most recent R version.\nR can be extended through packages. Packages work with R and are usually written with a specific purpose: importing data, data visualization, application of statistical techniques. These packages are usually made available via CRAN. You can think of these packages as apps that you would install on your smartphone or tablet. All smartphones and tablets include an operating system (iOS or Android). These include a number of apps that you can use to make calls, connect to the internet or read and write emails. The operating system and the apps allow you to use your device out of the box. In addition, you can download third party apps through Apple’s app store, Google’s play store or other services that allow you to buy or download apps. Using these apps, you can use your smartphone or tablet to read your newspaper, find your way in a foreign city, make a sketch or play a game. Just like your smartphone or tablet, if you download R, it comes with a number of packages pre-installed. So, you can use R “out of the box”. But you can also download and install other packages. Usually, these packages are developed for a specific purpose: data cleaning, data visualization, statistical modelling, preparing reports, … .Like the apps you install on your smartphone or tablet, you need to download and install these apps in your computer. The repository for these packages is usually CRAN.\nRStudio is an Integrated Development Environment (IDE). R Studio was launched in 2009 by RStudio, a public benefit corporation now called Posit. In IDE helps you to use programming languages as they combine a number of tools in a user interface. For instance, the editor of an IDE usually shows the structures, keywords and code errors using color and font effects (syntax highlighting) and it suggests code (code completion). RStudio also shows that variables that you use in your code, plots and you create. RStudio also includes Quarto. Quarto is an open source publishing system that allows you to create reports and other publications that include R code. The R code is executed within that document. In doing so, Quarto allows you to create e.g. reproducible or dynamic publications. Quarto documents allow you to share your code with your research. In other words, others are capable of producing the exact same results as you did. In addition, tables and figures in a report will change if the underlying data changes. Quarto allows you to publish books, websites, articles, dashboards in HTML, PDF, MS Word, … . As an example, this book was written using Quarto.\nThis book covers the major components of a data science project. This workflow is shown in Figure 1 which was taken from Wickham Hadley, Cetinkaya-Rundel, and Grolemund (2023).\n\n\n\n\n\nFigure 1: A data science project\n\n\n\n\n\n\n\n\nFirst, you need to import the data into R. Usually, data are stored in other files: spreadsheets, sheets in a spreadsheet, parts of a spreadsheet, a database or can be accessed via an API. Files can be stored in various formats: Excel, Google sheets, CSV and many more.\nThe second step involves tidying the data. Tidy data refers to a dataset where each row is an observation and each column if a variable. Although “one row = one observation and one variable = one column” might seem a rule that is observed by most datasets, we’ll see that this is often not the case.\nThe third step includes tranforming your data: creating new variables from existing ones, summary statistics or filtering observations to select only those observations that are relevant for the problem at hand (e.g. all data from the most recent 2 quarters, sales data for a specific product or brand).\nUsing the transformed variables, in the fourth step, you want to understand the data. Visualizing the data is one way to do this. Modelling, the application of statistical and quantitative models, is a second way. Visualization and modeling are often complementary. For instance, a bar chart could suggest that two groups of customers are different with respect to the number of items they buy during the summer season sales. To test if this is the case, you could apply statistical techniques to see if this is the case.\nThe last step involves communication: sharing your results with others.\nThis book largely follows these steps. The next chapter introduces R and RStudio. To illustrate how you can use these tools, the second chapters includes 2 applications to illustrate the steps of a typical data science project and also illustrate how you use RStudio. The third chapter of the introductory part introduces the data types and structures, functions and models as they are used in R.\nThe second part includes chapters on data import and export, tidying and cleaning data and data transformation. The third part covers data visualization. The fourth part introduces programming skills. The fifth part concludes with two chapters on communication.\nModelling is not covered in this book. I leave that for classes in Statistics or econometrics.\nThis book it not intended to be absolute guide to R. There are many other very good books on R. In the Big Book of R you can find references to a lot of material including applications in various fields. The Geeksforgeeks site has a large tutorial on R. Good starting points if you have questions are the [Posit Community] (https://forum.posit.co/) or Stackoverflow. On this site you’ll see that you are often not the first one to have a problem but find it hard to come with a solution. Thulin (2024) is a good starting point to learn statistics using R. If you need econometric methods, you can use Introduction to Econometrics with R. If you want to dive deep into R: I would suggest H. Wickham (2019).\n\n\n\n\n\n\nThulin, Mans. 2024. Modern Statistics with r (2nd Edition). London, UK: Chapman & Hall/CRC Press.\n\n\nWickham, Hadley. 2019. Advanced r (2nd Edition). London, UK: Chapman & Hall/CRC Press.\n\n\nWickham, Hadley, Cetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (2nd Edition). Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Installing R and R Studio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Introduction.html#installing-r-and-r-studio",
    "href": "Introduction.html#installing-r-and-r-studio",
    "title": "1  Introduction",
    "section": "2.1 Installing R and R Studio",
    "text": "2.1 Installing R and R Studio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Examples.html",
    "href": "Examples.html",
    "title": "2  Examples",
    "section": "",
    "text": "To illustrate the content of the next chapters of this book, we use two examples. In the first example, we use the …. The second example shows how you can import data from the web. In addition, it shows how you can use time series data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html",
    "href": "01_Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Installing R and RStudio\nIn this introduction we will first show how you can install R and R Studio. After the installation is complete, this chapter continues with an overview of the most important panes in the R Studio IDE. In the third section, you’ll learn how to start a new project and how to organize the files in that project.\nR is a programming language. It tells the computer what to do. Like any other language, it has a vocabulary and grammar rules. The vocabulary includes names for e.g. functions. For instance, if you want to create a vector (1, 2, 3) and call it my_first_vector you would write my_first_vector &lt;- c(1, 2, 3). Here, the vocabulary includes c and stands for combine of concatenate and combines the 1, 2 and 3 in a vector. The items that you want to collect in the vector are shown between (). If you would use [] R wouldn’t know what to do and would show a error. The vector is assigned a name. The assignment operator &lt;- assigns the name my_first_vector to that vector. To separate the values 1, 2 and 3, you use a , and, for instance, not a space or a ;. There is also a style guide. In the example, the style include a space after a comma but not before the comma, a space before and after the assignment operator and no space after the first ( or before the last ). The style guide is a guide. In other words, you can write my_first_vector&lt;-c( 1,2,3 ).\nRStudio is an Integrated Development Environment or IDE. Technically, you don’t need it to write R code. As a matter of fact, you can write R code in any word processor, save the code as an .R file and run the code from R’s console. In other words, you can use R without R Studio. However, you can not run RStudio without a programming language such as R. There are also other IDE’s that you can use. We use RStudio because it is widely used. An IDE helps you to work with R. After you have installed R and RStudio in the next section, you’ll see that RStudio creates 4 panes: you can use one pane to write scripts, one pane that shows all your variables, a pane where every command is immediately executed and one pane where you can see your files, plots and where you can find the help function. In addition, RStudio helps you as it makes suggestions to complete functions or variables, adds the closing bracket ) if you type the opening one ( and changes the color of some words you use (e.g. if or for). If you launch R without RStudio, you wouldn’t see these panes. If you enter a command in R, R executes that command.\nThere are two ways to use RStudio. The first installs a local copy on your machine. R and RStudio are available for Windows, Mac, Linux … . If you can not find a copy for your devise (e.g. iPad), you can access RStudio via the cloud. We’ll first focus on the local installation. RStudio cloud is next.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_Examples.html",
    "href": "02_Examples.html",
    "title": "2  A tour of R",
    "section": "",
    "text": "2.1 Data types\nIn Chapter 1 we covered the installation of R, RStudio and packages and introduced projects, scripts, the assignment operator, quarto files and the style guide and names guide. In this chapter, we will take a quick tour of R. We’ll introduce the basic data types and structures, functions, the pipe operator, plots and tables as well as some basic programming structures. In subsequent chapters, these parts will be covered in depth. However, introducing them here allows to use them before we cover them in depth.\nTo start, you can open a new R script: from File select New File and R script. In your project (see Chapter 1) you can save this script in your scripts folder. In case you use quarto for note taking, you can start from File select New File and Quarto Document to start a new document. Add a name for the document and an author (your name) and save this document in your lecture notes folder in your project. Leave the YAML as it is but delete the text in the document. You are now ready to add notes. Recall that you can type notes, add code using Ctrl + Alt + i and use #| eval: FALSE or #| echo: FALSE after the {r} row to prevent R from executing your code or to hide the code and only show output.\nThere are 7 data types: doubles, integer, string (or character), boolean (or logical), date/time, complex and raw. The last two are not used in these lectures notes so we will not cover these two. In this introduction, we’ll quickly look at the first five.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#installing-r-and-rstudio",
    "href": "01_Introduction.html#installing-r-and-rstudio",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Local installation\nYou can download R and Rstudio from the Posit website. First, you need to install R, then you can install RStudio. Doing it the other way around won’t work: RStudio needs R to install. If you click the download and install R button (Figure 1.1) you will be redirected to the The Comprehensive R Archive Network or CRAN. CRAN includes all R versions and packages. For this course, you can install the most recent R version: 4.5.2.. Although there wouldn’t be too much issues if you would update to a newer version if it becomes available, keep this version for the reminder of the course. Doing so avoids that changes alter some functions that we discuss during the lectures.\n\n\n\n\n\nFigure 1.1: Posit: installing and downloading R\n\n\n\n\n\n\n\n\nAs shown in Figure 1.2, you can download R for Linux, MacOS and Windows. If you use Linux, first check if your distribution doesn’t already include R. If that is not the case, you can select your distribution and download the installation file. For MacOS, the file you need to download depends on your system (i.e. modern Apple Silicon or older Intel Macs). For Windows, you can download and install the base version.\n\n\n\n\n\nFigure 1.2: CRAN: download options for R\n\n\n\n\n\n\n\n\nInstalling R is usually quite straightforward. You can accept the standard configuration settings. All you need to do is select the directory where you want to install R.\nIf the installation of R is finished, you can install RStudio. On the Posit website, you can now click on the Install RStudio button (see Figure 1.3). Here, this button refers to my operating system (Windows).\n\n\n\n\n\nFigure 1.3: Posit: installing and downloading RStudio\n\n\n\n\n\n\n\n\nIf you are on a Mac or Linux device and if that button does not show your operating system, you will find it at the bottom of that page (Figure 1.4).\n\n\n\n\n\nFigure 1.4: Posit: download options for RStudio\n\n\n\n\n\n\n\n\nInstalling RStudio is usually very straightforward. You can accept the standard configuration settings. All you need to so is select the directory where you want to install RStudio. We’ll use Version 2026.01.0+392. As with R, you might want to keep that version fixed for the semester.\n\n\n1.1.2 Posit Cloud\nIf you can not install RStudio, e.g. because there is no version available for your system, you can run RStudio via Posit Cloud. RStudio will behave a little different from a local installation, but the experience will essentially be the same.\nThere is a free plan as well as a student plan ($5 per month). You can see a summary of these plans in Figure 1.5.\n\n\n\n\n\nFigure 1.5: Posit cloud: plans\n\n\n\n\n\n\n\n\nThe only difference is the amount of compute time. For the free plan, you have 25 hours per month. This increases to 75 hours in the student plan. Given the settings of the plan, compute time is very close to the actual time that you will use RStudio to do calculations. Note that this will be less than the number of hours that you use R. Writing code for instance does not require processor capacity and will not add to your compute time. You can always switch plans if you feel the free plan offers you too little compute hours. You will not need to other (more expensive) options. These usually include more computing power (e.g. 8 CPU’s) and are a good option is you work with very large datasets or run large and processor intensive code. This is not what we will do here. To use RStudio cloud, you need to register. Once registered, you will be able to log in. The opening screen looks like the one in Figure 1.6.\n\n\n\n\n\nFigure 1.6: Posit cloud: opening screen",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#r-studio",
    "href": "01_Introduction.html#r-studio",
    "title": "1  Introduction",
    "section": "1.2 R Studio",
    "text": "1.2 R Studio\nIf you installed a local copy, you should have to new icons on your desktop.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: R Logo\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.8: R Logo",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_Introduction.html#rstudio",
    "href": "01_Introduction.html#rstudio",
    "title": "1  Introduction",
    "section": "1.2 RStudio",
    "text": "1.2 RStudio\n\n1.2.1 A tour of RStudio\nIf you installed a local copy, you should have two new icons on your desktop: the R logo and the RStudio logo. We’ll always use RStudio. If you open R studio, you’ll see a screen like the one shown in Figure 1.7 (other than the version of R, the opening screen will be very similar).\n\n\n\n\n\nFigure 1.7: Your first RStudio session\n\n\n\n\n\n\n\n\nFrom File in the top left cornder of the menu, select New file and R Script as shown in Figure 1.8. This opens a new window.\n\n\n\n\n\nFigure 1.8: Your first RStudio session\n\n\n\n\n\n\n\n\nYou can also use the keyboard shortcut Ctrl + Shift + N (Windows) or Command + Shift + N (Mac).\nIn Posit Cloud, you first need to setup a new project. To do so you click on New projects in the top right of Figure 1.5 and select New RStudio project. You will then see a screen similar to Figure 1.7. From the File menu, select New file and R Script as shown in Figure 1.8. This opens a similar window as the one in the desktop version.\nAs shown in Figure 1.9, the RStudio windows has 4 panes:\n\nConsole pane\nSource pane (or editor): allows to edit (editor) and view data\nEnvironment pane (with tabs such as Environment, History, … )\nOutput pane (with tabs such as Files, Plots, Help Packages, … )\n\nIf you want, you can modify the tabs in each window. From Tools, select Global options and Pane layout on the right hand side. However, for now, you can leave the panes as they are.\n\n\n\n\n\nFigure 1.9: Your first RStudio session\n\n\n\n\n\n\n\n\nLet’s start at the bottom left of RStudio: the console. The console allows you to interactively execute code. For instance if you type 2 + 2 the console will show [1] 4. Every line you type in the console is immediately executed and the result is shown. If you type 2 + 2 you’ll see the result. If you then type 4 + 4, R will immediately show the output [1] 8.\nNow let’s do the same in the editor in the top left: write 3 + 3 in that pane and press enter. As you can see, nothing happens: the editor does not show any result nor is the result shown in the console. After enter, the cursor moved to the next line, but RStudio didn’t return any result. Add a second line of code in the editor: 4 + 4 and press enter. Again, other than the cursor moving to the next line, noting happens. Now select both lines and press the small button on the top right corner of the editor that says Run with a green arrow (Figure 1.10).\n\n\n\n\n\nFigure 1.10: Running code in RStudio\n\n\n\n\n\n\n\n\nIf you look at the console now, you’ll see the result for both lines of code\n&gt; 3 + 3 \n[1] 6 \n&gt; 4 + 4 \n[1] 8\nThe console shows the lines that you ran (3 + 3 and 4 + 4) and after each line the result [1] 6 and [1] 8.\nIn other words, the editor waits to execute the code until you explicitly ask RStudio to run the code. R executes the code line by line and shows the output in the console.\nTo run the code in the previous example, you selected the lines you wanted to run and then pressed Run. There are several useful keyboard shortcuts to run parts of all code that you write in the editor:\n\n\n\n\n\n\n\n\nRun current line/selection\nCtrl+Enter\nCommand+Return\n\n\nRun current line/selection (retain cursor position)\nAlt+Enter\nOption+Return\n\n\nRun from document beginning to current line\nCtrl+Alt+B\nOption+Command+B\n\n\nRun from current line to document end\nCtrl+Alt+E\nOption+Command+E\n\n\n\nTo illustrate, write 5 + 5 in the editor on new line after the two lines 3 + 3and 4 + 4 and keep the cursor at the end of this line:\n\nCtrl + Enter (Command + Return) will run the line 5 + 5 and show the result in the console. The cursor will move to the next line\nAlt + Enter (Option + Return) will run the line 5 + 5 and show the result in the console. The cursor will stay at the end of the line\nCtrl + Alt + B (Option + Command + B) will run all lines 3 + 3, 4 + 4 and 5 + 5 and show the result for each line in the console. The cursor stays at the end of the line. This option executes all code before the cursor. It runs to code from the beginning (the B in Ctrl + Alt) to the position of the cursor.\n\nNow move your cursor to the start of the first line (before the first 3 in 3 + 3):\n\nCtrl + Alt + E (Option + Command + E) will run all lines 3 + 3, 4 + 4 and 5 + 5 and show the result for each line in the console. The cursor stays at the beginning of the first line. This option executes all code after the cursor. This command runs all the code from the position of the cursor to the end (the E in Ctrl + Alt).\n\nYou can save the code that you wrote in the editor as an R script via File and Save as. R will then save your code in an your-file_name.R file. The .R extension shows that his file includes R code. If you want to continue with your script, you follow Files and Open to select the script you want to open.\nNote that RStudio doesn’t autosave your work. If you would accidently try to close a file in the editor, RStudio will warn that there are unsaved parts in that file, but it does not save your work unless you explicitly ask it to. To save a file, you can use Ctrl + S (Command + S) or click on the small blue disk in the top left corner.\nIn the editor, you can have multiple scrips open at the same time. Their name (or Untitled) will be shown below RStudio’s menu.\nIn RStudio, you will usually work in the editor. Recall from Chapter 1 that a typical data science project includes various steps: importing data, tidying data, data transformation, running a model, preparing a data visualization and output. These steps often require various lines of code. Using the console for that purpose would be very inefficient. First, you will make mistakes. If you do so in the middle of a long line of commands, you’ll need to restart from the first line if you use the console. In an R script, you can correct that mistake and continue. Second, you can reuse a script. This is not possible if you use the console. The ability so save and rerun code is crucial. Suppose that your job requires you to present monthly sales data in a powerpoint presentation. If you include all steps in a script, you can reuse that script every month. Your first presentation will take some time to code, but to prepare all other presentations the effort will be limited: open the script, change a couple of lines (e.g. the name of an excel file) and hit “run”.\nThere are two panes left: the environment pane and the files pane. To illustrate what the environment pane does, use the editor and type and run the following line:\n\na &lt;- 100\n\nDon’t worry too much about the exact meaning of &lt;- here, we will cover that shortly. In short, this line says that we will assign the value of 100 to a variable called a.\nNote that two things happen as you run a &lt;- 100. First, as expected, you see the result in the console:\n&gt; a &lt;- 100\nSecond, in the environment pane in the top right, you see that variable a was created and that the value assigned to that variable is 100. The value can now be used in any part of your R session. If you type and run a in the editor\n\na\n\n[1] 100\n\n\nThe console shows the value of a:\n[1] 100\nYou can now use a in your code. For instance, if you type and run the next line in the editor, the console will show the result:\n\na + 10\n\n[1] 110\n\n\nNow add the following line in the editor:\n\ndf &lt;- mtcars\n\nmtcars is one of the many datasets that are included in the R/RStudio package. Here we copy that dataset and assign that copy to the object df (which stands for data frame, a data structure we’ll cover in e.g. Chapter 2). As we assign mtcars to df, df and mtcars are copies. However, if we change df, that will not affect mtcars.\nThe environment pane now shows 2 parts: Data and Values. The Data part shows the datasets that you have opened. The environment pane shows that this dataset includes 32 observations for 11 variables. If you click on the blue arrow, you see the variables in the dataset. (Figure 1.11).\n\n\n\n\n\nFigure 1.11: Your first RStudio session\n\n\n\n\n\n\n\n\nThe environment pane now lists all variables included in the dataset: the df dataset (a copy of mtcars) includes mpg, cyl, disp, hp, … . You can also see that all these variables are numerical (num). The environment pane also shows the first values (observations) of each variable are shown. In the Values part, you can see that we still have a.\nIf you need to take a closer look at the data you can use View() with the name of the object you would like to inspect between ( ). Note that R is case sensitive: View() is not equal to view()! If you run the following command,\n\nView(mtcars)\n\nyou’ll see the dataset in the source pane (Figure 1.12):\n\n\n\n\n\nFigure 1.12: Output from View()\n\n\n\n\n\n\n\n\nThe output shows all the observations (32) and all variables (11). The names of the rows refer to the observations, the names of the columns include the variable names. If you move your cursor to the variables names, you’ll see additional information, e.g. for the variable mpg, you will see Column 1: numeric with range 10-35: mpg is a numeric variable with a minimum value of 10 and a maximum value of 35. You can sort a dataset for one variable using the two triangles next to the variable name. To undo the sort, you can click on the triangle in the column with the row names.\nAll data and values that are in the environment pane can be used in an R script. In other words, the environment pane shows you which datasets were imported and which variables were created during the active session. These datasets and variables can be used throughout the session. Even if you save and close the script in the editor and start a new one via file, New file, R script, you will still be able to access these datasets and variables.\nTo remove a dataset of variable from the session, you can use the remove function rm(). For instance, if you want to remove a from the session, you can use:\n\nrm(a)\n\nAs you removed the variable a, it is no longer shown in the environment pane. If you would use a in an calucation, e.g. a + 2, the console will show the error message\nError: object 'a' not found\nThe history tab in the environment pane allows you to see all code that you ran during the active session.\nThe fourth pane, in the right bottom shows the directory and the files in that directory. In Figure 1.11 this is a rather long directory referring to the syllabus for this course. In that folder, I have 3 sub-directories: Data-and-programming-skills, docs and Lectures. Under the files tab, you can create a new folder, delete existing folders, rename folders, … .\nIf you create a plot, that plot will be shown in the plot tab. To see how the Plots tab works, run the following lines in the editor (don’t worry yet if you don’t understand the code, the code is written to show where you can see a plot, not to learn you how to create a plot):\n\nplot(hp ~ mpg, \n     data = mtcars, main = \"Horse power and fuel economy\", \n     xlab = \"Horse power\", \n     ylab = \"Miles per gallon\")\n\n\n\n\n\n\n\n\nThe plot which is shown in the output here, also appears in the Plots tab. If you haven’t done so as part of your code, you can export the plot to a pdf or a jpeg or png file using the Export tab in the Plots pane. When we introduce the package {ggplot2} you will learn how to add titles and subtitles to the plot, change the colors of the plot, add titles to the axis, change the legend, save these plots in your code or add them to a powerpoint presentation.\nThe help tab allows you to search in the R documentation. If you need help on e.g. the function mean, you can type help(mean) of ?mean()in the console or you can use the search bar in the help tab.\n\n\n1.2.2 Packages\n\n1.2.2.1 Installing packages\nR includes a lot of functions out of the box. We will refer to these functions as “base R” functions. However, it is almost impossible to include all possible functions that a data scientist might need in one application. Statistical procedures that you find in one field (e.g. marketing, accounting or finance) are often not used in other fields (biology, engineering or medicine). This is why R is extendable through packages.\nPackages are an essential R tool as they include functions for specific applications. These functions allow you to perform a wide range of tasks e.g. importing data, tidying data, transforming data, visualizing data, statistical modelling, … . In that way, you don’t have to write all the code to perform those tasks on your own. You can find all these packages on CRAN’s list of available packages by name. If you look at this list, you’ll notice that the number of packages is huge. However, for most applications, the number of packages that you will use is limited. For that reason, R or RStudio doesn’t install all these packages out of the box, but you need to install them if you need them. To install a package, you can use the Tools menu. The first option, Install packages, allows you select the packages that you need. As an alternative, you can type use the command install.packages() in the editor or console. Within the ( ) you include the name of the packages you want to install between \" \".\nWe will use a number of packages. For now, we will install the packages that we will use often such as {Tidyverse}, {here}, {nycflights13} and {nycflights23}. To install these packages you can enter the following line in the console, run it from the editor or select the packages in the Tools, Install packages menu in RStudio.\n\ninstall.packages(c(\"tidyverse\",\n                   \"readxl\",\n                   \"glue\", \n                   \"here\",\n                   \"janitor\",\n                   \"gt\",\n                   \"officer\", \n                   \"viridis\",\n                   \"rnaturalearth\", \n                   \"gganimate\", \n                   \"sf\", \n                   \"nycflights13\", \n                   \"nycflights23\",\n                   \"magrittr\"))\n\nAfter installation, these packages are available to use in all your subsequent sessions. In other words, you don’t need to install them very time you use R or RStudio. The packages are updated from time to time. In other words, it is sufficient to check for updates and install those in case they are available. As with R or RStudio, keep all packages fixed as you complete the course.\nThe package {tidyverse} installs a number of packages that help you to\n\nimport data: readr, Wickham, Hester, and Bryan (2024)\nimport data from Excel: readxl, Wickham and Bryan (2023)\ntidy data: tidyr, Wickham, Vaughan, and Girlich (2024)\ncreate tibbles: tibble, Müller and Wickham (2024)\ntransform data: dplyr, Wickham et al. (2023)\nvisualize data: ggplot2, Chang (2025),\nexamine and clean data:janitor, Firke (2024),\n\nwork with special types of data:\n\nstring variables: stringr, Wickham (2023b) and glue, Hester and Bryan (2024)\nfactors: forcats, Wickham (2023a)\ndates and times: lubridate, Grolemund and Wickham (2011)\n\nor add programming tools: purrr, Wickham and Henry (2023).\nThe other packages help you to\n\nreport data in tables: gt, Iannone et al. (2025)\nchoose color scales in plots: viridis, Garnier et al. (2024), or\nwork with word and powerpoint: officer, Gohel, Moog, and Heckmann (2024)\nuse animations: gganimate, Pedersen and Robinson (2024)\nuse maps in your data visualisation: sf, Pebesma (2018) and rnaturalearth, Massicotte and South (2024)\n\nThe {here} package allows you to find and save files. It is especially useful in a project based workflow. We”ll use this package often to import, export and save files.\nThe two nycflights packages nycflights13 and nycflights23 will be used to illustrate code. These datasets include all flights that departed from New York city airports in 2013 and 2023 to airports in the United States, Puerto Rico and the American Virgin Islands. The data include information on the flights (departure time, arrival time, delays), weather, construction information for each plane, the airport names and locations, and the carrier names.\n\n\n1.2.2.2 Using packages\nThe functions defined in the packages are not immediately available after installation. There are two ways to use them. First, you can load the package in your computer’s memory using the library() function. For instance, if your work requires you to produce a plots, you can load {ggplot2} using library(ggplot2). Using the library() function loads the package into the computer’s memory and you can use all its functions in your code without reference to the package.\nA second way to use a function from a package is to call it explicitly using packagename::function() in your code. This is especially useful if you only need a limited number of functions from a package and you don’t use them often. The {here} package for instance makes opening and saving files easier. As we’ll see in the next section, it also allows you to share code with others. As you don’t import, read or save datasets, scripts or plots often as part of the code, it is not necessary to load that package in the memory of your computer. In that case it is more efficient to you use here::here() to use the here()function from the {here} package.\nLet’s load the {tidyverse} package. You can copy and run the following code:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIn the console, you can see that this command produces the following output (Figure 1.13):\n\n\n\n\n\nFigure 1.13: Loading the tidyverse\n\n\n\n\n\n\n\n\nThe first part shows all packages that are loaded as part of the tidyverse package. You can now use all the functions that are defined in each of these packages. You can also see the version (e.g. ggplot 4.0.1). Second, there are a couple of conflicts. These conflicts occur because some packages include functions with the same name as functions in base R or in other packages. In this case, both {dplyr} and base R include a function filter() and lag(). This warning tells you that if you use filter() you’ll be using {dplyr}, not base R. In case you need to use base R’s filter function, you will have to instruct R to do so explicitly and use base::filter(). The same holds for the lag()function. Because you often want to use the package function and not the one from base R (why would you load a package if you don’t want to use its functions?), this is usually not a problem. However, if loading a package causes a conflict message, it is always advisable to read it carefully.\n\n\n\n1.2.3 Quarto\nWriting code, analyzing data or producing nice plots is only one part of the workflow. You also need to communicate these results with others. This is what quarto allows you to do. Quarto is an open source publishing system that allow you to mix text, code and the output from the code in one document. It allows you to create documents in many formats, including html, pdf, docx, … .\nWe will cover quarto more in depth in one of the last chapters, but it is useful to introduce it here as it will allow you to take notes during class.\nYou can open a new quarto document via File, New file and Quarto document. A new window opens (Figure 1.14).\n\n\n\n\n\nFigure 1.14: Creating a quarto document\n\n\n\n\n\n\n\n\nYou can add a title and your name. For now, you can use the html options and create the document. In the source pane, a new tab opens Figure 1.15:\n\n\n\n\n\nFigure 1.15: A new quarto document\n\n\n\n\n\n\n\n\nThe first part (between - - -) is called the YAML (Yet another markup language). In addition to your name and the title of the document, the YAML includes information that quarto will use to render the document (e.g. In this case, it will render as html). There are many options that you can add to the YAML, but for now, this will do. The part below the YAML is where you write your text and code. You can delete the text that you see and write your own text. You can subdivide your text using headings. If you add headings (under the normal tab), they will appear on the right of your html page.\nIf you include an executable cell (via the insert tab or Ctrl-alt-I (Command-option-I), you have the option to choose a programming language. In this case, you need to select R. The code block shows the programming language {r} and looks like\n\na &lt;- runif(10)\nmean_a &lt;- mean(a)\nmean_a\n\nIf you edit the quarto document in the source view (tab in the top left) the code will be included between ```. You can save the quarto file and give it a name (which doesn’t need to be the title in the YAML). A quarto document has a .qmd extension. If you render the document, the code will run and the document will show all the output from the code in the Viewer tab in the Files pane.\nAt his point, a quarto document is a good option to take notes in class. All notes that you take and code that you include will be saved in a quarto file. If allows you to add notes or comments at a later stage.\nTo illustrate how you can use quarto,\n\nstart a new quarto document, add my_first_quarto_text ,\nwith the exception of the YAML part, delete all text,\nwrite “My first quarto document shows code to produce a scatter plot for the horse power and miles per gallon variables in the mtcars dataset.”\npress Ctrl-alt-I/Command-option-I , a code block opens\nwrite or copy paste the following lines in the code block in your document\n\n\nplot(hp ~ mpg, \n     data = mtcars, \n     main = \"Horse power and fuel economy\", \n     xlab = \"Horse power\", \n     ylab = \"Miles per gallon\")\n\n\n\n\n\n\n\n\nIf you hit render (Figure 1.16)\n\n\n\n\n\nFigure 1.16: Quarto\n\n\n\n\n\n\n\n\nquarto asks you to save your file and shows the html version of your text in your Viewer tab in the Environment pane (Figure 1.17)\n\n\n\n\n\nFigure 1.17: Quarto\n\n\n\n\n\n\n\n\nQuarto is ideal to communicate your code. For instance, if you need to write a report in a team where you use R to do statistical tests, quarto files allow you to share the code as well as the output. Doing so, other team members can review and change the code, alter visualization, … . Note that one person you communicate with is “you” or the “future you”. You often switch between tasks: you prepare a report now, switch to a presentation tomorrow, … . A quarto file is a good way to explain your code: what is the purpose, why did you write it the way you did, are there any todo’s left. In doing so, you document your script (what, why, how, intended outcome) as you code. If you need to switch between tasks, a well documented workflow will make live easier as you pick up where you left.\nThere are two useful option: if you add #| echo: false on the line following {r}, the code will run and you’ll see the output but quarto will hide the code.\nType, below the previous part in my_first_quarto_text: “Here we use #| echo: false. The quarto document shows the output, but doesn’t show the code.” and copy paste the following lines in a new executable cell (make sure you delete the second {r} if that would show) and add the line #| echo: false on the line immediately below {r}. Make sure there is no empty line between {r} and #| echo: false.\n\n\n\n\n\n\n\n\n\nYour code should look like Figure 1.18:\n\n\n\n\n\nFigure 1.18: Adding ‘#|echo: false’ to your code\n\n\n\n\n\n\n\n\nIf you save and render the document, you’ll see that the documents shows the plot but doesn’t show the code.\nIf you don’t want to run the code (and only show the lines of the code), you can add #| eval: false in your first line.\nAdd the text: “If I add #| eval: false to the code, then the code is shown, but it doesn’t run.” and copy paste the following lines in a new executable cell (again, delete the second {r} is that would show and add #| eval: false below {r} without a empty line (see also Figure 1.18):\n\nplot(hp ~ mpg, \n     data = mtcars, \n     main = \"Horse power and fuel economy\", \n     xlab = \"Horse power\", \n     ylab = \"Miles per gallon\")\n\nIf you save and render the document, you’ll see the sentence that you wrote as well as the lines of code but your document will not show the output. This is useful is you want to show code, you don’t need to run. For instance, if you work on a team project and you would use R, adding this option would allow you to share code that causes an error.\n\n\n1.2.4 Projects\nAn R workflows includes importing and tidying data, using the editor to write code to analyse or visualize data or to write reports. As you step through these stages, you will need to access various files (e.g. data) and you will create different files that you want to save. If you don’t organize these activities, some files will be stored in one place on your hard drive, while another files is stored elsewhere. At some point, you’ll forget where these files are.\nThe way you organize your files is different from the way others do. You might save all your work in a c:/user/my_name/documents folder, others save all their work in c:/my_documents/work. If you import a file stored on your computer, you will point R to the location of that file. If you do this in a script and you use absolute references (i.e. c:/user/my_name/documents/R/work) your colleagues will have a hard time running that script. On their computer, there is no folder c:/user/my_name/documents/R/work and a command such as read.csv(\"c:/user/my_name/documents/R/work/data/sales.csv\") will fail if they run that code.\nAn R project is one way to solve both these issues: it allows you to store all files in one location and you don’t need absolute references to locate a file.\n\n1.2.4.1 Creating a new project\nIf you start R, R will look for and store all files in the working directory on your hard drive. You can see which directory R uses at its working directory at the top of the console. That directory will also show if you run getwd():\n\ngetwd()\n\nR will show something that looks like c:/users/my_name documents/R. It is unlikely that this will be what you see as the way you organize your files on your hard drive is different from the way others do. Note that R uses the Linux and Mac / and not the Windows \\\\ . However, R can work with both.\nThe working directory is where R will look for files and save files unless you point it to another directory on your hard drive. If you use R occasionally, that is not much of a problem. However, if you use R for various tasks, your work directory will be cluttered with files: data files for all projects that you did, scripts, plots, various drafts of reports and papers, … .\nThis is where R projects offer a very efficient solution. You can think of an R project as a storage cabinet where you store all items related to one specific project in one cabinet, where every shelve collects related items. If you need an item for a specific project, you only have to open the project’s cabinet and look on its shelves to find what you were looking for. An R project is the cabinet; the shelves in that cabinet are the folders in the R project.\nLike a storage cabinet, an R project stores all files in a separate cabinet (directory). Usually, a storage cabinet allows you to adjust the shelves. In an R project, you can create as many subdirectories as you need. In other words, R projects allow you to collect all the files in a separate directory. This directory is called the the project root directory (the cabinet). Within that directory, you can add as many subdirectories (or shelves) as you need. If you need to work on a specific project, all you need to do is open the associated R project. In doing so, R will use the project root directory as the working directory.\nTechnically, you could also set the work directory using the set working directory commend setwd().\n\nsetwd(\"c:/users/documents/R/my_new_project\")\n\nFor multiple reasons, this is not the most efficient thing to do. First, everytime you need to work on my_new_project you will need to run that command. Suppose that you have a project with multiple R script files. If you forget to add the command in a file and that file includes a line that saves a plot, that plot will be saved in a different directory. You would then have to run getwd() to find where that plot was saved. Second, if you collaborate on a project and you set the working directory using the setwd() command in every script, others in your team will need to change that line in your script. It is very unlikely that your working directory, c:/users/documents/R/my_new_project, will exist on their computer. Using a R project also avoids that problem.\nTo create a new project, follow File, New Project and RStudio will open a new window (Figure 1.19):\n\n\n\n\n\nFigure 1.19: Creating a new project\n\n\n\n\n\n\n\n\nIf you continue with New Directory and New Project, you will be able to give that project a name (new directory in the Create New Project window) and select where you want to save that project on your hard drive (browse if you want to change). If you then click on Create Project, your project will be created. If you ask for the working directory getwd()you will see that the working directory is equal to the project directory. R also add a new_project_name.Rproj file to that directory. This directory is the project’s root directory. If you click on that file, the project options window opens (Figure 1.20). Here, you need to make a couple of changes:\n\n\n\n\n\nFigure 1.20: Project options\n\n\n\n\n\n\n\n\nThe first options avoids that you will reload all the data in your project (i.e. restore all variables in the environment from your previous session). For projects with large datasets, that causes a substantial slowdown loading R. Second, all variables should be created using code you save in scripts. If R restores your environment, then that could be one reason why you forget to write and save a variable into code. This is a recipe for trouble. Suppose that you want to re-use your code in another project. Variables that you created in the old project but forgot to save will be lost and you’ll have to rewrite parts of your code to create and save them.\nThe last options saves your history.\nIf you need to change projects, you can do so via File and Open Project or Recent Project.\nIn the files pane you can now add new directories to organize you project’s files. An example of a project structure is shown in Figure 1.21.\n\n\n\n\n\nFigure 1.21: Directories for an R project\n\n\n\n\n\n\n\n\nAlthough there are many possibilities to organize a project, there are a couple of general points that you can make:\n\nVia new files and Text file (in the files pane) add a readme.txt file to the project. You can use this file to share information on the project, the various directories, major data sources, … .\nAs a general rule: never change your raw data. Store them in a separate (read only) file or in a separate directory. If you change your raw data files, it will be impossible to rerun your analysis and it is impossible to replicate what you did. To avoid that you overwrite a raw data file with an edited file, you can store your raw data in a raw subdirectory of your project’s data directory. As you clean and tidy data, save these files in a tidy or clean subdirectory of your project’s data directory. In doing so, you will always keep your original raw data files as you had them at the start of the project.\nMost projects include multiple scripts. In the example in Figure 1.21, you can see these scripts in the scripts subdirectory. You can organize your scripts in the order in which they have to run (as in Figure 1.21), you can add a date and organize them in the order in which they were created or changed, … . For very large projects, you can include more subdirectories: e.g. one for all your tidying and cleaning scripts, one for the analysis scripts, … .\nIf you need to write a report, you can add a reports subdirectory. There you can save, e.g. the plots or tables that you need to communicate, quarto files that you need to share, or powerpoint presentations that you create within R.\n\nFor this class, you should create specific project. In that way, all your files will be stored on the same location. You can add the folders as shown in Figure 1.21: data with raw and tidy subfolders and scripts. If you use quarto for your lecture notes, you can add a folder lecture_notes and organize your quarto files using a number (01_, 02_, …) or a date and a description (e.g. lecture_intro, lecture_import, …). You can organize your scripts in the script folder in a similar way. I will assume that this project structure exists for your exam.\n\n\n1.2.4.2 The {here} package\nYou installed the {here} package. This package makes it easy to save files in a project. Say you want to open a dataset called sales.csv which is in your project’s data directory. Here is a line of code that loads that files:\n\nread.csv(\"data/sales.csv\")\n\nUsing the {here} package, you can rewrite that code as\n\nread.csv(here::here(\"data\", \"my_data.csv\"))\n\nThe {here} package automatically builds your file’s location path relative to your projects directory. The command here(\"data\", \"my_data.csv\")) builds a path starting from your project folder’s root path (the one your created in Figure 1.19). This is especially useful if you have multiple subdirectories in your project. For instance, suppose that you store your data covering sales for a specific product in a file product.csv in a directory which is organized per year and per region, e.g. .../data/2024/sales/nordics/. Using {here}, you can type\nread.csv(here::here(\"data\", \"2024\", \"sales\", \"nordics\" \"product.csv\")).\nThere {here} package will build the reference as if you would have written /data/2024/sales/nordics/product.csv.\nBecause you are in an R project, you always start from the project’s root directory. In other words, with {here} you are building paths to files relative to that directory. In that way you can share projects with colleagues. As long as their project directory structure is equal to yours, it will be irrelevant where they have created their project on their hard drive. Without a project’s ability to use relative paths, that would be impossible. If your files are in c:/users/documents/R/my_new_project and theirs are in c:/R/projects/marketing/my_new_project you would load your sales data using:\n\nread.csv(\"c:/users/documents/R/my_new_project/data/sales.csv\")\n\nAs your collegues don’t have these folders, running your script on their computer would cause an error. To read the files, they would have to change c:/users/documents/R/my_new_project/data/sales.csv in c:/R/projects/marketing/my_new_project/data/sales.csv. If your code includes a command to save a file, that line would cause a similar error. Again, the location where you save your files would be different from theirs. With R’s projects, you avoid these errors. Because R now looks at your files relative to the project’s root folder, R will look for your files in c:/users/documents/R/my_new_project. If your colleagues use R projects as well R will look for files on their computer and start from their project’s root folder c:/R/projects/marketing/my_new_project.\n\n\n1.2.4.3 Scripts\nIf you take another look at Figure 1.21, there are 3 directories. We will cover importing and exporting data files that you would find in and save to your data folders in Chapter 6. Reports and plots are covered in e.g. Chapter 9. Here we will focus on the use of scripts in a project.\nRecall that the editor in the source pane allows you to write code and that you can save that code in a .R file (a script). You can reuse that script every time you need to run the same analysis. Suppose that you have to analyze and present sales data every month. A script would allow you to write all the code to import and tidy your sales data, run the analysis and create and save the plots and tables you need to build your powerpoint presentation. An R project can include multiple scripts. For shorter projects, you can usually include all your code in one script. However, if your code is very long, splitting up your code in various scripts is usually the best option. It allows you to collect all code relevant to one part of your work in one script. In doing so, it avoids that you have to scan through long lines of to find a specific part. In addition, it helps you to debug your code: if you see an error, it is easier to locate exactly which part of your code (i.e. which script) caused that error.\nYou can start a script via File and R Script or via Ctrl-Shift-N (Command-Shift-N). In the source pane, you will now see a new tab Untitled1. The editor also shows the line numbers. As you haven’t written any code, that line number for now is 1.\nThe editor helps you coding. For instance, (slowly) write for in the editor. RStudio’s editor shows Figure 1.22.\n\n\n\n\n\nFigure 1.22: R Studio code suggestions\n\n\n\n\n\n\n\n\nRStudio’s editor suggests a couple of “words” that include the word “for”. If you hit tab on the word forRStudio shows you the for loop template: a for loop requires you to include a\n\nthe word for\nbetween brackets ( )\n\na variable\nthe word in\na vector\n\nbetween curly brackets { } you will write your code. The code is intended with 2 spaces\n\nRStudio’s editor also suggests where the curly brackets are: at the end of the for line and on the last line. If you need to include a for loop in your code and hit tab as RStudio suggests for, you will have the basic template ready. To illustrate, let’s fill in the parts:\n\ni &lt;- 1\n\nfor (i in 1:5) {\n  print(\"Hello world\")\n}\n\nHere the ‘variable’ in the template is i and the vector is (1, 2, 3, 4, 5). The code that R will run is between the curly brackets. It will first print “Hello world”. As we’ll see in Chapter 13, the loop will continue as long as i is smaller then or equal to 5. R increments the value of i each time it finished the code between curly brackets. The first line i &lt;- 1 sets i equal to one. You need to add this line. To see why, run this code once. In the environment pane, you’ll see that the value of i is equal to 6. Now try to run the code again but detele i &lt;- 1. You’ll see that is does not produce any results: as i = 6 and the for loop repeats the code only for values of i less than or equal to 5, R will not execute the code.\nYou’ll see these suggestions often. For instance, write the word mean in the editor. RStudio will now show additional information for the function mean() (see Figure 1.23).\n\n\n\n\n\nFigure 1.23: R Studio code suggestions for mean\n\n\n\n\n\n\n\n\nRStudio’s editor also helps with brackets. In the editor, type 1 bracket ( . RStudio’s editor adds a second ) bracket. The same would happen is you use a curly bracket {, a a square bracket [ or a quotation mark \". In addition, it automatically intends code in e.g. a for loop. RStudio also makes suggestions for functions. For instance, type\n\nvec1 &lt;- (1:5)\nmean(vec1, na.rm = TRUE)\n\nand see how many times RStudio will make a suggestion.\nR will alert you to problems or errors with a red squiggly line and a red circle with a white cross in the sidebar. If you hover over the cross, RStudio will show why the error occurs. If you use Tools, Global Options and then select Code and the diagnostics tab (Figure 1.24) you can select when R will issue a warning.\n\n\n\n\n\nFigure 1.24: R Studio diagnostics\n\n\n\n\n\n\n\n\nA last observation: you can see that RStudio adds color to various parts of the code. For instance, the words forand in are shown in blue, the word TRUE is shown in red and the 2 subsequent brackets are in a different color. You can change the look for the editor via Tools, Global Options and then select Appearance. The appearance is personal and has no impact on the way in which RStudio runs.\nNotice that the tab Untitled1 is red. That means that you haven’t saved your code yet or that you have unsaved changes to your code. You can save the script as my_first_script and save it as an .R file in your project’s scripts directory.\n\n\n1.2.4.4 File and variable names\nWith respect to the name of your scripts or variables in your scripts there are no or little hard coded rules. However, note that R is case sensitive. In other words, var1 and Var1 are two different variables. In addition, avoid spaces or special characters such as $, €,%,&, … as some of these characters could cause problems in some applications, packages or operating systems.\nTo name files or variables, there are four popular case types:\n\nCamel case (or lowerCamel case): you start a name with a small letter. If a name includes multiple words, you use a capital letter for the second, third, … word. For example: scriptToLoadLibrary, salesRawData\nPascal case (or UpperCamel case): is similar to Camel case, but includes a capital letter for the first word. For example: ScriptToLoadLibrary, SalesRawData\nSnake case: all letters are small case and words are separated with an underscore. For example: script_to_load_library, sales_raw_data\nKebab Case: is similar to Snake case but uses a hyphen (-) instead of an understore (_). For example: script-to-load-library, sales-raw-data\n\nThe most important rule when naming files and variables is to be consistent and to think of a name that is both human readable as well as machine readable. A file or variable are human readable is the name suggests the content of the file or variable. For instance a file name 2024_sales_sweden_chocolates.csv is human readable as long as the data in the file refer to sales of chocolates in Sweden for the year 2024. Note that what a human can read depends on the circumstances. If you work for a firm who sells chocolates a short product code rather than a long product name could be very informative for those who work for that company. For others, it is highly unlikely that a product code is sufficient to decipher what is in a file.\nWith respect to variables, names such as total_sales_2024 or averageCost are informative with respect to values that these variables (obviously, assuming that the first includes the sum of sales for 2024 and the second the average cost of something). However, try to limit the length of your variable names: tot_sales_24 or aveCost reduce number of keystrokes if you write code.\nIf you need to import various files, it is often very convenient if you can do so in one or a couple of lines of code. If your file names are machine readable, you will be able to do so. Suppose that your data directory includes the following data files\n2024_sales_sweden_chocolates.csv , 2024_sales_norway_chocolates.csv 2024_sales_finland_chocolates.csv , 2024_sales_denmark_chocolates.csv 2024_sales_sweden_cookies.csv , 2024_sales_norway_cookies.csv 2024_sales_finland_cookies.csv , 2024_sales_denmark_cookies.csv 2023_sales_sweden_chocolates.csv , 2023_sales_norway_chocolates.csv 2023_sales_finland_chocolates.csv , 2023_sales_denmark_chocolates.csv 2023_sales_sweden_cookies.csv , 2023_sales_norway_cookies.csv 2023_sales_finland_cookies.csv , 2023_sales_denmark_cookies.csv\nIf you need only data for Denmark, you would be able to write code that selects only those files which include denmark. Likewise if you need only the files whose name includes cookies and 2024, you will be able write code that imports only those files. The same holds for variable names. For instance, if you need to select all the columns in a dataset that include the name of a specific product, it will help if these variables include the name of that product in a consistent way or if these names all start or end with e.g. prod. Doing so, you’ll be able to select these variables with a couple of words of code.\nThird, by default, files are alphabetically ordered in your directory. In your directory, a file such as import_data.R will be after generating_plots.R. Usually, you import data before you generate a plot. It could be convenient if the first file you see imports the data and the second generates the plot. If you work with multiple R scripts, you can order them if you add a number at the start of the script 01_import_data.R and 02_generating_plots.R. For plots or tables, you would order them as they appear in the text t01_name_of_table or f1_name_of_figure For reports, you can add a date before their name e.g. 20240215_report or 2024Q1_report. If you use numbers, make sure that you think about the total number of files you will need. For instance, if you think you’ll need 14 files, start the first number as 01 and not as 1; if your report includes 15 figures, use figure_01 for your first figure, … . If you do so, your files will be shown in the order in which you use them or in which they appear in the report. If you have 15 figures and you’ll see figure_1 and then figure_11 … figure_15 before you see figure_2.\nWith respect to variable names avoid names that are equal to names of functions (e.g. mean, summary, … ) or reserved words such as if, for, while, else, … .\nMixing cases is usually not recommended. One use case includes functions. Some people prefer to differentiate variables or files from the functions they write. They would then use e.g. snake_case for their variables but myFunction() or MyFunction() in other words (camelCase of PascaleCase) for their functions. As long as you are consistent, this is a matter of taste.\n\n\n\n1.2.5 The assignment operator &lt;-\nRun the next line\n\nvar1 &lt;- 25\n\nYou should read this statement as “the object var1 is assigned the value of 25” or “the object var1 gets the value of 25”. In this case, the object is a numerical variable, but could be any other object in R: a vector, matrix, data frame, a plot or a table. If you assign a value to an object, you can use the object in your code. For instance, if you multiply var1 with 4\n\nvar1 * 4\n\n[1] 100\n\n\nthe result is 100. So, as long as you don’t change the value of var1, it’s value will be 25.\nThe assignment operator &lt;- (arrow pointing left and minus sign) is widely used in R to assign a value to an object. Recall for instance that you wrote df &lt;- mtcars. Here this statement says that we will assign the dataset mtcars to an object that we will call df. In doing so, the object df is a copy of the dataset mtcars. If you assign a value to a an object, you’ll see that object in the environment pane. If you ran the code var1 &lt;- 25 your environment pane showed in the Values section var1 and a value of 25. If you ran df &lt;- mtcars, you saw the object df in the Data section of the environment pane.\nTechnically = would do the same trick. For instance,\n\nvar2 = 125\n\nwill create a variable whose value is 125 (see its value in the environment pane). If you do math\n\nvar2/25\n\n[1] 5\n\n\nyou’ll see that the result is equal to 125/25.\nAs a rule, if you want to assign a value to an object, always use the assignment operator. We will use = often but almost always within a function. To see the importance, let’s create a vector with 10 (n = 10) random draws from a normal distribution (rnorm(): r for random, norm for normal distribution) with mean 2 (mean = 2) and standard deviation 4 (sd = 4):\n\nvec1 &lt;- rnorm(n = 10, mean = 2, sd = 4)\n\nIf you run this line, you’ll see that vec1 shows up in the environment. Now run the next line\n\nvec2 &lt;- rnorm(n = 10, mean &lt;- 2, sd &lt;- 4)\n\nNote that the environment pane now shows vec2 as well as mean and sd. These are now assigned a value: 2 for mean and 4 for sd. However, in this case, we don’t want to assign a value of 2 to the mean. What we want is a random draw from a normal distribution with mean 2. In other words, we don’t want R to remember that mean was set to 2 to draw the random numbers. Within a function, using = tells R it can forget what mean was as soon as the function is completed. In doing so, the value mean with a value of 2 will only exist within the function but will not have any impact once the function is complete.\nWe met a similar case when we wrote the code to draw the horse power - miles per gallon plot:\n\nplot(hp ~ mpg, \n     data = mtcars,\n     main = \"Horse power and fuel economy\", \n     xlab = \"Horse power\", \n     ylab = \"Miles per gallon\")\n\n\n\n\n\n\n\n\nIn this code, we didn’t use the &lt;- in the data = mtcars part. Again, we don’t want R to remember that data was set equal to mtcars to produce the plot. If we would have used the assignment operator then the mtcars dataset would be have been assigned to the object data and would have shown up in the environment pane. In other words, that variable would continue to live outside of the function call.\nAs a result, almost all people who use R use the assingment operator &lt;- to assign a value to an object and use = inside function. In that case, you avoid that you create a variable such as mean which continues to live beyond the function call in your environment. With the assignment operator, you are explicit in your goal: you want to assign a value to an object (a vector to an object vec1, a dataset to a on object which we will call a data frame df, … .\nYou can also use -&gt;:\n\n3 -&gt; var3\n\nHowever, in most cases, you would use &lt;- as its aligns with the way you read your code: first the object’s name followed by the value.\nLet’s create two variables, var1 and assign it the value of 125 and var2 two times the value of var1:\n\nvar1 &lt;- 125\nvar2 &lt;- 2 * var1\n\nAs you would expect, var1 is 125 and var2 equals 250. Now change the value of var1 from 125 in 25:\n\nvar1 &lt;- 25\n\nand check the value of var2:\n\nvar2\n\n[1] 250\n\n\nAs you can see, the value of var2 was 250 before we changes the value of var1 and is still 250 after we changed var1’s value. This shows that the value assigned to a variable or object equals the value at the time the expression was evaluated. Here, the expression var2 &lt;- 2 * var1 was evaluated with var1 equal to 125. Changing the value of var1 after this evaluation does not affect the value of var2. In other words, once created, var2 forgets where it came from. It does not remember that is was the result of the expression 2 * var1 but only remembers its value when this expression was evaluated.\n\n\n1.2.6 Other conventions and style guide\n\n1.2.6.1 Case senstivity and decimals\nI already mentioned the fact that R is case sensitive. In other words, my_var1and My_var1 are two different objects.\n\nmy_var1 &lt;- 100\nMy_var1 &lt;- 1000\n2 * my_var1\n\n[1] 200\n\n2 * My_var1\n\n[1] 2000\n\n\nIf you write this code in the editor, RStudio suggests auto completions after you have typed the first 3 letters, my_ and will show both my_var1 and My_var1. If you use both, this will almost surely cause mistakes. You can avoid these mistakes if you are consistent in your use of capitals for your variable names.\nSecond, R uses a point for decimals: 10.25 and not 10,25.\n\na &lt;- 10.25\n\nif you use a , you will see an error:\n\nb &lt;- 10,25\n\nError in parse(text = input): &lt;text&gt;:1:8: unexpected ','\n1: b &lt;- 10,\n           ^\n\n\n\n\n1.2.6.2 Style guide\nYou’ll write code, but you’ll often need to read code as well. To help people, including your future self, with the last activity, there are a couple of conventions (i.e. a style guide). In English or in any other language, you are familiar with the fact that a sentence starts with a capital and ends with a dot, a question or exclamation mark; that you add a white space after a comma or that a new paragraph always starts on a new line. The R style guide has similar rules.\nIn R, like in any other language, you will use commas. As in most other languages, you use a space after a comma, but never before. For instance, you would write\n\nx[, 1]\n\nbut not\n\nx[,1]\nx[ ,1]\nx[ , 1]\n\nIn function calls, you don’t add a space before of after parentheses. For instance, you write\n\nsd(x, na.rm = TRUE)\n\nbut not\n\nsd (x, na.rm = TRUE)\nsd( x, na.rm = TRUE )\nsd ( x, na.rm = TRUE ) \n\nThis is not always the case. If you use e.g. for, while or if, you would use a space before the opening parenthesis( and after the closing parenthesis ):\n\nfor (i in 1:10) {\n  do_something_with(i)\n}\n\nif (condition) {\n  do_somehting_with(x)\n} \n\nwhile (i &lt; 100) {\n  do_something_with(i)\n}\n\nWe’ll write our own functions. In that case, you add a space after the closing parenthesis ) of the function call:\n\nfunction(a,b) { }\n\nYou wouldn’t write\n\nfunction(a,b){ }\nfunction (a,b){ }\nfunction (a,b) { }\n\nFor other operators such as +, -, /, =, |&gt;, %&gt;% or *, you can often use a space before and after the operator:\n\nx &lt;- 4\nmean(x, na.rm = TRUE)\na == b\n2 + 4\n6 - 8\n24 / 6\n1 * 4\nx |&gt; y\nz %&gt;% q\n\nThere are a couple of exceptions.\n\n2^2\n1:10\nhere::here\n~x\n!=\n\nCurley braces { } in code blocks show a hierarchy in R. To show this more explicit, there are a couple of conventions:\n\nafter e.g. if and the condition or for and the loop, { should be the last character\nthe next lines should be intended with 2 spaces\nthe closing } should be the first character on the line\n\n\nif (condition) {\n  do_something\n  } else {\n  do_something_else\n}\n\nThis is especially useful if you have more than one of these conditions:\n\nfor (i in x) {\n  \n  if (condition) {\n    do_somthing\n  } else {\n    if (another_condition) {\n      do_something\n    } else {\n      do_something_else\n    }\n    do_something_else\n  }\n}\n\nHere you immediately see the hierarchy. Every condition is within curley braces and intended with 2 spaces.\nSometimes function calls are long. This is usually the case if you have many named arguments (e.g. mean = 5 in the function rnorm(). In general, it is a good idea to keep the length of a single line short. In that case, you can use multiple lines. For instance you could use\n\nprint(paste0(\"The mean of the vector is equal to \", \n             round(mean(1:250), \n                   digits = 4)))\n\ninstead of\n\nprint(paste0(\"The mean of the vector is equal to \", round(mean(1:250), digits = 4)))\n\nThe same holds for code using the pipe operator. To avoid lengthy lines, if is often a good idea to start a new line after every |&gt;\n\ndf |&gt; \n  filter(y == 25) |&gt;\n  ggplot(aes(x = something, y = something)) +\n  geom_line()\n\nsometimes an exception is made for the first line because this line usually starts with the dataset you want to use. You can add your first operation and have two pipes on the same line:\n\ndf |&gt; filter(y == 25) |&gt;\n  ggplot(aes(x = something, y = something)) +\n  geom_line()\n\nThe same holds for a line that starts with an assignment\n\nplot_df &lt;- df |&gt; filter(y == 25) |&gt;\n  ggplot(aes(x = something, y = something)) +\n  geom_line()\n\nHowever, try to keep the first line as short as possible (i.e. not add too many pipes).\n\n\n\n\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.\n\n\nFirke, Sam. 2024. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://github.com/sfirke/janitor.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2024. viridis(Lite) - Colorblind-Friendly Color Maps for r. https://doi.org/10.5281/zenodo.4679423.\n\n\nGohel, David, Stefan Moog, and Mark Heckmann. 2024. Officer: Manipulation of Microsoft Word and PowerPoint Documents. https://ardata-fr.github.io/officeverse/.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nHester, Jim, and Jennifer Bryan. 2024. Glue: Interpreted String Literals. https://glue.tidyverse.org/.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. Gt: Easily Create Presentation-Ready Display Tables. https://gt.rstudio.com.\n\n\nMassicotte, Philippe, and Andy South. 2024. Rnaturalearth: World Map Data from Natural Earth. https://docs.ropensci.org/rnaturalearth/.\n\n\nMüller, Kirill, and Hadley Wickham. 2024. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPedersen, Thomas Lin, and David Robinson. 2024. Gganimate: A Grammar of Animated Graphics. https://gganimate.com.\n\n\nWickham, Hadley. 2023a. Forcats: Tools for Working with Categorical Variables (Factors). https://forcats.tidyverse.org/.\n\n\n———. 2023b. Stringr: Simple, Consistent Wrappers for Common String Operations. https://stringr.tidyverse.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel Files.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional Programming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#sales-data",
    "href": "02_Examples.html#sales-data",
    "title": "2  Examples",
    "section": "",
    "text": "2.1.1 Project setup\nFirst, let’s create a new project in R: file, New Project, call the project “sales_data”. After you finish creating the project, in the Files pane: create the following folders (see e.g. Figure 1.21):\n\ndata, include subdirectories\n\nraw\ntidy\n\nscripts\nreports\n\nYou can download the file nike_sales_2024.csv from Toledo or you can download it via Kaggle: Nike Global Sales Data (2024). To download the data, you might have to create a (free) account. Copy paste the file in your data, raw directory.\nFirst, we need to load the packages that we need for this example. You installed all of them in the previous chapter. In case you have done so, you can continue. If you have not yet done so, you need to do this first. You can copy the next lines of code in the console:\n\ninstall.packages(c(\"tidyverse\", \"here\", \"janitor\", \"officer\", \"viridis\", \"gt\"))\n\nThe packages that we need are included in the tidyverse, janitor, gt, officer and viridis\n\n\n2.1.2 Importing the data\nThe nike_sales_2024.csv file should be located in your data, raw directory in your project. If you open this file in excel, you’ll see that it is a comma delimited file. From the first line, you can see that the dataset includes a name for each variable. In excel, the dataset will open in one column with all data for each variable in the same column A (Figure 2.1):\n\n\n\n\n\nFigure 2.1: A csv file in Excel\n\n\n\n\n\n\n\n\nIn excel, you would use the Text to columns button under Data to split this single column in one column per variable. In RStudio, we can use read_csv from the {readr} package to import the data in R. The read_csv function allows you to specify a lot of details on how and what R should import. We will discuss some of these option more in depth in the next chapters. For now, the defaults are fine. We need to specify the location on the file name. Recall from the previous chapter that we can use the {here} package. As you put your file in the data\\raw directory, we can use here::here(\"data\", \"raw\", \"nike_sales_2024.csv\" to tell read_csv which file it needs to read (nike_sales_2024.csv) and where it is located: starting from your project root directory: in the data\\raw\\directory. To import the file, we can now use:\n\nnike_df &lt;- read_csv(here::here(\"data\", \"raw\", \"nike_sales_2024.csv\"))\n\nRows: 1000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Month, Region, Main_Category, Sub_Category, Product_Line, Price_Tier\ndbl (4): Units_Sold, Revenue_USD, Online_Sales_Percentage, Retail_Price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe data in the file is read into a tibble (a special type of data frame (or df)) called nike_df. There is nothing special about the name nike_df. You can choose any name you like. I chose to start the name with a reference to the dataset nike and added _df to indicate that it is a data frame. Other than the usual name file or variable name limitations (Chapter 1) there are little constraints in terms of the name you use for a dataset. In the next chapter, we’ll introduce data frames and tibbles more in depth. For now, I’ll refer to a data frame. Recall the meaning of &lt;- or the assignment operator: the datatset in the file nike_sales_2024.csv is assigned to the object nike_df. In the console, you can see the dataset as well as its size: the dataset includes 1000 rows and 10 columns. The delimiter that was used was the comma “,”. Out of these 10 columns, 6 include characters and 4 include numbers. You can think about a character column as a column filled with letters: “January”, “Equipment”, “India”, … . Character variables are enclosed between \" \". Numeric columns include numbers. You can inspect the the data in a number of ways.\n\n\n2.1.3 A first inspection of the data\nThe first is to check the internal structure of the nike_df object using the str() function:\n\nstr(nike_df)\n\nspc_tbl_ [1,000 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Month                  : chr [1:1000] \"November\" \"January\" \"October\" \"December\" ...\n $ Region                 : chr [1:1000] \"India\" \"India\" \"India\" \"Greater China\" ...\n $ Main_Category          : chr [1:1000] \"Equipment\" \"Equipment\" \"Apparel\" \"Footwear\" ...\n $ Sub_Category           : chr [1:1000] \"Bags\" \"Accessories\" \"Tops\" \"Cricket\" ...\n $ Product_Line           : chr [1:1000] \"Gym Sack\" \"Hats\" \"Tech Fleece\" \"Vapor Cricket\" ...\n $ Price_Tier             : chr [1:1000] \"Budget\" \"Budget\" \"Mid-Range\" \"Premium\" ...\n $ Units_Sold             : num [1:1000] 48356 9842 25079 41404 33569 ...\n $ Revenue_USD            : num [1:1000] 14506800 2066820 1755530 8694840 5371040 ...\n $ Online_Sales_Percentage: num [1:1000] 73 50 90 58 53 73 50 55 78 86 ...\n $ Retail_Price           : num [1:1000] 300 210 70 210 160 140 230 150 150 230 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Month = col_character(),\n  ..   Region = col_character(),\n  ..   Main_Category = col_character(),\n  ..   Sub_Category = col_character(),\n  ..   Product_Line = col_character(),\n  ..   Price_Tier = col_character(),\n  ..   Units_Sold = col_double(),\n  ..   Revenue_USD = col_double(),\n  ..   Online_Sales_Percentage = col_double(),\n  ..   Retail_Price = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThis function shows the name of each variable ($ Month, $ Region, …), the variable type (charachter, numeric), the number of observations in each column: one observation per row, with 1000 observations per variable (1:1000). In addition you see the first values for each variable. The $ plays a special role in R. Within a project you’ll often import many datasets. For instance, a dataset with sales data for various countries, a employee dataset for various production locations, … . If these datasets include the same variable names, R doesn’t know which variable you want to use. To avoid this, to have to identify a variable using dataframe$variable. If you want to use the Main_Category variable in nike_df, you would identify this variable as nike_df$Main_Category.\nThe output is useful to check if the imported data meet your expectations. For instance, did you expect 1000 observations for 10 variables and did you expect that data on units sold or revenue is shown as numeric, while data on regions or categories are character variables? If you expected say 10.000 observations, it is useful to return to your raw data and check if and to what extent the file is correct. If observations are shown as characters and you would expect numeric variables, you’ll need to change the variable type.\nYou can see the first 10 observations if you type the name of the object nike_df\n\nnike_df\n\n# A tibble: 1,000 × 10\n   Month    Region Main_Category Sub_Category Product_Line Price_Tier Units_Sold\n   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;\n 1 November India  Equipment     Bags         Gym Sack     Budget          48356\n 2 January  India  Equipment     Accessories  Hats         Budget           9842\n 3 October  India  Apparel       Tops         Tech Fleece  Mid-Range       25079\n 4 December Great… Footwear      Cricket      Vapor Crick… Premium         41404\n 5 May      Great… Equipment     Socks        Performance… Premium         33569\n 6 October  Japan  Apparel       Performance  Dri-FIT      Premium         39344\n 7 December Japan  Footwear      Cricket      Vapor Crick… Budget          30197\n 8 July     Great… Apparel       Tops         Therma-FIT   Mid-Range       42814\n 9 April    Great… Footwear      Cricket      Vapor Crick… Mid-Range       16489\n10 February Europe Footwear      Cricket      Vapor Crick… Mid-Range       43626\n# ℹ 990 more rows\n# ℹ 3 more variables: Revenue_USD &lt;dbl&gt;, Online_Sales_Percentage &lt;dbl&gt;,\n#   Retail_Price &lt;dbl&gt;\n\n\nThat you see the first 10 observations is a characteristic of a tibble. Other data frames show all observations.\nFor datasets with a lot of variables, this might not be the most efficient way to check your data, but here, with 10 variables, you can inspect the dataset using the first 10 lines. As we will see, there are alternatives, for instance, you can ask for the 15 first lines, the 10 last lines or you can ask for a sample of 20 or any other number of random observations using slice_head(nike_df, n = 15), slice_tale(nike_df, n = 10), slice_sample(nike_df, n = 20. If you order the data first, you can inspect the values for numeric variables. For instance, nike_df[order(nike_df$Retail_Price), ] orders the data on retail prices from low to high (ascending). If you include this statement in slice_head() you’ll see the dataset starting with e.g. the 15 observations with the lowest retail price. slice_tail() shows the highest prices. Using these commands, you can inspect if the values “make sense”. If you use slice_tail(nike_df[order(nike_df$Retail_Price), ], n = 15, you will see the observations with the highest retail price. For a company as Nike, you wouldn’t expect retail prices per unit equal to, say $50.000. If those are included in your dataset, you need to double check: although it could be the case that $50.000 is correct and refers to an exceptional item, at least you need to make sure. In addition, it is probably a good idea to add a note to your project that these prices were checked and that they do refer to an exceptional item in the companies catalog.\nThere are 1000 observations in the dataset. We saw that number in the str() output. Sometimes we need the total number of observations in our code. We can count this number using\n\nnobs_tot &lt;- summarise(nike_df, n = n())\n\nLet’s look at this statement more closely. summarise() or summarize() (R knows both) is a function included in the {dplyr} package. The summarise(nike_dr) part tells R we want a summary for the dataset nike_df. There are many ways to summarise data (mean, median, min, max, …). Here we ask one specific summary statistics: the total number of observations. To do so, we use n = n(). The last statement says to we want to know the total number of observations (n()) and that R can store this number in a column called n (n = n() of a tibble called nobs_tot (see the &lt;-). In other words, the output of this command is a new dataset (a tibble). If you run this command, you’ll see that the outcome is a tibble with one observation called n (the first n in the summarise statement) and that n is equal to 1000, the total number of observations calculated by n(). Why do we have a tibble in this case? Usually, we will ask for more than one summary statistic. In that case, each summary statistics will be stored in a separate column in a new tibble. Here, as we only ask for one summary statistics, our tibble nobs_tot includes one column and one row. The column in called n and the single observation is 1000.\nUsing the pipe operator, you can write this line also as\n\nnobs_tot &lt;- nike_df |&gt; summarise(n = n())\n\nRecall that the |&gt; operator puts whatever is on its left hand side as the first input on its right hand side. The summarise() function includes the data it needs to summarise as its first argument or input. In other words, the |&gt; takes nike_df and puts it as the first argument in summarise(): summarise(nike_df, ...).\nIn addition to the number of observations, it is also relevant to check if there are missing values. Do do so, we use\n\nnobs_na &lt;- sum(is.na(nike_df))\n\nLet’s read this statement: is.na(nike_df) checks if an observation is missing or not. If it is missing, it returns a value equal to TRUE. If the value is not missing, it returns a value equal to FALSE. In other words, R checks all observations and marks them as TRUE: if missing or FALSE if not missing. In most programming languages, TRUE is also equal to 1 and FALSE is equal to 0. We use this fact as we ask R to sum across all observations: sum(is.na(nike_df)). The outcome will be 0 is there are no missing observations. If there are 12 missing observations, the command will sum to 12 as R marks these 12 missing observations as missing (is.na() = TRUE i.e. 1). We assign the total number of missing observations in an object called nobs_na.\nHere, the number of missing values is 0. However, in case the dataset would have included missing observations, we could use the\n\napply(nike_df, MARGIN = 2, function(x) sum(is.na(x)))\n\n                  Month                  Region           Main_Category \n                      0                       0                       0 \n           Sub_Category            Product_Line              Price_Tier \n                      0                       0                       0 \n             Units_Sold             Revenue_USD Online_Sales_Percentage \n                      0                       0                       0 \n           Retail_Price \n                      0 \n\n\ncommand so show us in which variables the missing values are. This command tells R to apply a function, identified by function(x) sum(is.na(x)), to the dataset nike_df. The argument, MARGIN = 2 tells R that the function function(x) sum(is.na(x)) has to be applied to all columns (variables). In other words, the command will count the number of missing values (sum(is.na(x)) in every column (MARGIN = 2) of nike_df. If MARGIN = 1, then R will apply the function of every row. So,\n\napply(nike_df, MARGIN = 1, function(x) sum(is.na(x)))\n\n   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [482] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [519] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [667] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [704] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [741] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [778] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[1000] 0\n\n\nshows if a row has missing values and if so, how many missing values there are.\n\n\n2.1.4 Tidying your data\nBefore you start your analysis, you need to check your data (see Figure 1). If you look at the dataset, there are a couple of things that we can change. First, if you look at the output from str(nike_df) or nike_df, you’ll notice that rows such as Main_Category or Product_Line use both capital letters at the start of a word as well as an underscore between words. This is applies consistently across columns. However, if you would like to change these names in e.g. snake you can use the clean_names() function from the {janitor} package. You installed this package in the previous chapter. This function takes the a data frame as input and changes column names into snake case. Although it is not necessary as snake case if the default case if you use clean_names, in the next command, the case is included explicitly. If we use the native pipe |&gt;, we can write\n\nnike_df &lt;- nike_df |&gt; clean_names(case = \"snake\")\n\nWithout the native pipe, the command would have been nike_df &lt;- clean_names(nike_df, case = 'snake'). The alternatives for snake case are (see also Chapter 1):\n\nsnake_case: “snake”: default (is not other is specified, R will choose this option).\nlowerCamel: “lower_camel” or “small_camel”\nUpperCamel: “upper_camel” or “big_camel”\nALL_CAPS: “all_caps” or “screaming_snake”\nlowerUPPER: “lower_upper”\nUPPERlower: “upper_lower”\nSentence case: “sentence”\nTitle Case: “title” - This one is basically the same as sentence case, but abbreviations are always turned into upper\n\nA quick look at nike_df will show that column names are now snake case.\nRecall from the str() function that some variables are character variables while others are numeric. If you sort character variables, R will sort them alphabetically. Sometimes this doesn’t make sense. If you take a look at nike_df, you’ll see that the variable month includes the months of the year. We can ask R to sort this variable. Because there are many observations that include a month, we will first select the unique months. In other words, if February is included 10 times among the observations, the function unique() will only include that month once. The same holds for December, January and all other months. To identify a column in a data frame, you need to add it to the data frame’s name with a $ sign: nike_df$month selects the variable month from nike_df. Using unique(nike_df$month) we select every month only once. If we then sort these unique variables (using sort()) R returns:\n\nnike_df$month |&gt; \n  unique() |&gt; \n  sort()\n\n [1] \"April\"     \"August\"    \"December\"  \"February\"  \"January\"   \"July\"     \n [7] \"June\"      \"March\"     \"May\"       \"November\"  \"October\"   \"September\"\n\n# Note: as an alternative you can write sort(unique(nike_df$month))\n\nYou can see that with April, August, December, … . It would be preferable if R sorts these months according to the calender year (January, February, …), the academic year (October, December, …) or any other meaningful order (start of the season in sports or tourism, accounting year in case the accounting year is different from the calender year, …).\nTo do so, we will change the type of the variable month from character into a factor and order it. As we’ll see in the next chapter, a factor variable takes a limited number of values or levels and you can tell R that these levels are ordered.\n\nnike_df$month &lt;- factor(nike_df$month, \n                             levels = c(\"January\", \"February\", \"March\", \n                                        \"April\", \"May\", \"June\", \"July\", \n                                        \"August\", \"September\", \"October\", \n                                        \"November\", \"December\"), \n                             ordered = TRUE)\n\nIf you now run\n\nsort(unique(nike_df$month))\n\n [1] January   February  March     April     May       June      July     \n [8] August    September October   November  December \n12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December\n\n# Note: as an alternative you can write nike_df$month |&gt; unique() |&gt; sort()\n\nYou’ll see that the months are ordered.\nThe dataset includes another variable that implies a natural order: price_tier. Its values Budget, Mid-Range or Premium refer to the pricing segment. However, as you can see, if you sort this variable,\n\nnike_df$price_tier |&gt; unique() |&gt; sort()\n\n[1] \"Budget\"    \"Mid-Range\" \"Premium\"  \n\n\nthe order is consistent the the pricing segment.\nThe other variables, region, main_catetory, sub_category and product_line do not require a change in their order (unless you would like to show the regions or categories in particular order in a graph or a table).\nNote you can use the unique() function to check your data. Suppose for instance that the month of April is included as both April as well as april. As R is case sensitive, R will treat April as distinct from april. The unique function allows you to check for these typo’s. The same holds for typo’s in e.g. category variables, or other character variables.\n\n\n2.1.5 Data quality and validation\nData validation refers to processes to ensure the accuracy, consistency and completeness of the data. We already performed a couple of quality checks. For instance, we checked if the type of a variable is correct (character, numeric). We also checked if there aren’t any missing values (completeness). In terms of the completeness, we could also verify if we have observations for all regions or for all subcategories. To do so, we could use the tabyl() function included in the {janitor} package. If you include one variable, the function shows the number of observations for every level of that variable as well as the percentages. If you add show_na = TRUE it also shows the missing variables as a separate category:\n\nnike_df |&gt; tabyl(region, show_na = TRUE)\n\n         region   n percent\n        America 147   0.147\n         Europe 133   0.133\n  Greater China 161   0.161\n          India 145   0.145\n          Japan 144   0.144\n    South Korea 146   0.146\n Southeast Asia 124   0.124\n\n\nIf you include 2 variables, the function shows a cross table with the total number of observations:\n\nnike_df |&gt; tabyl(region, main_category, show_na = TRUE)\n\n         region Apparel Equipment Footwear\n        America      49        53       45\n         Europe      49        48       36\n  Greater China      53        54       54\n          India      46        60       39\n          Japan      54        38       52\n    South Korea      50        57       39\n Southeast Asia      42        42       40\n\n\nThese examples allow you to check is your observations are complete or if a region or product category is underrepresented. For instance, if you know that 20% of all transactions are usually from Southeast Asia, the results from the first tabel() command could indicate that some transactions from Southeast Asia are missing from the data.\nWith respect to the correctness of the data, there are usual couple of validation checks that you can do. In the example here, the variable revenue_usd is calculated as the product of the variable units_sold and retail_price. Using this observation, we can check if units_sold * retail_price - revenue_usd equals zero for all observations in the dataset. To do so, we will first create tibble including only these three variables and call it nike_df_val. Here we will use the select() function from {dplyr} which you installed as part of the tidyverse collection in Chapter 1. Using the variables, we then use the mutate() function from the same package to create a new variable rev_calc equal to units_sold * retail_price - revenue_usd:\n\nnike_df_val &lt;- nike_df |&gt; \n  select(retail_price, units_sold, revenue_usd) |&gt;\n  mutate(rev_calc = retail_price * units_sold - revenue_usd)\n\nIf we sum all observations in the rev_calc variable, that sum should be equal to 0. The data could be wrong in two ways: reported revenue is higher than the calculated revenue in rev_calc or it is lower. Because positive and negative values could cancel each other out, we use the sum of the absolute value of the difference. If that sum is 0, calculated revenue equals reported revenue for all observations. If not, we will have to find the observations where the revenue_usd is less or greater than retail_price * units_sold. We can calculate the absolute value using the abs()function and the sum of the abs(rev_calc) variable across observations using the sum()function. If this sum is not equal to 0, the validation check will point to a mistake in the data. In R, “not equal to” is a written as !=. In other words, if the condition sum(nike_df_val$rev_calc) != 0 holds, then the validation check fails. If that condition does not hold and the sum is in fact equal to 0, the tests passes the validation check. We can include this condition in a couple of lines of code:\n\nif (sum(abs(nike_df_val$rev_calc)) != 0) {\n  print(\"Data validation check revenue failed.\")\n} else {\n  print(\"Data validation check revenue passed.\")\n}\n\n[1] \"Data validation check revenue passed.\"\n\n\nThe first line includes the condition. If it holds, then R will print “Data validation check revenue failed”. The if condition does not hold and the sum if equal to 0, then R will move to the third line starting with else and print “Data validation check revenue passed.”. If you include such a statement after every validation check, you’ll see the results immediately in the console.\nAs the validation check didn’t suggest errors in the data, we can remove nike_df_val using\n\nrm(nike_df_val)\n\nLet’s now turn our attention to the numeric variables units_sold, revenue_usd, online_sales_percentage and retail_price and ensure that their values are inline with expectations. To do so, we will first create a new data frame nike_df_num with these numeric variables. To select all numeric variables from a data frame, we use the where()function. This function helps to select variables for which a function inside the ()returns TRUE. In this case, we need the variables that are numeric. The is.numeric(x) function is TRUE if x is numeric and FALSE otherwise. Combining both, where(is.numeric)) will help to select the numerical variables in nike_df using:\n\nnike_df_num &lt;- nike_df |&gt; select(where(is.numeric))\n\nWe first look at a summary of the numeric variables. In R, we can use\n\nsummary(nike_df_num)\n\n   units_sold     revenue_usd       online_sales_percentage  retail_price  \n Min.   : 5028   Min.   :  287400   Min.   :50.00           Min.   : 50.0  \n 1st Qu.:17554   1st Qu.: 2344675   1st Qu.:60.00           1st Qu.:110.0  \n Median :28685   Median : 4328020   Median :71.00           Median :180.0  \n Mean   :28499   Mean   : 5039576   Mean   :70.04           Mean   :176.3  \n 3rd Qu.:40026   3rd Qu.: 7264942   3rd Qu.:80.00           3rd Qu.:240.0  \n Max.   :49992   Max.   :14864700   Max.   :90.00           Max.   :300.0  \n\n\nThis summary can now be used to further check the data: for this dataset, all numeric variables should be positive (as you would expect that a company charges a postive retail price, doesn’t sell negative units and hence reports revenue in usd which is positive and online sales percentage can not be negative) . The minimum value (first line in the output) shows, for each variable, if that condition is met. Second, the variable online_sales_percentage can not exceed 100%. The last line in the summary shows that this is indeed not the case: the maximum value equals 90%. To assess the other variables, you would need further information or you need to be familiar with the products or data. For instance, if you work for the company, a quick look at the maximum for retail_price could suggest that there are errors in the data if this value is too high relative to the retail prices charged by the company. In this dataset, we only have 4 numeric variables. However, often you have more and you can check more conditions: age should be positive but less than, say 110; in accounting assets equal liabilities and total revenue equals turnover and other revenue, …); in finance, you wouldn’t expect a daily percentage change in the price of a stock equal to, say 200% and a postal code for Belgium includes 4 numbers (e.g. 1000 Brussels), … As the number of variables grows, and the size of the summary table increases, you’ll need to automate these checks and program them as validation rules similar to the one we used.\nThe summary() function automatically shows the summary statistics. If the dataset would include missing values, the function will show them as an na. Here, as there are no missing values, this line was left out.\nIt is always important to check you data for outliers: values that are very high of very low. The maximum value is one check, but there could be other observations whose value is extremely high. In addition, to assess the maximum value, you need to have some sence of the magnitude of your data. Outlier detection shows you which values are high relative to the other values for the same variable in the dataset. Outliers are not necessarily wrong, but the only way to determine if they are true outliers or data errors is to take a closer look at these observations that include outliers. There are many ways to determine if an extreme observation is an outlier. For instance, if a variable is normally distributed, 0.5% of observations will be more than 3 standard deviations from the mean, 5% more than 2 standard deviations, … . Using this observation, you could, for instance, consider all values more than 3 standard deviation from the mean as an outlier that requires further inspection.\nLet’s use this observations to see if there are outliers in the retail price variable. To do so, we need to calculate the mean and standard deviation. In R, you can calculate the mean using the function mean() and the standard deviation using the function sd(). Within the () in both function, we need to include the variable as well as na.rm = TRUE. If we wouldn’t add this to the function and our variable includes missing values, R would show an error: it can not calculate the mean or the standard deviations due to missing observations. If you add na.rm = TRUE, you tell R that it can remove (“rm”), missing observations (“na”) from the calculation. Using the mean and the standard deviation, we can calculate the z-score and add a column with z-scores to the data frame. To do so, we use the mutate() function:\n\nave_price &lt;- mean(nike_df_num$retail_price, na.rm = TRUE)\nsd_price &lt;- sd(nike_df_num$retail_price, na.rm = TRUE)\n\nnike_df_num &lt;- nike_df_num |&gt; \n  mutate(z_price = (retail_price - ave_price)/sd_price)\n\nWe can now re-arrange the dataset and order all observations from high to low z-price values. R will show the 10 observations with the highest value for z_price if you run:\n\nnike_df_num |&gt; arrange(desc(z_price))\n\n# A tibble: 1,000 × 5\n   units_sold revenue_usd online_sales_percentage retail_price z_price\n        &lt;dbl&gt;       &lt;dbl&gt;                   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1      48356    14506800                      73          300    1.64\n 2      41901    12570300                      63          300    1.64\n 3      14434     4330200                      77          300    1.64\n 4       9747     2924100                      55          300    1.64\n 5      27680     8304000                      68          300    1.64\n 6      16989     5096700                      83          300    1.64\n 7      21835     6550500                      88          300    1.64\n 8      49549    14864700                      75          300    1.64\n 9      23339     7001700                      84          300    1.64\n10      12202     3660600                      84          300    1.64\n# ℹ 990 more rows\n\n\nand the 10 smallest value if you run\n\nnike_df_num |&gt; arrange(z_price)\n\n# A tibble: 1,000 × 5\n   units_sold revenue_usd online_sales_percentage retail_price z_price\n        &lt;dbl&gt;       &lt;dbl&gt;                   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1      47805     2390250                      52           50   -1.68\n 2       8478      423900                      57           50   -1.68\n 3      29827     1491350                      79           50   -1.68\n 4      47626     2381300                      66           50   -1.68\n 5      28038     1401900                      63           50   -1.68\n 6      19197      959850                      57           50   -1.68\n 7      27232     1361600                      60           50   -1.68\n 8      15325      766250                      70           50   -1.68\n 9      31290     1564500                      87           50   -1.68\n10      30892     1544600                      65           50   -1.68\n# ℹ 990 more rows\n\n\nThe z-scores show that all observations with respect to price are withing reasonable limits of the mean (e.g. less than 2 standard deviations). You can include other outlier checks for all other numerical variables.\nNote that you don’t need three lines of code but you could do it in one. If you repeat the analysis for revenue:\n\nnike_df_num &lt;- nike_df_num |&gt; \n  mutate(z_rev = (revenue_usd - (mean(nike_df_num$revenue_usd, na.rm = TRUE)))/(sd_price &lt;- sd(nike_df_num$revenue_usd, na.rm = TRUE)))\n\nIf you only want to inspect the values for the revenue variable with outliers, you could select the revenue_usd as well as the z_rev variable and order them and show the 10 smallest,\n\nnike_df_num |&gt; \n  select(revenue_usd, z_rev) |&gt; \n  arrange(z_rev)\n\n# A tibble: 1,000 × 2\n   revenue_usd z_rev\n         &lt;dbl&gt; &lt;dbl&gt;\n 1      287400 -1.45\n 2      313500 -1.45\n 3      313980 -1.45\n 4      316350 -1.45\n 5      358300 -1.43\n 6      403860 -1.42\n 7      404250 -1.42\n 8      423900 -1.41\n 9      464950 -1.40\n10      506940 -1.39\n# ℹ 990 more rows\n\n\nor then highest z_rev scores:\n\nnike_df_num |&gt; \n  select(revenue_usd, z_rev) |&gt; \n  arrange(desc(z_rev))\n\n# A tibble: 1,000 × 2\n   revenue_usd z_rev\n         &lt;dbl&gt; &lt;dbl&gt;\n 1    14864700  3.01\n 2    14608200  2.93\n 3    14506800  2.90\n 4    14453100  2.88\n 5    14321700  2.84\n 6    14264810  2.82\n 7    13997760  2.74\n 8    13989310  2.74\n 9    13871760  2.70\n10    13821000  2.69\n# ℹ 990 more rows\n\n\nHere you can see that there are outliers that need further inspection as their z-scores are larger than 2 or even 3. Suppose you would like to see all observations for a z score larger than 1.96 for revenue, you can use the filter() function from the {dplyr} package:\n\nnike_df_num_out &lt;- nike_df_num |&gt; filter(z_rev &gt; 1.96)\n\nAs you can see, there are 49 observations where total revenue in usd is reported to be in excess of 1.96 time the standard deviation. You can use the nike_df_num_out dataset to futher analyse if these observations are correct or if they include an error and should be corrected or removed from the dataset.\nIf you are satisfied with your data, you can now remove nike_df_num as well as nike_df_num_out.\n\nrm(nike_df_num, nike_df_num_out)\n\n\n\n2.1.6 Transforming and manipulating data\nUsing the mutate() function from {dplyr} we added variables to a dataset (e.g. rev_calc and z_price) and used them in the quality and validation part of the workflow. Often we will need to calculate other variables for our analysis. Suppose for instance you need the natural logarithm of a variable. The log() function in R allows you to calculate this variable. For instance, to the log of the retail price and units sold:\n\nnike_df &lt;- nike_df |&gt; \n  mutate(log_price = log(retail_price), log_unit = log(units_sold))\n\nNote that in R, the log() function has more than one argument: log(x, base = exp(1)). Here, the default for the base is exp(1) which is equal to e. In other words, the default log() function calculates the natural logarithm. If you need the logarithm with base 10, you can use log10() of change the base in 10: log(x, base = 10).\nIn R it is quite easy to use more complex expressions to add variables to a data frame. For instance, suppose you would like to recode the online_sales_percentage variable into a low (online sales less than 60%), mid (online sales less than 80% and high (online sales higher than or equal to 80%) category variable. Again you would use mutate() but here you would add the case_when()function:\n\nnike_df &lt;- nike_df |&gt; \n  mutate(oneline_cat = case_when(\n    online_sales_percentage &lt; 60 ~ \"Low\", \n    online_sales_percentage &lt; 80 ~ \"Mid\", \n    online_sales_percentage &gt;= 80 ~ \"high\",\n    .default = \"none\"))\n\ncase_when() evaluates the expression between () and if the expression returns TRUE (e.g. online_sales_percentage &lt; 60 ) returns the value after the ~ (tilde). It there are missing observations or is there is an observation which doesn’t meet any condition, the .default = part tells R what need to happen in that case.\nWe will have a closer look at these and other expressions in the next chapter and in Chapter 8.\nYou can save your data as a csv or Rdata file. The latter option is good if you are the only person working on the project or if all others who work on the same project use R. The former option is good is these conditions don’t hold. You can save your work in the tidy subdirectory of your data folder:\n\nwrite_csv(nike_df, here::here(\"data\", \"tidy\", \"nike.csv\"))\n\nYou can now use this file using the read_csv function.\nAs an alternative, you can use the save function to save the data as an Rdata file:\n\nsave(nike_df, file = here::here(\"data\", \"tidy\", \"nike.Rdata\"))\n\nTo reload the data, you can use\n\nload(file = here::here(\"data\", \"tidy\", \"nike.Rdata\"))\n\n\n\n2.1.7 Answering questions with data\nIf you are satisfied with the quality of your data, you can now start to use it to answer questions.\nYou have used the use the tabyl() function from the {janitor} package to see how many observations we have for various categories. For instance, to see the number of observations per region, we can use\n\nnike_df |&gt;\n  tabyl(region)\n\n         region   n percent\n        America 147   0.147\n         Europe 133   0.133\n  Greater China 161   0.161\n          India 145   0.145\n          Japan 144   0.144\n    South Korea 146   0.146\n Southeast Asia 124   0.124\n\n\nHere, the output shows both the absolute number of observations as well as the relative number (percentage). If you add a variable, you’ll see more detail. For instance, the number of observations per sub category and price tier:\n\nnike_df |&gt;\n  tabyl(sub_category, price_tier)\n\n sub_category Budget Mid-Range Premium\n  Accessories     40        32      51\n         Bags     47        39      30\n   Basketball     21        16      21\n      Cricket     20        20      21\n     Football     25        25      13\n    Lifestyle     28        17      21\n    Outerwear     37        46      42\n  Performance     42        34      33\n      Running     20        20      17\n        Socks     29        31      53\n         Tops     39        42      28\n\n\nAs you can see, there are quite some differences in the number of observations for sub category - price tier pair. You can test if this is indeed the case using a chi-square test. To do so, save the table and you can perform and print the chi-square test:\n\nprtier_subcat_test &lt;- nike_df |&gt;\n  tabyl(sub_category, price_tier)\n\nchi_square_result &lt;- chisq.test(prtier_subcat_test)\n\n# Print the results\nprint(chi_square_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  prtier_subcat_test\nX-squared = 30.532, df = 20, p-value = 0.06169\n\n\nLet’s see if there is a difference in the average retail price and the average percentage of online sales per product sub category. To answer that question you need to calculate the average retail price per and the average share of online sales product sub category. In R, you can use {dplyr} group_by() to group observations per item. In this case, we need to group per sub category: group_by(sub_category). For each sub category, we can now calculate the averages we want. We do so using summarise(ave_price = mean(retail_price), ave_online = mean(online_sales_percentage). Note that we use the native pipe |&gt; operator here. You can read this code as using nike_df, apply the group_by() function and use the result to calculate the values in summarise(). The outcome of this operation is a data frame. We assign this data frame to nike_df_cat.\n\nnike_df_cat &lt;- nike_df |&gt;\n  group_by(sub_category) |&gt;\n  summarise(ave_price = mean(retail_price), \n            ave_online = mean(online_sales_percentage))\n\nThere are multiple ways to communicate the result of this exercise. We will illustrate two: a table and a graph. For the table, we’ll use the {gt} package. You have installed that package in the previous chapter Chapter 1. We will start from the data frame nike_df_cat and produce a table nike_gt_cat. To do so we introduce the data frame in the gt() function. We then add a table header (a title and subtitle) and column labels. Using fmt_currency() and fmt_percent() we limit the number of decimals in the table to 2 and add “$” and “%”. gt() scales any value in a percent column with 100 by default (i.e. 0.52 will be shown as 52%). Because the variable online_sales_percentage is expressed as a value between 0 and 100 and not between 0 and 1, we need to turn this scaling of. The last line shows the table.\n\n# Create the table\n\nnike_gt_cat &lt;- gt(nike_df_cat) |&gt;\n  tab_header(\n    title = \"Average retail price and share online per Sub category\",\n    subtitle = \"2024\") |&gt;\n  cols_label(\n    sub_category = \"Sub category\", \n    ave_price = \"Average retail price\", \n    ave_online = \"Average share online\") |&gt;\n  fmt_currency(\n    columns = \"ave_price\", \n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_percent(\n    columns = \"ave_online\", \n    scale_values = FALSE,\n    decimals = 2)\n  \n\n# Show the table\n\nnike_gt_cat\n\n\n\n\n\n\n\nAverage retail price and share online per Sub category\n\n\n2024\n\n\nSub category\nAverage retail price\nAverage share online\n\n\n\n\nAccessories\n$175.45\n72.36%\n\n\nBags\n$180.17\n67.82%\n\n\nBasketball\n$161.21\n70.17%\n\n\nCricket\n$199.51\n72.00%\n\n\nFootball\n$180.00\n69.73%\n\n\nLifestyle\n$176.36\n69.77%\n\n\nOuterwear\n$186.16\n70.14%\n\n\nPerformance\n$167.43\n71.18%\n\n\nRunning\n$163.51\n68.91%\n\n\nSocks\n$182.74\n69.20%\n\n\nTops\n$163.76\n69.17%\n\n\n\n\n\n\n\nAgain note that we used the |&gt; operator to connect the various parts of the table. As a matter of fact, we could have calculated the averages and produced the table in one code block. So show this, let’s create the table again, now called nike_gt2_cat but in one code block.\n\n# Create the table\n\nnike_gt2_cat &lt;- nike_df |&gt;\n  group_by(sub_category) |&gt;\n  summarise(ave_price = mean(retail_price), \n            ave_online = mean(online_sales_percentage)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Average retail price and share online per sub category\",\n    subtitle = \"Another way to create the table\")|&gt;\n  cols_label(\n   sub_category = \"Sub category\", \n    ave_price = \"Average retail price\", \n    ave_online = \"Average share online\") |&gt;\n    fmt_currency(\n    columns = \"ave_price\", \n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_percent(\n    columns = \"ave_online\", \n    scale_values = FALSE,\n    decimals = 2)\n\n# Show the table\n\nnike_gt2_cat\n\n\n\n\n\n\n\nAverage retail price and share online per sub category\n\n\nAnother way to create the table\n\n\nSub category\nAverage retail price\nAverage share online\n\n\n\n\nAccessories\n$175.45\n72.36%\n\n\nBags\n$180.17\n67.82%\n\n\nBasketball\n$161.21\n70.17%\n\n\nCricket\n$199.51\n72.00%\n\n\nFootball\n$180.00\n69.73%\n\n\nLifestyle\n$176.36\n69.77%\n\n\nOuterwear\n$186.16\n70.14%\n\n\nPerformance\n$167.43\n71.18%\n\n\nRunning\n$163.51\n68.91%\n\n\nSocks\n$182.74\n69.20%\n\n\nTops\n$163.76\n69.17%\n\n\n\n\n\n\n\nYou can save the table in various formats, e.g. pdf, docx, html using the gtsave() function. This function doesn’t overwrite existing files. In case you ran the code, the table will exist. So we first remove it. To delete a file from a directory, you can use unlink(). As an alternative, in the files pane, you can select the file and delete it using the Delete button.\n\nfi &lt;- here::here(\"reports\", \"table_1.docx\")\nif (file.exists(fi)) {\n  file.remove(fi)\n}\n\n[1] TRUE\n\n\n\nnike_gt_cat |&gt; gtsave(filename = \"table_1.docx\",\n              path = here::here(\"reports\"))\n\nWe can also communicate the same results with e.g. a bar chart. In this example, we’ll produce a bar chart with the average price per sub category. We’ll use the viridis color palette. In that way, the chart will be readable for color blind. We’ll save this chart using nike_pl_aveprice. If you want to see the various options of the viridis palette, you can change the “B” into “A”, “B”, … “H”.\n\n# Create the plot\n\nnike_pl_aveprice &lt;- nike_df |&gt;\n  group_by(sub_category) |&gt;\n  summarise(ave_price = mean(retail_price),\n            ave_online = mean(online_sales_percentage)) |&gt;\n  ggplot(aes(x = sub_category, y = ave_price, fill = sub_category )) + \n  geom_col() +\n  scale_fill_viridis(option = \"B\", discrete = TRUE) +\n  labs(title = \"Average price per sub category\", \n       x = \" \", \n       y = \"Aveage price\",\n       fill = \"Sub category\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        panel.background = element_rect(fill = \"white\"))\n                                   \n \n# Show the graph\n\nnike_pl_aveprice\n\n\n\n\n\n\n\nnike_pl_aveprice |&gt; ggsave(filename = \"figure_1\", \n       device = \"png\", \n       path = here::here(\"reports\"))\n\nIn the ggplot() call, we didn’t specify the input names. If we would have done so, we would have written the first line in the ggplot() code as ggplot(mapping  = aes(...). Because ggplot is so widely used, hardly anybody uses these named inputs. Likewise, because ggplot is often used with the pipe operator, the first argument (ggplot(data = df, ...) (with df the name of the dataset where R can find the variables) is often left out. Even if there is not pipe before the call to ggplot, data = df is almost never used.\nLet’s see if we can determine the most sold product lines? Here we need to group by product line. For each product line, we calculate total revenue as the sum of revenue. To see which products lines are most sold, we can re-order the data frame using total revenue. We order in descending order. To show only the highest 10 product lines, we use slice(1:10). After these steps, we have a data frame, nike_df_prod_top with the top 10 selling product lines. Using ggplot, we can plot these product lines in a bar chart. The graph shows the 10 best selling products, starting at 10 and moving to the first along the horizontal axis. Here, we remove the legend and remove the label from the x axis. We do the latter as we add x = \" \" in the labs part.\n\n# Top selling product lines (top 10)\n\nnike_df_prod_top &lt;- nike_df |&gt; \n  group_by(product_line) |&gt;\n  summarise(tot_rev = sum(revenue_usd)) |&gt;\n  arrange(desc(tot_rev)) |&gt;\n  slice(1:10)\n\nggplot(nike_df_prod_top, aes(x = reorder(product_line, tot_rev), \n                             y = tot_rev, \n                             fill = product_line)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Top 10 selling products in revenue\", \n       x = \" \", \n       y = \"total revenue (usd)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nUntil now, we worked with the full data frame. You can also use a subset of the observations. To illustrate, let’s try to show the regional share in total Apparel sales. First, we need a data frame with only apparel sales. Here we use {dplyr}’s filter() function and keep observations if the main category is equal to apparel. To check the equality, R uses ==. The part of the first line filter(main_category == \"Apparel\") will filter all observations in the apparel main category.\n\nappar_df &lt;- nike_df |&gt; filter(main_category == \"Apparel\")\n\ntot_appar &lt;- sum(appar_df$revenue_usd)\n\nappar_df_reg &lt;- appar_df %&gt;% group_by(region) %&gt;%\n  summarize(appar_sh = sum(revenue_usd)/tot_appar * 100)\n\nIn the second step, we calculate total apparel sales and assign that value to tot_appar. We’ll use this value to calculate regional shares. In the third step, we group the apparel data frame by region and calculate regional shares as sum(revenue_usd)/tot_appar * 100. The first part calculates total sales for every region (variable used to group). We store these regional sales shares in a data frame appar_df_reg. This data frame has 2 columns (the region and the revenue share) and 7 rows, one for each region. There are various ways you can visualize these shares. Here we will use a pie chart. A pie chart is a column or bar chart but with polar coordinates.\n\nggplot(appar_df_reg, aes(x = \"\", y = appar_sh, fill = region)) +\n  geom_col() +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(label = round(appar_sh, 2)),\n            position = position_stack(vjust = 0.5)) +\n  scale_fill_discrete() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe pie chart’s code is somewhat different from the code you saw in previous examples. A pie chart is a circle where the size of the slices show the relative size of a variable. In other words, there is no x-axis. The size of the slices is determined by the share of regional sales. The color of the slices should change with the region. This is the first line in the code. The second line creates a column chart. Let’s look at the column chart as we have it now;\n\nggplot(appar_df_reg, aes(x = \"\", y = appar_sh, fill = region)) +\n  geom_col()\n\n\n\n\n\n\n\n\nAs you can see, the regional shares are shown as stacked blocks. To create a pie chart, the we change the coordinate system intro polar coordinates. The theta = \"y\" tells R to use the y-variable (in this case the regional revenue shares) to fill the circle. The other parts include the label shares in the various slices of the pie. If you replace theme_minimal with theme_void() the plot will loose all the axis and other backgrounds. In this graph, you can read the regional trade shares on the outer circle. Starting at 0, you almost get to 25 if you add Southeast Asia and South Korea. If you further add Japan and India, you have more than 50% of total Apparel sales. Further adding Greater China en Europe, move the total regional trades shares above 75%. Add America and you are back on top with 100%.\nLet’s see what happens if we include an x and use x in the polar coordinates. As the data frame has two variables, region and appr_sh, we can only use `region as the x-variable. This is what the graph would look like in the usual coordinate system.\n\nggplot(appar_df_reg, aes(x = region, y = appar_sh, fill = region)) +\n  geom_col()\n\n\n\n\n\n\n\n\nNow we are going to tell R to use the x-variable (region) to fill the circle:\n\nggplot(appar_df_reg, aes(x = region, y = appar_sh, fill = region)) +\n  geom_col() +\n  coord_polar(theta = \"x\") + \n    geom_text(aes(label = round(appar_sh, 2)),\n            position = position_stack(vjust = 0.5)) +\n  scale_fill_discrete() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, the circle is now divided in equal parts, with one part per region (the x-axis). To measure the revenue share, you need to look at the inner circles. If you now walk around the circle, you first meet America, then Europe, … To see the revenue share, you need to start from the origin. Every circle you pass represents an increase of 5% points in market share. As you can see, America is between the 3rd and 4th circle, Europe between the 2nd and the 3rd, … .\nLet’s look at what would happen if we include region and change the theta to “y”:\n\nggplot(appar_df_reg, aes(x = region, y = appar_sh, fill = region)) +\n  geom_col() +\n  coord_polar(theta = \"y\") + \n    geom_text(aes(label = round(appar_sh, 2)),\n            position = position_stack(vjust = 0.5)) +\n  scale_fill_discrete() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis doesn’t make a lot of sense as you need to read the graph as a race track with start and finish on top. All observations start at the “start”. The closer to the finish, the larger the share. However, this visualisation fails to tell the story. Graphs shouldn’t need a lot of comments to see what they show. Here, without any additional comment, you wouldn’t know how to interpret this graph. Second, note that the inner circle is the smallest, but is also closest to the finish line. In other words, the smallest part represents the largest share. This is not consistent with the story you are telling: you are looking at relative size of regions in terms of total apparel revenue and you would expect that largest color in the graph to represent the largest region.\nYou can add a plot to a powerpoint presentation using e.g. the {officer} package. To do so you need to save the file as an object. Let’s use the pie chart and assign its value to appar_pl_reg.\n\nappar_pl_reg &lt;- ggplot(appar_df_reg, aes(x = \"\", y = appar_sh, fill = region)) +\n  geom_col() +\n  coord_polar(theta = \"y\") +\n  geom_text(aes(label = round(appar_sh, 2)),\n            position = position_stack(vjust = 0.5)) +\n  scale_fill_discrete() +\n  theme_void()\n\nYou can now add this slide to a powerpoint presentation. Suppose that you don’t have the powerpoint yet. The first command in the next block my_pres &lt;- read_pptx() creates a powerpoint presentation called my_pres. In the second line, you add a slide to that presenation. It has the standard layout: a title and content and you use a master called office theme (a blank slide). In the third line, you add the title to the slide. The content, the plot, is added in the fourth line. Using the print command, you save that powerpoint in your project as my_presentation.pptx. If you look at the directory, you’ll see the powerpoint. You can open it to see the file.\n\n# Add to powerpoint\nmy_pres &lt;- read_pptx()\n\nmy_pres &lt;- add_slide(my_pres, \n                     layout = \"Title and Content\", \n                     master = \"Office Theme\")\n\nmy_pres &lt;- ph_with(my_pres, \n                   value = \"Apparel sales per region\", \n                   location = ph_location_type(type = \"title\"))\n\nmy_pres &lt;- ph_with(my_pres, \n                   value = appar_pl_reg, \n                   location = ph_location_type(type = \"body\"))\n\nprint(my_pres, target = here::here(\"reports\", \"my_presentation.pptx\"))\n\nYou can add multiple slides:\n\nmy_pres2 &lt;- read_pptx()\n\nmy_pres2 &lt;- add_slide(my_pres2, \n                     layout = \"Title and Content\", \n                     master = \"Office Theme\")\n\nmy_pres2 &lt;- ph_with(my_pres2, \n                   value = \"Apparel sales per region\", \n                   location = ph_location_type(type = \"title\"))\n\nmy_pres2 &lt;- ph_with(my_pres2, \n                   value = appar_pl_reg, \n                   location = ph_location_type(type = \"body\"))\n\nmy_pres2 &lt;- add_slide(my_pres2, \n                     layout = \"Title and Content\", \n                     master = \"Office Theme\")\n\nmy_pres2 &lt;- ph_with(my_pres2, \n                   value = \"Average price per sub category\", \n                   location = ph_location_type(type = \"title\"))\n\nmy_pres2 &lt;- ph_with(my_pres2, \n                   value = nike_pl_aveprice, \n                   location = ph_location_type(type = \"body\"))\n\nprint(my_pres2, target = here::here(\"reports\", \"my_presentation2.pptx\"))\n\nAs we’ll see in the final chapter Chapter 17, you can add more content to your slides.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#life-expectancy-at-birth",
    "href": "02_Examples.html#life-expectancy-at-birth",
    "title": "2  Examples",
    "section": "2.2 Life expectancy at birth",
    "text": "2.2 Life expectancy at birth\nFor this example, you can continue to use the project you set up for the previous sales example or you can create a new project. Again, you need to include two date folders, one script folder and one reports folder.\nIn the previous example, we used data in a spreadsheet. Not all data are stored in a spreadsheet. Some datasets are avaiable online. You can access them from R using an Application Prorgramming Interface or API. In general, API’s allow two applications to communicate with one another. Here, R will communicate with other servers that store data. Doing so allows you to retrieve data. In this and the next example, that is what we will do: access and import data using an API. We’ll use two sources: the World Bank and Yahoo Finance. In Chapter 6, we’ll see how you can access data from major central banks (e.g. exchange rates or interest rates). Using the World Bank’s life expectancy at birth dataset, the second example will illustrate how you can use animations in your data visualisation and how you can produce maps. API are a very powerful tool. Most of the sources that you can access using an API also allow you to download the data in excel. An API skips this step and reduces the number of mistakes that you can make. Second if you have access to an API, you can download and use the most recent data without the need to go online, look for the webportal, download the excel file and import the excel file in R. All you need to do is rerun your script and you’ll have access to the most recent observations.\nYou need to install and load four packages. If you haven’t done so, you can copy paste the line of code in your console and install {wbstats}, {gganimate}, {rnaturalearth} and {sf}. If you have done so already, you can load them into your session.\n\ninstall.packages(\"wbstats\", \"gganimate\", \"rnaturalearth\", \"sf\")\n\nIf you continue from the previous example without interruption, the packages you installed are loaded. However, if you quit R and you are now re-opening it for this example, you need to load these packages again:\n\n2.2.1 Life expectancy at birth: the data\nThe {wbstats} package allows you to access data from the World Bank through an API connection. We will cover API’s more in depth in Chapter 6. The World Bank collects a lot of data. To access the data, you need to now the exact code of the variables you need. There are multiple ways to finding that code and in Chapter 6 we will discuss a number of alternative. Suppose, for now, that you know the data you need: life expactancy at birth, per capita GDP in constant USD and total population and that you also know the codes. The World Bank uses the following codes to identify the variables you need:\n\nlife expectancy at birth: SP.DYN.LE00.IN\nper capita GDP in constant USD: NY.GDP.PCAP.CD\ntotal population: SP.POP.TOTL\n\nYou can collect the indicators you need in an object my_indicators using:\n\nmy_indicators &lt;- c(\n  life_exp = \"SP.DYN.LE00.IN\", \n  gdp_capita =\"NY.GDP.PCAP.CD\", \n  pop = \"SP.POP.TOTL\"\n)\n\nIf you include a start data and end data, you can use the wb_data() function to import the data from the World Bank’s data portal and assign it to a data frame (in this case life_df).\n\nlife_df &lt;- wb_data(my_indicators, start_date = 1960, end_date = 2022)\n\nYou can save this dataset as an Rdata file in your raw data folder:\n\nsave(life_df, file = here::here(\"data\", \"raw\", \"life_df.Rdata\"))\n\nAn Rdata file is a very good option if you are the only one using the data or if all others who cooperate on a project use R. If this is not the case, you can save the file as an csv file:\n\nwrite_csv(life_df, here::here(\"data\", \"raw\", \"mtcars.csv\"))\n\nIf you need to use your data, you can use the load function to open the Rdata file or the read_csv() function you used in the previous examle to open the csv vile.\nIn the environment pane, you can see that this dataset has 13.671 observations for 7 variables. Why 7 variables and not 3 as you asked? Let’s check the structure of the dataset\n\nstr(life_df)\n\ntibble [13,671 × 7] (S3: tbl_df/tbl/data.frame)\n $ iso2c     : chr [1:13671] \"AW\" \"AW\" \"AW\" \"AW\" ...\n $ iso3c     : chr [1:13671] \"ABW\" \"ABW\" \"ABW\" \"ABW\" ...\n $ country   : chr [1:13671] \"Aruba\" \"Aruba\" \"Aruba\" \"Aruba\" ...\n $ date      : num [1:13671] 1960 1961 1962 1963 1964 ...\n $ gdp_capita: num [1:13671] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"label\")= chr \"GDP per capita (current US$)\"\n $ life_exp  : num [1:13671] 64.2 64.5 64.8 65.1 65.3 ...\n  ..- attr(*, \"label\")= chr \"Life expectancy at birth, total (years)\"\n $ pop       : num [1:13671] 54922 55578 56320 57002 57619 ...\n  ..- attr(*, \"label\")= chr \"Population, total\"\n\n\nAs you can see, in addition to the variables we want, the World Bank also includes an ISO country code with 2 letters (alpha 2 code e.g. AW, FR, BE), and ISO code with 3 letters (alpha 3 code e.g. ABW, FRA, BEL) and the name of the country (Aruba). The date column shows the years. ISO codes are unique country codes set by the International Organisation for Standardisation. These codes are widely used throughout the world to identify countries.\nYou can check the individual countries included in the dataset if you use the unique() function:\n\nunique(life_df$country)\n\n  [1] \"Aruba\"                          \"Afghanistan\"                   \n  [3] \"Angola\"                         \"Albania\"                       \n  [5] \"Andorra\"                        \"United Arab Emirates\"          \n  [7] \"Argentina\"                      \"Armenia\"                       \n  [9] \"American Samoa\"                 \"Antigua and Barbuda\"           \n [11] \"Australia\"                      \"Austria\"                       \n [13] \"Azerbaijan\"                     \"Burundi\"                       \n [15] \"Belgium\"                        \"Benin\"                         \n [17] \"Burkina Faso\"                   \"Bangladesh\"                    \n [19] \"Bulgaria\"                       \"Bahrain\"                       \n [21] \"Bahamas, The\"                   \"Bosnia and Herzegovina\"        \n [23] \"Belarus\"                        \"Belize\"                        \n [25] \"Bermuda\"                        \"Bolivia\"                       \n [27] \"Brazil\"                         \"Barbados\"                      \n [29] \"Brunei Darussalam\"              \"Bhutan\"                        \n [31] \"Botswana\"                       \"Central African Republic\"      \n [33] \"Canada\"                         \"Switzerland\"                   \n [35] \"Channel Islands\"                \"Chile\"                         \n [37] \"China\"                          \"Cote d'Ivoire\"                 \n [39] \"Cameroon\"                       \"Congo, Dem. Rep.\"              \n [41] \"Congo, Rep.\"                    \"Colombia\"                      \n [43] \"Comoros\"                        \"Cabo Verde\"                    \n [45] \"Costa Rica\"                     \"Cuba\"                          \n [47] \"Curacao\"                        \"Cayman Islands\"                \n [49] \"Cyprus\"                         \"Czechia\"                       \n [51] \"Germany\"                        \"Djibouti\"                      \n [53] \"Dominica\"                       \"Denmark\"                       \n [55] \"Dominican Republic\"             \"Algeria\"                       \n [57] \"Ecuador\"                        \"Egypt, Arab Rep.\"              \n [59] \"Eritrea\"                        \"Spain\"                         \n [61] \"Estonia\"                        \"Ethiopia\"                      \n [63] \"Finland\"                        \"Fiji\"                          \n [65] \"France\"                         \"Faroe Islands\"                 \n [67] \"Micronesia, Fed. Sts.\"          \"Gabon\"                         \n [69] \"United Kingdom\"                 \"Georgia\"                       \n [71] \"Ghana\"                          \"Gibraltar\"                     \n [73] \"Guinea\"                         \"Gambia, The\"                   \n [75] \"Guinea-Bissau\"                  \"Equatorial Guinea\"             \n [77] \"Greece\"                         \"Grenada\"                       \n [79] \"Greenland\"                      \"Guatemala\"                     \n [81] \"Guam\"                           \"Guyana\"                        \n [83] \"Hong Kong SAR, China\"           \"Honduras\"                      \n [85] \"Croatia\"                        \"Haiti\"                         \n [87] \"Hungary\"                        \"Indonesia\"                     \n [89] \"Isle of Man\"                    \"India\"                         \n [91] \"Ireland\"                        \"Iran, Islamic Rep.\"            \n [93] \"Iraq\"                           \"Iceland\"                       \n [95] \"Israel\"                         \"Italy\"                         \n [97] \"Jamaica\"                        \"Jordan\"                        \n [99] \"Japan\"                          \"Kazakhstan\"                    \n[101] \"Kenya\"                          \"Kyrgyz Republic\"               \n[103] \"Cambodia\"                       \"Kiribati\"                      \n[105] \"St. Kitts and Nevis\"            \"Korea, Rep.\"                   \n[107] \"Kuwait\"                         \"Lao PDR\"                       \n[109] \"Lebanon\"                        \"Liberia\"                       \n[111] \"Libya\"                          \"St. Lucia\"                     \n[113] \"Liechtenstein\"                  \"Sri Lanka\"                     \n[115] \"Lesotho\"                        \"Lithuania\"                     \n[117] \"Luxembourg\"                     \"Latvia\"                        \n[119] \"Macao SAR, China\"               \"St. Martin (French part)\"      \n[121] \"Morocco\"                        \"Monaco\"                        \n[123] \"Moldova\"                        \"Madagascar\"                    \n[125] \"Maldives\"                       \"Mexico\"                        \n[127] \"Marshall Islands\"               \"North Macedonia\"               \n[129] \"Mali\"                           \"Malta\"                         \n[131] \"Myanmar\"                        \"Montenegro\"                    \n[133] \"Mongolia\"                       \"Northern Mariana Islands\"      \n[135] \"Mozambique\"                     \"Mauritania\"                    \n[137] \"Mauritius\"                      \"Malawi\"                        \n[139] \"Malaysia\"                       \"Namibia\"                       \n[141] \"New Caledonia\"                  \"Niger\"                         \n[143] \"Nigeria\"                        \"Nicaragua\"                     \n[145] \"Netherlands\"                    \"Norway\"                        \n[147] \"Nepal\"                          \"Nauru\"                         \n[149] \"New Zealand\"                    \"Oman\"                          \n[151] \"Pakistan\"                       \"Panama\"                        \n[153] \"Peru\"                           \"Philippines\"                   \n[155] \"Palau\"                          \"Papua New Guinea\"              \n[157] \"Poland\"                         \"Puerto Rico\"                   \n[159] \"Korea, Dem. People's Rep.\"      \"Portugal\"                      \n[161] \"Paraguay\"                       \"West Bank and Gaza\"            \n[163] \"French Polynesia\"               \"Qatar\"                         \n[165] \"Romania\"                        \"Russian Federation\"            \n[167] \"Rwanda\"                         \"Saudi Arabia\"                  \n[169] \"Sudan\"                          \"Senegal\"                       \n[171] \"Singapore\"                      \"Solomon Islands\"               \n[173] \"Sierra Leone\"                   \"El Salvador\"                   \n[175] \"San Marino\"                     \"Somalia\"                       \n[177] \"Serbia\"                         \"South Sudan\"                   \n[179] \"Sao Tome and Principe\"          \"Suriname\"                      \n[181] \"Slovak Republic\"                \"Slovenia\"                      \n[183] \"Sweden\"                         \"Eswatini\"                      \n[185] \"Sint Maarten (Dutch part)\"      \"Seychelles\"                    \n[187] \"Syrian Arab Republic\"           \"Turks and Caicos Islands\"      \n[189] \"Chad\"                           \"Togo\"                          \n[191] \"Thailand\"                       \"Tajikistan\"                    \n[193] \"Turkmenistan\"                   \"Timor-Leste\"                   \n[195] \"Tonga\"                          \"Trinidad and Tobago\"           \n[197] \"Tunisia\"                        \"Turkiye\"                       \n[199] \"Tuvalu\"                         \"Tanzania\"                      \n[201] \"Uganda\"                         \"Ukraine\"                       \n[203] \"Uruguay\"                        \"United States\"                 \n[205] \"Uzbekistan\"                     \"St. Vincent and the Grenadines\"\n[207] \"Venezuela, RB\"                  \"British Virgin Islands\"        \n[209] \"Virgin Islands (U.S.)\"          \"Viet Nam\"                      \n[211] \"Vanuatu\"                        \"Samoa\"                         \n[213] \"Kosovo\"                         \"Yemen, Rep.\"                   \n[215] \"South Africa\"                   \"Zambia\"                        \n[217] \"Zimbabwe\"                      \n\n\nThere are missing values in the dataset. Recall that you can count the number of missing values using the is.na() function and then sum across observations:\n\nsum(is.na(life_df))\n\n[1] 3106\n\n\nTo see the number of missing observations per variable, we can use the summarise() function:\n\nlife_df_na &lt;- life_df |&gt; \n  summarise(na_life = sum(is.na(life_exp)), \n            na_gdp = sum(is.na(gdp_capita)), \n            na_pop = sum(is.na(pop))            )\n\nlife_df_na holds the number of missing values per variable in a data frame.\n{wbstats} can be used to add additional information to the dataset. life_df only includes country data, but doesn’t show e.g. the continent or region in the world, it doesn’t show the capital or the location of that capital city. wb_countries() includes this type of information. You can see the information if you run\n\nstr(wb_countries())\n\ntibble [296 × 18] (S3: tbl_df/tbl/data.frame)\n $ iso3c             : chr [1:296] \"ABW\" \"AFE\" \"AFG\" \"AFR\" ...\n $ iso2c             : chr [1:296] \"AW\" \"ZH\" \"AF\" \"A9\" ...\n $ country           : chr [1:296] \"Aruba\" \"Africa Eastern and Southern\" \"Afghanistan\" \"Africa\" ...\n $ capital_city      : chr [1:296] \"Oranjestad\" NA \"Kabul\" NA ...\n $ longitude         : num [1:296] -70 NA 69.2 NA NA ...\n $ latitude          : num [1:296] 12.5 NA 34.5 NA NA ...\n $ region_iso3c      : chr [1:296] \"LCN\" NA \"SAS\" NA ...\n $ region_iso2c      : chr [1:296] \"ZJ\" NA \"8S\" NA ...\n $ region            : chr [1:296] \"Latin America & Caribbean\" \"Aggregates\" \"South Asia\" \"Aggregates\" ...\n $ admin_region_iso3c: chr [1:296] NA NA \"SAS\" NA ...\n $ admin_region_iso2c: chr [1:296] NA NA \"8S\" NA ...\n $ admin_region      : chr [1:296] NA NA \"South Asia\" NA ...\n $ income_level_iso3c: chr [1:296] \"HIC\" NA \"LIC\" NA ...\n $ income_level_iso2c: chr [1:296] \"XD\" NA \"XM\" NA ...\n $ income_level      : chr [1:296] \"High income\" \"Aggregates\" \"Low income\" \"Aggregates\" ...\n $ lending_type_iso3c: chr [1:296] \"LNX\" NA \"IDX\" NA ...\n $ lending_type_iso2c: chr [1:296] \"XX\" NA \"XI\" NA ...\n $ lending_type      : chr [1:296] \"Not classified\" \"Aggregates\" \"IDA\" \"Aggregates\" ...\n\n\nIn addition to the country identifiers, you see that the data includes the name of the capital city and its location (longitude and latitude), information on the region of a country (e.g. Latin America & Caribbean including region ISO codes), the administrative region or the income level and World Bank lending type.\nWe will first create a dataset with a couple of indicators for each country. To do so, we use the select() function from {dplyr} to create a dataset that only includes the columns from wb_countries() that we need: iso3c, capital_city, longitude, latitude, region, income_level:\n\nwbcountry &lt;- wb_countries() |&gt;\n  select(iso3c, capital_city, longitude, latitude, region, income_level)\n\nWe now have two datasets: one including the data and one including the country characteristics. To join both, we use the left_join() function from {dplyr}. left_join(df1, df2, by = \"var1\") joins two datasets using a common variable var1. In other words, it looks for that variable in both df1 and df2 and joins the two datasets if a value in the var1 row matches that same value in both df1 and df2. Using left_join(), R keeps all the observation in the dataset on the left (df1). In our case, we have a common country identifier: iso3c. This variable is included in both datasets. We want to add the information in wbcountry to the information in life_df. In other words, we want to keep all observations in life_df. In left_join() df1 is our “left” dataset and df2 is our “right” dataset. If you run\n\nlife_df &lt;- left_join(life_df, wbcountry, \"iso3c\")\n\nR will add columns to the life_df dataset which it takes from wbcountry. If it finds the iso3c code for, way Aruba (ABW) in life_df, if will search if that country code is also included in wbcountry. If that is the case, it will add the row in the latter dataset df2 which includes the country code ABW, to the former df1.\nTo see the structure:\n\nstr(life_df)\n\ntibble [13,671 × 12] (S3: tbl_df/tbl/data.frame)\n $ iso2c       : chr [1:13671] \"AW\" \"AW\" \"AW\" \"AW\" ...\n $ iso3c       : chr [1:13671] \"ABW\" \"ABW\" \"ABW\" \"ABW\" ...\n $ country     : chr [1:13671] \"Aruba\" \"Aruba\" \"Aruba\" \"Aruba\" ...\n $ date        : num [1:13671] 1960 1961 1962 1963 1964 ...\n $ gdp_capita  : num [1:13671] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"label\")= chr \"GDP per capita (current US$)\"\n $ life_exp    : num [1:13671] 64.2 64.5 64.8 65.1 65.3 ...\n  ..- attr(*, \"label\")= chr \"Life expectancy at birth, total (years)\"\n $ pop         : num [1:13671] 54922 55578 56320 57002 57619 ...\n  ..- attr(*, \"label\")= chr \"Population, total\"\n $ capital_city: chr [1:13671] \"Oranjestad\" \"Oranjestad\" \"Oranjestad\" \"Oranjestad\" ...\n $ longitude   : num [1:13671] -70 -70 -70 -70 -70 ...\n $ latitude    : num [1:13671] 12.5 12.5 12.5 12.5 12.5 ...\n $ region      : chr [1:13671] \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" \"Latin America & Caribbean\" ...\n $ income_level: chr [1:13671] \"High income\" \"High income\" \"High income\" \"High income\" ...\n\n\nRecall that the pipe operator takes whatever is on its left side and puts it as the first argument on the right side. You could have also merged both datasets using life_df &lt;- life_df |&gt; left_join(wbcountry, \"iso3c\").\nWe will not do a lot of data tyding or cleaning, but there are a couple of things that you could do. For instance, recall that R uses the alphabet to order character variables. For the level of income, the implies an order which is not consistent:\n\nsort(unique(life_df$income_level))\n\n[1] \"High income\"         \"Low income\"          \"Lower middle income\"\n[4] \"Not classified\"      \"Upper middle income\"\n\n\nWe can change the order using\n\nlife_df &lt;- life_df |&gt;\n  mutate(income_level = factor(income_level, levels = c(\"Low income\", \"Lower middle income\", \"Upper middle income\", \"High income\", \"Not classified\")))\n\ndate is a numeric variable. In other words, it will show with decimals. We can change this into an integer (doesn’t have decimals) using\n\nlife_df$date &lt;- as.integer(life_df$date)\n\nIf you want to check the data, you could use e.g. a violin plot. It show the full distribution of a variable. For instance, if you draw a violin plot for life expectancy at birth per income level and the year 2020:\n\nggplot(filter(life_df, date == 2020 & income_level != \"Not classified\"), aes(x = income_level, y = life_exp)) +\n  geom_violin(color = \"grey\", fill = \"lightyellow\") +\n  geom_point(color = \"grey\") + \n  labs(title = \"A violin plot for life expectancy at birth\", \n       subtitle = \"2020\",\n       caption = \"Source: World Bank\",\n       x = \"Income level\",\n       y = \"Life expectancy at birth\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can see the full distribution of life expectancy at birth for each income level in the dataset. As you can see, life expectancy increases with inome. In high income economies, a lot of observations are above 80 years. In upper middle income countries, you see that the same holds for life expectancy between 70 and 75 while in the lower income group, a lot of countries have life expectancy levels around 60. You can also see how stretched the distribution is. Note also that income of not the “only” variable that matters. As you can see, life expectancy at birth is higher in some low income countries than in all other three country groups.\n\n\n2.2.2 Life expectancy at birth: an animated plot\nLet’s now try to show the evolution of life expectancy over time and income. We could use a 3D chart to do so with life expectancy on the z-axis, and gdp per capita and time on the x- an y-axis. However, this wouldn’t produce a nice result: the number of observations is so large that you wouldn’t be able to show the change over time. Another way to show the evoluation over time is to present the data in various plots. Doing so, you can show various plots for different years. Each plot has per capita GDP on the horizontal axis and life expectancy on the vertical axis. Here we have various years. So we can create one plot per year and then show each plot one after the other. Before we we create a plot per year, we’ll first setup the plot for one year. If we are satisfied with the way it looks, we can use this chart as the basis to build an animation. Here, we’ll show that you can change almost everything in a plot if you use {ggplot2}.\nWe’ll use 2000 for the first part and plot a scatterplot with per capita GDP on the horizontal axis and life expectancy at birth on the vertical axis. To do so, we use {ggplot2}. We have to tell {ggplot2} where it can find the data: the dataset is life_df and we only need data for 2000 (filter(life_df, date == 2000) what to plot (x = gdp_capita and y = life_exp) and how to plot is (scatter = geom_point):\n\nggplot(filter(life_df, date == 2000), \n       aes(x = gdp_capita, y = life_exp)) +\n  geom_point()\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nEvery ‘dot’ in the plot represents a country. However, not all countries are equal. As you can see on the horizontal axis, some countries are much richer than others. Second, some countries are much larger in terms of their population than others. This is not shown in the graph. To do so, we can tell {ggplot2} to add that information by changing the size of the dot using the population of a country. We do so by adding size = pop in the aesthetics part:\n\nggplot(filter(life_df, date == 2000), \n       aes(x = gdp_capita, y = life_exp, size = pop)) +\n  geom_point()\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow the size of the dot is larger as the population increases. {ggplot2} adds a legend showing the dot size and population.\nWe can add more data. All dots have the same color. With the data that we have, we could show the location of a country using the regionvariable. To do so, we tell {ggplot2} to show every region with a different color for the country dots. We do so by adding color = region in the aesthetics part:\n\nggplot(filter(life_df, date == 2000), \n       aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThere are a couple of things that we need to change. First, as you can see from the large number of observations on the left of the graph and there are a couple of outliers in the right top panel. In other words, the horizontal axis is quite long because it shows countries that are poor as well as a couple of countries that are very rich (per capita GDP larger than 40.000). We can reduce that “stretch” if we use the natural logarithm of per capita GDP. In that way, the difference between, say 5.000 and 50.000 will not show as 1 in 10 but as 1 in 1.27:\n\nlog(50000)/log(5000)\n\n[1] 1.270346\n\n\nHowever, we want to show non logged dollar values. In other words, we don’t want to add a variable to the dataset log(per capita gdp) and use that variable for our x-asis. If we did so, the x-axis wouldn’t show income in USD, but income in log(USD). We can change the x-axis using the scale_x_continuous() function. If we add transform = \"log\" {ggplot2} transforms the horizontal axis by taking the natural log of per capita GDP but continues to show the non-logged values as labels. We can also tell {ggplot2} which levels it needs to show by adding breaks = c(100, 1000, 10000, 100000). The horizontal axis will show these values for per capita GDP. The horizontal axis also shows dollar amounts. In the graph, we can make that explicit anc include the $ sign. This is what we do on the last line. Using the {scales} package in R we tell {ggplot2} that the horizontal axis is a currency and that has to add $ before the value.\n\nggplot(filter(life_df, date == 2000), \n       aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000), \n    labels = scales::label_currency(prefix = \"$\"))\n\n\n\n\n\n\n\n\nSecond, we can change the size of the dots and increase the size of dots for larger countries. Here we use the scale_size_continuous() function (recall that the size of the dot changes with population and population is a continuous variable). We will change the dot size so that the largest dots are 20 times as large as the smaller dots using range = c(1,20). We can also manually set the upper values for each dot and change the way in which the values are shown in the legend. We do the former by adding breaks = c(50000000, 250000000, 500000000, 750000000, 1000000000). We’ll show the legend in “millions” and add a suffix m” using labels = scales::label_number(scale = 1/1000000, suffix = \"m\").\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  scale_size_continuous(\n    range = c(1, 20),\n    breaks = c(50000000, 250000000, 500000000, 750000000, 1000000000),\n    labels = scales::label_number(scale = 1/1000000, suffix = \"m\"))\n\n\n\n\n\n\n\n\nWe can now change the other parts of the layout and add e.g. titles, labels for the axis and legends. We’ll show the title in bold and size 14 and remove the background from the plot:\n\nggplot(filter(life_df, date == 2000), \n       aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  scale_size_continuous(\n    range = c(1, 20),\n    labels = scales::label_number(scale = 1/1000000, suffix = \"m\"),\n    breaks = c(50000000, 250000000, 500000000, 750000000, 1000000000)\n  ) +\n  labs(\n    title = \"GDP per capita and life expectancy at birth\", \n    x = \"GDP per capita (constant USD, log scale)\", \n    y = \"Life expectancy at birth\", \n    caption = \"Source: World Bank\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"), \n    plot.title = element_text(face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\nIf you are satisfied with the graph, we can start with the animation. We will show the graph for every year. In other words, we’ll create this graph for every year in the data and show each graph one after another in an animation. Do to so, you need to remove the year filter. Second, you need to add two additional lines. The first, transition_time(date) tells R it has create a graph for every year (date in our dataset). The second, ease_aes(\"linear\") sets the speed with which graphs will be shown. Here, they will be shown one after another with equal time between plots. Last, to show the year in the title, we add {frame_time} to the title. If you run this code - which might take some time - you’ll see the end result:\n\nggplot(life_df, \n       aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log10\",\n    labels = scales::label_currency(prefix = \"$\")) +\n  scale_size_continuous(\n    range = c(1, 15),\n    labels = scales::label_number(scale = 1/1000000, suffix = \"m\"),\n    breaks = seq(from = 1e8, to = 1e9, by = 2e8)) +\n  theme(\n    panel.background = element_rect(fill = \"white\"), \n    plot.title = element_text(face = \"bold\", size = 14)\n    ) +\n  labs(\n    title = \"GDP per capita and life expectancy at birth in {frame_time}\", \n    x = \"GDP per capita (constant USD, log scale)\", \n    y = \"Life expectancy at birth\", \n    caption = \"Source: World Bank\") +\n  transition_time(date) + \n  ease_aes(\"linear\")\n\n\n\n\n\n\n\n\nIf you want to include this animation in a powerpoint or website, you can save the plot and render it as a gif. The first line in this code saves the plot. The last 2 lines render the plot and save the animation as a gif in the reports folder in the project directory. You can also save the animation as an mp4.\n\nlife_plot &lt;- ggplot(life_df, aes(x = gdp_capita, y = life_exp, size = pop, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log10\",\n    labels = scales::label_currency(prefix = \"$\")) +\n  scale_size_continuous(\n    range = c(1, 15),\n    labels = scales::label_number(scale = 1/1000000, suffix = \"m\"),\n    breaks = seq(from = 1e8, to = 1e9, by = 2e8)) +\n  theme(\n    panel.background = element_rect(fill = \"white\"), \n    plot.title = element_text(face = \"bold\", size = 14)\n    ) +\n  labs(\n    title = \"GDP per capita and life expectancy at birth in {frame_time}\", \n    x = \"GDP per capita (constant USD, log scale)\", \n    y = \"Life expectancy at birth\", \n    caption = \"Source: World Bank\") +\n  transition_time(date) + \n  ease_aes(\"linear\")\n\nanimate(life_plot,\n        renderer = gifski_renderer(file = here::here(\"reports\", \"avg_life_exp_cont_year.gif\"), loop = FALSE))\n\n\n\n\n\n\n\n\nYou can now use the animation in e.g. a powerpoint, a website or social media post.\n\n\n2.2.3 Life expectancy at birth: a map\nWe will now use life_dfto show life expectancy at birth using a map of the world. This dataset includes data on life expectancy, but doesn’t include data that would allow to draw a map. The {rnaturalearth} package includes this type of data. Using its ne_countries() we can create a dataset which allows to draw a map of all countries on the planet:\n\nworld = ne_countries(returnclass = \"sf\") |&gt;\n   select(iso_a3)\n\nIn the command, we specify that the returnclass should be “sf” which stand for Simple Features. A simple feature locates and object (or feature) on earth in 2D. Let’s take a closer look at this dataset\n\nView(world)\n\nAs you can see, this data set include a column geometry. If we take a close look at this column for e.g. Belgium\n\nbel &lt;- filter(world, iso_a3 == \"BEL\")\nView(bel)\n\nRecall that the centre of Belgium is longitude 4.47 and latitude 50.50. As you can see, the geometry for Belgium includes the borders, the most western point is longitude 2.51 and the most eastern point is longitude 6.15 while the most southern point is located at latitude 49.52 and the most northern point is 51.47. Using these coordinates, the geometry shows R were Belgium is located on the map and what its borders are.\nWe can now join the dataset we wish to show: life expectancy at birth per country (life_df) with the dataset which includes that location of the various countries on the map (world) using left_join(). Here we add the fact that the country ISO3 code in life_df is included in the iso3c column, while the it is included in the iso_a3 column in the world dataset:\n\nlife_df_sf &lt;- life_df |&gt;\n  left_join(world, by = c(\"iso3c\" = \"iso_a3\"))\n  \nlife_df_sf &lt;- st_as_sf(life_df_sf)\n\nThe last line creates sets the file as a simple features file.\nWe can now create a map. To do so, we will use data for 2000. This is the first part of the ggplot() command: it where it can find the data. The second line creates a map (geom_sf()) and includes the variable we want to show (fill = life_exp). The other lines pick a color from the viridis scale, set the limits for the legend starting at 40 and ending at 80 and add labels.\n\nggplot(filter(life_df_sf, date == 2000)) +\n geom_sf(aes(fill = life_exp)) +\n scale_fill_viridis_c(limits = c(40, 80),option = \"plasma\") +\n labs(\n   title = \"GDP per capita and life expectancy at birth in 2000\",\n   caption = \"Source: World Bank\",\n   fill = \"Life expectancy at birth\")\n\n\n\n\n\n\n\n\nIn this map, you can see that the ‘x-asis’ as the longitude and the ‘y-axis’ as the latitude. If you want to remove those, you can add theme_void(): it removes all axis and backgrounds:\n\nggplot(filter(life_df_sf, date == 2000)) +\n  geom_sf(aes(fill = life_exp)) +\n  scale_fill_viridis_c(limits = c(40, 80), option = \"plasma\") +\n  labs(\n    title = \"GDP per capita and life expectancy at birth in 2000\",\n    caption = \"Source: World Bank\",\n    fill = \"Life expectancy at birth\") +\n  theme_void()\n\n\n\n\n\n\n\n\nAs in the previous example, you can save this map, add it to a powerpoint, … . 1",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#using-specific-packages-tidyfinance",
    "href": "02_Examples.html#using-specific-packages-tidyfinance",
    "title": "2  Examples",
    "section": "2.3 Using specific packages: tidyfinance",
    "text": "2.3 Using specific packages: tidyfinance\nFor most applications, you can find packages that were written specifically with that application in mind. {tidyfinance} package (Scheuch, Voigt, and Weiss (2023)). Others in this field include {tidyquant} Dancho and Vaughan (2025) or {quantmod} Ryan and Ulrich (2025). These packages allow you to download data and analyse portfolio’s or calculate widely used models in finance. We will leave those applications for other courses. Here, I’ll use {tidyfinance} and show how these packages can help you. {tidyfinance} is, as the name suggests, integrated in the {tidyverse} approach in R (see Chapter 5).\nThe packages listed here are all related to finance. If you are in a different field, you’ll have to look for and install the packages that were developed for those fields. Given the large number of packages on CRAN, it would be very surprising if you don’t find what you need.\n\n2.3.1 Importing stock market data\n{tidyfinance} allows you to download data on e.g. stocks, indices and other financial variables from Yahoo Finance. In this example, we’ll upload data for Alphabet, Apple, Meta, Microsoft and Nvidia. These stocks are listed under the ticker symbols GOOG, AAPL, META, MSFT and NVDA. These symbols are used to identify these stocks.\nIf you haven’t already done so, you need to install the {tidyverse} package:\n\ninstall.packages(\"tidyfinance\", \"httr2\")\n\nFor this example, you need to load that package\n\nlibrary(tidyfinance)\n\nWe first set a vector with the ticker symbols we need to download:\n\ntickers &lt;- c(\"AAPL\", \"GOOG\", \"META\", \"MSFT\", \"NVDA\")\n\nUsing the download_data()function from the {tidyfinance} package, you can download the data from Yahoo Finance. You need to include the type of data (in this case “stock prices”), the ticker symbols and a start and end data. Here, the start and end data are given, but if you need to access the data including the most recent observation, you can use Sys.Date() (without quotation marks) for the end_date. If you need the last x observations (e.g. last 500 of last 250), you can use Sys.Date() - 500 in for start_date.\n\nstart &lt;- \"2000-01-01\"\nend &lt;- \"2024-12-31\"\nport &lt;- download_data(\n  type = \"stock_prices\",\n  symbols = tickers,\n  start_date = start,\n  end_date = end\n)\n\nLet’s check the data:\n\nhead(port)\n\n# A tibble: 6 × 8\n  symbol date          volume  open   low  high close adjusted_close\n  &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1 AAPL   2000-01-03 535796800 0.936 0.908 1.00  0.999          0.842\n2 AAPL   2000-01-04 512377600 0.967 0.903 0.988 0.915          0.771\n3 AAPL   2000-01-05 778321600 0.926 0.920 0.987 0.929          0.782\n4 AAPL   2000-01-06 767972800 0.948 0.848 0.955 0.848          0.715\n5 AAPL   2000-01-07 460734400 0.862 0.853 0.902 0.888          0.749\n6 AAPL   2000-01-10 505064000 0.911 0.846 0.913 0.873          0.735\n\n\nAs you can see, the data includes the symbol, a date, the volume and the open, low, high and closing price as well as the adjusted close. The adjusted close corrects the close for e.g. stock splits. All variables have the expected type: character for symbol, date for date and numeric for all other variables. The name of the company is not included.\n\n\n2.3.2 Summary statistics for stock market data\nUsing the adjusted close, we can calculate the (daily) return of each stock as \\(p_t/p_{t-1} - 1\\). To do so, we need to make sure that we group all stock per symbol and that the prices are arranged per date in ascending order (i.e. oldest observation first). This is what we do in the first two lines of the following code:\n\nport &lt;- port |&gt;\n  group_by(symbol) |&gt;\n  arrange(date) |&gt;\n  mutate(ret_daily = adjusted_close / lag(adjusted_close) - 1)\n\nThe last line creates a new variable, ret using the mutate() function. The lag() function allows to compute \\(p_{t-1}\\). There, we have daily prices. The ret_daily variable includes daily returns. We can summarise these daily returns for the full period or per year. In the former case, the outcome shows summary statistics for each stock for the entire time period, the latter shows them grouped per month and per stock. To calculate the mean, standard deviation (or volatility), the maximum or minimum, you can use\n\ndaily_returns &lt;- port |&gt;\n  group_by(symbol) |&gt;\n  summarise(\n    daily_mean = mean(ret_daily, na.rm = TRUE),\n    daily_vol = sd(ret_daily, na.rm = TRUE),\n    daily_max = max(ret_daily, na.rm = TRUE), \n    daily_min = min(ret_daily, na.rm = TRUE)\n  )\n\nBecause the first observations can not have its lag, there is at least one missing observations even if the stock price is known for the full period. In addition, not all stocks where listed in early 2000. Because these also generate missing values, we have to include na.rm = TRUE.\nTo see this statistics, you can use {gt}:\n\ndaily_returns |&gt; gt() |&gt;\n  tab_header(\n    title = \"Summary of daily stock return data\",\n    subtitle = glue::glue(\"{start} (or earliest listing) to {end}\")\n  ) |&gt;\n  fmt_percent(\n    columns = starts_with(\"daily\"), \n    decimals = 2,\n  ) |&gt;\n  cols_label(\n    daily_mean = \"Mean\",\n    daily_vol = \"Volatility\", \n    daily_max = \"Maximum\",\n    daily_min = \"Minimum\"\n  )\n\n\n\n\n\n\n\nSummary of daily stock return data\n\n\n2000-01-01 (or earliest listing) to 2024-12-31\n\n\nsymbol\nMean\nVolatility\nMaximum\nMinimum\n\n\n\n\nAAPL\n0.12%\n2.44%\n13.90%\n−51.87%\n\n\nGOOG\n0.10%\n1.93%\n19.99%\n−11.61%\n\n\nMETA\n0.12%\n2.51%\n29.61%\n−26.39%\n\n\nMSFT\n0.06%\n1.90%\n19.57%\n−15.60%\n\n\nNVDA\n0.19%\n3.74%\n42.41%\n−35.23%\n\n\n\n\n\n\n\nThis table illustrates the use of {glue}: a package that you can use to glue text parts together. As you can see using {start} and {end} in this command, allows to automate parts of the title of the table. Here we further use the fact that we want to include all columns that start with ‘daily’ to be shown in percentages with 2 decimals. The last part also changes the table headings.\nThe previous table shows the average data for the full period. You can also calculate per year averages. To do so, you need to add an additional grouping variable: date:\n\ndaily_returns_per_year &lt;- port |&gt;\n  group_by(symbol, year = year(date)) |&gt;\n  summarise(\n    daily_mean = mean(ret_daily, na.rm = TRUE), \n    daily_vol = sd(ret_daily, na.rm = TRUE), \n    max_ret = max(ret_daily, na.rm = TRUE), \n    min_ret = min(ret_daily, na.rm = TRUE))\n\n`summarise()` has grouped output by 'symbol'. You can override using the\n`.groups` argument.\n\ndaily_returns_per_year$year &lt;- as.integer(daily_returns_per_year$year)\n\nThe last line sets the year variable as an integer (without decimals).\nWe can visualise the yearly return data using e.g. a boxplot:\n\nggplot(daily_returns_per_year, \n       aes(x = symbol, y = daily_mean)) +\n  geom_boxplot(color = \"lightgrey\") +\n  geom_point(aes(y = daily_mean, color = year), size = 2.5) +\n  scale_color_viridis_c(option = \"turbo\") + \n  scale_y_continuous(\n    breaks = seq(-0.0060, 0.1, by = 0.001), \n    labels = scales::label_percent(accuracy = 0.01, suffix = \"%\")\n    ) +\n  labs(\n    title = \"Yearly returns\", \n    subtitle = glue::glue(\"{start} (or earliest listing) to {end}\"),\n    x = \" \", \n    y = \"Average daily return\"\n  ) +\n  theme_light()\n\n\n\n\n\n\n\n\nThe box in plot show the first (bottom) and third (top) quartile. The line in the middle represents the median (i.e. the second quartile). The difference between the third and first quartile is called the interquartile range. The whiskers are the lines on both ends of the box. The equal the first quartile minus 1.5 times the interquartile range (bottom-) and the third quartile plus 1.5 times the interquartile range. Observations under or above the extreme whisker points are outliers.\nSuppose your are interested in month on month percentage changes where you want to calculate the percentage difference between the last day of the previous month and the last day of the current month. You can use the date variable to calculate the last day of the month. To do that you use ceiling_date(date, \"month\"). This function calculates the first day of the following month in the date variable\n\ndate &lt;- \"2024-06-1\"\ndate &lt;- as.Date(date)\nceiling_date(date, \"month\")\n\n[1] \"2024-07-01\"\n\n\nSubtracting 1 show the last day of the month\n\ndate &lt;- \"2024-06-1\"\ndate &lt;- as.Date(date)\nceiling_date(date, \"month\") - 1\n\n[1] \"2024-06-30\"\n\n\nThe second line uses this function to create a variable month which equals the last day of the month for each observation. If we then filter all observations using the condition that the date = month we only keep observations for the last day of the month. After grouping by symbol, the code calculate the monthly return:\n\nport_mon &lt;- port |&gt;\n  arrange(date) |&gt;\n  mutate(month = ceiling_date(date, \"month\")-1) |&gt;\n  filter(date == month) |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret_monthly = adjusted_close / lag(adjusted_close) - 1)\n\nYou can now use port_mon to calculate similar statistics as for the daily return data set, e.g. mean statistics for the monthly return portfolio\n\nmonthly_returns &lt;- port_mon |&gt;\n  group_by(symbol) |&gt;\n  summarise(\n    monthly_mean = mean(ret_monthly, na.rm = TRUE),\n    monthly_vol = sd(ret_monthly, na.rm = TRUE),\n    monthly_max = max(ret_monthly, na.rm = TRUE), \n    monthly_min = min(ret_monthly, na.rm = TRUE))\n\nor averages per year\n\nmonthly_returns_per_year &lt;- port_mon |&gt;\n  group_by(symbol, year = year(date)) |&gt;\n  summarise(\n    monthly_mean = mean(ret_monthly, na.rm = TRUE),\n    monthly_vol = sd(ret_monthly, na.rm = TRUE),\n    monthly_max = max(ret_monthly, na.rm = TRUE), \n    monthly_min = min(ret_monthly, na.rm = TRUE))\n\n`summarise()` has grouped output by 'symbol'. You can override using the\n`.groups` argument.\n\n\n\n\n2.3.3 Creating portfolio’s\nSuppose that you want to simulate the performance of different portfolio’s which include these 5 stocks. For this example, we will only use the adjusted close. We use the select()function to select the date, symbol and adjusted_close variables. If you look at the port dataset, you can see it is a long format. However, since we’ll be using matrix operations, we would prefer a dataset in wide format (i.e. where each stock is its own variable. Because we can to include all stocks in the simulation, we have to drop all observations where one or more stocks have missing values. These three steps are includes in the next lines of code:\n\nport_wide &lt;- port |&gt; select(\"date\", \"symbol\", \"adjusted_close\")\n\nport_wide &lt;- port_wide |&gt; pivot_wider(names_from = symbol, \n                                      values_from = adjusted_close)\n\nport_wide &lt;- port_wide |&gt; drop_na()\n\nSay want to to simulate 100 portfolio’s with these 5 stocks. We can generate the weight of each portfolio using a random number generator. Here we use\n\nset.seed(1000)\nport_share &lt;- matrix(runif(500, min = 0, max = 1), \n                     nrow = 100, \n                     ncol = 5, \n                     byrow = TRUE)\n\nwhere set.seed()is included to make sure that each time this code run, it will generate the same random variables. The runif(500, min = 0, max = 1) creates 500 (5 times 100) random draws from a uniform distribution. The matrix(..., nrow = 100, ncol = 5 and byrow = TRUE) command fills a matrix with these random draws. The matrix has 100 rows, 5 columns and these are filled by row. In other words, the first 5 random draw are used to fill the first row first. If byrow = FALSE the matrix is filled by column.\nThe sum of the portfolio shares is equal to 1. However, this is not the case if we use random draws from a uniform distribution: the row sums could be equal to 0 (in case all 5 draws are 0) but also 5 (in case all 5 draws are 1). The force the rowsum to equal 1, we divide each observations by its rowsum:\n\nrow_sums &lt;- rowSums(port_share)\nfor (i in 1:100) {\n  for (j in 1:5){\n    port_share[i,j] &lt;- port_share[i,j] / row_sums[i]\n  }\n}\n\nIn this loop: for each row in the matrix (i in 1:100) starts in the first column (j in 1:5) and divides that element by the rowsum for the ith row. The loop then proceeds to the second column of the ith row and performs the same calculation. After this loop, all rowsums are equal to 1:\n\nrow_sums &lt;- rowSums(port_share)\n\nWe can use this matrix as our matrix with portfolio shares.\nThe stocks are in port_wide and we would like to use them for generate 100 portfolio’s. To do so, we first select all stocks prices. Because we would like to perform matrix operations, we change their type from a data frame into a matrix:\n\nports &lt;- port_wide |&gt; select(AAPL:META)\nports &lt;- as.matrix(ports)\n\nTo simulate the 100 portfolio’s, we use matrix calculations and multiply the ports matrix (3174 x 5) with the transpose of the share matrix (100 x 5), the outcome of a 3174 x 100 matrix where each column represents one portfolio and the rows show all daily values. Matrix multiplication in R is written as %*% and the transpose is t():\n\nport_mat &lt;- ports %*% t(port_share)\n\nThe names of each column in port_mat equals v1, v2, … v100. We can change these names into port_1, port_2, … port_100 using:\n\ncolnames(port_mat) &lt;- paste0(\"port_\", seq(1:100))\n\ncolnames() tells R we can to change the column names. paste0() add port_ to 1, 2, 3, . This last part is written as seq(1:100): a sequence of 100 starting at 1 and ending at 100 in steps of 1.\nThe portfolio’s are now done. To analyse them or visualise them, we need to add the date and recreate a date frame:\n\nportfolios_wide &lt;- cbind(port_wide$date, as_tibble(port_mat))\n\nportfolios_wide &lt;- portfolios_wide %&gt;% rename_at(\"port_wide$date\", ~\"year\")\n\nIn the last line, we change the name of the date column in “year”. To use the data frame in e.g. ggplot we need to recreate the long format (with all portfolio’s in 1 column). To do so, we can use:\n\nportfolios_long &lt;- portfolios_wide |&gt; \n  pivot_longer(!year, names_to = \"port\", values_to = \"value\")\n\nWe now have a data frame that we can use in e.g. ggplot():\n\nggplot(portfolios_long, aes(x = year, y = value, color = port)) +\n  geom_line() +\n  theme(\n    panel.background = element_rect(fill = 'transparent'),\n    plot.background = element_rect(fill = 'transparent', color=NA),\n    legend.position = \"none\") +\n  labs(\n    title = \"Value of 100 portfolio's\", \n    x = \" \",\n    y = \"Value\"\n  )\n\n\n\n\n\n\n\n\nor where we can use summary return statistics such as averages:\n\nportfolios_long &lt;- portfolios_long |&gt;\n  group_by(port) |&gt;\n  arrange(year) |&gt;\n  mutate(ret_daily = value / lag(value) - 1) |&gt;\n  ungroup()\n\nor total returns from the first day to the last day:\n\ntot_return &lt;- portfolios_long |&gt;\n  group_by(port) |&gt;\n  arrange(year) |&gt;\n  filter(year == max(year) | year == min(year)) |&gt;\n  mutate(tot_ret = (log(value) - lag(log(value)))) |&gt;\n  filter(year == max(year)) |&gt;\n  select(year, port, value, tot_ret) \n\n\n\n\n\n\n\nDancho, Matt, and Davis Vaughan. 2025. Tidyquant: Tidy Quantitative Financial Analysis. https://business-science.github.io/tidyquant/.\n\n\nRyan, Jeffrey A., and Joshua M. Ulrich. 2025. Quantmod: Quantitative Financial Modelling Framework. https://github.com/joshuaulrich/quantmod.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with r. 1st ed. Chapman; Hall/CRC. https://doi.org/https://doi.org/10.1201/b23237.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "03_Data_types_and_structures.html",
    "href": "03_Data_types_and_structures.html",
    "title": "3  Data types and structures",
    "section": "",
    "text": "3.1 Data types and classes\nR has 6 basic data types: numeric or double, integer, character, logical, data/time and complex and raw. We’ll focus on the first 5.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types and structures</span>"
    ]
  },
  {
    "objectID": "03_Data_types_and_structures.html#data-types-and-classes",
    "href": "03_Data_types_and_structures.html#data-types-and-classes",
    "title": "3  Data types and structures",
    "section": "",
    "text": "3.1.1 Numeric\n\n3.1.1.0.1 Definition\nA numeric or double precision variable stores numbers. A computer only knows 0 or 1 or “on” of “off”. A 1 or a 0 are know as a bit. The double precision refers to the number of bits used to store the value in your computer’s memory. Double precision or floating point (FP64, float64) occupy 64 bits (or 8 bytes) in your computer’s memory. Note that this number is limited. In other words, is doesn’t allow your computer to store all numbers with the highest precision but is accurate up to 15 to 17 decimal places. This last point will be important when we introduce boolean operators and compare if two values are equal.\nLet’s define two double precision numeric variables:\n\na &lt;- 10.25\nb &lt;- -25\n\nRecall that R uses a . for decimals and not a comma. Negative numbers are shown with a -.\nYou can check the type of the variable is you use typeof():\n\ntypeof(a)\n\n[1] \"double\"\n\n\nAs you can see, a is stored as a double. The class of a variable refers to what kind of object it is. In R, the class of a double is numeric You can check the class of a variable using class():\n\nclass(a)\n\n[1] \"numeric\"\n\n\nRecall that in ?sec-examples you selected columns from nike_df using the is.numeric() function. This function allows you to check if the type of a variable is numeric. The function returns TRUE if a variable is numeric and FALSE if this is not the case. You can also check if a variable is double using is.double():\n\nis.numeric(a)\n\n[1] TRUE\n\nis.double(a)\n\n[1] TRUE\n\n\nSuppose we have a number but that number is shown as a character variable. You can recognize character variables because they are usually shown within quotation marks \" \". You can force these variables into numeric variables using as.numeric() or into double precision using as.double(). Let’s define a number as a character variable:\n\na &lt;- \"12.25\"\na\n\n[1] \"12.25\"\n\n\nAs you can see, a is shown as “12.25” i.e. a character variable. You can change the type using\n\na &lt;- as.numeric(a)\na\n\n[1] 12.25\n\n\nAs you can see, the quotation marks are gone. In other words, a is now numeric (double precision):\n\ntypeof(a)\n\n[1] \"double\"\n\n\nIf you use the as.double() function, the outcome would be the same.\n\n\n3.1.1.0.2 Mathematical operators and functions\nMost commonly used mathematical operators and functions are build into base R. To illustrate, let’s define two values:\n\na &lt;- 50\nb &lt;- 20\n\n\naddition:\n\n\na + b\n\n[1] 70\n\n\n\nsubtraction:\n\n\na - b\n\n[1] 30\n\n\n\nmultiplication:\n\n\na * b\n\n[1] 1000\n\n\n\ndivision:\n\n\na / b \n\n[1] 2.5\n\n\n\ninteger division (the quotient part of a division without the fractional part):\n\n\na %/% b\n\n[1] 2\n\n\n\nmodulus (the fractional part of a division)\n\n\na %% b\n\n[1] 10\n\n\nNote that the result of the integer division * b + modulus = a:\n\nc &lt;- a %/% b\nd &lt;- a %% b\nc * b + d\n\n[1] 50\n\n\nThe modulus is often used to check if a number of even. If the modulus from division by 2 is zero, the number of even, else it is uneven:\n\n31 %% 2\n\n[1] 1\n\n30 %% 2\n\n[1] 0\n\n\nFor these operators, the usual order applies:\n\nmultiplication (division) before addition (subtraction)\nparenthetical subexpressions are evaluated first\nexponentiation before multiplication\n\nA couple of examples of often used mathematical function\n\nabsolute value\n\n\nb &lt;- -50\nabs(b)\n\n[1] 50\n\n\n\nlogarithm base e (natural logarithm)\n\n\nlog(a)\n\n[1] 3.912023\n\n\n\nlogarithm base 10\n\n\nlog10(a)\n\n[1] 1.69897\n\nlog(a, base = 10)\n\n[1] 1.69897\n\n\n\nsquare root\n\n\nsqrt(a)\n\n[1] 7.071068\n\n\n\npower, e.g. 2\n\n\na^2\n\n[1] 2500\n\n\n\nexponent (e to the power n (e.g. n = 10)\n\n\nexp(10)\n\n[1] 22026.47\n\n\nRecall that log(exp(n)) = n:\n\nlog(exp(10))\n\n[1] 10\n\n\n\n\n3.1.1.0.3 Statistical functions\nR also includes a number of statistical probability functions for the normal, t, Chi-square, F, binomial, Poisson, uniform, … distribution. For the normal distribution, these include pnorm(), dnorm(), qnorm() and rnorm() . Similarly, for the t-distribution, these functions include pt(), dt(), qt() and rt(); for the uniform distribution punif(), dunif(), qunif() and runif(); pf(), df(), qf() and rf() for the F-distribution or pchisq(), dchisq(), qchisq() and rchisq()for the Chi-square distribution. You can find a good explanation for these functions in Sean Finn’s Visual guide to pnorm, dnorm, qnorm, and rnorm function in R and Visual guide to uniform distribution functions in R (punif, dunif, qunif, and runif). The equilvant functions for the other distributions have an analogous interpretation. Let’s see what these functions do. Here, we will focus on the normal distribution, but you can extend the interpretation to other distributions. However, note that a distribution has parameters: the mean and standard deviation for the normal distribution, the degrees of freedom for the t-distribution of Chi-Square distribution. For the normal distribution, the parameters are the mean and the standard deviation.\nSuppose you have a variable with a normal distribution with mean 0 and standard deviation equal to 1; the probability that this value is smaller then or equal to q is given by:\npnrom(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nThe first three arguments in this function are self explanatory. The fourth lower.tail = TRUE means that you want to calculate the probability that the value is smaller than or equal to q. In other words, you want to calculate the area under the density function left of q. Changing this into lower.tail = FALSE show the probability that a value will be larger than q. In other words, it shows the area under the density curve right of q. If we would set log.p = TRUE, the result of this function would the the log of p. Usually, you can disregard this option and leave it equal to the default. In other words, you don’t have to repeat it in the function call.\nSuppose you have a value x = 1.75 and you know that it comes from a standard normal distribution with. Now you want to know what the probability is that you find a number equal to of less than 1.75:\n\npnorm(q = 1.75, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 0.9599408\n\n\nThe probability that you find a value less than or equal to 1.75 is 0.9599408. What is the probability that you find a value greater than 1.75? There are two ways to find out. First, you use the fact that the probability that a value is greater equals 1 minus the probability that the value is equal to or smaller than:\n\n1 - pnorm(q = 1.75, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n[1] 0.04005916\n\n\nYou can also get this result is you change lower.tail = TRUE into lower.tail = FALSE:\n\npnorm(q = 1.75, mean = 0, sd = 1, lower.tail = FALSE, log.p = FALSE)\n\n[1] 0.04005916\n\n\nTo find the probability for a given value, you can use\ndnorm(x, mean = 0, sd = 1, log = FALSE)\nHere we added the log = FALSE argument for the sake of completeness, but usually the default value is what you need.\nSuppose you have a value x = 1.75 and you know that it comes from a standard normal distribution with. Now you want to know what the probability is that you find a number equal to 1.75. In that case you would use:\n\ndnorm(x = 1.75, mean = 0, sd = 1)\n\n[1] 0.08627732\n\n\nThe probability is 0.08627732.\nqnorm()is about quantiles of percentiles. Suppose you have a score of 1.75 on a test and you know that the test results follow a standard normal distribution. Are you within the top 5% of students? Using qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)you can find out\n\nqnorm(p = 0.95, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 1.644854\n\n\nThe answer: yes you are. 95% of students have a score equal to or smaller then 1.644854. With your 1.75, you are solidly within the top 5%. Note that this function is closely related to the prnorm() function:\n\npnorm(q = 1.6448545, mean = 0, sd = 1)\n\n[1] 0.9500001\n\n\nThe rnorm() functions draws random draws from a normal distribution with mean = 0 and standard deviation = 1):\n\nset.seed(1000)\nrnorm(n = 1, mean = 0, sd = 1)\n\n[1] -0.4457783\n\n\nIn this code block set.seed(1000) ensures that the random draw will always be the same.\nYou can perform similar calculation for the other distributions.\nSuppose that you find a t-value equal to 1.85 and you know that it follows a t-distribution with 15 degrees of freedom. What is the probability that you find a value for this variable larger then 1.85?\n\n\nCode\npt(q = 1.85, df = 15, lower.tail = FALSE)\n\n\n[1] 0.04205604\n\n\nWhat is the 95th percentile of a Chi-square distribution with 12 degrees of freedom?\n\n\nCode\nqchisq(p = 0.95, df = 12, lower.tail = TRUE)\n\n\n[1] 21.02607\n\n\nSuppose that you calculate a test statistics. The test statistic follows a t-distribution with 18 degrees of freedom. Your t-statistics equals 1.89. Is this statistically significantly different from 0 at the 5% level?\n\n\nCode\npt(q = 1.89, df = 24, lower.tail = FALSE) * 2\n\n\n[1] 0.07089497\n\n\nNote that you need to multiply the outcome of pt() with 2. The function gives the area under the curve to the right of 1.89 (lower.tail = FALSE). However, the area to the left of -1.89 is as large of the area to the right of 1.89. So you need to double the value.\nThe probability that you find a t-statistic larger than 1.89 is 0.0708. This is larger than the 5% level of confidence.\n\n\n3.1.1.0.4 Rounding\nIf you want to round numbers, you can use round(x, digits = n) to round to the nearest n decimal places. In case you have .5, the rule “go the closest even digit” applies and is in line with the international ISO standard IEC 60559(although you may want to check your operating system):\n\nr &lt;- 0.5\nround(r)\n\n[1] 0\n\ns &lt;- 1.5\nround(s)\n\n[1] 2\n\nt &lt;- -0.5\nround(t)\n\n[1] 0\n\nu &lt;- -1.5\nround(u)\n\n[1] -2\n\n\nIf you apply this more in general\n\nr &lt;- 4.5\nround(r)\n\n[1] 4\n\ns &lt;- 5.5\nround(s)\n\n[1] 6\n\n\nAs you can see from these examples, R rounds 4.5 to the closest even integer: 4 and rounds 5.5 to the closest even integer 6.\nGiven this result, the other rounding rules are straightforward. For instance, let’s round 12.34567 to 4, 3, 2, 1, 0 and -1 (round before the decimal):\n\nto 4 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 4)\n\n[1] 12.3457\n\n\n\nto 3 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 3)\n\n[1] 12.346\n\n\n\nto 2 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 2)\n\n[1] 12.35\n\n\n\nto 1 digit: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 1)\n\n[1] 12.3\n\n\n\nto 0 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 0)\n\n[1] 12\n\n\n\nto -1 digits= 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = -1)\n\n[1] 10\n\n\nAs an alternative, you can use the floor() function to round to the largest integer (no decimal places), not greater than the value itself:\n\nr &lt;- 12.34567\nfloor(r)\n\n[1] 12\n\n\nor the ceiling()function to round to the smallest integer (no decimal places), not smaller than the value itself\n\nr &lt;- 12.34567\nceiling(r)\n\n[1] 13\n\n\nThe trunc()function is a rounding function that removes the decimal places. In other words, it only returns the integers:\n\nr &lt;- 12.34567\ntrunc(r)\n\n[1] 12\n\n\nNote that if you use trunc() 12.9999 is rounded to 12:\n\ns &lt;- 12.9999\ntrunc(s)\n\n[1] 12\n\n\nsignif(x, digits = n = 6) rounds to the specified number of significant digits (default = 6). For instance:\n\nt &lt;- 123456789.12\nsignif(t, digits = 9)\n\n[1] 123456789\n\nsignif(t, digits = 8)\n\n[1] 123456790\n\nsignif(t, digits = 7)\n\n[1] 123456800\n\nsignif(t, digits = 6)\n\n[1] 123457000\n\nsignif(t, digits = 5)\n\n[1] 123460000\n\nsignif(t, digits = 4)\n\n[1] 123500000\n\nsignif(t, digits = 3)\n\n[1] 1.23e+08\n\nsignif(t, digits = 2)\n\n[1] 1.2e+08\n\nsignif(t, digits = 1)\n\n[1] 1e+08\n\n\nAs you can see, with 9 significant digits, 123,456,789.12 gets rounded to 123,456,789. If you reduce the number of significant digits from 9 to 8, t is rounded to 123,456,790. With 1 significant digit, t is rounded to 100,000,000 of 1 * 10 ^8 (also written in scientific notation: 1e+08).\n\n\n3.1.1.0.5 A note on NaN and Inf\nThere are a couple of special numbers: Infinity and NaN. Let’s start with Infinity.\n\nz &lt;- Inf\ntypeof(z)\n\n[1] \"double\"\n\nclass(z)\n\n[1] \"numeric\"\n\n\nAs you can see, Infinity is a numeric (double precision). In this example, we assigned the value of Infinity to the variable z. You also get Infinity is you divide by 0:\n\na &lt;- 50\nb &lt;- a/0\nb\n\n[1] Inf\n\ntypeof(b)\n\n[1] \"double\"\n\n\nAs you can see from this example, unless you ask for the result or unless you look at the environment pane, R doesn’t warn that b equals Infinity. To check if that is the case, you can use two functions: is.infinite() which returns TRUE is a value if Inf of is.finite() which returns FALSE of a variable is Inf:\n\nis.infinite(b)\n\n[1] TRUE\n\nis.finite(b)\n\n[1] FALSE\n\n\nThe outcome of a calculation with Infinity is always Infinity of NaN\n\nc &lt;- b * 2\nc\n\n[1] Inf\n\nd &lt;- b/b\nd\n\n[1] NaN\n\n\nNaN or Not a Number is used when the outcome of a calculation doesn’t exist. For instance:\n\nf &lt;- 0/0\nf\n\n[1] NaN\n\ntypeof(f)\n\n[1] \"double\"\n\nclass(f)\n\n[1] \"numeric\"\n\n\nAgain, R doesn’t include a warning: it assings the NaN to the variable f. If you would miss this calculation and you didn’t check the environment panel (where f is shown as NaN). You can check if a number exists if you use the function is.nan(). Note that R will show a warning if you don’t assign:\n\nlog(-10)\n\nWarning in log(-10): NaNs produced\n\n\n[1] NaN\n\n\n\n\n3.1.1.0.6 NaN and NA?\nNote that NaN is different from NA: NA is used for missing values (not available). Datasets often include missing data: data that is not available, e.g. because a respondent in a survey did not complete a question (e.g. “What is your income”), sales data that were not (yet) reported at the time you are writing a report, or country data that is missing because the dataset includes years where a country didn’t exist or didn’t report data (say GDP for the USA in 1500). NA’s are missing in a sense that you usually would expect that there is a value: if the respondent would have answered the question with respect to his or her income, that value would have shown a positive value (say €2000), sales data will be reported and the country did exist in a sense that there were people running around between the boundaries of what is now the USA and these people produced goods and earned an income, but the land within these boundaries was not yet known as the USA. So these NA’s are not equal to 0 (there was an income, there were sales, …) but we don’t know their value.\nMost computations that involve NA’s will return NA’s:\n\nNA\n\n[1] NA\n\nNA *5\n\n[1] NA\n\nNA /2 \n\n[1] NA\n\n\nBecause of this, often we will have to include ‘na.rm = TRUE’ in functions, e.g. to show summary statistics. Adding na.rm = TRUE tells the function to disregard NA’s. If we wouldn’t and there are NA’s, the result would be NA. Don’t worry too much about the c(10 …), we’ll see what it does soon, but let’s try to calculate the mean of 10, 20, 30, 40, and NA:\n\nc &lt;- c(10, 20, 30, 40, NA)\nmean(c)\n\n[1] NA\n\n\n\nc &lt;- c(10, 20, 30, 40, NA)\nmean(c, na.rm = TRUE)\n\n[1] 25\n\n\nYou can check is a value is an NA using is.na.\n\na &lt;- NA\nis.na(a)\n\n[1] TRUE\n\n\nNote that NaN are also NA’s\n\ng &lt;- log(-10)\n\nWarning in log(-10): NaNs produced\n\nis.na(g)\n\n[1] TRUE\n\n\n\n\n\n3.1.2 Integers\nIntegers are numeric but take values such as 1, 2, 3, …. In other words, an integer does not have a decimal part. If you don’t tell R that a variable is an integer, it will set its type equal to double:\n\na &lt;- 10\ntypeof(a)\n\n[1] \"double\"\n\n\nTo tell R that it needs to set a value as integer, you can add and L: for instance:\n\na &lt;- 10L\ntypeof(a)\n\n[1] \"integer\"\n\n\nAs with numeric variables, you can check if a variable is an integer using is.integer()\n\nis.integer(a)\n\n[1] TRUE\n\n\nIf that is not the case, you can tell R it needs to change the type into an integer. Here we use as.integer() to change the type of a character and numeric variable into an integer:\n\nb &lt;- \"10\"\nc &lt;- 10\nas.integer(b)\n\n[1] 10\n\nas.integer(c)\n\n[1] 10\n\n\nYou used this function in ?sec-examples to change the type of the variable data in life_df into an integer.\nIntegers are numeric. You can see that if you use is.numeric()\n\nis.numeric(a)\n\n[1] TRUE\n\n\nIn other words, if you select columns as you did in ?sec-examples using the condition is.numeric() this selection will include double as well as integer variables.\nBecause they are numeric, you can use them in calculations. For instance:\n\na &lt;- 50L\nb &lt;- 25\nc &lt;- a / b\ntypeof(c)\n\n[1] \"double\"\n\n\nIf you do calculations with two integers and the result is an integer, the type of that result will change to double:\n\nr &lt;- 100L\ns &lt;- 10L\nt = r/s\ntypeof(t)\n\n[1] \"double\"\n\n\nBefore we continue with the logical values, let’s remove some variables\n\nrm(a, b, c, t, s, t)\n\nWarning in rm(a, b, c, t, s, t): object 't' not found\n\n\n\n\n3.1.3 Logical values\n\n3.1.3.1 Boolean or logical values\nLogical values or boolean values take the value of TRUE or FALSE.\n\nbool_1 &lt;- TRUE\nbool_1\n\n[1] TRUE\n\n\nYou can verify that bool_1 is a logical value using:\n\nclass(bool_1)\n\n[1] \"logical\"\n\n\n\ntypeof(bool_1)\n\n[1] \"logical\"\n\n\nor\n\nis.logical(bool_1)\n\n[1] TRUE\n\n\nYou can use T and F as shorthand for TRUE als FALSE:\n\nbool_2 &lt;- F\nbool_2\n\n[1] FALSE\n\n\nNote that these logical values take the value of 1 for TRUE and 0 for FALSE. In other words, you can use them in math:\n\nbool_1 + 1\n\n[1] 2\n\n\nWe already met multiple functions whose output is a boolean operator: is.numeric(), is.logical(), is.integer(), is.na(), is.nan(), is.finite(), is.infinite(), … .\n\n\n3.1.3.2 Boolean operators\nA statement is TRUE or FALSE. Suppose for instance that you have the following values\n\na &lt;- 100\nb &lt;- 25\nc &lt;- 125\n\nUsing these, you can test for equality or other relations.\n\nIs a equal to b: ==\n\n\na == b\n\n[1] FALSE\n\n\n\nis a larger than or equal to b: &gt;= (if only larger than: &gt;)\n\n\na &gt;= b\n\n[1] TRUE\n\n\n\nIs a smaller than or equal to b: &lt;=\n\n\na &lt;= b\n\n[1] FALSE\n\n\nIn addition to these relational operators, boolean operators also include & (and), | (or) and ! (not). Using these, you can test for, e.g.\n\nis not equal to: != (i.e. is smaller or larger than):\n\n\na != b\n\n[1] TRUE\n\n\n\nis not smaller than or equal to (i.e. is larger than):\n\n\n!(a &lt;= b)\n\n[1] TRUE\n\n\n\nis not larger than: (i.e. is smaller than or equal to):\n\n\n!(a &gt; b)\n\n[1] FALSE\n\n\nOr you can test if a condition does not hold:\n\n!is.numeric(a)\n\n[1] FALSE\n\n\nUsing & and | you can combine conditions. A statement with the & operator is TRUE is all its components are TRUE and FALSE if one of them is false. A statement with the | operator is TRUE is one of the components is TRUE and FALSE is all components are FALSE. For instance:\n\na is larger than both b and c\n\n\n(a &gt; b) & (a &gt; c)\n\n[1] FALSE\n\n\nHere the first component of the statement is TRUE (100 &gt; 10), the second component (100 &gt; 125). is FALSE Here & is FALSE.\n\na is smaller than b or c\n\n\n(a &lt; b) | (a &lt; c)\n\n[1] TRUE\n\n\nHere the first component of the statement (100 &gt; 10) is FALSE, the second component is TRUE (100 &lt; 125). FALSE | TRUE is TRUE.\nNote that combining conditions you need to deviate from “traditional language”. The statement in the first example: a is larger than b and c is not written as a &gt; b & c. R reads the latter statement as a &gt; b and evaluates that expression as TRUE. The second statement, c is TRUE as there is not condition involved. As TRUE & TRUE equal TRUE, R evaluates this statement as TRUE, not as FALSE. With a the condition as (a &gt; b) & (a &gt; c) this is not the case. The first condition is TRUE, the second condition is FALSE: and TRUE and FALSE result in FALSE\nCombining ! (not) with & (and) or | (or) :\n\n!( a & b) = !a | !b (“not a and b” equals “not a or not b”)\n!(a | b) = !a | !b (“not a or b” equals “not a and not b”)\n\nLogical values equal 1 if TRUE and 0 if FALSE. This allows you to do math. Recall from ?sec-examples that we used the fact the determine the number of missing values in a dataset sum(if.na()). In other words you can use:\n\nbool_1 + 1\n\n[1] 2\n\n\n\n2 * bool_1\n\n[1] 2\n\n\n\nTRUE + TRUE\n\n[1] 2\n\n\n\n\n3.1.3.3 A note on numeric\nRecall that numeric values are stored as series of 1 and 0 and the precision is limited to 15-17 digits. For boolean operators, this precision has implications. For instance, let’s compare\n\na &lt;- 1/3\n\nwith (15 digits)\n\nb &lt;- 0.333333333333333\n\nand (16 digits)\n\nc &lt;- 0.3333333333333333\n\n\na == b\n\n[1] FALSE\n\na == c\n\n[1] TRUE\n\n\nAs you can see, the first condition is FALSE. In other words, for R, 1/3 is not equal to 0.333333333333. However, add one digit and R evaluates the second condition as TRUE. The same issue can occur if you check the results of two different calculations are equal. If you use a condition such as res_cal_1 == res_calc_2, the outcome could be false due to the limited precision of numeric variables. There are a couple of ways to deal with this. The first uses round(). If you add sufficient digits (e.g. 9 or 12), the test will be TRUE is the rounding error is every small:\n\nround(a, digits = 12) == round(b, digits = 12)\n\n[1] TRUE\n\n\nA second way is to calculate the difference and check if the difference is small enough treat it as 0. Here we test if the difference between a and b is smaller than 0.000000000001 (10^(-12)). If that is the case, we assume that a and b are sufficiently close be the equal:\n\na - b &lt; 10^(-12)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nDoes demand equal supply?\n\n\n\nTo illustrate that you need to be careful with == in calculation, consider this simple Economics 101 demand and supply model. Suppose that demand is shown as\n\\[\nQ_D = D_0 - d_1 * P\n\\]\nwith \\(D_0 = 700\\) and \\(d_1 = 0.80\\)\nand the inverse supply function is equal to\n\\[\nQ_S = S_0 + s_1 * P\n\\]\nwith \\(S_0 = 10\\) and \\(s_1 = 2.50\\). The following code calculates the equilibrium.\n\n# Demand function Q_D = D0 - d1 * P\nD0 &lt;- 700\nd1 &lt;- 0.80\n\n# Invese supply function: Q_S = S0 + s1 * P\nS0 &lt;- 10\ns1 &lt;- 2.50\n\n# Solve model: using the inverse supply function, determine the supply function\n# Supply function: P = (1/s1) * Q_S - (1/s1) * S0\n# Insert supply function demand and determine equilibrium quantity via Demand\n\neq_Q &lt;- (s1 / (s1 + d1)) * D0 + (s1 / (s1 + d1)) * (d1 / s1) * S0\n\nprint(glue::glue(\"Equilibrium quantity is \", {eq_Q}))\n\nEquilibrium quantity is 532.727272727273\n\n# Use inverse demand to calculate the equilibrium price eq_P\n\neq_P &lt;- -(1/d1) * (eq_Q - D0)\n\nprint(glue::glue(\"Equilibrium price is \", {eq_P}))\n\nEquilibrium price is 209.090909090909\n\n# Using the equilibrium price, determine total demand\n\nQ_D = D0 - d1 * eq_P\n\nprint(glue::glue(\"Demand equals \", {Q_D}))\n\nDemand equals 532.727272727273\n\n# and total supply\n\nQ_S = S0 + s1 * eq_P\nglue::glue(\"Supply equals \", {Q_S})\n\nSupply equals 532.727272727273\n\n\nLet’s check if there is an equilibrium using the condition that Q_S == Q_D:\n\nif (Q_S == Q_D) {\n  print(\"Market is in equilibrium\")\n} else {\n  print(\"No market equilibrium\")\n}\n\n[1] \"No market equilibrium\"\n\n\nAs you can see, R shows no equilibrium. Even if you can spot from the output of the model that demand equals supply. Using an alternative - the difference is very small - we have equilibrium\n\nif(Q_S - Q_D  &lt; 10^(-8)) {\n  print(\"Market is in equilibrium\")\n} else {\n  print(\"No market equilibrium\")\n}\n\n[1] \"Market is in equilibrium\"\n\n\n\n\nBefore we start with character variables, let’s remove some values:\n\nrm(bool_1, bool_2, a, b, c)\n\n\n\n\n3.1.4 Character variables or strings\n\n3.1.4.1 The packages: {stringr} and {glue}\nCharacter or string variables include letters, words, sentences or a space. The {stringr} and {glue} packages include a lot of functions that you can use to work with strings. You can load these packages using the library() function. They should be available as you installed them in the Chapter 1. If you didn’t install these packages, you can do so now. If you did, you can skip this part and load the packages.\nSome of these functions - with a different name - are also available in base R. If your work doesn’t require a lot of string manipulation, you can use the base R string functions. To differentiate between base R functions and functions from {stringr} and {glue}, the latter always include the package name and are written as stringr::and glue::. This clearly shows where the function comes from. In addition, you don’t have to load these packages. If you do, you can leave out these package references.\n{stringr} has the advantage that it is consistent with a lot of other tidyverse packages: all function are written as a verb “extract,”replace”, the first argument is always the data, … . The base R function often do not follow this logic. This is also the case here. For instance, we’ll use the grepl() function to look for patterns in a character variable. As you can see, this is not a verb and when we discuss the function, you’ll see that the first argument is not a character that you will use to apply the function to. However, as they don’t require you to call a package - either using the library function or through the package::function approach - they are generally faster then the {stringr} functions.\n\n\n3.1.4.2 String variables\nTo assign characters, you use \" \". Anything between \" \" will be stored as a character: you can also use them for numbers. However, if you do so, the number is not stored as a numeric type, but as a character type. R will also treat anything between ' ' as a string. However, most people use \" \" and this is what we’ll do here as well. We will use the words “character” and “string” variables interchangeably. Here are a couple of examples:\n\nchar1 &lt;- \"A\"\nchar2 &lt;- \"2.25\"\nchar3 &lt;- \"Hello World!\"\nchar4 &lt;- \" \"\n\nYou can verify is a variable is a character in two ways. First, you can ask for its type:\n\ntypeof(char1)\n\n[1] \"character\"\n\n\nSecond, you can use is.character() to verify a variable is a string:\n\nis.character(char2)\n\n[1] TRUE\n\n\nHow long are these strings? You would think that the length is char1 is 1 and the lenght of char3 is 12. If you ask for the length of these variables:\n\nlength(char1)\n\n[1] 1\n\nlength(char2)\n\n[1] 1\n\nlength(char3)\n\n[1] 1\n\nlength(char4)\n\n[1] 1\n\n\nyou’ll see that they are all equal to 1. In other words, they are all atomic vectors or vectors of length 1 even if “Hello World!” includes 10 letters, 2 words, an exclamation mark and a spaceNote that a space is treated as a character. If you look at char4:\n\nprint(char4)\n\n[1] \" \"\n\n\nYou’ll see that is prints a space. As spaces are treated as a separate character, it follows that\n\nchar5 &lt;- \"name:\"\nchar6 &lt;- \"  name:  \"\n\nare treated as two different strings. char2 is “2.25”. Although it looks like a number, R will treat it as a string. In other words, you can not apply mathematical functions:\n\nchar2 * 2\n\nError in char2 * 2: non-numeric argument to binary operator\n\n\n\n\n3.1.4.3 Useful string operations\nData management and analysis is often associated with numbers, statistics and math. This is only part of the story. Text based data is widely available and can be retrieved from e.g. emails, pdf or word files, social media posts or in the reviews left in Tripadvisor. Here, we’ll introduce some functions that you can use to analyse text based data. We”ll focus on single (atomic) characters. We’ll use both base R function as well as function included in the {stringr} package. If you don’t need a lot of string operations, you can often use the former. Loading {stringr} takes up memory. If you don’t need its full functionality, base R function will be less memory intensive. I’ll refer to {stringr} functions using the stringr:: way of using them. Function that do not start with stringr:: are base R functions. I’ll write the function in full, including the argument names. Recall from Chapter 1 that as long as you keep the order of the arguments unchanged, you don’t need to add them. Here, I’ll add them anyway. As you become more familiar with these functions, you can drop them. In addition, you recall that you don’t need to default values. Here, if they matter, I’ll do that anyway.\n{stringr} was designed to work with other packages including e.g. {dplyr} or {tidyr}. In subsequent chapters, we’ll use that fact to tidy and clean data. For instance, if there is a value “USD 250”, we’ll need to split this up in a variable currency which includes that value “USD” and a variable amount, which includes the number “250”.\n\n3.1.4.3.1 Change characters to upper or lower\nSuppose you have a character variable\n\nchar_a &lt;- \"SALES NOVEMBER\"\n\nand you need lowercase. Using tolower()allows you to change all letters to lowercase\n\nchar_b &lt;- tolower(char_a)\nchar_b\n\n[1] \"sales november\"\n\n\nYou can also change lowercase to uppercase using toupper():\n\nchar_c &lt;- toupper(char_b)\nchar_c\n\n[1] \"SALES NOVEMBER\"\n\n\n{stringr} includes functions with a similar outcome: stringr::str_to_upper() and stringr::str_to_lower(). In addition, stringr::str_to_title() adds an upper case to the first letter of a word:\n\nstringr::str_to_title(char_c)\n\n[1] \"Sales November\"\n\n\n\n\n3.1.4.3.2 Combine multiple strings\nThe base R paste()and paste0() functions allows you to combine two or more strings. If you use paste, you need to include the strings you want to concatenate as well as the separator (i.e. what you want to include between these two strings (e.g. a space, an underscore “_”, …)). The default value here is a space paste(..., sep = \" \", collapse = NULL, recycle0 = FALSE). With paste0, you don’t need to include the separator: paste0(..., collapse = NULL, recycle0 = FALSE). In both functions the ... show where you include the strings that you want to combine in a single string. Let’s see what these functions do. First define two strings:\n\nchar_a &lt;- \"Your name is\"\nchar_b &lt;- \"Name\"\n\nFor now, you can disregard the collapse = NULL and recycle0 = FALSE part and accept these default values. If you use paste() and you also accept the default `sep = \" \" to concatenate these two string variable, the outcome shows:\n\npaste(char_a, char_b)\n\n[1] \"Your name is Name\"\n\n\nAs you can see, the first string “Your name is” and the second string “Name” are now one string. Between the first and the second string, R put a space. This is the default value for the separator (see sep = \" \"). Here, we included two character variables, but you could add more. In addition, you don’t need to define your character variables first but you can type them in the function as well. However, in most cases, you’ll have a character variable in your data.\n\npaste(\"one string\", \"a second string\", \"another string\", \"yet another string\")\n\n[1] \"one string a second string another string yet another string\"\n\n\nSuppose you would like a : as a separator. In that case you would change the default in the sep = argument and include the seperator you need, e.g. :, _, |, & or any other sign, word or letter:\n\npaste(char_a, char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\npaste(char_a, char_b, sep = \"and \")\n\n[1] \"Your name isand Name\"\n\npaste(char_a, char_b, sep = \"& \")\n\n[1] \"Your name is& Name\"\n\npaste(char_a, char_b, sep = \"a couple of words \")\n\n[1] \"Your name isa couple of words Name\"\n\n\npaste() can be use to e.g. create names of files. Suppose you need to read a file “market_share_month_year.csv” as part of your monthly workflow. Using paste() you can build that files name:\n\ndataset_1 &lt;- \"market_share\"\nmm &lt;- \"january\"\nyy &lt;- \"2026\"\nname_file &lt;- paste(paste(dataset_1, mm, yy, sep = \"_\"), \"csv\",  sep = \".\")\n\nUsing {here}, you can now use here::here(\"data\", \"raw\", file_name). In your code, you only need to change mm and yy to read the file. Using the same code, you can build names to as in a command where you save datasets of scripts.\nIn addition, you can use this to e.g. change variable names, or change character values in observations.\nIf you use paste0, R puts both strings one after the other without a separator.\n\npaste0(char_a, char_b)\n\n[1] \"Your name isName\"\n\n\nIn other words, you need to include the separator in your strings:\n\nchar_c &lt;- \"Your name is: \"\npaste0(char_c, char_b)\n\n[1] \"Your name is: Name\"\n\n\nRecall that the pipe operators puts what its finds on the left hand side as the first argument of the function on the right hand side. In other words, you can use paste() or paste0() also using that operator:\n\nchar_c |&gt; paste0(char_b)\n\n[1] \"Your name is: Name\"\n\n\n{stringr}’s str_c() function performs the same operation as paste():\n\nstringr::str_c(char_a, char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\n\nHowever, it is designed to be used with other {tidyverse} packages and functions such as {dplyr} ’s mutate() function and you can use the pipe operator:\n\nchar_a |&gt; stringr::str_c(char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\n\n{glue} can be used to “insert” a variable in a string using { }. Say you have a variable\n\nsurname &lt;- \"Alex\"\n\nand you want to add that surname to the string char_a. You could use the previous functions, but those wouldn’t allow you to insert that name in the middle of a string. Glue does.\n\nglue::glue(char_a, \": \", {surname})\n\nYour name is: Alex\n\n\nWe used this functionality in ?sec-examples to fill in the year in the animated plot. Using {glue} you can insert many variables:\n\nname &lt;- \"Anna\" \nage &lt;- 22 \nanniversary &lt;- as.Date(\"2025-10-12\") \nglue::glue('My name is {name}, ',   \n           'my age next year is {age}, ',   \n           'my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.')\n\nMy name is Anna, my age next year is 22, my anniversary is zondag, oktober 12, 2025.\n\n\nNote that my computer is dutch. So here, it puts the day as “zondag” and not sunday, and add “oktober” and not october. This {glue} functionality was incorporated in the {stringr} package in the str_glue() function. For instance, the first example using {stringr}’s str_glue() function:\n\nstringr::str_glue(char_a, \": \", {surname})\n\nYour name is: Alex\n\n\nFor the second example, the changes to the {glue} code are similarly small:\n\nname &lt;- \"Anna\" \nage &lt;- 22 \nanniversary &lt;- as.Date(\"2025-10-12\") \nstringr:::str_glue('My name is {name}, ',   \n                   'my age next year is {age}, ',   \n                   'my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.')\n\nMy name is Anna, my age next year is 22, my anniversary is zondag, oktober 12, 2025.\n\n\nIf you don’t need to load {stringr}, you can use {glue}. Although the former includes more functions (as we’ll she shortly), the latter is more focused end doesn’t take up much memory.\nIf you need to duplicate individual strings, str_dub(), can be used to do so. Suppose you need to duplicate the string “abc” 5 times. Let’s first define this string:\n\nchar_d &lt;- \"abc\"\n\nwith str_dub(string = str, times = n) you can duplicate the string “str” n times. Here, there are no default values so you need to add both arguments:\n\nstringr::str_dup(string = char_d, times = 5)\n\n[1] \"abcabcabcabcabc\"\n\n\nNote that the outcome is one character, not 5 character variables.\n\n\n3.1.4.3.3 Whitespaces\nWhitespaces, or spaces for short, are sometimes added to a string. For instance, suppose you have\n\nchar_space1 &lt;- \" year\"\n\nRecall that R sees a space as a separate character. In other words “year” and ” year” are treated as different. You can remove these whitespaces using {stringr}’s str_trim(string, side = c(\"both\", \"left\", \"right\"))and str_squish(string) functions. Using the first, you need to add which spaces you want to remove: those on the left, the right or both. Both is here default. Removing the space in the char_space string:\n\nstringr::str_trim(string = char_space1, side = \"left\")\n\n[1] \"year\"\n\n\nstr_squish() removes all strings to the left and right and replaces internal spaces with a single space. For instance,\n\nchar_space2 &lt;- \" Here, there are    multiple spaces   left, right and   middle  \"\n\nUsing str_squish(), you can remove all those:\n\nstringr::str_squish(char_space2)\n\n[1] \"Here, there are multiple spaces left, right and middle\"\n\n\nYou can eliminate all white spaces using str_remove_all(string, pattern = \" \").\n\nstringr::str_remove_all(string = char_a, pattern = \" \")\n\n[1] \"Yournameis\"\n\n\n\n\n3.1.4.3.4 Identifying patterns in a string\nLet’s define new strings, a quote by JM Keynes:\n\nchar_a &lt;- \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\nRecall that for R, char_a is an atomic vector even is there are multiple words, spaces, … :\n\nlength(char_a)\n\n[1] 1\n\n\nHere, I used a quote with two sentences but this character variable could be an email with multiple lines, a product or service review. We can calculate the number of letters in this string using base R’s nchar(x, type = c(\"chars\", \"bytes\", \"width\"), allowNW = FALSE, keepNA = NA). You can disregard the latter two options and keep their default values. The option “chars”, which is the default, calculates the number of characters in a string. The other two show the number of bytes needed to store the string and “width” shows the number of columns you need to print the string in monospaced font. Usually, you won’t need those two options. How long is char_a?\n\nnchar(x = char_a, type = \"chars\")\n\n[1] 123\n\n\nchar_a has 123 characters (including white spaces).\nTo detect a pattern in a character variable, you can use base R’s grep() and grepl() functions. grep stand for Global Regular Expression Print. We will cover regular expressions. The difference between both is that the first shows either the position of a pattern or its value while the last will evaluate to TRUE if a pattern is detected and FALSE if that is not the case. The most important arguments of grep() include pattern, x, ignore.case = FALSE, value = FALSE. These arguments include the pattern that you are looking for in string x. By default, R ignores the case and shows the position of the character. Here, we only have one character variable, so the position will always be 1. Let’s use this function to check if the character variable char_a includes the pattern “pounds”, “pou” or “POU”:\n\ngrep(pattern = \"pounds\", x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\ngrep(pattern = \"pou\", x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\ngrep(pattern = \"POU\", x = char_a, ignore.case = FALSE, value = FALSE)\n\ninteger(0)\n\n\nAs you can see, R detect the patterns “pounds” and “pou” while it can not detect the patter “POU”. Here, the R returns an empty position integer(0). Chaging the default ignore.case = FALSE in TRUE, would change that:\n\ngrep(pattern = \"POU\", x = char_a, ignore.case = TRUE, value = FALSE)\n\n[1] 1\n\n\nR detects the pattern “POU” in char_a. Changing the default value = FALSE in TRUE, R shows the character where it detects the pattern. Because we have one character, R returns the full character.\n\ngrep(pattern = \"pou\", x = char_a, ignore.case = FALSE, value = TRUE)\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nYou don’t have to add the pattern within grep(). If you define it outside of the function, you can use that definition within the function.\n\npat1 &lt;- \"pou\"\ngrep(pattern = pat1, x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\n\ngrepl() has the same arguments, except for the value argument. As grepl returns a logical value TRUE of FALSE in case the pattern was detected or nor, if always shows the same output: TRUE of FALSE. In other words, you don’t need to tell R what the output should be:\n\ngrepl(pattern = \"mana\", x = char_a)\n\n[1] TRUE\n\n\nChanging the ignore.case = FALSE to TRUE, glepl() will show TRUE if the pattern occurs, whether there are capital letters or not:\n\ngrepl(pattern = \"MANA\", x = char_a)\n\n[1] FALSE\n\n\nWe can also use {stringr} to detect if a pattern occurs. For instance, does the character variable include a word where “pou” occurs? To check this, you can use the str_detect(string, pattern, negate = FALSE). Here, you need to fill out the string (char_a in this example) and the pattern (pou in this example). You can keep the negate = FALSE default:\n\nstringr::str_detect(string = char_a, pattern = \"pou\")\n\n[1] TRUE\n\n\nAs you can see, the pattern “pou” is included in the string. Note that R is case sensitive. {stringr} will fail to detect “Pou”:\n\nstringr::str_detect(string = char_a, pattern = \"Pou\")\n\n[1] FALSE\n\n\nIf you would change the default negate = FALSE to negate = TRUE, the results of these functions would be FALSE and TRUE. This can be useful if you want to keep strings where a given pattern does not occur. In that case, str_detect() will be FALSE if it detects a pattern and TRUE if it doesn’t detecte a pattern.\nThis function is useful is you want to detect the presence of a string in e.g. an if statement. If string “x” occurs, then do something, else do something else. For instance, suppose you want to import files where the name includes “sales”, e.g.\n\nchar_file &lt;- \"2024_november_sales\"\n\n\nstringr::str_detect(string = char_file, pattern = \"sales\")\n\n[1] TRUE\n\n\nR returns true. You can use this statement to import files. Recall from ?sec-examples, that we used is.numeric() and the where() function to select numeric variables in a data frame. In a similar way, you use grepl() or str_detect() to select variables whose name include a pattern.\nWe now know that “pounds” is included in char_a. Let’s now count the number of times “pounds” occurs in this string. To do so, we use the str_count(string, pattern = \" \") function:\n\nstringr::str_count(string = char_a, pattern = \"pounds\")\n\n[1] 2\n\n\nNote that pattern = \"pou\" would show the same result. However, this is due to the fact that “pound” is the only word that includes this pattern. If a string would also include “vapour” or “spouse” that would not the case. If you know the number of occurrences, you can e.g. create a word cloud or a chart show how many times a specific word of pattern is used in a text.\nLet’s find the location of the word “pounds”. To do so we will use str_locate(string, pattern) of str_locate_all(string, pattern). The former finds the first occurrence and then stops, the second shows all occurrences. So, if you only want to know the first occurrence of “pounds”, you can use str_locate():\n\nstringr::str_locate(string = char_a, pattern = \"pounds\")\n\n     start end\n[1,]    41  46\n\n\nThe pattern “pounds” is located between position 41 (letter p) and position 46 (letter s) in char_a. Where is the second “pounds”? Using str_locate_all() you can find out:\n\nstringr::str_locate_all(string = char_a, pattern = \"pounds\")\n\n[[1]]\n     start end\n[1,]    41  46\n[2,]    96 101\n\n\nNote that in this case pattern = \"pou\" would show a different result: here R would show position 41 to 43 as it wouldn’t include the last letter “nds”. You already knew the pattern “pounds” occurred twice and you knew the location of the first, now you know the location of the second: between the 96th and 101st position.\nIf you want to extract the word, you can use {stringr} ’s str_extract(string, pattern, group = NULL) or str_extract_all(string, pattern, simplify = FALSE) function. The first function extracts the first occurrence (and only the first occurrence), the second extracts all occurrences:\n\nstringr::str_extract_all(string = char_a, pattern = \"pounds\", simplify = FALSE)\n\n[[1]]\n[1] \"pounds\" \"pounds\"\n\n\nstr_extract() extracts a pattern. Recall that we now where the word “pounds” is. We can use that information to extract that word using its location. To do so, we can use base R’s substr(x, start, stop) function or {stringr}’s str_sub(string, start, end). The first function needs the string and the position where it needs to start extracting and the position where it needs to stop doing so:\n\nsubstr(x = char_a, start = 41, stop = 46)\n\n[1] \"pounds\"\n\n\nstr_sub()needs a string, a start and end position to extract the pattern:\n\nstringr::str_sub(string = char_a, start = 41, end = 46)\n\n[1] \"pounds\"\n\n\nYou also use negative numbers for start and end. In that case, str_sub() counts backwards from the end:\n\nstringr::str_sub(string = char_a, start = -11, end = -1)\n\n[1] \"your mercy.\"\n\n\nThis can be useful if you want to extract the middle part of a string. Suppose you have a character variable\n\nchar_mid1 &lt;- \"1000 Brussels (BEL)\"\n\nand you need to extract the city name “Brussels”. Brussels starts at location 6 (starting from the left) en ends at location -7 (starting from the right):\n\nstringr::str_sub(string = char_mid1, start = 6, end = -7)\n\n[1] \"Brussels\"\n\n\nStarting from the left, removes “1000”, starting from the right removes ” (BEL)“. If all you observations follow a similar pattern, you can extract all city names:\n\nchar_mid2 &lt;- \"2000 Antwerpen (Berchem) (BEL)\"\n\n\nstringr::str_sub(string = char_mid2, start = 6, end = -7)\n\n[1] \"Antwerpen (Berchem)\"\n\n\nThis function can be usefull to extract information from e.g. the name of a file. Suppose that you have a file “sales_2025.csv”. The dataset doesn’t include the reference year as one of its variables, but you would like to add the year. You know that the year starts at position -8 and ends at position -5. In other words:\n\nstringr::str_sub(\"sales_2025.csv\", start = -8, end = -5)\n\n[1] \"2025\"\n\n\nextract the year. If you store this value as.numeric() you can add it to your dataset as a separate variable.\nIf you want to remove a pattern, you can use str_remove(string, pattern) or str_remove_all(string, pattern). As in the previous case, the former removes only the first occurrence, the second removes all occurrences. We’ll save the outcome in a new string:\n\nchar_b &lt;- stringr::str_remove(string = char_a, pattern = \"pounds\")\nchar_b\n\n[1] \"If you owe your bank manager a thousand , you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\n\nchar_c &lt;- stringr::str_remove_all(string = char_a, pattern = \"pounds\")\nchar_c\n\n[1] \"If you owe your bank manager a thousand , you are at his mercy. If you owe him a million , he is at your mercy.\"\n\n\nThis function is useful is you need to remove, for instance, values from a string. Suppose you have a data frame where observations are recorded as\n\nchar_obs &lt;- \"20 cm\"\n\nIf you need the “20” to do calculations, you’ll need to remove the space and cm from that string. Using stringr::str_remove() as as.numeric() you can do this:\n\nchar_obs &lt;- stringr::str_remove(string = char_obs, pattern = \" cm\")\nchar_obs &lt;- as.numeric(char_obs)\nchar_obs\n\n[1] 20\n\ntypeof(char_obs)\n\n[1] \"double\"\n\n\nCentimeter (cm) is only one example. Often, datasets include values such as “$20”, “30 usd”, “eur 30”, “20l”, “size 30x32”, … . These all prevent you from using the “20”, “30” or “32” in calculations.\nIn addition, you can use this function to remove whitespaces. You’ll find a lot of whitespaces in your work: “sales november”, “long filename.xls”, “stock returns per month”, … . Sometimes it is usefull to remove them.\n\nstringr::str_remove_all(string = char_a, pattern = \" \")\n\n[1] \"Ifyouoweyourbankmanagerathousandpounds,youareathismercy.Ifyouowehimamillionpounds,heisatyourmercy.\"\n\n\nYou can remove a pattern, but also replace a pattern. To do so, you can use base R functions sub() and gsub() of {stringr}’s. str_replace(string, pattern, replacement) or str_replace_all(string, pattern, replacement).\nsub()and gsub()are both base R function that allow you to replace a pattern in a string. A pattern could be, e.g. a couple of letters, a word, … . We’ll also discuess regular expressions. These too allow you to build patterns. For now, we’ll use recognizable patterns such as a word or a couple of letters. Both function’s arguments include pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE. The first tells R which patters it needs to identify and replace. The second includes the pattern it needs use to replace the pattern in the character variable x. By default, this search is case sensitive. You can change that is you change the default FALSE to TRUE in ignore.case = FALSE.\nLet’illustrate these thwo functions using char_a. Suppose you would like to change the word “pounds” in “dollar”. Using sub() you could do that using:\n\nsub(pattern = \"pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nAs you can see, the first “pounds” has been replaced with “dollars”. However, the second wasn’t. This is where sub() and gsub() differ. The first will replace the first occurrence of the pattern, but not the next. gsub() replaces all occurrences:\n\ngsub(pattern = \"pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nTo illustrate the use if the default case sensitivity, Let’s change “pounds” into “Pounds”:\n\ngsub(pattern = \"Pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nAs you can see, gsub() didn’t replace “pounds” as due to R’s case sensitivity. Let’s now change the default ignore.case from FALSE in TRUE\n\ngsub(pattern = \"Pounds\", replacement = \"dollars\", x = char_a, ignore.case = TRUE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nHere, you can see that gsub() replaced pounds as it ignores R’s sensitivity.\nYou can also use {stringr} function to replace patterns. Suppose again you want to replace “pounds” with “dollars”. The first function str_replace() replaces the first occurrence and is very similar to sub():\n\nchar_d &lt;- stringr::str_replace(string = char_a, pattern = \"pounds\", replacement = \"dollars\")\nchar_d\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nstr_replace_all() is similar to base R’s gsub() and replaces all patterns:\n\nchar_d &lt;- stringr::str_replace_all(string = char_a, pattern = \"pounds\", replacement = \"dollars\")\nchar_d\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nIf you want replace whitespaces with an underscore “_”, using this function or gsub():\n\nstringr::str_replace_all(string = char_a, pattern = \" \", replacement = \"_\")\n\n[1] \"If_you_owe_your_bank_manager_a_thousand_pounds,_you_are_at_his_mercy._If_you_owe_him_a_million_pounds,_he_is_at_your_mercy.\"\n\n\nIn char_a we have two sentences and many words. Using str_split() and boundary() we can split this string in two sentences or break it in words. Using pattern = boundary(\"sentence\") R will (try to) break up char_a in sentences. Using pattern = boundary(\"words\") breaks char_a up in words. There are two more alternatives: boundary(\"character\") breaks the string up in single characters and boundary(\"lines\") breaks the string up in line. The str_split() also accepts other pattern. The function has two default values: n = Inf and simplify = FALSE. Let’s split up in sentences:\n\nchar_e &lt;- stringr::str_split(string = char_a, pattern = boundary(\"sentence\"), n = Inf, simplify = FALSE)\nchar_e\n\n[[1]]\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. \"\n[2] \"If you owe him a million pounds, he is at your mercy.\"                 \n\n\nNow you can access the first sentence using\n\nchar_e[[1]][1]\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. \"\n\n\nand the second sentence\n\nchar_e[[1]][2]\n\n[1] \"If you owe him a million pounds, he is at your mercy.\"\n\n\nIf you split is up in words:\n\nchar_f &lt;- stringr::str_split(string = char_a, pattern = boundary(\"word\"), n = Inf, simplify = FALSE)\nchar_f\n\n[[1]]\n [1] \"If\"       \"you\"      \"owe\"      \"your\"     \"bank\"     \"manager\" \n [7] \"a\"        \"thousand\" \"pounds\"   \"you\"      \"are\"      \"at\"      \n[13] \"his\"      \"mercy\"    \"If\"       \"you\"      \"owe\"      \"him\"     \n[19] \"a\"        \"million\"  \"pounds\"   \"he\"       \"is\"       \"at\"      \n[25] \"your\"     \"mercy\"   \n\n\nYou can know access the words. For instance, the 20th word is:\n\nchar_f[[1]][20]\n\n[1] \"million\"\n\n\nLet’s see what happens if we include another pattern, e.g. “pounds”:\n\nchar_g &lt;- stringr::str_split(string = char_a, pattern = \"pounds\", n = Inf, simplify = FALSE)\nchar_g\n\n[[1]]\n[1] \"If you owe your bank manager a thousand \"         \n[2] \", you are at his mercy. If you owe him a million \"\n[3] \", he is at your mercy.\"                           \n\n\nAs you can see, char_a is now broken up in 3 pieces: the first equal the part before the first “pounds”, the second, the piece after the first and before the second “pounds” and the last part is the rest of char_a after the second “pounds”.\nThe string in char_a is long. You can shorten this string using str_trunc(string, width, side = c(\"right\", \"left\", \"center\"), ellipsis = \"...\"). Here, you take a string, determine how long it can be (width) and, if the string is longer, which parts has be be shown: the first (right), the last (left) or the middle (center). The last option allows you to show that a part of the string was removed. If you want to reduce the width of char_a to 25, this is how you can do it:\n\nstr_trunc(string = char_a, width = 25, side = \"left\")\n\n[1] \"..., he is at your mercy.\"\n\nstr_trunc(string = char_a, width = 25, side = \"right\")\n\n[1] \"If you owe your bank m...\"\n\nstr_trunc(string = char_a, width = 25, side = \"center\")\n\n[1] \"If you owe ...your mercy.\"\n\n\n\n\n3.1.4.3.5 Regular expressions: a first look\nIn the previous example, the pattern we looked for was always “pou” or “pounds”, i.e. a word or part of a word. But a word is not the only pattern you can look for. This is what regular expressions do: a short way to look for patterns in a string. The terms “regular expression” is often replaced by “regex”. For instance the world “RStudio” starts with a capital letter R, then a capital letter S and small letters u, d, i and o. This is a pattern. An email adress is written as “someone@somethingelse.xyz”. Again, there is a pattern: someone, then an @ followed by something else a dot and com, edu, or two letters referring to a country; a postal code for Belgium includes 4 digits, a phone number is written as e.g. 0123 45 67 89 and a student number look like r0987654. A regular expression can be used to identify this type of patterns. Regular expressions are widely used. They are also included in e.g. excel.\nLet’s look at how we can use them. Suppose we have a character:\n\nchar_reg &lt;- \"1000 Brussels, 2000 Antwerp, 3000 Leuven\"\n\nWe’ll try to extract information from this string. As you can see, there are three cities in the string, each with their postal code. Suppose you want to extract all city names. The city postal coded are numbers, the names are letters. With a regular expression, you search for patterns, not words. In this case, you can not search for a common pattern as all city names are different (length, start with different letters, …). This where regular expressions enter. If you want to search for letters, you can use “[a-z]”. The regular expression “[a-z]” looks in the character variable and identifies all characther that include one of the letters a - z non-capitalized. To search for multiple letters (the same of different), you add a plus “[a-z]+”. This regular expressions searches for patterns such as “a”, “b”, … , “z”, “aa”, “cd”, … “ccc”, “ddd”, “qrrt …. zt”. In other words, is searchers the character variable and checks if it includes one or a series of non-capitalized letter. Recall that R is case sensitive: [a-z] refers to all normal letters. I.e. it excludes capital letters. To add the capital letter, we need [A-Z]. Without a + after [A-Z], the regular expression is equal to “1 capital letter A - Z”. If you add a plus, the regular expression search for one of more capitalized letters. As there is only one capital letter in char_reg and that capital is at the start, we don’t need to allow for repetition and we put the capital letter before the normal ones: [A-Z][a-z]+. This is an example of a regular expression: it searches for all occurrances of the pattern “1 capital letter + one or more non-captalized letters”. In other words, it refers to any pattern in words where the word starts with one capital letter A, B, C, … Z and continues with any number of normal letters. We can now use this to extract the values from our character. Using {stringr}’s str_extract_all()\n\nstringr::str_extract_all(string = char_reg, pattern = \"[A-Z][a-z]+\")\n\n[[1]]\n[1] \"Brussels\" \"Antwerp\"  \"Leuven\"  \n\n\nWe can extract the names of the cities.\nWhat about the postal codes? Here “[0-9]” allows you to search for numbers. If you add a + you can search for multiple numbers:\n\nstringr::str_extract_all(string = char_reg, pattern = \"[0-9]+\")\n\n[[1]]\n[1] \"1000\" \"2000\" \"3000\"\n\n\nWe know that the costal code includes 4 numbers. Using this information, we can refine the regular expression. The pattern includes 4 and only 4 numbers. To do so, we add the number of repetitions between {} after \\[0-9\\]:\n\nstringr::str_extract_all(string = char_reg, pattern = \"[0-9]{4}\")\n\n[[1]]\n[1] \"1000\" \"2000\" \"3000\"\n\n\nThere is an alternative. If you use \\\\d you search for any digit character:\n\nstringr::str_extract_all(string = char_reg, pattern = \"\\\\d{4}\")\n\n[[1]]\n[1] \"1000\" \"2000\" \"3000\"\n\n\nA couple of words on the double \\\\. Where does it come from? The \\ is an escape sign. If you type “d” for digit, R wouldn’t know that you refer to a digit or the letter d. That is why you use an escape backslash \\. It tells R not to look for a d, but to look for a digit. In other words, it allows you to “escape” the usual meaning of the letter d. However, \\ is a special sign in R. So R could interpret that sign in the wrong way. To avoid that, you add a second escape backslash: to tell R that the first backslash isn’t a normal backslash but an escape backslash. In other words, the second escape sign is used the escape the usual interpretation of the backslash in R. In other words, if you want to add a pattern: any digit: you need \\\\d.\nHere is a list of regular expressions:\n\n[a-z] : any letter, non capitalized, a - z, changing a or z reduces the range\n[A-Z] : any letter, capitalized, A-Z, chaning A or Z reduces the range\n[ad2$z]: string included all letters, numbers and symbols within []\n\\\\d : any digit character, 0, 1, 2 …\n\\\\D : any nog digit character: letters, question marks, spaces, …\n\\\\w : any alphanumeric character\n\\\\W : any non-alphanumeric character (symbols, punctuation, …)\n\\\\s : a whitespace\n\\\\S : any non-whitespace\n\n  : refers to one of more repetitions\n\n* : refers to zero or more repetitions (i.e. it doesn’t occur but it can also occur multiple times).\n\n^ : the string starts with the expression following ^,\n$ : the string ends with the expression before $\n\n{x} : in case you want to include the number of repititions\n{x, y} : in case there are x to y repetitions.\n\nHere, I used extract as an example, but all previous functions that include a patters can be used with regular expressions as well.\nUsing regular expressions, you can build very complex patterns. Let’s illustrate a couple:\n\nchar_abc &lt;- \"abcdefghijklmnopqrstuvwxyz0123456789\"\n\n\nsearching for only a, b or c: [abc] as individual characters or abc as a group if you add +\n\n\nstringr::str_extract_all(string = char_abc, pattern = \"[abc]\")\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\nstringr::str_extract_all(string = char_abc, pattern = \"[abc]+\")\n\n[[1]]\n[1] \"abc\"\n\n\n\nsearching for all, except a, b or c: [^abc] as individual characters or ^abc as a group if you add +\n\n\nstringr::str_extract_all(string = char_abc, pattern = \"[^abc]\")\n\n[[1]]\n [1] \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\"\n[20] \"w\" \"x\" \"y\" \"z\" \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\"\n\nstringr::str_extract_all(string = char_abc, pattern = \"[^abc]+\")\n\n[[1]]\n[1] \"defghijklmnopqrstuvwxyz0123456789\"\n\n\n\nsearch for all letters a, b, c … k: [a-k], as individual characters or as a group if you add +\n\n\nstringr::str_extract_all(string = char_abc, pattern = \"[a-k]\")\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\"\n\nstringr::str_extract_all(string = char_abc, pattern = \"[a-k]+\")\n\n[[1]]\n[1] \"abcdefghijk\"\n\n\n\nsearch for all numbers 0, 1, … 5: [0-5], as individual characters or as a group if you add +\n\n\nstringr::str_extract_all(string = char_abc, pattern = \"[0-5]\")\n\n[[1]]\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\"\n\nstringr::str_extract_all(string = char_abc, pattern = \"[0-5]+\")\n\n[[1]]\n[1] \"012345\"\n\n\n\nsearch for xyz012 [a-z]{3}[0-9]{9}. Here you ask for a sequence that includes first three letters (a, b, c, … or z) using [a-z]{3} followed by a sequence of three numbers 0, 1, 2, … 9 using [0-9}{3}\n\n\nstringr::str_extract_all(string = char_abc, pattern = \"[a-z]{3}[0-9]{3}\")\n\n[[1]]\n[1] \"xyz012\"\n\n\nSuppose a phone number of written as 0032-xxxx-xxxxxx. You have a character\n\nchar_tel &lt;- \"my phone is 0032-4859-202545\"\n\nIs there a phone number in this string?\nFirst, notice the pattern:\n\n4 numbers: 0032: regular expression: “0032”\n-: regular expression: “-”\n4 numbers, no limit: regular expression “[0-9]{4}”\n-: regular expression: “-”\n6 numbers, no limit: “[0-9]{6}”\n\n\nstringr::str_detect(string = char_tel, pattern = \"0032-[0-9]{4}-[0-9]{6}\")\n\n[1] TRUE\n\n\nIs there a phone number in this string?\n\nchar_tel2 &lt;- \"my id is 3256-2256-659822\"\n\n\nstringr::str_detect(string = char_tel2, pattern = \"0032-[0-9]{4}-[0-9]{6}\")\n\n[1] FALSE\n\n\nBefore we start with logical values, let’s remove some values first:\n\nrm(age, anniversary, name, char_a, char_abc, char_b, char_c, char_reg, char_space2, char_space3, char_tel, char_tel2, pat1)\n\nWarning in rm(age, anniversary, name, char_a, char_abc, char_b, char_c, :\nobject 'age' not found\n\n\nWarning in rm(age, anniversary, name, char_a, char_abc, char_b, char_c, :\nobject 'anniversary' not found\n\n\nWarning in rm(age, anniversary, name, char_a, char_abc, char_b, char_c, :\nobject 'name' not found\n\n\nWarning in rm(age, anniversary, name, char_a, char_abc, char_b, char_c, :\nobject 'char_space2' not found\n\n\nWarning in rm(age, anniversary, name, char_a, char_abc, char_b, char_c, :\nobject 'char_space3' not found\n\n\n\n\n\n\n3.1.5 Dates and times\nDates are often used in data management. They are stored in various formats, e.g. March 1, 2021; 2021-03-01; 01/03/2021; 1 march 2021; 1 march 21; 21-03-01 … The same holds for times. In addition, the world is not sharing one common time zone. In other words, if it is 12:00 in one location, in many other locations the time is different. To work with dates and times, we’ll use both base R function as well as functions from the {lubridate} package. This package was specifically designed to work with date/time variables. If a {lubridate} function is used, you’ll see that because that function will be called with lubridate::. As was the case with {stringr}, I’ll also base R functions. Often these functions will do fine. However, like any other tidyverse package, {lubridate} has the advantage that it integrates very will with other tidyverse packages;\n\n3.1.5.1 Dates\n\n3.1.5.1.1 Which day is it today?\nTo determine which day it is today, you can use the Sys.Date() function:\n\nSys.Date()\n\n[1] \"2025-02-21\"\n\n\nThis function return the current date in ISO 8601 format: YYYY-MM-DD: first the year, written with 4 digits, then the month, always written with 2 digits and the day always written with 2 digits with year, month and day separated by a dash of hyphen (-). As an alternative, the date can also be shown without the hyphen as 20250215. Using this standard avoids a lot of uncertainty. For instance, 2/3/2025 could mean March 2nd 2025 (day, month, year) but also February 3th 2025 (month, day, year). Using the ISO standard avoids this confusion: the first 4 digits represent the year, digits 5-6 the month and digits 7-8 the day. All months and days include two digits, even if they refer to, e.g. January (written as 01) of the third day of the month (written as 03).\nAs an alternative, you can use {lubridate}’s today(tzone = \" \") function. Using this function, you can add the timezone. For instance, if you need the current date in the Greenwhich Meantime timezone (GMT), you can use\n\nlubridate::today(tzone = \"GMT\")\n\n[1] \"2025-02-21\"\n\n\nIf you want to now the current date in Kiribati (time zone UTC + 14) or in Honulu (time zone UTC - 10) in you can use their time zones in the today() function. For Kiribati, this timezone is Pacific/Kiritimati and for Honululu it is Pacific/Honolulu (you can find a list of timezones in the IANA time zone database (tzdatabase) or in the base R file Olsonnames()\n\ndat_k &lt;- lubridate::today(tzone = \"Pacific/Kiritimati\")\ndat_k\n\n[1] \"2025-02-22\"\n\ndat_h &lt;- lubridate::today(tzone = \"Pacific/Honolulu\")\ndat_h\n\n[1] \"2025-02-21\"\n\n\n\n\n3.1.5.1.2 Converting strings and numbers to a date\nSuppose you have a character variable\n\nchar_dat &lt;- \"2025-06-25\"\n\nand you want to convert it to a date. Here, dat_1 is a character variable.\n\nclass(char_dat)\n\n[1] \"character\"\n\n\nThere are various ways to change that into a date. Let’s start with base R’s as.Date(x, format, tryformats = (\"%Y-%m-%d\", \"%Y/%m/%d\"), optional = FALSE, ...) function. The function needs a variable to convert (x) but also a format, e.g. %Y-%m-%d. This format stands for 4 digit year (%Y, e.g. 2025), month as a number (%m, e.g. 06) and day as a number (%d, e.g. 25). If no format is given, as.Date()will try one of the formats (\"%Y-%m-%d\", \"%Y/%m/%d\"). The optional = FALSE arguments determines what R needs to do in case it can not convert a string into a date. The default, FALSE, means that R will show and NA. If TRUE, R will show an error. Using char_dat and entering the format and accepting the default for optional:\n\ndat_1 &lt;- as.Date(x = char_dat, format = \"%Y-%m-%d\")\n\nLet’s see what the outcome is:\n\ndat_1\n\n[1] \"2025-06-25\"\n\n\nNote that dat_1 is written as a string (it is within ” “) but its class has changed\n\nclass(dat_1)\n\n[1] \"Date\"\n\n\nAs you can see, dat_1 is now a Date and it is shown using the ISO standard. In other words, as.Date() converts a string in a date and the output is consistent with the ISO standard.\nYou can check that `dat_1´ is a date using {lubridate} is.Date() function:\n\nlubridate::is.Date(dat_1)\n\n[1] TRUE\n\n\nIf you check the type of dat_1\n\ntypeof(dat_1)\n\n[1] \"double\"\n\n\nyou’ll see that it is a double. In order words, dat_1 ’ class is date but it’s type is a number. The fact that dat_1is a double is caused by the way computers store dates and times. For dates, applications use a reference data or epoch date. Although there are differences, for a lot of software packages, that date is 1 January 1970. A date is then recorded as the number of days since Thursday 1 January 1970 00:00:00 UT (where UT stands for Universal Time). This date and, as well see, also time, is also known as Unix epoch or POSIX. To see this, let’s convert dat_1 in a number and show the result:\n\ndat_num &lt;- as.numeric(dat_1)\ndat_num\n\n[1] 20264\n\n\nAs you can see, 20264 days have passes since 1 January 1970. As a date is stored as a number, you can convert any number to a date. Here, you also use the as.Date(x, origin = )function, but now, you can specify the origin. For the latter argument, the default value is 1 January 1970, but you can use other dates. For instance, Microsoft Excel’s origin is 1 january 1900. In other words, if you import data from excel with the date is stored in a numeric format, you’ll have to add Excel’s origin. If we accept the default origin, converting a number to a data is straightforward:\n\ndat_2 &lt;- as.Date(25000)\ndat_2\n\n[1] \"2038-06-13\"\n\n\nas.Date() adds the number of days, 25000, to January 1, 1970 and shows that date in ISO format. Doing so, as.Date() takes into account e.g. leap years.\nYou can now verify the origin:\n\ndat_or &lt;- as.Date(0)\ndat_or\n\n[1] \"1970-01-01\"\n\n\nWhat happens with dates before 1 January 1970. These are stored as negative numbers. For instance Niel Armstrong was the first man to set foot on the moon on 20 July 1969 at 20:17 UTC. To see how this day is stored:\n\nchar_moon &lt;- \"1969-07-10\"\ndat_moon &lt;- as.Date(char_moon)\ndat_num_moon &lt;- as.numeric(dat_moon)\ndat_num_moon\n\n[1] -175\n\n\nIn other words, if you enter a negative number in as.Date()and you accept the default origin, you’ll get a date before 1 january 1970. For instance, 25 000 days before 1 January 1970, the date was:\n\nas.Date(-250000)\n\n[1] \"1285-07-10\"\n\n\nNot all dates are stored as 2025-06-30. Some are written as 2025/06/30, as 30-06-2025, as 30 July 2025, July 30, 2025, … . Using as.Date() you can specify the format. The following table shows a variety of symbols that you can use to format dates:\n\n\n\nTable 3.1: Date symbols\n\n\n\n\n\nSymbol\nDefinition\nExample\n\n\n\n\n%d\nDay as a number\n15\n\n\n%a\nAbbreviated weekday\nMon\n\n\n%A\nUnabbreviated weekday\nMonday\n\n\n%m\nMonth as a number\n04\n\n\n%b\nAbbreviated month\nApr\n\n\n%B\nUnabbreviated month\nApril\n\n\n%y\n2-digit year\n25\n\n\n%Y\n4-digit year\n2025\n\n\n\n\n\n\nTo see how this works, let’s create a date for 01/03/2025. The format of the date is “day as number”/“month as number”/“4 digit year”. In R, this is the format %d/%m/%Y (capital Y). To read that string “01/03/2025” as a date, you can use the format option in as.Date():\n\ndat_1 &lt;- \"01/03/2025\"\ndat_1 &lt;- as.Date(dat_1, format = \"%d/%m/%Y\")\ndat_1\n\n[1] \"2025-03-01\"\n\nclass(dat_1)\n\n[1] \"Date\"\n\n\nIf your date is written as 010325 (March 1, 2025) but with format “day as a number”“month as a number”“2 digit year”) you set the format as %d%m%y (note the small y):\n\ndat_2 &lt;- \"01032025\"\ndat_2 &lt;- as.Date(dat_2, format = \"%m%d%y\")\ndat_2\n\n[1] \"2020-01-03\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nIf you are not sure which format it is, you can include various formats that R will test using the tryformats = c() argument in as.Date(). If you don’t specify the format, R will try one of the formats in included in the c() vector. By default, the function includes two formats to try using tryformats “%Y-%m-%d” and “%Y/%m/%d” but you can add more. Suppose you have a date “25 April 2025”. Here, the date is written as “day as a number” “unabbreviated month” “4 digit year”. You can add this format “%d %B %Y” to the formats R will test (note that you can delete the others if you are sure)\n\ndat_3 &lt;- \"25 April 2025\"\ndat_3 &lt;- as.Date(dat_3, tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d %B %Y\"))\ndat_3\n\n[1] \"2025-04-25\"\n\nclass(dat_3)\n\n[1] \"Date\"\n\n\nEvery time we used as.Date() R shows the date as %Y-%m-%d. To print dates, that if often not the most convenient way to show the a date as you read your day backward (e.g. year first) relative to how most people refer to a date (e.g. year last). You can change that format using format(x, format = \"%Y-%m-%d). This function allows you to format a date. The default format is %Y-%m-%d. However, suppose you want to write dat_3 including the month as an unabbreviated month (%B), the full name of the day (%A) as well as the day as a number (%d) and a 4 digit year (%Y), you could use:\n\ndat_3\n\n[1] \"2025-04-25\"\n\ndat_4 &lt;- format(dat_3, format = \"%A %d %B %Y\")\ndat_4\n\n[1] \"vrijdag 25 april 2025\"\n\n\nNote the R uses your system language to show the name of the day and the month.\n{lubridate} includes a wide range of function set dates. The “ymd()”-family which includes: ymd(), ydm(), mdy(), myd(), dmy(), dym(), yq(), ym() and my() (where y refers to year, m refers to month, d refers to day and q refers to quarter). The structure of these functions is the same: ymd(..., quiet = FALSE, tz = NULL). For now, you can accept all defaults. This family of functions outputs a date with the traditional %Y-%m-%d format. To illustrate how this family works, we’ll use the dmy() function. This function will try to convert a value (numeric or character) from a format with day first, month second and year last to a date, written in ISO format. Let’s write a date (2025-03-25) in multiple day-month-year formats, e.g. as character “25/03/2025” written %d/%m/%, a numeric written as 25032025, a character “25 March 2025” written as %d %B %Y and a character “25-03-25” written as %d-%m-%y. lubridate::dmy() tries to identify the day, month and year and returns a date with %Y-%m-%d format\n\ndat_11 &lt;- \"25/03/2025\"\ndat_12 &lt;- 25032025\ndat_13 &lt;- \"25 March 2025\"\ndat_14 &lt;- \"25-03-25\"\ndat_21 &lt;- lubridate::dmy(dat_11)\ndat_22 &lt;- lubridate::dmy(dat_12)\ndat_23 &lt;- lubridate::dmy(dat_13)\ndat_24 &lt;- lubridate::dmy(dat_14)\ndat_21\n\n[1] \"2025-03-25\"\n\ndat_22\n\n[1] \"2025-03-25\"\n\ndat_23\n\n[1] \"2025-03-25\"\n\ndat_24\n\n[1] \"2025-03-25\"\n\n\nAnother useful {lubridate} function is make_date(year = , month = , day = ). If a dataset includes a separate variable for the day, the month and the year, this function allows you to collects these into one variable as a date.\n\nyear &lt;- 2025\nmonth &lt;- 3\nday &lt;- 25\ndat_3 &lt;- lubridate::make_date(year = year, month = month, day = day)\ndat_3\n\n[1] \"2025-03-25\"\n\nclass(dat_3)\n\n[1] \"Date\"\n\n\n\n\n3.1.5.1.3 Extracting information from a date\nOften you need to work with date aggregated per year, semester, quarter, month, week … . If you datasets includes a date, you can extract the relevant period using base R’s weekdays(), months() or quarters() function {lubridate}’s year(), semester(), quarter(), month(), week(), isoweek(), day(), wday(), yday(). Each function has its own arguments. Base R’s include the date you will use to extract information as well as abbreviate = which allows you to determine if you need an abbreviated e.g. month or not. By default base R’s functions show the full name. Let’s apply these function to a specific date, March 25, 2025:\n\ndat_1 &lt;- as.Date(\"2025-03-25\")\n\nLet’s start with base R’s three functions:\n\nweekdays(x = dat_1, abbreviate = TRUE)\n\n[1] \"di\"\n\nmonths(x = dat_1, abbreviate = FALSE)\n\n[1] \"maart\"\n\nquarters(x = dat_1, abbreviate(TRUE))\n\n[1] \"Q1\"\n\n\n{lubridate} includes similar functions but adds more options. Using year() you can extract the year:\n\nlubridate::year(dat_1)\n\n[1] 2025\n\n\nThe `semester(w, with_year = FALSE) allows you to show the semester as well as the year. The default does not show the year. If you change this default to true, the function shows the year.semester, e.g. 2025.1\n\nlubridate::semester(dat_1, with_year = FALSE)\n\n[1] 1\n\nlubridate::semester(dat_1, with_year = TRUE)\n\n[1] 2025.1\n\n\nThe quarter(x, type = \"quarter\", fiscal_start = 1) has more options. The type allows you to specify the way the function will show the output. The default is the quarter (1, 2, 3, 4). As an alternative, you can use e.g. “year.quarter” to show the year as well as the quarter, “date_first” or “date_last” to show the first or last day of the quarter or “year_start/end”. In accounting and finance, years do not always coincide with the calender year. If a firm’s fiscal year starts in November, its first quarter will end in January. To accommodate for this, you can specify when the fiscal year starts using “fiscal_start = x”, with x the month when the fiscal year starts. The default if January (i.e. fiscal year equals calender year).\n\n# Changing the way the quarter looks:\n\nlubridate::quarter(dat_1, type = \"quarter\", fiscal_start = 1)\n\n[1] 1\n\nlubridate::quarter(dat_1, type = \"year.quarter\", fiscal_start = 1)\n\n[1] 2025.1\n\nlubridate::quarter(dat_1, type = \"date_first\", fiscal_start = 1)\n\n[1] \"2025-01-01\"\n\nlubridate::quarter(dat_1, type = \"date_last\", fiscal_start = 1)\n\n[1] \"2025-03-31\"\n\nlubridate::quarter(dat_1, type = \"date_first\", fiscal_start = 1)\n\n[1] \"2025-01-01\"\n\nlubridate::quarter(dat_1, type = \"year_start/end\", fiscal_start = 1)\n\n[1] \"2024/25 Q1\"\n\n# Changing the fiscal year to november: \nlubridate::quarter(dat_1, type = \"quarter\", fiscal_start = 11)\n\n[1] 2\n\nlubridate::quarter(dat_1, type = \"year_start/end\", fiscal_start = 11)\n\n[1] \"2024/25 Q2\"\n\n\nThe month(x, label = FALSE, abbr = TRUE) function allows you to show the label (e.g. January or Jan). If you specify abbr = TRUE the function shows “Jan” if abbre = FALSE it will show “January”. By default the output shows the month as a number:\n\nlubridate::month(dat_1, label = FALSE, abbr = TRUE)\n\n[1] 3\n\nlubridate::month(dat_1, label = TRUE, abbr = FALSE)\n\n[1] maart\n12 Levels: januari &lt; februari &lt; maart &lt; april &lt; mei &lt; juni &lt; ... &lt; december\n\nlubridate::month(dat_1, label = TRUE, abbr = TRUE)\n\n[1] mrt\n12 Levels: jan &lt; feb &lt; mrt &lt; apr &lt; mei &lt; jun &lt; jul &lt; aug &lt; sep &lt; ... &lt; dec\n\n\nNote that the months are ordered.\nweek()and isoweek() show the number of the week. The former show the complete seven day periods that have occurred between the date and January 1 of the same year, plus one. The former returns the week using the ISO 8601 standard. The first day of the week in the ISO standard is a Monday and the last day of the week is a Sunday. The first week of the year using the ISO week numbering is the first week with 4 days or more in January. In other words, the first week of the year includes January 4. The first week also includes at least 3 working days (all days except Saturday, Sunday and January 1). In other words, the earliest first week of the year starts Monday December 29 and ends Sunday January 4th. The latest possible week starts on Monday January 4 and lasts to Sunday January 10. If that is the case, January 1 is part of the last week of the previous year. The last week is week 52 or week 53. In 2025, there will be a difference. week() counts 7 day periods since January 1. In 2025, week 1 according to the ISO standard started on December 30, 2024 and ended on Sunday January 5. In other words, as ISO weeks start on Monday, the week() starts counting on Wednesday, there will be a difference in week numbering for Mondays and Tuesdays. As March 25 is on a Tuesday, there will be a difference.\n\nlubridate::week(dat_1)\n\n[1] 12\n\nlubridate::isoweek(dat_1)\n\n[1] 13\n\n\nI you would use, e.g. March 28, that wouldn’t be the case:\n\nlubridate::week(dat_1 + 3)\n\n[1] 13\n\nlubridate::isoweek(dat_1 + 2)\n\n[1] 13\n\n\nWith respect to the day, there are different ways you can look at the day: a day as part of a week, month, … year. With respect to the day as part of the month, that is how we usually look at the day: March 25 (25th day of March). With respect to the day of the week, there are usually two ways to count, depending on where your week starts: Monday or Sunday. In some countries and in line with the ISO standard, Sunday is the last day of the week. In other words, weekday 1 is Monday. In other countries, the week starts on Sunday and Monday would be weekday 2. If you want to show the day of the week number, you’ll need to take that into account. The function wday(x, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 7)) allows you to show the day of the week using a label, an abbreviated label or a number. In the latter case, you need to specify the start of the week using the argument (\"lubridate.week.start\", 7). As a default, Sunday is the first day of the week. If you want to change that into Monday, you need to change the 7 in 1. If you want to show a label (label  = TRUE) the starting day of the week does not affect the outcome as the function shows Monday or Mon. For instance:\n\nlubridate::wday(dat_1, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 7))\n\n[1] 3\n\nlubridate::wday(dat_1, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 1))\n\n[1] 2\n\nlubridate::wday(dat_1, label = TRUE, abbr = FALSE)\n\n[1] dinsdag\n7 Levels: zondag &lt; maandag &lt; dinsdag &lt; woensdag &lt; donderdag &lt; ... &lt; zaterdag\n\nlubridate::wday(dat_1, label = TRUE, abbr = TRUE)\n\n[1] di\nLevels: zo &lt; ma &lt; di &lt; wo &lt; do &lt; vr &lt; za\n\n\nyday() and qday() show the day as the xth day in the year (yday()) or quarter (qday()). Here, the quarter refers to the calender year. The show the difference let’s add 50 days to March 25 so that we have a date in the second quarter. The result of these functions is:\n\nlubridate::yday(dat_1 + 50)\n\n[1] 134\n\nlubridate::qday(dat_1 + 50)\n\n[1] 44\n\n\n\n\n3.1.5.1.4 Calculations using dates\nRecall that dates are stored as numbers.\n\ndat_1 &lt;- as.Date(\"2025-03-25\")\ntypeof(dat_1)\n\n[1] \"double\"\n\n\nIf you add 5 to dat_1 the result will be a new date:\n\ndat_2 &lt;- dat_1 + 5\ndat_2\n\n[1] \"2025-03-30\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nAs you can see, R adds 5 days to dat_1. Here, R uses the fact that the type of dat_1 is numeric and is stored as the number of days since 1 January 1970. To calculate the new date, it add 5 to that numeric value and shows that numeric value as a date:\n\ndat_1n &lt;- as.numeric(dat_1)\ndat_1n\n\n[1] 20172\n\ndat_2n &lt;- dat_1n + 5\ndat_2n\n\n[1] 20177\n\ndat_2 &lt;- as.Date(dat_2n)\ndat_2\n\n[1] \"2025-03-30\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nLikewise, if you subtract 250 from dat_1 R will show the date 250 days before dat_1\n\ndat_1 - 250\n\n[1] \"2024-07-18\"\n\n\nSuppose you have start date and you know that a period will last 3 years, 15 weeks and 4 days. Using {lubridate}’s years(), weeks(), and days() function you can determine the and of that period. Using this approach - as opposed to adding the number of days - {lubridate} will take into account leap years. For instance, suppose that the start January 1, 2025. The period will end\n\nstart_time &lt;- lubridate::ymd(\"2025-01-01\")\nend_time1 &lt;- start_time + lubridate::years(3) + lubridate::weeks(15) + lubridate::days(4)\nend_time1\n\n[1] \"2028-04-19\"\n\n\nIn addition to these more traditional calculation, {lubridate} also includes a number or rounding functions: round_date(), floor_date()and ceiling_date(). These function include three arguments. The first is the date to round; the second is the unit to round to (e.g. day, week, month, quarter, halfyear, season of year) and the last argument specifies the start of the week and is similar to the one we already met in wday(): week_start = getOption(\"lubridate.week.start\", 7). The see what these functions do, let’s round 2025-03-25 using all three of them:\n\ndat_1 &lt;- lubridate::ymd(\"2025-03-25\")\n\nUsing round_date() you can round a date to the nearest unit. Here, the unit is the week, month, quarter, halfyear (semester), season of year.\n\nlubridate::round_date(dat_1, unit = \"year\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-01-01\"\n\nlubridate::round_date(dat_1, unit = \"halfyear\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-01-01\"\n\nlubridate::round_date(dat_1, unit = \"season\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-01\"\n\nlubridate::round_date(dat_1, unit = \"quarter\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::round_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::round_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-24\"\n\nlubridate::round_date(dat_1, unit = \"week\")\n\n[1] \"2025-03-23\"\n\n\nHere, the nearest year is “2025” as 2025-03-25 is closer to the start of 2025 than to the start of 2026. The same holds for the nearest halfyear or semester. For the season, round_date() rounds to the month where the season starts (March 21, June 21, September 21 or December 21). As 2025-03-25 is near the end of the first quarter, round_date() rounds to the start of the second quarter. Likewise, the 25th is the month is closer to the end of the month than it is to the start of the month and the function returns the start of the following month. The nearest week is the current week and round_date will round to the start of the week. For the unit “week” the option (\"lubridate.week.start\", 1) sets the start of the week on Monday. The default would show the week starting on Sunday. This is what you see on the last line of the code block.\nfloor_date() rounds the date down to the unit. Here we’ll use “week” and “month” to illustrate its output:\n\nlubridate::floor_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-01\"\n\nlubridate::floor_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-24\"\n\n\nAs you can see, if the unit if “month”, this function rounds to the start of the month. Likewise, is the unit is week, the function rounds to the start of the current week. For the unit “week” the option (\"lubridate.week.start\", 1) sets the start of the week on Monday. The default would show the week starting on Sunday.\nceiling_date() rounds up to the unit.\n\nlubridate::ceiling_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::ceiling_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-31\"\n\n\nHere, the month is rounded to the start of the next month and the week is rounded to the start of the next week (Monday 2025-03-31).\n{lubridate} includes two other functions that you can use to change the date to the last day or the previous month of the first day of the current month (rollbackward(dates, roll_to_first = FALSE, preserve_hms = TRUE)) or to the first day of the next month or the last day of the current month (rollforward(dates, roll_to_first = FALSE, preserve_hms = TRUE)). Both function need a date as their first argument. By default, they roll (back or forward) to the last of the previous month (back) or the last or the current month (forward). If you change roll_to_first = FALSE to TRUE the functions will roll to the first of the current (back) of next (forward) month. The last argument, preserve_hms = TRUE will be covered when we discuss time.\nUsing rollbackward() you can identify the last day of the previous month or the first day of the current month. This is useful is you want to calculate e.g. average for the month or a change since the last day of the previous month or the first day of the current month:\n\nlubridate::rollbackward(dat_1, roll_to_first = FALSE)\n\n[1] \"2025-02-28\"\n\nlubridate::rollbackward(dat_1, roll_to_first = TRUE)\n\n[1] \"2025-03-01\"\n\n\nUsing rollforward() you can identify the last day of the current month or the first day of the next month. This is useful is you want to calculate e.g. the difference between a level end of month and the current level.\n\nlubridate::rollforward(dat_1, roll_to_first = FALSE)\n\n[1] \"2025-03-31\"\n\nlubridate::rollforward(dat_1, roll_to_first = TRUE)\n\n[1] \"2025-04-01\"\n\n\n\n\n3.1.5.1.5 Time spans\nTime spans can be measured in 3 ways:\n\nintervals set by two dates, a start and end date\nperiods, measured in years, months, days, hours, minutes and seconds\ndurations, measured in seconds\n\nHere, we will focus on the first two. Using {lubridate}’s interval(start, end, tz = \"\") you can create time intervals. For instance, let’s set the start of the academic year (open) at September 18, 2024 and the end of the academic year at September 13, 2025. The time zone is Brussels.\n\naca_open &lt;- \"2024-09-18\"\naca_close &lt;- \"2025-09-13\"\n\naca_year &lt;- lubridate::interval(start = aca_open, end = aca_close, tzone = \"Europe/Brussels\")\naca_year\n\n[1] 2024-09-18 CEST--2025-09-13 CEST\n\n\n{lubridate} creates a new “Interval” object whose type is double:\n\nclass(aca_year)\n\n[1] \"Interval\"\nattr(,\"package\")\n[1] \"lubridate\"\n\ntypeof(aca_year)\n\n[1] \"double\"\n\n\nUsing this object, you can now determine e.g. the length of that interval using time_length(x, unit = \" \"). For the unit you can insert e.g. “second”, “day”, “week” or “year”. To measure the length of the academic year in various units, you can use\n\nlubridate::time_length(aca_year, unit = \"month\")\n\n[1] 11.83871\n\nlubridate::time_length(aca_year, unit = \"week\")\n\n[1] 51.42857\n\nlubridate::time_length(aca_year, unit = \"day\")\n\n[1] 360\n\nlubridate::time_length(aca_year, unit = \"hour\")\n\n[1] 8640\n\n\nAs you can see, the academic year as we defined it here, lasts 11.83 months, 51.42 weeks, 360 days or 5640 hours. This function can be useful to calculate e.g. an age. Suppose you have the date of birth, e.g. September 25, 1997. To calculate the age, you can set an interval\n\ndate_birth &lt;- \"1997-09-25\"\nnow &lt;- lubridate::today()\nage &lt;- lubridate::interval(start = date_birth, end = now)\n\nRecall that you can round a number of various ways. Here, we’ll use trunc() which rounds a number by removing the decimals. Measuring the length of the interval in years and rounding by eliminating the decimals, you have a person’s age:\n\ntrunc(lubridate::time_length(age, \"years\"))\n\n[1] 27\n\n\nYou can check if a date is part of an interval using %within%(date, interval). For instance, to check if “2025-03-25” is included in the interval aca_year:\n\nlubridate::`%within%`(dat_1, aca_year)\n\n[1] TRUE\n\n\nAs an alternative, you can define a period using {lubridate}’s period() function. Using this function, you need to specify the number of years, weeks, days, hours, minutes or seconds. Focusing on the date components, you can specify this period as\n\nper &lt;- lubridate::period(year = 3, week = 15, day = 4)\nper\n\n[1] \"3y 0m 109d 0H 0M 0S\"\n\n\nYou can now use that period to e.g. calculate an end date:\n\nend_time2 &lt;- start_time + per\nend_time2\n\n[1] \"2028-04-19\"\n\n\nNote that {lubridate} includes leap years when it calculates end_time1 or end_time2.\n\n\n3.1.5.1.6 Other usefull functions\nThere are a couple of other useful functions on the {lubridate} package. The first, days_in_month(x) shows the number of days on a month for the month includes in x. The output shows the month as well as the number of days in that month. For instance:\n\ndays_in_mar &lt;- lubridate::days_in_month(dat_1)\ndays_in_mar\n\nMar \n 31 \n\n\nTo extract the number of days, you can use\n\ndays_in_mar[[1]]\n\n[1] 31\n\n\nFor most months, you can determine that number in many other ways as the number of days is fixed. However, for the month of February, you need to now if a year is a leap year. ´days_in_month()` takes leap years into account to calculate the number of days in February. As 2024 was a leap year, we can use this a day in February 2024 to show this:\n\nfeb_leap &lt;- lubridate::ymd(\"2024-02-15\")\nlubridate::days_in_month(feb_leap)\n\nFeb \n 29 \n\n\nTo check is a year if a leap year, you can use leap_year(). For a date, the output will be TRUE is the date is part of a leap year and FALSE if that isn’t the case:\n\nlubridate::leap_year(dat_1)\n\n[1] FALSE\n\nlubridate::leap_year(feb_leap)\n\n[1] TRUE\n\n\n\nrm(dat_1, dat_2, dat_3, dat_4, dat_11, dat_12, dat_13, dat_14, dat_21, dat_22, dat_23, dat_24, dat_moon, dat_k, dat_num, dat_num_moon, end_time1, end_time2, aca_open, aca_close, day, date_birth, open, x, tt, days_in_mar, feb_leapn, aca_year)\n\nWarning in rm(dat_1, dat_2, dat_3, dat_4, dat_11, dat_12, dat_13, dat_14, :\nobject 'open' not found\n\n\nWarning in rm(dat_1, dat_2, dat_3, dat_4, dat_11, dat_12, dat_13, dat_14, :\nobject 'tt' not found\n\n\nWarning in rm(dat_1, dat_2, dat_3, dat_4, dat_11, dat_12, dat_13, dat_14, :\nobject 'feb_leapn' not found\n\n\n\n\n\n3.1.5.2 Times\nTo a large extent, we’ll be able to use what we know from dates and apply that knowledge to times. A number of functions that you can use with dates, also apply to times. However, time is a bit more complicated. First, there are many different time zones around the world. The time in a time zone is usually as a difference with Greenwhich Mean Time (GMT) or Coordinated Universal Time (UCT). GMT or Coordinated Universal Time (UTC) are the same. However, GMT is a timezone and refers to the local time at the Royal Observatory in London’s Greenwhich. UTC is a time standard and was developped in the late 1960’s. Today, most times zones are expressed in terms of an offset relative to UTC. The offset shows the difference between UTC and the local time, e.g. UTC01:00. UTC divides time into days, hours, minutes and seconds. Each day contains 24 hours and each hour 60 minutes. Usually, every minute counts 60 seconds, although there are leap seconds. If there is a leap second, a minute can count 59 or 61 seconds. Although we mentioned time zones in relation to dates, they were of a minor issue in that case and are only relevant if you look at the current date round the world. Here, for some countries the local date might be “Monday”, while in others it is still “Sunday”. For time however, time zones are relevant. For instance, if you schedule a meeting for 14:00, you need to add the time zone if that meeting includes people from e.g. the UK, US and India. In addition to time zones, a second complicating issue in terms of time is daylight savings time or summer time: advancing the clock by one hour in spring and retarding the clock by one hour in autumn. Not all countries know this practice. In addition, not all countries have used daylight savings time every year since it was introduced.\nThe ISO standard for dates was “%Y-%m-%d”: 4 digit year, 2 digit month and 2 digit day of the month separated by a hyphen (-). The ISO standard for date/time values set in local time is “%Y-%m-%dT%H:%M:%S”, e.g. 2025-03-25T17:30:15. In other words, the date and time are separated by a “T” and time is written as %H (hours (24)), %M (minutes (0-59)) and %S (seconds (0-59)) separated by a colon. These and other time formats are included in the next table/\n\n\n\nTable 3.2: Time symbols\n\n\n\n\n\nSymbol\nDefinition\nExample\n\n\n\n\n%H\nDecimal hours (24 hours)\n15\n\n\n%I\nDecimal hours (12 hours)\n3\n\n\n%M\nDecimal minute\n58\n\n\n%S\nDecimal second\n36\n\n\n%p\nAM/PM\nAM/PM\n\n\n**%z%%\nOffset from UTC\n+0130\n\n\n**%Z%%\nFull time zone name\nAmerica/New York\n\n\n\n\n\n\nIf a date/time refers to UTC time, a Z is added at the end “%Y-%m-%dT%H:%M:%SZ”, e.g. 2025-03-25T17:30:15Z. For other time zones, the ISO standard uses the UTC offset: “%Y-%m-%dT%H:%M:%S+/-hh:mm” (with hh the number of hours and mm the number of minutes a time zone’s time differs from UTC (with + for times zones ahead and - for time zones behind UTC)), e.g. 2025-03-25T17:30:15+01:00 for a timezone 1 hour ahead of UTC.\n\n3.1.5.2.1 What is the time now?\nTo see the current time, you can use Sys.time()\n\ntime_1 &lt;- Sys.time()\ntime_1\n\n[1] \"2025-02-21 23:23:54 CET\"\n\n\nAs you can see, time is stored using various components: the date (year - month - day), a space followed by the hour, minute and second, separated by a colon then a space and the time zone (CET). This format deviated from the ISO standard as it includes a space between the date and time component and it includes the name of the time zone but not the UTC offset.\nWith respect to the timezone, you can check the current timezone using Sys.timezone()\n\nSys.timezone()\n\n[1] \"Europe/Brussels\"\n\n\nTo check if daylight savings time is on for a specific date, you can use {lubridate}’s dst(). This function returns TRUE if daylight savings time is in force and FALSE otherwise. In August, daylight savings time is in force. If you include this date in dst()the result will be TRUE\n\nlubridate::dst(\"2025-08-25\")\n\n[1] TRUE\n\n\nIn august, daylight savings time is in force.\n{lubridate}’s now(tzone = \" \") shows the current time for the timezone included in tz = \" \". The default timezone is your system’s timezone. To change that into UTC, you can use tz = \"UTC\":\n\nlubridate::now()\n\n[1] \"2025-02-21 23:23:54 CET\"\n\nlubridate::now(tzone = \"UTC\")\n\n[1] \"2025-02-21 22:23:54 UTC\"\n\n\nIf you need to know the time in a different time zone, you can use {lubridate} with_tz(time, tzone = \" \") function. For instance, finding the time in Australia/Brisbane for the local time is possible using\n\nlubridate::with_tz(Sys.time(), tzone = \"Australia/Brisbane\")\n\n[1] \"2025-02-22 08:23:54 AEST\"\n\n\nIf you replace Sys.time() with another time, the function shows the time in the tzone for that specific time.\n\n\n3.1.5.2.2 Converting strings and numbers to date/time\nThe class of a date/time value is known as POSIX.\n\nclass(time_1)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nNote that the type of time_1 is double:\n\ntypeof(time_1)\n\n[1] \"double\"\n\n\nPOSIX is a portable operating system interface that allows to store data/time values like the time_1 but allows for time zones. It stores values to the nearest second. POSIX does so in two ways: POSIXct (ct for calender time) and POSIXlt (lt for local time). The former stores data/time values as the number of seconds since 1 January 1970, midnight UTC. POSIXlt stores a list of day, month, year, hour, minute, second, … . As the latter is requires more memory, base R’s POSIXct is usually more efficient.\nTo change a character into a date/time format, you can use as.POSIXct(x, tz = \"\", format, tryformats = c()). Notice that the overall structure of this function is similar to the as.Date() function for dates. The first argument is the string that you want to convert. If you know the date/time format, you can include that format in the format = \" \" part. You develop the format, you can use the symbols in ?tbl-times. If no format is included, the function will try one of the formats in tryFormats = c(\"%Y-%m-%d %H:%M:%S\", \"%Y/%m/%d, %H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y/%m/%d %H:%M\", \"%Y-%m-%d\", \"%Y/%m/%d\") where you can add your own format. Usually time values start with the hour, then minute and often also the second. In other words, usually, it is safe to assume that the hour is written first, followed by the minute and then, if present, the second.\n\ndat_tim_1 &lt;- \"2025-03-25 17:30:00\"\ndat_tim_1ct &lt;- as.POSIXct(dat_tim_1, format = \" %Y-%m-%d %H:%M:%S\")\ndat_tim_1ct\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\nAs was the case with dates, date/time values are stored as a number:\n\ntime_1_num &lt;- as.numeric(dat_tim_1ct)\ntime_1_num\n\n[1] 1742920200\n\n\nAs times are stores as numeric variable, you can also convert numbers to a date/time format. As you had to do with dates, you need to add the origin if that origin differs from the standard January 1, 1970. For instance, if you convert the number 1 to a date/time variable, you’ll see\n\ntime_1 &lt;- as.POSIXct(1, tz = \"UTC\", origin = \"1970-01-01\")\ntime_1\n\n[1] \"1970-01-01 00:00:01 UTC\"\n\n\n{lubridate} includes a number of function to read date/time variables: ymd_hms(), ymd_hm(), ymd_h(), dmy_hms(), dmy_hm(), dmy_h(), mdy_hms(), mdy_hm(), mdy_h(), ydm_hms(), ydm_hm() and ydm_h() where y refers to year, m to month, d to day, h to hour, m to minute and s to second. For instance, for dat_tim_1, which was written as ymd_hms, you can use ymd_hms(..., tz = \"UTC) function family to convert that string into a date/time variable. Note that the default time zone is UTC:\n\ndat_tim_lub &lt;- lubridate::ymd_hms(dat_tim_1, tz = \"Europe/Brussels\")\ndat_tim_lub\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\n{lubridate}’s make_datetime()function is very similar to make_date(). With make_datetime() you can add the hour, minute and second as well as the time zone:\n\ndat_tim_1 &lt;- lubridate::make_datetime(year = 2025,\n  month = 3, day = 25, hour = 17, min = 30, \n  sec = 0, tz = \"Europe/Brussels\")\ndat_tim_1\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\nAs with the make_date() function, this is a very useful function is a dataset includes date/time values, but stores each component in a different variable.\nWith respect to formatting a date/time value, the ISO format reads and shows date/time values as “2025-03-25T17:30:15+0100”. The first part before the T is the date in ISO format. The T introduces the time which includes the hour, minute and second. The last part of the UTC offset: the difference between UTC and the time in the timezone. To show data/time in ISO format, you can use {lubridate}’s format_ISO8601() function. The function’s first argument is the date/time value. The second specifies if there the output show the timezone offset. To use UTC, usetz = \"Z\". The default is FALSE. The last argument, the precision allows you to set the precision of the output, e.g. day (show ydm), hour (include ymd as well as the hour). Let’s see what this function shows for dat_tim_1:\n\nlubridate::format_ISO8601(dat_tim_1)\n\n[1] \"2025-03-25T17:30:00\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = TRUE)\n\n[1] \"2025-03-25T17:30:00+0100\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = TRUE, precision = \"ymdh\")\n\n[1] \"2025-03-25T17+0100\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = FALSE, precision = \"ymdh\")\n\n[1] \"2025-03-25T17\"\n\n\n\n\n3.1.5.2.3 Extracting information from date/times\nWith dates, you can extract the week, day, month, … . With date/time values, you can also extract the hour using hour(), the minute (minute()) and second (second()) as well as the time zone (tz()). These functions have no other arguments than the date/time value:\n\nlubridate::hour(dat_tim_1)\n\n[1] 17\n\nlubridate::minute(dat_tim_1)\n\n[1] 30\n\nlubridate::second(dat_tim_1)\n\n[1] 0\n\n\n\n\n3.1.5.2.4 Calculations using times\nAs times are stored as numbers, you can add and subtract seconds. For instance, to determine the time 100000 seconds after 2025-03-25 17:30:15, you can use the add that number to the time variable:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\ntime_2 &lt;- time_1 + 100000\ntime_2\n\n[1] \"2025-03-26 21:16:55 UTC\"\n\n\nLikewise, if you want to subtract one million seconds:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\ntime_3 &lt;- time_1 - 1000000\ntime_3\n\n[1] \"2025-03-14 03:43:35 UTC\"\n\n\nRecall from the dates section that you can add time using the number of years, months and days. For time, you can add the number of hours, minutes, seconds (and even milli-, micro-, nano- and picoseconds). Suppose that you want to add 36 hours, 55 minutes and 30 seconds to time_1 (025-03-25 17:30:15):\n\ntime_4 &lt;- time_1 + lubridate::hours(36) + lubridate::minutes(55) + lubridate::seconds(30)\ntime_4\n\n[1] \"2025-03-27 06:25:45 UTC\"\n\n\nIn addition, you can round times. We already covered these function for dates: round_date(), floor_date()and ceiling_date(). These functions also allow you to use “second”, ’minute” or “hour” as unit. Note that in this case, you don’t need to add the week option. To illustrate, we’ll use time_1 (2025-03-25 17:30:15). This value doesn’t have any sub-second units, so we”ll round to the nearest hour and minute:\n\nlubridate::round_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 18:00:00 UTC\"\n\nlubridate::round_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:30:00 UTC\"\n\n\nAs you can see from the output, the nearest hour is 18:00:00, the nearest minuut is 17:30. Using floor_date()and ceiling_date() rounds down and up to the nearest hour or minute:\n\nlubridate::floor_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 17:00:00 UTC\"\n\nlubridate::floor_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:30:00 UTC\"\n\nlubridate::ceiling_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 18:00:00 UTC\"\n\nlubridate::ceiling_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:31:00 UTC\"\n\n\nRounding down to the hour rounds to 17:00:00 and down to the minute to 17:30:00 while rounding up to the hour shows 18:00:00 and to the minute shows 17:31:00.\n\n\n3.1.5.2.5 Time spans\nWe met time spans at the date level. You can also work with intervals and periods at the time level. For instance, let’s define interval of one lecture:\n\nlec_start &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\", tz = \"Europe/Brussels\")\nlec_ends &lt;- lubridate::ymd_hms(\"2025-03-25T19:30:15\", tz = \"Europe/Brussels\")\nlec_int &lt;- lubridate::interval(start = lec_start, end = lec_ends, tzone = \"Europe/Brussels\")\nlec_int\n\n[1] 2025-03-25 17:30:15 CET--2025-03-25 19:30:15 CET\n\n\nAs you could with dates, you can now calculate the length\n\nlubridate::time_length(lec_int, unit = \"hour\")\n\n[1] 2\n\nlubridate::time_length(lec_int, unit = \"minute\")\n\n[1] 120\n\nlubridate::time_length(lec_int, unit = \"second\")\n\n[1] 7200\n\n\nNote that where, the time zone matters. Let’s see what happens if we leave the time zone out of our interval when we set the start and end time, but add it when we determine the interval:\n\nlec_starttz &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\nlec_endstz &lt;- lubridate::ymd_hms(\"2025-03-25T19:30:15\")\nlec_inttz &lt;- lubridate::interval(start = lec_starttz, end = lec_endstz, tzone = \"Europe/Brussels\")\nlec_inttz\n\n[1] 2025-03-25 18:30:15 CET--2025-03-25 20:30:15 CET\n\n\nAs you can see, the times are pushed forward one hour. This is because we set the start and end time without a reference to a time zone and time is measured in UTC. As we define the interval including the time zone, {lubridate} updates the time to Europe/Brussels, which is UTC+01:00 and shows 18:30 and 20:30.\nDefining a period with times follows a similar approach as a period with dates. Using {lubridate}’s period() function, we can set a period equal to, e.g. 3 hours, 30 mintues and 15 seconds:\n\nper &lt;- lubridate::period(hours = 3, minutes = 30, seconds = 15)\nper\n\n[1] \"3H 30M 15S\"\n\n\nWe can now use that period to determine an end time for given start time, e.g;\n\nlec_endspe &lt;- lec_start + per\nlec_endspe\n\n[1] \"2025-03-25 21:00:30 CET\"\n\n\nAgain note that it is important that you specify the time zone. Here we used lec_start which was defined referring to the Europe/Brussels time zone. If you wouldn’t include that reference, the end result would also by in UTC:\n\nlec_endstz &lt;- lec_starttz + per\nlec_endstz\n\n[1] \"2025-03-25 21:00:30 UTC\"\n\n\n{lubridate}’s duration() function calculate the number of seconds in a given time span. You can define the time span using second, minute, hour, day, week, month, year). For instance, how many seconds are there in a time span of 2 weeks, 10 days, 21 hours, 6 minutes and 25 seconds?\n\ndur_1 &lt;- lubridate::duration(week = 2, day = 10, hour = 21, minute = 6, second = 25)\ndur_1\n\n[1] \"2149585s (~3.55 weeks)\"\n\n\nYou can now use that duration to, e.g. determine when an activity started if it ended on 2025-03-25 17:30:15:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25 17:30:15\")\ntime_2 &lt;- time_1 - dur_1\ntime_2\n\n[1] \"2025-02-28 20:23:50 UTC\"\n\n\nHere you wee when an activity started given iets end date and duration.\nBefore we continue, we’ll remove some variables.\n\nrm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1, end_time2, start_time, lec_int, lec_inttz, lec_start, lec_starttz, lec_endspe, open, dur_1, lec_ends, lec_endspe, dat_tim_1, dat_tim_lub, dat_tim_1ct)\n\nWarning in rm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1,\n: object 'end_time' not found\n\n\nWarning in rm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1,\n: object 'end_time1' not found\n\n\nWarning in rm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1,\n: object 'end_time2' not found\n\n\nWarning in rm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1,\n: object 'open' not found\n\n\nWarning in rm(time_1, time_1_num, time_2, time_3, time_4, end_time, end_time1,\n: object 'lec_endspe' not found",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types and structures</span>"
    ]
  },
  {
    "objectID": "03_Data_types_and_structures.html#data-structures",
    "href": "03_Data_types_and_structures.html#data-structures",
    "title": "3  Data types and structures",
    "section": "3.2 Data structures",
    "text": "3.2 Data structures\nWe now move to data structures. In the previous section, the focus was on individual numbers, booleans, strings and date/times. Data structures hold multiple values. R knows the following data structures:\n\nvectors\nfactors\nmatrix\narray\nlists\ndataframes\ntibbles\n\n\n3.2.1 Vectors\n\n\n3.2.2 Creating a vector\nA vector is a one dimensional data structure: it has one row and n columns. To create a vector we use the c() function to combine the elements within this function in one data structure. Let’s create a vector with numbers:\n\nvec_num &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\nYou can see the total number of elements in this vector using the length() function:\n\nlength(vec_num)\n\n[1] 10\n\n\nHere, we nave a total of 10 columns. You can see in the environment pane that the vector is a 1x10 vector: 1 row and 10 columns.\nIf you check the type of this vector, you’ll see that its type is “double”. In other words, it is a numeric vector.\n\ntypeof(vec_num)\n\n[1] \"double\"\n\n\nYou can check if an object is a vector using\n\nis.vector(vec_num)\n\n[1] TRUE\n\n\nIn addition, you can check if a vector is of a given type by including the mode (numeric, logical, …) in is.vector():\n\nis.vector(vec_num, mode = \"numeric\")\n\n[1] TRUE\n\n\nThe type of vec_num is “double”. You can create a vector for other data types:\n\nvec_char &lt;- c(\"cat\", \"mouse\", \"dog\", \"bird\")\nvec_log &lt;- c(TRUE, FALSE, TRUE, TRUE)\nvec_int &lt;- c(1L, 10L, 50L)\nvec_dat &lt;- c(as.POSIXct(\"2025-03-25\"), as.POSIXct(\"2025-04-25\"))\n\ntypeof(vec_char)\n\n[1] \"character\"\n\ntypeof(vec_log)\n\n[1] \"logical\"\n\ntypeof(vec_int)\n\n[1] \"integer\"\n\ntypeof(vec_dat)\n\n[1] \"double\"\n\n\nThe type of a vector is the type common to all individual elements. In other words, a vector only holds elements of the same type. If this is not the case, R will change the type of all elements in the vector to a type that fits all. For instance, suppose that we have a vector\n\nvec_1 &lt;- c(1, \"2\", 3)\n\nHere, we mix two numeric values with 1 character value “2”. If you take a look at this vector, you’ll see that R changes all elements in characters:\n\nvec_1\n\n[1] \"1\" \"2\" \"3\"\n\n\nYou can verify this by checking the type\n\ntypeof(vec_1)\n\n[1] \"character\"\n\n\nAs you can see, vec_1 is not a numeric vector, but a character vector. Let’s take another example:\n\nvec_2 &lt;- c(TRUE, FALSE, 5, as.POSIXct(\"2025-03-25\"))\nvec_2\n\n[1]          1          0          5 1742857200\n\n\nIn this example, we have a mix of logical values (TRUE, FALSE), a numeric value and a date/time value. R uses a common type and sets TRUE equal to 1 and FALSE equal to 0 and show the number of seconds since January 1, 1970. In other words, R forces the vector into a double vector.\n\ntypeof(vec_2)\n\n[1] \"double\"\n\n\nLet’s see what happens if we mix logical, character and numeric values:\n\nvec_3 &lt;- c(TRUE, FALSE, \"a\", 5)\nvec_3\n\n[1] \"TRUE\"  \"FALSE\" \"a\"     \"5\"    \n\n\nHere, from the quotation marks, you can see that R changes the type of all individual elements into character values. These three examples are examples of implicit coercion: R tries to find a way to represent the elements in a vector using a common type. Sometimes, this implicit coercion makes sense, sometimes it doesn’t. For instance, combining a numeric value and a character representation of a numeric value creates a character vector. The reason why R changes numbers into characters is that usually, you can represent a number as a character, while you can not always represent a character as a number.\nYou can coerce the type of a vector using an as. function: as.numeric(), as.integer(), as.character(), as.logical() or as.Date() or as.POSIXct(). Here, the coercion is explicit. In that case, R will try to change all elements into the same type. In case this is impossible, R produces NA’s. For instance, let’s try to change the three vectors vec_1, vec_2 and vec_3 in numeric:\n\nas.numeric(vec_1)\n\n[1] 1 2 3\n\nas.numeric(vec_2)\n\n[1]          1          0          5 1742857200\n\nas.numeric(vec_3)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA  5\n\n\nFor vec_1and vec_2 R could change all the elements in type numeric: in vec_1 R managed to change the character “2” in a number 2. The same holds for vec_2. Here R could change the type of TRUE and FALSE in a number format, 1 and 2 and set the date/time variable in numeric format. For vec_3, changing all elements in numeric was impossible. As a matter of fact, with the exception of the number 5, R didn’t manage to change the type at all. Why couldn’t R change “TRUE” or “FALSE” in 1 and 0 as it could in vec_2. Here, TRUE and FALSE were character values, not boolean. R didn’t manage to change the “TRUE” into a boolean TRUE and then into a number. As this was not possible, it replaced that value with an NA.\nYou can change an object (e.g. a column in a data frame) into a vector using the as.vector() function. This function takes two arguments: the object that you want to convert into a vector and the vector type. For instance\n\nvec_4 &lt;- as.vector(vec_1, mode = \"numeric\")\nvec_4\n\n[1] 1 2 3\n\n\ncreates a numeric vector from vec_1. Note that here this operation was not as useful as vec_1 is a numeric vector. However, in later chapters we will convert variables or column in a date frame in vectors.\nSo far, all vectors were created using c() and including all elements in this function. Using the vector(type, length = ) function, you can create an empty vector of a given length and type. For instance, to create an empty numeric vector of length 10:\n\nvec_1 &lt;- vector(\"numeric\", length = 10)\nvec_1\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nAs you can see, this vector is filled with 0. Note that this is a numeric vector but only for now. If you would change one of its elements in a character, the full vector would change from numeric into character. If you want to create an empty character vector:\n\nvec_2 &lt;- vector(\"character\", length = 10)\nvec_2\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nHere, you can see that empty is a space (recall that a space of a character).\nCreating a vector with “0” values can be very useful before a for loop. Suppose that you have a for loop where each ‘loop’ adds the result of a calculation to a vector. Here, you have two option. First, you allow the vector to ‘grow’ in every loop. Second, you define an empty vector with the same length as the number of loops and you fill each element as you run through the loop. The second option is not very efficient as R will copy the entire vector you have each time you expand it with one element. This is not the case if you create the vector before the loop. Here, R fills one element after the other but doesn’t need to grow the vector.\nRecall that NA are missing observations. If a vector includes NA values, that will not change the vector’s type. To see this, let’s create two vectors, one numeric and one character, which both include NA and show their type:\n\nvec_1 &lt;- c(10, 30, NA, 40)\nvec_2 &lt;- c(\"dog\", NA, \"cat\")\ntypeof(vec_1)\n\n[1] \"double\"\n\ntypeof(vec_2)\n\n[1] \"character\"\n\n\nThe same hold for NaN (not a number ) and Inf (infinity). If these and NA are part of a character vector, they will become character values “NA”, “Nan” of “Inf”. In other words, they’ll be considered characters and not special values.\n\nrm(vec_1, vec_2, vec_3)\n\n\n\n3.2.3 Creating a vector by replicating elements of another vector\nUsing rep(x, times, length.out, each) you can replicate the values in a vector x. Suppose you want a vector repeat a value 10 times. The first argument is the values yo want to replicate. This can be any value: number, character, a vector … . The second to last argument determines how many times or how x needs to be replicated. To create a vector with 10 columns and all values equal to 25\n\nvec_rep &lt;- rep(x = 25, times = 10)\n\nHere, x was a number, but you can also replicate characters or other vectors:\n\nvec_rep_char &lt;- rep(x = \"ABC\", times = 5)\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), times = 5)\nvec_rep_char\n\n[1] \"ABC\" \"ABC\" \"ABC\" \"ABC\" \"ABC\"\n\nvec_rep_vec\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3\n\n\nlength.out sets the length of the vector. If x is a single numeric, character, date/time using length.out and times is equivalent. If x is a vector, this is not the case. In the previous example, c(1, 2, 3) was replicated 5 times. In other words, the length of the output vector was 15. Using length.out you can set the total length. In doing so, R will replicate the vector, but will do so only partially on the last replication. For instance, if you set the length.out = 10, the length of the output vector is 10:\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), length.out = 10)\nvec_rep_vec\n\n [1] 1 2 3 1 2 3 1 2 3 1\n\n\nWith times and length.out you replicate the full vector on every replication. Using each you replicate each element of the vector each times. In other words, the output vector will show the first element of the input vector each times before it changes to the second element of the input vector.\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), each = 3)\nvec_rep_vec\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nAdding length.out sets a limit on the total length of the output vector. It does so by reducing the number of replications of the last element in the input vector:\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), length.out = 7, each = 3)\nvec_rep_vec\n\n[1] 1 1 1 2 2 2 3\n\n\n\n\n3.2.4 Named vectors\nYou can define names for the columns of a vector. You can do so when you create the vector or at a later stage using the setNames() function. Suppose that you have a vector with exam results for three courses, A, B and C. Using a named vector, allows you to identify the columns:\n\nvec_1 &lt;- c(A = 15, B = 13, C = 17)\n\nThe vector now includes column names. You can see that this is the case in the environment pane where vec_1 is now identified as a Numed num [1:3]. These columns are also included if you ask R to show the vector:\n\nvec_1\n\n A  B  C \n15 13 17 \n\n\nsetNames() needs a vector with values and a vector with names. Using the previous example:\n\nvec_2 &lt;- setNames(c(15, 13, 17), c(\"A\", \"B\", \"C\"))\n\nIn a final example, we’ll use the names() function to add names after the vector was created. Let’s first create a vector:\n\nvec_3 &lt;- c(15, 13, 17)\n\nTo add names, we include them in a another vector and use names() to assign names to vec_3:\n\nnames(vec_3) &lt;- c(\"A\", \"B\", \"C\")\nvec_3\n\n A  B  C \n15 13 17 \n\n\nYou can use the names() function to get the names of a vector:\n\nvar_names &lt;- names(vec_1)\nvar_names\n\n[1] \"A\" \"B\" \"C\"\n\n\nDoing so allows you to store these names in a character vector that you can use in your work flow. With many columns, you can see the names using e.g. str():\n\nstr(vec_1)\n\n Named num [1:3] 15 13 17\n - attr(*, \"names\")= chr [1:3] \"A\" \"B\" \"C\"\n\n\nYou can see that names are defined as an attribute. In other words, you can also see the names of columns if you ask R to show the attibute “name”:\n\nattr(vec_1, \"names\")\n\n[1] \"A\" \"B\" \"C\"\n\n\nTo remove the names of columns, you can use unname(obj, force = FALSE). The first arguments is the object (e.g. vector) whose names you want to remove; the second is a specific option to remove names even if the object is a dataframe. You can usually keep the default value FALSE.\n\nvec_3 &lt;- unname(vec_3)\nvec_3\n\n[1] 15 13 17\n\n\n\n\n3.2.5 Subsetting a vector\n\n3.2.5.1 Subsetting an unnamed vector\nIf you subset a vector, you select one or more columns of a vector (to possibly store them in a new one). Here, we will use the numeric vector vec_num. Note that you can apply most of these ways to subset to other vector types. There are two subset operators: [] and [[ ]].\n\n3.2.5.1.1 Subsetting by position\nTo access an individual element of a vector, you include its position (or index number) between square brackets of the subscript operator [] after the name of the vector. In R, vector indexing starts at 1. In other words, the first element of a 1x10 vector is at position 1, the second at position 2, … This is not always the case. In Python for instance, the first element of a vector is at position 0, the second at position 1, …\nLet’s look at the element 5 of the element in the fifth column of vec_num:\n\nvec_num[5]\n\n[1] 3\n\n\nIf you want to extract that element to use it in part of your code, you would assign it to a different vector using the &lt;- operator:\n\na &lt;- vec_num[5]\na\n\n[1] 3\n\n\nNote that subsetting leaves the original vector intact. If you subset a vector, you copy the value in a new vector, but that value stays in the original vector.\nYou can subset more than one column. Suppose that you want to subset columns 1 to 4. To do so, you can use 1:4 within the subscript operator:\n\nvec_num[1:4]\n\n[1] 0 1 1 2\n\n\nAgain, you could assign this new vector. Here, this new vector would have 1 row and 4 columns. These 4 columns would be equal to the first 4 columns of the original vector.\nThe third way to access elements in a vector using their position is to combine these position via the c() function within the subsetting operator. The c() function allows you to define the columns you need. The subscript operator will then access these columns and extract their value. Suppose that you want to extract the elements in columns 1 and 4. Note that here, you will extract to columns: 1 and 4. In the previous example you extracted 4 columns: 1 to 4 or column 1, 2, 3, and 4. To extract columns 1 and 4 you need to include those position in the c() function: c(1, 4) and use:\n\nvec_num[c(1, 4)]\n\n[1] 0 2\n\n\nNote that you can mix various ways to subset a vector. For instance, if you need the first to third, fifth and seventh to last element, you can combine the various way to subsetting the vector:\n\nvec_num[c(1:3, 5, 7:10)]\n\n[1]  0  1  1  3  8 13 21 34\n\n\n\n\n3.2.5.1.2 Subsetting using negative positions\nYou can also use negative numbers for the index elements. In that case, R will show all elements, except those in the negative index (negative index range). For instance,\n\nAccessing all elements except for the first:\n\n\nvec_num[-1]\n\n[1]  1  1  2  3  5  8 13 21 34\n\n\n\nAccessing all elements except for the first, second, third and fourth\n\n\nvec_num[-1:-4]\n\n[1]  3  5  8 13 21 34\n\n\n\nAccessing all elements except for the second and fourth\n\n\nvec_num[(c(-2, -4))]\n\n[1]  0  1  3  5  8 13 21 34\n\nvec_num[-c(2, 4)]\n\n[1]  0  1  3  5  8 13 21 34\n\n\n\n\n3.2.5.1.3 Subsetting by using a logical vector\nThe fourth way to subset columns in a vector uses a logical vector of the same length as the vector to subset. To see how this works, let’s first define two vectors: one numeric and one logical:\n\nvec_1 &lt;- c(1, 2, 3, 4, 5)\nvec_log &lt;- c(TRUE, FALSE, FALSE, FALSE, TRUE)\n\nYou can now subset vec_1 using vec_log:\n\nvec_1[vec_log]\n\n[1] 1 5\n\n\nIf the value on position x in vec_log is “TRUE”, the result of vec_1[vec_log] is equal to the value in the xth column of vec_1. This is the case for the first and last value. If vec_log’s yth element is false, vec_1’s yth element is not extracted.\nIn the example, we defined the logical vector ourselfs. However, there are many other ways to create such a vector. Recall that the outcome of any boolean operation is either TRUE or FALSE. Applying a boolean operation to every column of a vector creates a logical vector of the same length as the vector where the operation was applied to. You can now select those columns that meet that condition. For instance, suppose you want to work with the elements of vec_num who are larger than 5. There are two ways to do so. First, you create a logical vector of the same length as vec_num where an element is TRUE is the element in vec_num on the same position meets the condition and false otherwise. To create that vector, you use logical vector &lt;- original vector + condition. Doing so, the logical vector will have the same length as the vector whose elements you want to extract.\n\ncond &lt;- vec_num &gt; 5\ncond\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNow we have a logical vector cond whose values are TRUE if the element in the same position in vec_num meets the condition (&gt; 5) and FALSE otherwise. We can now use this vector to subset vec_num:\n\nvec_num[cond]\n\n[1]  8 13 21 34\n\n\nHere, the TRUE-FALSE elements of cond are used to subset vec_num. Is an element in cond is TRUE, vec_num[cond] extracts that element from vec_num. If the element in cond is FALSE, the element in the same position in vec_num is not extracted.\nThe second option to use a condition is shorter and uses the condition within the subscript operator:\n\nvec_num[vec_num &gt; 5]\n\n[1]  8 13 21 34\n\n\nNote that you can use more than one boolean operator. For instance extracting all elements larger than 3 and not equal to 13 can be done using:\n\ncond &lt;- vec_num &gt; 3 & vec_num != 13\nvec_num[cond]\n\n[1]  5  8 21 34\n\n\nor\n\nvec_num[vec_num &gt; 3 & vec_num != 13]\n\n[1]  5  8 21 34\n\n\nNote that you can use these conditions also in the case of character vectors. For instance, to see if “cat” and “dog” are values in vec_char:\n\nvec_char[vec_char == \"dog\" | vec_char == \"cat\"]\n\n[1] \"cat\" \"dog\"\n\n\nIf you don’t know the exact location and you don’t have an explicit condition that you can use, but you know which values you want to extract, you can use the %in% operator. Here, you first define a vector with values, e.g. 1, 8 and 143 using c(1, 8, 143). Using the %in% operator, you can now subset the vector vec_num:\n\nvec_num %in% c(1, 8, 143)\n\n [1] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\n\nThis code extract all elements from vec_num that are also in c(1, 8, 143). The result of this operation is a logical vector with the same length as vec_num where the elements equal TRUE is an element in vec_num is included in c(1, 8, 143) and FALSE otherwise. This vector now allows you to extract these elements using, e.g. \n\nvec_num[vec_num %in% c(1, 8, 143)]\n\n[1] 1 1 8\n\n\nAs an alternative, you can do so in two steps:\n\ncond &lt;- vec_num %in% c(1, 8, 143)\nvec_num[cond]\n\n[1] 1 1 8\n\n\nHere we included the elements to extract in a c() function. However, you can use any other vector.\n\na &lt;- c(1, 8, 143)\nvec_num[vec_num %in% a]\n\n[1] 1 1 8\n\n\nNote that you can include values in %in% c() which are not part of the vector. If this is the case, R won’t find them. Hence, adding them wouldn’t change the outcome. If no element in the original vector matches the condition, R will output numeric(0) or a vector of length 0.\nTo illustrate this and the use of %in% for a character vector, suppose you want to extract “dog”, “fish” from vec_char:\n\nvec_char[vec_char %in% c(\"dog\", \"fish\")]\n\n[1] \"dog\"\n\n\nAs “fish” is not included in vec_char but “dog” is, R outputs “dog”.\nBoolean operators allow you define many conditions. For instance, if you have a vector that includes missing values, you can extract all non missing values using !is.na() or is not (!) NA:\n\nvec_1 &lt;- c(1, 20, 20, NA, NA, 50)\nvec_1[!is.na(vec_1)]\n\n[1]  1 20 20 50\n\n\nAs a second example, suppose that you want to extract all even numbers. Recall that a number of even if modulus after division by 2 is zero:\n\n10 %% 2\n\n[1] 0\n\n11 %% 2\n\n[1] 1\n\n\nYou can use this to create a condition\n\n10 %% 2 == 0\n\n[1] TRUE\n\n11 %% 2 == 0\n\n[1] FALSE\n\n\nthat you can use to subset elements of a vector:\n\ncond &lt;- vec_num %% 2 == 0\nvec_num[cond]\n\n[1]  0  2  8 34\n\n\nAs a third example, recall that you can use grepl() or stringr::string_detect() if a pattern occurs in a string. If the is the case, these function output TRUE. Suppose you have a character vectors\n\nvec_char &lt;- c(\"sales_shoes\", \"sales_trousers\", \"sales_shirts\", \"sales_jackets\")\n\nand you want to extract the column which includes “shoes”. Using grepl() you can identify the elements in the character vector that include the word “shoes”:\n\ngrepl(pattern = \"shoes\", x = vec_char)\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nYou can use this function to extract the element “sales_shoes” from vec_char.\n\nvec_char[grepl(pattern = \"shoes\", x = vec_char)]\n\n[1] \"sales_shoes\"\n\n\n\n\n3.2.5.1.4 Index positions\nIn all examples we either used an exact index position or a logical vector to extract the values of a vector. What if you are not interested in a value but in an index position? To show an index position rather than its value or TRUE or FALSE, you can use the which() function. For instance, suppose you want to know the position of value 1 in vec_num. To find this position, you can use\n\nwhich(vec_num == 1)\n\n[1] 2 3\n\n\nThe result shows the index positions where you can find 1 in vec_num. Note that you can save the output in a new vector with positions. You can now subset that vector to find the first occurrence. As an alternative, as which() outputs a vector, you can find the first occurrence subsetting the which() function. For instance, to find the first 1 in vec_num:\n\nwhich(vec_num == 1)[1]\n\n[1] 2\n\n\nWhat is you want to find multiple values. Here, you can use the %in% operator. Suppose you want to know the position of the values 1, 2, 8 and 55. First you collect these values in a vector using c(1, 2, 8, 55). You can now use that vector in the which() function:\n\nwhich(vec_num %in% c(1, 2, 8, 55))\n\n[1] 2 3 4 7\n\n\nwhich() shows every occurrence. Using match() you can find the first occurrence. For instance, the first occurrence of “1” in vec_num is in position\n\nmatch(1, vec_num)\n\n[1] 2\n\n\nUsing which() allows you to extract the positions of e.g. missing values. Suppose you have a vector vec_1 which includes missing values:\n\nvec_1 &lt;- c(10, 10, 20, NA, 30, 40, NA, 50, 50)\n\nTo locate these missing value, you can use\n\nwhich(is.na(vec_1), vec_1)\n\n[1] 4 7\n\n\nThere are two variants of the which() function that allow you to find the location of the (first) maximum or minimum values: which.max() and which.min:\n\nwhich.max(vec_1)\n\n[1] 8\n\nwhich.min(vec_1)\n\n[1] 1\n\n\nUsing locigal values, you can find the first occurrence of specific value. Here, which.max() uses the fact that TRUE = 1 and FALSE = 0. In other words, this function will show the first occurrence of TRUE:\n\nwhich.max(vec_1 &gt; 30)\n\n[1] 6\n\n\n\n\n\n3.2.5.2 Subsetting an named vector\nWith a named vector, you can also use the column names to subset. Suppose that you have a vector\n\nvec_1 &lt;- c(A = 10, B = 30, C = 50, D = 70)\n\nFirst you can use the ways you would use to subset an unnamed vector, e.g. \n\nvec_1[3]\nvec_1[2:4]\nvec_1[vec_1 &lt; 50]\n\nAs you can see using [] preserves the structure of the vector: the output shows both the column name as well as its value.\nYou can also use the name of the column to subset the vector using vec_1[\"column name\"]:\n\nvec_1[\"A\"]\n\n A \n10 \n\n\nThe output shows both the column name as well as the value. In other words, here too, the structure of the vector is preserves.\nTo subset more than one column, you can use\n\nvec_1[c(\"A\", \"D\")]\n\n A  D \n10 70 \n\n\nTo extract the value, you have refer to the column using subsetting operator [[]]. You can do so using both the column name or number. These lines extract the value for the second column\n\nvec_1[[\"B\"]]\n\n[1] 30\n\nvec_1[[2]]\n\n[1] 30\n\n\nThe output shows the value without the column. The [[]] operator simplyfies the structure of the vector: it returns the simpelst possible data structure: here this is the value of the column.\nYou can also subset column whose name includes a pattern. Recall that names() allow you to extract the names of the columns in a named vector. Using grepl() you can check if these names include a pattern. For instance, let’s check if the names of vec_1 include “A”. Using grepl():\n\ngrepl(pattern = \"A\", x = names(vec_1))\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nTo extract that column, you include that statement in vec_1[]:\n\nvec_1[grepl(pattern = \"A\", x = names(vec_1))]\n\n A \n10 \n\n\nThe result shows the name of the column and its value.\n\n\n\n3.2.6 Adding, removing and changing elements to a vector\n\n3.2.6.1 Adding elements to a vector\nAs in the previous section, I’ll use a numeric vector here, but you can apply the rules also to other types of vectors. Suppose that you have the 1x10 vector vec_num and you want to add a column with the value 55. The first way to do so is to use the c() function to create a new vector\n\nc(vec_num, 55)\n\n [1]  0  1  1  2  3  5  8 13 21 34 55\n\n\nIn this way you can add multiple columns and or multiple vectors:\n\nc(vec_num, c(55, 89, 144), c(233, 377, 610))\n\n [1]   0   1   1   2   3   5   8  13  21  34  55  89 144 233 377 610\n\n\nc() adds all elements in the order in which they appear in the function:\n\nc(c(610, 377, 233), c(144, 89, 55), vec_num)\n\n [1] 610 377 233 144  89  55   0   1   1   2   3   5   8  13  21  34\n\n\nNote that this doesn’t change the vec_num. c()creates a new vector. If you want to change vec_num you have to reassign it to the new vector. As an alternative, you can assign the new vector to a new object:\n\nvec_1 &lt;- c(vec_num, c(55, 89, 144), c(233, 377, 610))\nvec_1\n\n [1]   0   1   1   2   3   5   8  13  21  34  55  89 144 233 377 610\n\n\nIf you have a named vector, you can add a new named vector:\n\nvec_1 &lt;- c(A = 10, B = 30, C = 50, D = 70)\nc(vec_1, c(E = 90))\n\n A  B  C  D  E \n10 30 50 70 90 \n\n\nYou can also use the append() function to add new elements. By default, append will add an element after the last element in the existing vector. In other words, by default, append() is similar to c(). However, the arguments in the append(vector, value, after = length(x)) allow you to change that default position. If you want to add the new element after position 3, you can add this by changing the default length(x)in 3. Note that append()doesn’t change the original vector:\n\nappend(vec_num, 55)\n\n [1]  0  1  1  2  3  5  8 13 21 34 55\n\nvec_num\n\n [1]  0  1  1  2  3  5  8 13 21 34\n\n\nIf you want to change the original vector, you have to reassign it to its new values or assign the outcome to a new object:\n\nvec_1 &lt;- append(vec_num, 144)\nvec_1\n\n [1]   0   1   1   2   3   5   8  13  21  34 144\n\n\nTo add the value 88 as the first element or 143 after column 9, you can change the default location in append()’s after = argument:\n\nappend(vec_num, 88, after = 0)\n\n [1] 88  0  1  1  2  3  5  8 13 21 34\n\nappend(vec_num, 143, after = 9)\n\n [1]   0   1   1   2   3   5   8  13  21 143  34\n\n\nUsing the c() function, you can add multiple elements. For instance, if you want to add 88 and 143 as the first two columns of vec_num you combine these two values within c() and include them in the append statement:\n\nappend(vec_num, c(88, 143), after = 0)\n\n [1]  88 143   0   1   1   2   3   5   8  13  21  34\n\n\nNote that you can change the position where these new values are added. However, all elements are added after the same position and their position follows their position within the c() function. Note also that, if you add an element whose type of different from the vector type, R will change the vector type.\nYou can also add a named vector\n\nappend(vec_1, c(E = 50), after = 0)\n\n  E                                             \n 50   0   1   1   2   3   5   8  13  21  34 144 \n\n\n\n\n3.2.6.2 Removing elements from a vector\nThere are multiple ways to remove elements from a vector. We already covered two. First, if you know the position of the elements you want to remove, you can use a negative index. Recall that a negative index allows you to extract the elements of a vector except those included in the negative index. For instance, if you want to remove the first 4 columns of vec_num you can do this using\n\nvec_num[-1:-4]\n\n[1]  3  5  8 13 21 34\n\n\nTo remove column 1 and 4 (but not 2 and 3):\n\nvec_num[-c(1, length(vec_num))]\n\n[1]  1  1  2  3  5  8 13 21\n\n\nor\n\nvec_num[c(-1, -length(vec_num))]\n\n[1]  1  1  2  3  5  8 13 21\n\n\nYou can use this approach if you know the exact location (i.e. the columns) who want to remove.\nThe second way to remove elements uses a condition. For instance, the code to remove all elements larger than 3 and not equal to 0 is\n\nvec_num[!vec_num &gt; 3 & vec_num != 0]\n\n[1] 1 1 2 3\n\n\nor, using a specific vector including the condition:\n\ncond &lt;- !vec_num &gt; 3 & vec_num != 0\nvec_num[cond]\n\n[1] 1 1 2 3\n\n\nYou can use this approach if you know the condition that elements need to meet.\nIf you want to remove known values from a vector, e.g. 1, 8 and 143, you can use an approach which is very similar to the one you used to subset these elements. First, you collect them in a vector c(1, 8, 143). Second, you use %in% and not (!) to remove these elements:\n\ncond &lt;- vec_num %in% c(1, 8, 143)\nvec_num[!cond]\n\n[1]  0  2  3  5 13 21 34\n\n\nor, in one line of code\n\nvec_num[!vec_num %in% c(1, 8, 143)]\n\n[1]  0  2  3  5 13 21 34\n\n\nIn the last statement, 143 was included in the vector with values to remove but is not in vec_num. R doesn’t check if all values to be removed are also in the vector where they need to be removed.\n\n\n3.2.6.3 Changing elements in a vector\nSuppose that you know which column you want to change in your vector, e.g. you want to change the value in 4th column. To do this, you first subset that element using vec_num[4] and your reassign its value. For instance, changing the fourth element to 250:\n\nvec_num[4] &lt;- 250\nvec_num\n\n [1]   0   1   1 250   3   5   8  13  21  34\n\n\nAs you can see, fourth element is now 250. Note that the new value needs to be of the same type as the vector. If that is not the case, you”ll change the type of all other elements in the vector. For instance\n\nvec_num[4] &lt;- \"250\"\n\nchanges the type of the vector from double to character:\n\ntypeof(vec_num)\n\n[1] \"character\"\n\n\nIn that case, you have to change the vector’s type:\n\nvec_num &lt;- as.numeric(vec_num)\n\nUsing replace() you can change many values in a vector. Suppose you want to change columns 1, 8 and 10 in 50, 100, 150. The first argument in the replace() function is the vector you want to change. Here, this is vec_num. The second argument is a vector with index position. Using c(1, 8, 10) you can fix these position. The last argument is a vector with the values that will be used to replace the values in the index positions. Here you would use c(50, 100, 150). Using these in the replace() function:\n\nreplace(vec_num, c(1, 8, 10), c(50, 100, 150))\n\n [1]  50   1   1 250   3   5   8 100  21 150\n\n\nNote that the length of the index vector and the length of the vector with new values should be equal. If this is not the case, R will show an error:\n\nreplace(vec_num, c(1, 8, 10), c(50, 100, 150, 200))\n\nWarning in x[list] &lt;- values: number of items to replace is not a multiple of\nreplacement length\n\n\n [1]  50   1   1 250   3   5   8 100  21 150\n\n\nIf you want to replace all values that meet a certain condition with one single value, you can use the replace() function as well. Suppose you want to change all values larger than 25 with 50. Using recplace() you could do this with:\n\nreplace(vec_num, vec_num &gt; 25, 50)\n\n [1]  0  1  1 50  3  5  8 13 21 50\n\n\n\nrm(vec_1, vec_2, vec_3, vec_char, vec_dat, vec_int, vec_log)\n\n\n\n3.2.6.4 Sorting vectors\nTo sort a vector, R includes the sort(x, decreasing = FALSE, na.last = NA) function. Here, x is the vector to sort. By default, R sorts in increasing order. The last argument includes the treatement of “NA” values. By default, they are removed. Using TRUE missing values are retained, but added last. FALSE shows these values first.\n\nsort(x = vec_num, decreasing = FALSE)\n\n [1]   0   1   1   3   5   8  13  21  34 250\n\n\nCharacter vectors are sorted alphabetically by default:\n\nsort(x = c(\"zoo\", \"Zoo\", \"coast\", \"coAst\", \"cOAst\", \"lake\"))\n\n[1] \"coast\" \"coAst\" \"cOAst\" \"lake\"  \"zoo\"   \"Zoo\"  \n\n\nAs you can see, if the strings include copies where one includes a capital letter and the other one doesn’t, R orders those with the least capital letters first.\n\n\n\n3.2.7 Functions that generate a vector\n\n3.2.7.1 Generating a sequence\nTo create a vector, we used c() and included all its values. Some functions allow you to create a special vector. seq() allows you to fill a vector with a sequence of numbers. To do so, this function requires a start point (from), and endpoint (to) and either the increment of the sequence (by) or the length of the sequence (length.out). If length.out is specified, then R calculates the increments of the sequence. To see how this works, let’s create a vector which holds a sequence starting at 1, ending at 10 in steps of 1:\n\nvec_1 &lt;- seq(from = 1, to = 10, by = 1)\nvec_1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nAs an alternative, we can create the same sequence using the length.out argument:\n\nvec_2 &lt;- seq(1, 10, length.out = 10)\nvec_2\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhat happens if the last increment of the sequence, starting from the starting position, doesn’t end in the value given in the by argument. In that case, seq() stops before the value in to is reached. For instance:\n\nvec_3 &lt;- seq(1, 10, by = 8)\nvec_3\n\n[1] 1 9\n\n\nUsing the length.out = argument, the sequence always end in the value in to:\n\nvec_3 &lt;- seq(1, 10, length.out = 25)\nvec_3\n\n [1]  1.000  1.375  1.750  2.125  2.500  2.875  3.250  3.625  4.000  4.375\n[11]  4.750  5.125  5.500  5.875  6.250  6.625  7.000  7.375  7.750  8.125\n[21]  8.500  8.875  9.250  9.625 10.000\n\n\nYou can also use seq() with from, by and length.out. Here, you don’t specify the last value of the sequence. R will generate a sequence starting from the value in from and it will add the value in by the value in length.out times. Note that in this case, you need to add the arguments of the function as you skip the second argument.\n\nvec_4 &lt;- seq(from = 10, by = 10, length.out = 25)\nvec_4\n\n [1]  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190\n[20] 200 210 220 230 240 250\n\n\nNote that the increment can be negative. In that case, R will reduce the start value with the value of the increment until it reaches the end value or untill is reaches the number of increments in length.out:\n\nvec_5 &lt;- seq(from = 100, to = 50, by = -10)\nvec_5\n\n[1] 100  90  80  70  60  50\n\n\nor, as an alternative\n\nvec_6 &lt;- seq(from  = 100, by = -10, length.out = 5)\nvec_6\n\n[1] 100  90  80  70  60\n\n\nIn specific cases, you can create a sequence using shorter notation. For instance, suppose you want a vector of integers, where each increment is exactly 1. To generate this sequence, you can use\n\nvec_7 &lt;- 21:30\nvec_7\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nWe will use this short way to writing a sequence often in a for loop:\n\ni &lt;- 1\nfor (i in 1:5) {\n  print(\"Hello World\")\n  i &lt;- i + 1\n}\n\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n\n\nHere, i will adopt each value in 1:5, i.e. 1, 2, 3, 4 and 5 and print Hello World as long as i is smaller than or equal to 5. The counter i starts with a value 1 and the counter increases by 1 after every print of Hello World.\nIf the starting position is 1, sec.len() can be used as well:\n\nvec_8 &lt;- seq_len(10)\nvec_8\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nrm(vec_1, vec_2, vec_3, vec_4, vec_5, vec_6, vec_7, vec_8)\n\nYou can use seq.Date() to generate a sequence of dates. The arguments of this function are very similar to those for the seq() function. As a matter of fact, if you would use seq() and not seq.Date() R would recognise that you are using seq() to generate a sequence of dates and would use sec.Date() without problem. The from argument is the start date, the to the end date. If you use to, you need to specify the increment. Here, you can use “day”, “week”, “month”, “quarter” or “year”. Note that “days”, “weeks”, “months”, “quarters” or “years” is also accepted. If you add an integer, R will increment with a a multiple of “days”, … . To illustrate, let’s create three vectors, all start on Janaury 1, 2025 and and on December 31, 2025. The first increments in days, the second in 3 weeks and the last in quarters:\n\nstart_d &lt;- as.Date(\"2025-01-01\")\nend_d &lt;- as.Date(\"2025-12-31\")\n\nvec_d &lt;- seq.Date(from = start_d, to = end_d, by = \"day\")\nvec_w &lt;- seq.Date(from = start_d, to = end_d, by = \"3 weeks\")\nvec_q &lt;- seq.Date(from = start_d, to = end_d, by = \"quarter\")\n\nR generates a sequence and ends the sequence before the date in to. To see this, let’s ask the maximum value in each of these vectors:\n\nmax(vec_d)\n\n[1] \"2025-12-31\"\n\nmax(vec_w)\n\n[1] \"2025-12-24\"\n\nmax(vec_q)\n\n[1] \"2025-10-01\"\n\n\nIf you add increment with “day”, the last date is 2025-12-31. However, in both other cases, the last value of the sequence is before 2025-31-12. Using length_out, you determine the length of the sequence, but you allow R to determine the size of the increment if you include a value for to for the end point:\n\nvec_d10 &lt;- seq.Date(from = start_d, to  = end_d, length.out = 10)\nvec_d10\n\n [1] \"2025-01-01\" \"2025-02-10\" \"2025-03-22\" \"2025-05-02\" \"2025-06-11\"\n [6] \"2025-07-22\" \"2025-08-31\" \"2025-10-11\" \"2025-11-20\" \"2025-12-31\"\n\n\nIf combine a value for both by and length.out R will determine the end date. For instance, if you use 2025-01-01 as your start day, and increment 10 times with 1 week, R will produce:\n\nvec_w10 &lt;- seq.Date(from = start_d, by = \"weeks\", length.out = 10)\nvec_w10\n\n [1] \"2025-01-01\" \"2025-01-08\" \"2025-01-15\" \"2025-01-22\" \"2025-01-29\"\n [6] \"2025-02-05\" \"2025-02-12\" \"2025-02-19\" \"2025-02-26\" \"2025-03-05\"\n\n\nAs you would with seq() you can also use negative increments. In that case, R will count backwards in time. For instance, the generate a sequence starting on 2025-31-12 and ending at or before 2025-01-01 and steps of 5 weeks:\n\nvec_db &lt;- seq.Date(end_d, start_d, by = \"-5 weeks\")\nvec_db\n\n [1] \"2025-12-31\" \"2025-11-26\" \"2025-10-22\" \"2025-09-17\" \"2025-08-13\"\n [6] \"2025-07-09\" \"2025-06-04\" \"2025-04-30\" \"2025-03-26\" \"2025-02-19\"\n[11] \"2025-01-15\"\n\n\n\nrm(vec_d, vec_w, vec_q, vec_d10, vec_w10, vec_db)\n\nUsing seq.POSIXt you can generate date/time values. As was the case with seq.Date(), you can enter a starting date/time in the from argument, and end date/time in the to argument and supply the function with an increment “sec”, “min”, “hour”, “day”, “DSTday”, “week”, “month”, “quarter” of “year”. If you and an “s” that will not cause an error. In addition, you can add an integer to increment in multiples of “sec”. The difference between “day” and “DSTday” has to to be daylight savings time. DSTday takes daylight savings time into account. Is you include from, to and length.out, R determines the increment. With from, by and length.out R generates a sequence by adding the increment in by as many times and determined in length_out. If the time zone is not UTC, it has to be specified in from. Here are a couple of examples:\n\nstart_d &lt;- as.POSIXct(\"2025-01-01 12:00:00\")\nend_d &lt;- as.POSIXct(\"2025-01-05 12:00:00\")\n\nvec_dt_hour &lt;- seq.POSIXt(from =  start_d, to = end_d, by = \"6 hours\")\nvec_dt_10 &lt;- seq.POSIXt(from =  start_d, to = end_d, length.out = 10)\nvec_dt_20 &lt;- seq.POSIXt(from = start_d, by = \"5 mins\", length.out = 20)\nvec_dt_hour\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 18:00:00 CET\"\n [3] \"2025-01-02 00:00:00 CET\" \"2025-01-02 06:00:00 CET\"\n [5] \"2025-01-02 12:00:00 CET\" \"2025-01-02 18:00:00 CET\"\n [7] \"2025-01-03 00:00:00 CET\" \"2025-01-03 06:00:00 CET\"\n [9] \"2025-01-03 12:00:00 CET\" \"2025-01-03 18:00:00 CET\"\n[11] \"2025-01-04 00:00:00 CET\" \"2025-01-04 06:00:00 CET\"\n[13] \"2025-01-04 12:00:00 CET\" \"2025-01-04 18:00:00 CET\"\n[15] \"2025-01-05 00:00:00 CET\" \"2025-01-05 06:00:00 CET\"\n[17] \"2025-01-05 12:00:00 CET\"\n\nvec_dt_10\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 22:40:00 CET\"\n [3] \"2025-01-02 09:20:00 CET\" \"2025-01-02 20:00:00 CET\"\n [5] \"2025-01-03 06:40:00 CET\" \"2025-01-03 17:20:00 CET\"\n [7] \"2025-01-04 04:00:00 CET\" \"2025-01-04 14:40:00 CET\"\n [9] \"2025-01-05 01:20:00 CET\" \"2025-01-05 12:00:00 CET\"\n\nvec_dt_20\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 12:05:00 CET\"\n [3] \"2025-01-01 12:10:00 CET\" \"2025-01-01 12:15:00 CET\"\n [5] \"2025-01-01 12:20:00 CET\" \"2025-01-01 12:25:00 CET\"\n [7] \"2025-01-01 12:30:00 CET\" \"2025-01-01 12:35:00 CET\"\n [9] \"2025-01-01 12:40:00 CET\" \"2025-01-01 12:45:00 CET\"\n[11] \"2025-01-01 12:50:00 CET\" \"2025-01-01 12:55:00 CET\"\n[13] \"2025-01-01 13:00:00 CET\" \"2025-01-01 13:05:00 CET\"\n[15] \"2025-01-01 13:10:00 CET\" \"2025-01-01 13:15:00 CET\"\n[17] \"2025-01-01 13:20:00 CET\" \"2025-01-01 13:25:00 CET\"\n[19] \"2025-01-01 13:30:00 CET\" \"2025-01-01 13:35:00 CET\"\n\n\nAs you can see from these examples, way to use as.POSIXt() is very similar to the way you use seq.Date() or seq().\n\n\n3.2.7.2 Random numbers\nWe already covered statistical functions when we discussed numeric data. In that section, we showed how you can use pnorm(), dnorm(), qnorm() and rnorm(). However, with respect to the latter rnorm() we didn’t add too much detail. The same holds for the other function to generate random numbers from e.g. the t-distribution rt(), the uniform distribution runif(), the F-distribution rf() or rchisq()for the Chi-square distribution. In simulations, these random number generators are widely used. Before we move into these random number generates, a few words about the way to generate these numbers. Random number generators are not “random” but they follow an algorithm to generate a sequence of numbers whose properties approximate a random sequence. In other words, random numbers are not random, but their value is determined by and initial value that is used by the algorithm that generates this sequence. This is why random number generators are actually pseudo random number generators. They generate a sequence that mimics the properties of a random sequence, but the sequence is fully determined by and initial value. That initial value is called the seed. There are many pseudo random number generators, but the same pseudo random number generator will produce the same sequence of random numbers of the seed it the same. In R, you can select the pseudorandom number generator. The default is “Mersenne-Twister”. You can see all other pseudorandom number generators that are available if you use ?Random in the console. Using set.seed, you can make sure that R generates the same sequence of random numbers, every time you ask R to generate a series. This function sets the initial value for the pseudorandom number generator. Everytime you use this value, you’ll get the same results. This is useful is you want to replicate your results. In addition, if you build a simulation, it is often useful to have the same sequence every time to add components to the simulation’s model.\nEvery statistical distribution is characterized by its parameters. For the normal distribution, these are the mean and the standard deviation, to Student’s t-distribution as well as the Chi square distribution this parameter is the degrees of freedom, for the uniform distribution you need the minimum and the maximum and for the F-distribution, the ratio of two independent chi square distributed variables, you need two degrees of freedom. If you supply these parameters, you can generate random numbers of these distributions:\n\nset.seed(1000)\nv_norm &lt;- rnorm(n = 100, mean = 0, sd = 1)\nv_t &lt;- rt(n = 100, df = 5)\nv_unif &lt;- runif(n = 100, min = 0, max = 100)\nv_chi &lt;- rchisq(n = 100, df = 5)\nv_f &lt;- rf(n = 100, df1 = 10, df2 = 2)\n\nWith 100 random draws each, we can show the probability density distribution of each of these 5 randomly generated values using base R’s hist() function:\n\nhist(v_norm, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Normal\")\nlines(density(v_norm), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_t, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Student's t\")\nlines(density(v_t), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_unif, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Uniform\")\nlines(density(v_unif), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_chi, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Chi squared\")\nlines(density(v_chi), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_f, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"F distribution\")\nlines(density(v_f), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\n\n3.2.7.3 Sampling\nA sample refers to a subset of values from a vector that are drawn random. sample(x, size, replace = FALSE, prob = NULL) allows you to draw a random sample of size n, from a vector x. By default, sampling is done without replacement. In addition, all elements are equally likely to be drawn (prop = NULL). To illustrate this function, let’s use\n\nvec_1 &lt;- seq(1:48)\n\nand draw a sample, without replacement, of size = 10:\n\nsample(x = vec_1, size = 10)\n\n [1] 16 43 44 48 35 33 25 47 17 12\n\n\nIf you draw a sample with replacement (replace = TRUE), each draw is returned to the vector and could be drawn again.\n\nsample(x = vec_1, size = 10, replace = TRUE)\n\n [1] 26 10 47 10 37 42 46 27 23 33\n\n\nSampling is not limited to numeric vectors\n\nsample(x = c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"), size = 10, replace = TRUE)\n\n [1] \"g\" \"d\" \"f\" \"e\" \"b\" \"c\" \"c\" \"g\" \"f\" \"d\"\n\n\nWithout replacement, the sample size must be smaller than the length of the vector. With replacement, that is not the case. In the previous example for instance, the length of the vector was 8, while the size was 10. Without replacement, size = 8 would be equal to the vector and any size &gt; 8 would not leave sufficient values to sample from.\n\n\n3.2.7.4 Set operations\nSuppose you have two vectors,\n\nvec_1 &lt;- c(10, 20, 30, 40)\nvec_2 &lt;- c(20, 30, 40, 40)\n\nand you want to know if both share common elements. There are various ways to check if that is the case. The first uses the intersect() function. This function has two arguments: the vectors you want to compare.\n\nintersect(vec_1, vec_2)\n\n[1] 20 30 40\n\n\nThe output shows the values that these two vectors have in common. If you want to store these values, out assign them to a new vector. Note that this also allows to see how many values both vectors have in common. Using the length() function, you can verify how many (unique) values are common to both vectors:\n\nlength(intersect(vec_1, vec_2))\n\n[1] 3\n\n\nHere, we used numeric values, but it you can finds common strings in character vectors in a similar way:\n\nfriends &lt;- c(\"Monica\", \"Phoebe\", \"Joey\", \"Chandler\", \"Ross\", \"Rachel\") \ncollegues &lt;- c(\"Taylor\", \"David\", \"Joey\", \"Sandra\")\nintersect(friends, collegues)\n\n[1] \"Joey\"\n\n\nThis example also shows that the vectors don’t have to have the same length. If there are no common values, R will output the nul vector:\n\nintersect(c(10, 20), c(50, 60))\n\nnumeric(0)\n\n\nis.element(x, y) allows you to determine if elements of one vector, x, are included in the other y. The outcome is be a boolean vector whose values are TRUE if an element from x occurs in y and FALSE otherwise.\n\nis.element(vec_1, vec_2)\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\nThe values in the last three columns in vec_1 are also included in vec_2. Using the %in% operator has the same outcome as it checks which values on its left hand side vector are include in its right hand side vector:\n\nvec_1 %in% vec_2\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\nYou can also use this result to see how many elements from the first vector are also in the second. Here, you use the fact that TRUE is also 1 and FALSE is 0:\n\nsum(is.element(vec_1, vec_2))\n\n[1] 3\n\n\nUsing is.element(), you can subset a vector (the first) and extract all values that are both the the first and the second vector.\n\nvec_1[is.element(vec_1, vec_2)]\n\n[1] 20 30 40\n\n\nNote that the order of the vectors matters. If you use is.element(x, y) you check if the elements from x are included in y. With is.element(y, x) you determine the elements in y that are also in x. In the example, you can see that changing the order in the is.element() function shows a different output as 40 is includes in vec_2 twice, but is only once included in vec_1\n\nvec_1[is.element(vec_1, vec_2)]\n\n[1] 20 30 40\n\nvec_2[is.element(vec_2, vec_1)]\n\n[1] 20 30 40 40\n\n\nRecall that using the “!” you can check if a condition is not met. Here, you can use this to see which elements of x are not in y\n\n!is.element(vec_1, vec_2)\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nsetdiff(x y) allows you to look for elements that are different, in other words, which elements from x are not included in y. While !is.element(x, y)’s output is a boolean vector, setdiff() shows the values of x that are not included in y.\n\nsetdiff(vec_1, vec_2)\n\n[1] 10\n\n\nNote again that the order of the vectors matters.\nTo create a union of x and y, there is the union(x, y) function. This function shows the unique values after merging the values in x and y:\n\nunion(vec_1, vec_2)\n\n[1] 10 20 30 40\n\n\nIf you want to know positions of these common elements, you can use the which() function:\n\nwhich(is.element(vec_1, vec_2))\n\n[1] 2 3 4\n\n\nThe unique(x, incomparables = FALSE) function determines the unique values in a vector. Suppose that you have a vector\n\nvec_char &lt;- c(\"jan\", \"jan\", \"feb\", \"mar\", \"mar\", \"apr\")\n\nThis vector has 4 unique values: “jan”, “feb”, “mar” and “apr”. Using the unique() function, you can select the unique values:\n\nunique(x = vec_char)\n\n[1] \"jan\" \"feb\" \"mar\" \"apr\"\n\n\nIf you want to exclude one value, you can add it to the incomparables = argument. For instance, suppose that you want to see all unique values, except january, you can add incomparables = c(\"jan\"):\n\nunique(vec_char, incomparables = c(\"jan\"))\n\n[1] \"jan\" \"jan\" \"feb\" \"mar\" \"apr\"\n\n\nR will now show all occurrences of “jan” as well as the unique values of all others.\n\n\n\n3.2.8 Special vectors\nR includes a number of special vectors. For instance, the vectors “letters” and “LETTERS” include the letters of the alfabet. The first non capitalized, the second with capitals\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\n\n\n3.2.9 Vector functions\nMany operations in R are vectorized. This means that an operator works on an element by element basis. For function, that means that most function in R when applied to a vector will be applied to every element of that vector. For instance, is you take the logarithm of vec_num, R will apply that function to every element in that vector:\n\nvec_num1 &lt;- c(10, 10, 20, 30, 50, 80, 130, 210, 340, 550)\nlog(vec_num1)\n\n [1] 2.302585 2.302585 2.995732 3.401197 3.912023 4.382027 4.867534 5.347108\n [9] 5.828946 6.309918\n\n\nFor other operators, e.g. addition, subtraction, multiplication of division, this is also the case. For instance, multiplying vec_num1 with 2 is equal to multiplying each element of vec_num1 with 2:\n\nvec_num2 &lt;- vec_num1 * 2\nvec_num2\n\n [1]   20   20   40   60  100  160  260  420  680 1100\n\n\nLikewise, if you add vec_num1 to vec_num2, R will first check if both vectors are of the same length and, if that is the case, add the elements in the same position.\n\nvec_num3 &lt;- vec_num1 + vec_num2\nvec_num3\n\n [1]   30   30   60   90  150  240  390  630 1020 1650\n\n\nNote that this save a lot of work. Without vectorization, to add two vectors, you would have to write some code, e.g.:\n\nif (length(vec_num1) != length(vec_num2)) {               \n  print(\"Can not add vectors of a different length\")      \n} else {\n  vec_num4 &lt;- vector(\"numeric\", length = length(vec_num1))\n  for (i in 1:length(vec_num1)) { \n    vec_num4[i] &lt;- vec_num1[i] + vec_num2[i]\n   }\n}\nvec_num4\n\n [1]   30   30   60   90  150  240  390  630 1020 1650\n\n\nNote that this also holds for multiplication of division: R performs element by element multiplication of division if these operations include two vectors of the same length:\n\nvec_num1 * vec_num2\n\n [1]    200    200    800   1800   5000  12800  33800  88200 231200 605000\n\n\n\nvec_num2 / vec_num1\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\n\nBoolean operators work element wise. For instance, check if the values in vec_num1 are larger than 50:\n\nvec_num1 &gt; 50\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nRecall that we used this observation to subset a vector.\nreduce stats\nsample cumsum cumprod startWith endsWith\n\n\n3.2.10 Factors",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types and structures</span>"
    ]
  },
  {
    "objectID": "03_Data_types_and_structures.html#other-objects",
    "href": "03_Data_types_and_structures.html#other-objects",
    "title": "3  Data types and structures",
    "section": "3.3 Other objects",
    "text": "3.3 Other objects\n\nmodels\nformulas Say",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types and structures</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html",
    "href": "03_Data_types.html",
    "title": "3  Data types",
    "section": "",
    "text": "3.1 Numeric\nBefore we cover data structures, we will first discuss the basic data types and classes in R: double, integer, character, logical and dates and times. R has more basic data types: complex and raw. We will leave these out of this introduction. In this chapter, we will also introduce some functions that you can use with all these data types.\nUnlike other programming languages, R does not require you to declare a variable. In other programming languages, you’ll often have to write int a, float b or char c or dim a As Integer, dim b As Double of dim c As String. Doing so sets the type of a as an integer, the type of b as a double and c as a character. If you want to change the type in these languages, you need to do so explicitly. R is a dynamically typed language: in R, a variable is not declared: a variable gets the type of the object assigned to it. For instance, if you executed the code var1 &lt;- 25 in Chapter 1, R assigned the number 25 to that variable var1 and detected that var1 was numeric double precision. The type of a variable changes when you reassign a variable to a different type. The implication is that the type of a variable can change multiple times in your code. In languages that require you to declare the type, this is not the case.\nR has 7 basic data types: numeric or double, integer, character, logical, data/time and complex and raw. We’ll focus on the first 5.",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html#numeric",
    "href": "03_Data_types.html#numeric",
    "title": "3  Data types",
    "section": "",
    "text": "3.1.1 Definition\nA numeric or double precision variable stores numbers. A computer only knows 0 or 1 or “on” or “off”. A 1 or a 0 are known as a bit. The double precision refers to the number of bits used to store the value in your computer’s memory. Double precision or floating point (FP64, float64) occupy 64 bits (or 8 bytes) in your computer’s memory. Note that this number is limited. In other words, is doesn’t allow your computer to store all numbers with the highest precision but is accurate up to 15 to 17 decimal places. This last point will be important when we introduce boolean operators and compare if two values are equal.\nLet’s define two double precision numeric variables:\n\na &lt;- 10.25\nb &lt;- -25\n\nRecall that R uses a . for decimals and not a comma. Negative numbers are shown with a -.\nYou can check the type of the variable is you use typeof():\n\ntypeof(a)\n\n[1] \"double\"\n\n\nAs you can see, a is stored as a double. The class of a variable refers to what kind of object it is. In R, the class of a double is numeric. You can check the class of a variable using class():\n\nclass(a)\n\n[1] \"numeric\"\n\n\nRecall that in Chapter 2 you selected columns from nike_df using the is.numeric() function. This function allows you to check if the type of a variable is numeric. The function returns TRUE if a variable is numeric and FALSE if this is not the case. You can also check if a variable is double using is.double():\n\nis.numeric(a)\n\n[1] TRUE\n\nis.double(a)\n\n[1] TRUE\n\n\nSometimes, numbers are stored as characters. You can recognize character variables because they are shown within quotation marks \" \". You can coerce these variables into numeric variables using as.numeric() or into double precision using as.double(). R will then try to change the type of the variable from character to numeric or double. Let’s define a number as a character variable:\n\na &lt;- \"12.25\"\na\n\n[1] \"12.25\"\n\n\nAs you can see, a is shown as “12.25” i.e. a character variable. You can change the type using\n\na &lt;- as.numeric(a)\na\n\n[1] 12.25\n\n\nAs you can see, the quotation marks are gone. In other words, a is now numeric (double precision):\n\ntypeof(a)\n\n[1] \"double\"\n\n\nIf you use the as.double() function, the outcome would be the same. Note that this is not always possible. For instance, suppose that your “character number” was written as\n\na &lt;- \"12,25\"\n\nTrying to change this character in a number generates a warning;\n\nb &lt;- as.numeric(a)\n\nWarning: NAs introduced by coercion\n\nb\n\n[1] NA\n\n\nHere, R couldn’t recognize the “12,25” as a number due to the comma. In this case, you need to change to comma first. When we introduce character variables, you’ll see functions that allow you to do just that. For instance, using gsub():\n\nb &lt;- as.numeric(gsub(pattern = \",\", replacement = \".\", a))\nb\n\n[1] 12.25\n\n\nR is now able to convert the character in a number.\n\n\n3.1.2 Mathematical operators and functions\nMost commonly used mathematical operators and functions are build into base R. To illustrate, let’s define two values:\n\na &lt;- 50\nb &lt;- 20\n\n\naddition:\n\n\na + b\n\n[1] 70\n\n\n\nsubtraction:\n\n\na - b\n\n[1] 30\n\n\n\nmultiplication:\n\n\na * b\n\n[1] 1000\n\n\n\ndivision:\n\n\na / b \n\n[1] 2.5\n\n\n\ninteger division (the quotient part of a division without the fractional part):\n\n\na %/% b\n\n[1] 2\n\n\n\nmodulus (the fractional part of a division)\n\n\na %% b\n\n[1] 10\n\n\nNote that the result of the integer division * b + modulus = a:\n\nc &lt;- a %/% b\nd &lt;- a %% b\nc * b + d\n\n[1] 50\n\n\nThe modulus is often used to check if a number of even. If the modulus from division by 2 is zero, the number is even, else it is uneven:\n\n31 %% 2\n\n[1] 1\n\n30 %% 2\n\n[1] 0\n\n\nFor these operators, the usual order applies:\n\nmultiplication (division) before addition (subtraction)\nparenthetical subexpressions are evaluated first\nexponentiation before multiplication\n\nA couple of examples of often used mathematical function\n\nsum:\n\n\nsum(a,b)\n\n[1] 70\n\n\nNote that the sum() function includes the argument na.rm with default FALSE. Here, this default is fine as you know there are no missing values. However, when you apply this function to e.g. a vector where missing values can occur, you need to change that default into na.rm = TRUE.\n\nabsolute value:\n\n\nc &lt;- -50\nabs(c)\n\n[1] 50\n\n\n\nlogarithm base e (natural logarithm):\n\n\nlog(a)\n\n[1] 3.912023\n\n\n\nlogarithm base 10:\n\n\nlog10(a)\n\n[1] 1.69897\n\nlog(a, base = 10)\n\n[1] 1.69897\n\n\n\nsquare root:\n\n\nsqrt(a)\n\n[1] 7.071068\n\n\n\npower, e.g. 2:\n\n\na^2\n\n[1] 2500\n\n\n\nexponent (e to the power n (e.g. n = 10):\n\n\nexp(10)\n\n[1] 22026.47\n\n\nRecall that log(exp(n)) = n:\n\nlog(exp(10))\n\n[1] 10\n\n\nMost of these function allow you to use the pipe operator |&gt;. For instance:\n\na |&gt; log()\n\n[1] 3.912023\n\nb |&gt; sqrt()\n\n[1] 4.472136\n\na |&gt; log() |&gt; exp()\n\n[1] 50\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nCreate a variable a, assign it the value of 100 and apply the following functions and assign the result to a variable a_nl, a_lb2, a_sqrt and a_p2.\n\nnatural logarithm: assign to a_nl\nlogarithm base 2: assign to a_lb2\nsquare root of a: assign to a_sqrt\nraise a to the power 2: assign to a_p2\n\n\n\nCode\na &lt;- 100\na_nl &lt;- log(a)\na_lb2 &lt;- log(a, base = 2)\na_sqrt &lt;- sqrt(a)\na_p2 &lt;- a^2\n\n\nThen apply the\n\nexponential function to a_nl\nsquare root function to a_p2\n\n\n\nCode\nexp(a_nl)\n\n\n[1] 100\n\n\nCode\nsqrt(a_p2)\n\n\n[1] 100\n\n\nCan you use the pipe operator to calculate the last two results starting from a?\n\n\nCode\na |&gt; log() |&gt; exp()\n\n\n[1] 100\n\n\nCode\n# a |&gt; ^2 doesn't work. The pipe operator needs a function in the right hand side. \n# ^ is an operator and can not be used with the pipe operator\n\n\n\n\n\n\n\n3.1.3 Statistical functions\nR also includes a number of statistical probability functions for the normal, t, Chi-square, F, binomial, Poisson, uniform, … distribution. For the normal distribution, these include pnorm(), dnorm(), qnorm() and rnorm(); for the t-distribution, these functions include pt(), dt(), qt() and rt(); for the uniform distribution punif(), dunif(), qunif() and runif(); pf(), df(), qf() and rf() for the F-distribution or pchisq(), dchisq(), qchisq() and rchisq()for the Chi-square distribution. You can find a good explanation for these functions in Sean Finn’s Visual guide to pnorm, dnorm, qnorm, and rnorm function in R and Visual guide to uniform distribution functions in R (punif, dunif, qunif, and runif). There are many other distributions. Their statistical functions “rdist”, “pdist”, “ddist” and “qdist” have an analogous interpretation.\nLet’s see what these functions do. Here, we will focus on the normal distribution, but you can extend the interpretation to other distributions. However, note that a distribution has parameters: the mean and standard deviation for the normal distribution, the degrees of freedom for the t-distribution or Chi-square distribution. For the normal distribution, the mean and the standard deviation are sufficient to describe the full distribution. If the mean is 0 and the standard deviation equals 1, the normal distribution is also called the standard normal distribution.\nSuppose you have a variable which is normally distributed with mean 0 and standard deviation equal to 1; the probability that this value is smaller then or equal to q is given by: pnrom(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nThe first three arguments in this function are self explanatory. The fourth lower.tail = TRUE means that you want to calculate the probability that the value is smaller than or equal to q. In other words, you want to calculate the area under the density function left of q. Changing this into lower.tail = FALSE shows the probability that a value will be larger than q. In other words, it shows the area under the density curve right of q. If we would set log.p = TRUE, the result of this function would the the log of p. Usually, you can disregard this option and leave it equal to the default. In other words, you don’t have to repeat it in the function call.\nSuppose you have a value q = 1.75 and you know that it comes from a standard normal distribution. Now you want to know what the probability is that you find a number equal to of less than 1.75:\n\npnorm(q = 1.75, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 0.9599408\n\n\nThe probability that you find a value less than or equal to 1.75 is 0.9599408. What is the probability that you find a value greater than 1.75? There are two ways to find out. First, you use the fact that the probability that a value is greater equals 1 minus the probability that the value is equal to or smaller than:\n\n1 - pnorm(q = 1.75, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n[1] 0.04005916\n\n\nYou can also get this result is you change lower.tail = TRUE from its default TRUE into lower.tail = FALSE:\n\npnorm(q = 1.75, mean = 0, sd = 1, lower.tail = FALSE, log.p = FALSE)\n\n[1] 0.04005916\n\n\nTo find the probability of a given value, you can use dnorm(x, mean = 0, sd = 1, log = FALSE).Here we added the log = FALSE argument for the sake of completeness, but usually the default value log =  FALSE is what you need. Suppose you have a value x = 1.75 and you know that it comes from a standard normal distribution. Now you want to know what the probability is that you find a number equal to 1.75. In that case you would use:\n\ndnorm(x = 1.75, mean = 0, sd = 1)\n\n[1] 0.08627732\n\n\nThe probability is 0.08627732.\nqnorm() show the value, q, for which it holds that the probability that you find another values less than or equal to q is equal to the specified probability p: qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE). If you need to find the value of the standard normal distribution for which it holds that the probability that you find a value smaller than or equal to that value is 0.95, you can use:\n\nqnorm(p = 0.95, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 1.644854\n\n\nFor the standard normal distribution, 95% of the values are smaller than or equal to 1.6448. Note that this function is closely related to the prnorm() function:\n\npnorm(q = 1.6448545, mean = 0, sd = 1)\n\n[1] 0.9500001\n\n\nThe rnorm() functions draws random draws from a normal distribution with mean = 0 and standard deviation = 1):\n\nset.seed(1000)\nrnorm(n = 1, mean = 0, sd = 1)\n\n[1] -0.4457783\n\n\nIn this code block set.seed(1000) ensures that the random draw will always be the same.\nYou can perform similar calculation for the other distributions.\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nSuppose that you find a t-value equal to 1.85 and you know that it follows a t-distribution with 15 degrees of freedom. What is the probability that you find a value for this variable larger then 1.85?\n\n\nCode\npt(q = 1.85, df = 15, lower.tail = FALSE)\n\n\n[1] 0.04205604\n\n\nWhat is the 95th percentile of a Chi-square distribution with 12 degrees of freedom?\n\n\nCode\nqchisq(p = 0.95, df = 12, lower.tail = TRUE)\n\n\n[1] 21.02607\n\n\nSuppose that you calculate a test statistics. The test statistic follows a t-distribution with 18 degrees of freedom. Your t-statistics equals 1.89. Is this statistically significantly different from 0 at the 5% level?\n\n\nCode\npt(q = 1.89, df = 24, lower.tail = FALSE) * 2\n\n\n[1] 0.07089497\n\n\nNote that you need to multiply the outcome of pt() with 2. The function gives the area under the curve to the right of 1.89 (lower.tail = FALSE). However, the area to the left of -1.89 is as large of the area to the right of 1.89. So you need to double the value.\nThe probability that you find a t-statistic larger than 1.89 is 0.0708. This is larger than the 5% level of confidence.\nAssign a random draw from Student’s t distribution with 5 degrees of freedom to a variable t_random.\n\n\nCode\nt_random &lt;- rt(1, df = 5)\nt_random\n\n\n[1] -1.328866\n\n\nCalculate the probability that you have this value.\n\n\nCode\ndt(t_random, df = 5)\n\n\n[1] 0.1532039\n\n\n\n\n\n\n\n3.1.4 Rounding\nIf you want to round numbers, you can use round(x, digits = n) to round to the nearest n decimal places. In case you have .5, the rule “go the closest even digit” applies and is in line with the international ISO standard IEC 60559(although you may want to check your operating system):\n\nr &lt;- 0.5\nround(r)\n\n[1] 0\n\ns &lt;- 1.5\nround(s)\n\n[1] 2\n\nt &lt;- -0.5\nround(t)\n\n[1] 0\n\nu &lt;- -1.5\nround(u)\n\n[1] -2\n\n\nIf you apply this more in general\n\nr &lt;- 4.5\nround(r)\n\n[1] 4\n\ns &lt;- 5.5\nround(s)\n\n[1] 6\n\n\nAs you can see from these examples, R rounds 4.5 to the closest even integer: 4 and rounds 5.5 to the closest even integer 6.\nGiven this result, the other rounding rules are straightforward. For instance, let’s round 12.34567 to 4, 3, 2, 1, 0 and -1 (round before the decimal):\n\nto 4 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 4)\n\n[1] 12.3457\n\n\n\nto 3 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 3)\n\n[1] 12.346\n\n\n\nto 2 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 2)\n\n[1] 12.35\n\n\n\nto 1 digit: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 1)\n\n[1] 12.3\n\n\n\nto 0 digits: 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = 0)\n\n[1] 12\n\n\n\nto -1 digits= 12.34567\n\n\nr &lt;- 12.34567\nround(r, digits = -1)\n\n[1] 10\n\n\nNote that the rounding rule for ‘5’ is used there too. To see this, round 12.45 and 12.75 to the single digit:\n\nround(12.45, 1)\n\n[1] 12.4\n\nround(12.75, 1)\n\n[1] 12.8\n\n\nAs you can see, 12.45 is rounded to 12.4 - 4 being the closest even digit - while 12.75 is rounded up to 12.8 as 8 is now the closest even digit before 5.\nAs an alternative, you can use the floor() function to round to the largest integer (no decimal places), not greater than the value itself:\n\nr &lt;- 12.34567\nfloor(r)\n\n[1] 12\n\n\nor the ceiling()function to round to the smallest integer (no decimal places), not smaller than the value itself\n\nr &lt;- 12.34567\nceiling(r)\n\n[1] 13\n\n\nThe trunc()function is a rounding function that removes the decimal places. In other words, it only returns the integers:\n\nr &lt;- 12.34567\ntrunc(r)\n\n[1] 12\n\n\nNote that if you use trunc() 12.9999 is rounded to 12:\n\ns &lt;- 12.9999\ntrunc(s)\n\n[1] 12\n\n\nThe trunc() function behaves as a ceiling function for negative numbers\n\ntrunc(-12.25)\n\n[1] -12\n\nfloor(-12.25)\n\n[1] -13\n\nceiling(-12.25)\n\n[1] -12\n\n\nand floor function for positive numbers:\n\ntrunc(12.25)\n\n[1] 12\n\nfloor(12.25)\n\n[1] 12\n\nceiling(12.25)\n\n[1] 13\n\n\nsignif(x, digits = n = 6) rounds to the specified number of significant digits (default = 6). For instance:\n\nt &lt;- 123456789.12\nsignif(t, digits = 9)\n\n[1] 123456789\n\nsignif(t, digits = 8)\n\n[1] 123456790\n\nsignif(t, digits = 7)\n\n[1] 123456800\n\nsignif(t, digits = 6)\n\n[1] 123457000\n\nsignif(t, digits = 5)\n\n[1] 123460000\n\nsignif(t, digits = 4)\n\n[1] 123500000\n\nsignif(t, digits = 3)\n\n[1] 1.23e+08\n\nsignif(t, digits = 2)\n\n[1] 1.2e+08\n\nsignif(t, digits = 1)\n\n[1] 1e+08\n\n\nAs you can see, with 9 significant digits, 123,456,789.12 gets rounded to 123,456,789. If you reduce the number of significant digits from 9 to 8, t is rounded to 123,456,790. With 1 significant digit, t is rounded to 100,000,000 of 1 * 10 ^8 (also written in scientific notation: 1e+08).\n\n\n3.1.5 A note on NaN and Inf\nThere are a couple of special numbers: Infinity and NaN. Let’s start with Infinity.\n\nz &lt;- Inf\ntypeof(z)\n\n[1] \"double\"\n\nclass(z)\n\n[1] \"numeric\"\n\n\nAs you can see, Infinity is a numeric (double precision). In this example, we assigned the value of Infinity to the variable z. You also get Infinity is you divide by 0:\n\na &lt;- 50\nb &lt;- a/0\nb\n\n[1] Inf\n\ntypeof(b)\n\n[1] \"double\"\n\n\nAs you can see from this example, unless you ask for the result or unless you look at the environment pane, R doesn’t warn that b equals Infinity. To check if that is the case, you can use two functions: is.infinite() which returns TRUE is a value if Inf of is.finite() which returns FALSE of a variable is Inf:\n\nis.infinite(b)\n\n[1] TRUE\n\nis.finite(b)\n\n[1] FALSE\n\n\nThe outcome of a calculation with Infinity is always Infinity of NaN\n\nc &lt;- b * 2\nc\n\n[1] Inf\n\nd &lt;- b/b\nd\n\n[1] NaN\n\n\nNaN or Not a Number is used when the outcome of a calculation doesn’t exist. For instance:\n\nf &lt;- 0/0\nf\n\n[1] NaN\n\ntypeof(f)\n\n[1] \"double\"\n\nclass(f)\n\n[1] \"numeric\"\n\n\nAgain, R doesn’t include a warning: it assigns the NaN to the variable f. If you would miss this calculation and you didn’t check the environment panel (where f is shown as NaN). You can check if a number exists if you use the function is.nan(). Note that R will show a warning if you don’t assign:\n\nlog(-10)\n\nWarning in log(-10): NaNs produced\n\n\n[1] NaN\n\n\n\n\n3.1.6 NaN and NA?\nNote that NaN is different from NA: NA is used for missing values (not available). Datasets often include missing data: data that is not available, e.g. because a respondent in a survey did not complete a question (e.g. “What is your income”), sales data that were not (yet) reported at the time you are writing a report, or country data that is missing because the dataset includes years where a country didn’t exist or didn’t report data (say GDP for the USA in 1500). NA’s are missing in a sense that you usually would expect that there is a value: if the respondent would have answered the question with respect to his or her income, that value would have shown a positive value (say €2000), sales data will be reported and the country did exist in a sense that there were people running around between the boundaries of what is now the USA and these people produced goods and earned an income, but the land within these boundaries was not yet known as the USA. So these NA’s are not equal to 0 (there was an income, there were sales, …) but we don’t know their value.\nMost computations that involve NA’s will return NA’s:\n\nNA\n\n[1] NA\n\nNA * 5\n\n[1] NA\n\nNA / 2 \n\n[1] NA\n\n\nBecause of this, often we will have to include ‘na.rm = TRUE’ in functions, e.g. to show summary statistics. Adding na.rm = TRUE tells the function to disregard NA’s. If we wouldn’t and there are NA’s, the result would be NA. Don’t worry too much about the c(10 …), we’ll see what it does soon, but let’s try to calculate the mean of 10, 20, 30, 40, and NA:\n\nc &lt;- c(10, 20, 30, 40, NA)\nmean(c)\n\n[1] NA\n\n\n\nc &lt;- c(10, 20, 30, 40, NA)\nmean(c, na.rm = TRUE)\n\n[1] 25\n\n\nYou can check is a value is an NA using is.na.\n\na &lt;- NA\nis.na(a)\n\n[1] TRUE\n\n\nNote that NaN are also NA’s\n\ng &lt;- log(-10)\n\nWarning in log(-10): NaNs produced\n\nis.na(g)\n\n[1] TRUE",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html#integers",
    "href": "03_Data_types.html#integers",
    "title": "3  Data types",
    "section": "3.2 Integers",
    "text": "3.2 Integers\nIntegers are numeric but take values such as 1, 2, 3, …. In other words, an integer does not have a decimal part. If you don’t tell R that a variable is an integer, it will set its type equal to double:\n\na &lt;- 10\ntypeof(a)\n\n[1] \"double\"\n\n\nTo tell R that it needs to set a value as integer, you can add and L: for instance:\n\na &lt;- 10L\ntypeof(a)\n\n[1] \"integer\"\n\n\nAs with numeric variables, you can check if a variable is an integer using is.integer()\n\nis.integer(a)\n\n[1] TRUE\n\n\nIf that is not the case, you can tell R it needs to change the type into an integer. Here we use as.integer() to change the type of a character and numeric variable into an integer:\n\nb &lt;- \"10\"\nc &lt;- 10\nas.integer(b)\n\n[1] 10\n\nas.integer(c)\n\n[1] 10\n\n\nYou used this function in Chapter 2 to change the type of the variable data in life_df into an integer.\nIntegers are numeric. You can see that if you use is.numeric()\n\nis.numeric(a)\n\n[1] TRUE\n\n\nIn other words, if you select columns as you did in Chapter 2 using the condition is.numeric() this selection will include double as well as integer variables. Because they are numeric, you can use them in calculations. For instance:\n\na &lt;- 50L\nb &lt;- 25\nc &lt;- a / b\ntypeof(c)\n\n[1] \"double\"\n\n\nIf you do calculations with two integers and the result is an integer, the type of that result will change to double:\n\nr &lt;- 100L\ns &lt;- 10L\nt = r/s\ntypeof(t)\n\n[1] \"double\"",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html#logical-values",
    "href": "03_Data_types.html#logical-values",
    "title": "3  Data types",
    "section": "3.3 Logical values",
    "text": "3.3 Logical values\n\n3.3.1 Boolean or logical values\nLogical values or boolean values take the value of TRUE or FALSE.\n\nbool_1 &lt;- TRUE\nbool_1\n\n[1] TRUE\n\n\nYou can verify that bool_1 is a logical value using:\n\nclass(bool_1)\n\n[1] \"logical\"\n\n\n\ntypeof(bool_1)\n\n[1] \"logical\"\n\n\nor\n\nis.logical(bool_1)\n\n[1] TRUE\n\n\nYou can use T and F as shorthand for TRUE als FALSE:\n\nbool_2 &lt;- F\nbool_2\n\n[1] FALSE\n\n\nNote that these logical values take the value of 1 for TRUE and 0 for FALSE. In other words, you can use them in math:\n\nbool_1 + 1\n\n[1] 2\n\n\nWe already met multiple functions whose output is a logical value: is.numeric(), is.logical(), is.integer(), is.na(), is.nan(), is.finite(), is.infinite(), … .\n\n\n3.3.2 Boolean operators\nA statement is TRUE or FALSE. Suppose for instance that you have the following values\n\na &lt;- 100\nb &lt;- 25\nc &lt;- 125\n\nUsing these, you can test for equality or other relations.\n\nIs a equal to b: ==\n\n\na == b\n\n[1] FALSE\n\n\n\nis a larger than or equal to b: &gt;= (if only larger than: &gt;)\n\n\na &gt;= b\n\n[1] TRUE\n\n\n\nIs a smaller than or equal to b: &lt;=\n\n\na &lt;= b\n\n[1] FALSE\n\n\nIn addition to these relational operators, boolean operators also include & (and), | (or) and ! (not). Using these, you can test for, e.g.\n\nis not equal to: != (i.e. is smaller or larger than):\n\n\na != b\n\n[1] TRUE\n\n\nAs an alternative, you can also use\n\n!(a == b)\n\n[1] TRUE\n\n\n\nis not smaller than or equal to (i.e. is larger than):\n\n\n!(a &lt;= b)\n\n[1] TRUE\n\n\n\nis not larger than: (i.e. is smaller than or equal to):\n\n\n!(a &gt; b)\n\n[1] FALSE\n\n\nOr you can test if a condition does not hold:\n\n!is.numeric(a)\n\n[1] FALSE\n\n\nAs you can see, the way to code these statements is often different from how you use them in English: “a is not greater than or equal to” is not written as a !&lt;= b but as !(a &lt;= b).\nUsing & and | you can combine conditions. A statement with the & operator is TRUE is all its components are TRUE and FALSE if one of them is false. A statement with the | operator is TRUE is one of the components is TRUE and FALSE is all components are FALSE. For instance:\n\na is larger than both b and c\n\n\n(a &gt; b) & (a &gt; c)\n\n[1] FALSE\n\n\nHere the first component of the statement is TRUE (100 &gt; 10) and the second component (100 &gt; 125) is FALSE. FALSE & FALSE is FALSE. In general using & will only result in TRUE if all conditions are TRUE.\n\na is smaller than b or c\n\n\n(a &lt; b) | (a &lt; c)\n\n[1] TRUE\n\n\nHere the first component of the statement (100 &gt; 10) is FALSE, the second component is TRUE (100 &lt; 125). FALSE | TRUE is TRUE. In general, FALSE | FALSE will be FALSE, all other combinations are TRUE.\nNote that combining conditions you need to deviate from “traditional language”. The statement in the first example: a is larger than b and c is not written as a &gt; b & c. R reads the latter statement as a &gt; b and evaluates that expression as TRUE. The second statement, c is TRUE as there is no condition involved. As TRUE & TRUE equal TRUE, R evaluates this statement as TRUE, not as FALSE. With a the condition as (a &gt; b) & (a &gt; c) this is not the case. The first condition is TRUE, the second condition is FALSE: and TRUE and FALSE result in FALSE\nCombining ! (not) with & (and) or | (or) :\n\n!( a & b) = !a | !b (“not a and b” equals “not a or not b”)\n!(a | b) = !a | !b (“not a or b” equals “not a and not b”)\n\nLogical values equal 1 if TRUE and 0 if FALSE. This allows you to do math. Recall from Chapter 2 that we used the fact the determine the number of missing values in a dataset sum(if.na()). In other words you can use:\n\nbool_1 + 1\n\n[1] 2\n\n\n\n2 * bool_1\n\n[1] 2\n\n\n\nTRUE + TRUE\n\n[1] 2\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nCreate two variables, a and b and assign the values of 25 and 50 to them.\n\n\nCode\na &lt;- 25\nb &lt;- 50\n\n\nCheck if the following conditions hold:\n\na is larger than b\na is not larger then b\na is larger than 10 and b is larger than 50\na is larger than 10 or b is larger than 50\nb divided by a is even\na divided by b is equal to 0.50\nb divided by 3 is 16.666667\ngiven the last result: how could yet set this to TRUE with a precision of 1 billion-th? Tip: first read the next section.\n\n\n\nCode\na &gt; b\n\n\n[1] FALSE\n\n\nCode\n!(a &gt; b)\n\n\n[1] TRUE\n\n\nCode\n(a &gt; 10) & (b &gt; 50)\n\n\n[1] FALSE\n\n\nCode\n(a &gt; 10) | (b &gt; 50)\n\n\n[1] TRUE\n\n\nCode\nb %% a == 0\n\n\n[1] TRUE\n\n\nCode\na / b == 0.50\n\n\n[1] TRUE\n\n\nCode\nb / 3 == 16.666667\n\n\n[1] FALSE\n\n\nCode\nb / 3 - 16.666667 &lt; 10^(-9)\n\n\n[1] TRUE\n\n\n\n\n\n\n\n3.3.3 A note on numeric\nRecall that numeric values are stored as series of 1 and 0 and the precision is limited to 15-17 digits. For boolean operators, this precision has implications. For instance, let’s compare\n\na &lt;- 1/3\n\nwith b = 0.33333 ...(15 digits)\n\nb &lt;- 0.333333333333333\n\nand c = 0.33333 ...(16 digits)\n\nc &lt;- 0.3333333333333333\n\n\na == b\n\n[1] FALSE\n\na == c\n\n[1] TRUE\n\n\nAs you can see, the first condition is FALSE. In other words, for R, 1/3 is not equal to 0.333333333333. However, add one digit and R evaluates the second condition as TRUE. The same issue can occur if you check the results of two different calculations are equal. If you use a condition such as res_cal_1 == res_calc_2, the outcome could be FALSE due to the limited precision of numeric variables. Likewise, the results of some calculations will show “weird” results. For instance, we know that \\[(\\sqrt{10})^2 = 10\\]\nIf you test this condition in R:\n\n(sqrt(10))^2 == 20\n\n[1] FALSE\n\n\nThe result is FALSE. Note that this is not always the case, for instance\n\n(sqrt(4))^2 == 4\n\n[1] TRUE\n\n\nThere are a couple of ways to deal with this. The first uses round(). If you add sufficient digits (e.g. 9 or 12), the test will be TRUE if the rounding error is every small:\n\nround(a, digits = 12) == round(b, digits = 12)\n\n[1] TRUE\n\n\nA second way is to calculate the difference and check if the difference is small enough to treat it as 0. Here we test if the difference between a and b is smaller than 0.000000000001 (10^(-12)). If that is the case, we assume that a and b are sufficiently close be the equal:\n\na - b &lt; 10^(-12)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nTipDoes demand equal supply?\n\n\n\nTo illustrate that you need to be careful with == in calculations, consider this simple Economics 101 demand and supply model. Suppose that demand is shown as\n\\[\nQ_D = D_0 - d_1 * P\n\\]\nwith \\(D_0 = 700\\) and \\(d_1 = 0.80\\)\nand the inverse supply function is equal to\n\\[\nQ_S = S_0 + s_1 * P\n\\]\nwith \\(S_0 = 10\\) and \\(s_1 = 2.50\\). The following code calculates the equilibrium.\n\n# Demand function Q_D = D0 - d1 * P\nD0 &lt;- 700\nd1 &lt;- 0.80\n\n# Invese supply function: Q_S = S0 + s1 * P\nS0 &lt;- 10\ns1 &lt;- 2.50\n\n# Solve model: using the inverse supply function, determine the supply function\n# Supply function: P = (1/s1) * Q_S - (1/s1) * S0\n# Insert supply function demand and determine equilibrium quantity via Demand\n\neq_Q &lt;- (s1 / (s1 + d1)) * D0 + (s1 / (s1 + d1)) * (d1 / s1) * S0\n\nprint(glue::glue(\"Equilibrium quantity is \", {eq_Q}))\n\nEquilibrium quantity is 532.727272727273\n\n# Use inverse demand to calculate the equilibrium price eq_P\n\neq_P &lt;- -(1/d1) * (eq_Q - D0)\n\nprint(glue::glue(\"Equilibrium price is \", {eq_P}))\n\nEquilibrium price is 209.090909090909\n\n# Using the equilibrium price, determine total demand\n\nQ_D &lt;- D0 - d1 * eq_P\n\nprint(glue::glue(\"Demand equals \", {Q_D}))\n\nDemand equals 532.727272727273\n\n# and total supply\n\nQ_S &lt;- S0 + s1 * eq_P\nglue::glue(\"Supply equals \", {Q_S})\n\nSupply equals 532.727272727273\n\n\nLet’s check if there is an equilibrium using the condition that Q_S == Q_D:\n\nif (Q_S == Q_D) {\n  print(\"Market is in equilibrium\")\n} else {\n  print(\"No market equilibrium\")\n}\n\n[1] \"No market equilibrium\"\n\n\nAs you can see, R shows no equilibrium. Even if you can spot from the output of the model that demand equals supply, R shows that this is not the case. Using an alternative - the difference is very small - we have equilibrium\n\nif(Q_S - Q_D  &lt; 10^(-8)) {\n  print(\"Market is in equilibrium\")\n} else {\n  print(\"No market equilibrium\")\n}\n\n[1] \"Market is in equilibrium\"",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html#character-variables-or-strings",
    "href": "03_Data_types.html#character-variables-or-strings",
    "title": "3  Data types",
    "section": "3.4 Character variables or strings",
    "text": "3.4 Character variables or strings\n\n3.4.1 The packages: stringr and glue\nCharacter or string variables include letters, words, sentences or a space. The {stringr} and {glue} packages include a lot of functions that you can use to work with strings. You can load these packages using the library() function. They should be available as you installed them in the Chapter 1. If you didn’t install these packages, you can do so now. If you did, you can skip this part and load the packages. Recall that you used {glue} in Chapter 2 to plot the daily stock market returns. There, you used this package to insert a variable in the title of a graph.\nSome of these functions - with a different name - are also available in base R. If your work doesn’t require a lot of string manipulation, you can use the base R string functions. To differentiate between base R functions and functions from {stringr} and {glue}, if used here, the latter always include the package name and are written as stringr::and glue::. This clearly shows where the function comes from. In addition, you don’t have to load these packages. If you do, you can leave out these package references.\n{stringr} has the advantage that it is consistent with a lot of other tidyverse packages: all function are written as a verb “extract,”replace”, the first argument is always the data, … . The base R function often do not follow this logic. This is also the case here. For instance, we’ll use the grepl() function to look for patterns in a character variable. As you can see, this is not a verb and when we discuss the function, you’ll see that the first argument is not a character but a pattern. As {stringr} follows the tidyverse logic, it always starts with the variable and then the pattern. However, as base R functions don’t require you to call a package - either using the library function or through the package::function approach - they are generally faster than the {stringr} functions.\n\n\n3.4.2 String or character variables\nTo assign characters, you use \" \". Anything between \" \" will be stored as a character: you can also use them for numbers. However, if you do so, the number is not stored as a numeric type, but as a character type. R will also treat anything between ' ' as a string. However, most people use \" \" and this is what we’ll do here as well. We will use the words “character” and “string” variables interchangeably. Here are a couple of examples:\n\nchar1 &lt;- \"A\"\nchar2 &lt;- \"2.25\"\nchar3 &lt;- \"Hello World!\"\nchar4 &lt;- \" \"\n\nYou can verify is a variable is a character in two ways. First, you can ask for its type:\n\ntypeof(char1)\n\n[1] \"character\"\n\n\nSecond, you can use is.character() to verify a variable is a string:\n\nis.character(char2)\n\n[1] TRUE\n\n\nHow long are these strings? You would think that the length is char1 is 1 and the lenght of char3 is 12. If you ask for the length of these variables:\n\nlength(char1)\n\n[1] 1\n\nlength(char2)\n\n[1] 1\n\nlength(char3)\n\n[1] 1\n\nlength(char4)\n\n[1] 1\n\n\nyou’ll see that they are all equal to 1. In other words, they are all atomic vectors or vectors of length 1 even if “Hello World!” includes 10 letters, 2 words, an exclamation mark and a space. Note that a space is treated as a character. If you look at char4:\n\nprint(char4)\n\n[1] \" \"\n\n\nYou’ll see that R prints a space. As spaces are treated as a separate character, it follows that\n\nchar5 &lt;- \"name:\"\nchar6 &lt;- \"  name:  \"\n\nare treated as two different strings. char2 is “2.25”. Although it looks like a number, R will treat it as a string. In other words, you can not apply mathematical functions:\n\nchar2 * 2\n\nError in `char2 * 2`:\n! non-numeric argument to binary operator\n\n\n\n\n3.4.3 Useful string operations\nData management and analysis is often associated with numbers, statistics and math. This is only part of the story. Text based data is widely available and can be retrieved from e.g. emails, pdf or word files, social media posts or in the reviews left on Tripadvisor; transactions include a description of the product sold or include names of companies, countries, cities or regions. In addition, some observations include both characters as well as numbers which necessitates that have to split these in a character and number part to be able to use them in calculation. Here, we’ll introduce some functions that you can use to workd characters in a dataframe and to analyse text based data. We”ll focus on single (atomic) characters. We’ll use both base R function as well as function included in the {stringr} package. If you don’t need a lot of string operations, you can often use the former. Loading {stringr} takes up memory. If you don’t need its full functionality, base R function will be less memory intensive. I’ll refer to {stringr} functions using the stringr:: way of using them. Function that do not start with stringr:: are base R functions. I’ll write the function in full, including the argument names. Recall from Chapter 1 that as long as you keep the order of the arguments of a function unchanged, you don’t need to add them (rnorm(100, 5, 20) = rnorm(n = 100, mean = 5, sd = 20)) . Here, I’ll add these arguments in the function call. As you become more familiar with these functions, you can drop them. In addition, recall that you don’t need to specify default values. Here, if they matter, I’ll do that anyway.\n{stringr} was designed to work with other packages including e.g. {dplyr} or {tidyr}. In subsequent chapters, we’ll use that fact to tidy and clean data. For instance, if there is a value “USD 250”, we’ll need to split this up in a variable currency which includes that value “USD” and a variable amount, which includes the number “250”.\n\n3.4.3.1 Change characters to upper or lower case\nSuppose you have a character variable\n\nchar_a &lt;- \"SALES NOVEMBER\"\n\nand you need lowercase. Using tolower()allows you to change all letters to lowercase\n\nchar_b &lt;- tolower(char_a)\nchar_b\n\n[1] \"sales november\"\n\n\nYou can also change lowercase to uppercase using toupper():\n\nchar_c &lt;- toupper(char_b)\nchar_c\n\n[1] \"SALES NOVEMBER\"\n\n\n{stringr} includes functions with a similar outcome: stringr::str_to_upper() and stringr::str_to_lower(). In addition, stringr::str_to_title() adds an upper case to the first letter of a word:\n\nstringr::str_to_title(char_c)\n\n[1] \"Sales November\"\n\n\n\n\n3.4.3.2 Combine multiple strings\nThe base R paste()and paste0() functions allow you to combine two or more strings. If you use paste(), you need to include the strings you want to concatenate as well as the separator (i.e. what you want to include between these two strings (e.g. a space, an underscore “_”, …)). The default value here is a space paste(..., sep = \" \", collapse = NULL, recycle0 = FALSE). With paste0(), you don’t need to include the separator: paste0(..., collapse = NULL, recycle0 = FALSE). In both functions the ... show where you include the strings that you want to combine in a single string. For now, you can leave the collapse = NULL default and recycle0 = FALSE. We’ll use it with character vectors. Let’s see what these functions do. First define two strings:\n\nchar_a &lt;- \"Your name is\"\nchar_b &lt;- \"Name\"\n\nIf you use paste() and you also accept the default sep = \" \" to concatenate these two string variable, the outcome shows:\n\npaste(char_a, char_b)\n\n[1] \"Your name is Name\"\n\n\nAs you can see, the first string “Your name is” and the second string “Name” are now one string. Between the first and the second string, R included a space. This is the default value for the separator (see sep = \" \"). Here, we included two character variables, but you could add more. In addition, you don’t need to include your character variables first but you can define them in the function as well. However, in most cases, you’ll have a character variable in your data.\n\npaste(\"one string\", \"a second string\", \"another string\", \"yet another string\")\n\n[1] \"one string a second string another string yet another string\"\n\n\nSuppose you would like a : as a separator. In that case you would change the default in the sep = argument and include the separator you need, e.g. :, _, |, & or any other sign, word or letter:\n\npaste(char_a, char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\npaste(char_a, char_b, sep = \"and \")\n\n[1] \"Your name isand Name\"\n\npaste(char_a, char_b, sep = \"& \")\n\n[1] \"Your name is& Name\"\n\npaste(char_a, char_b, sep = \"a couple of words \")\n\n[1] \"Your name isa couple of words Name\"\n\n\npaste() can be used to e.g. create names of files. Suppose you need to read a file “market_share_month_year.csv” as part of your monthly workflow. Using paste() you can build that file name:\n\ndataset_1 &lt;- \"market_share\"\nmm &lt;- \"january\"\nyy &lt;- \"2026\"\nname_file &lt;- paste(paste(dataset_1, mm, yy, sep = \"_\"), \"csv\",  sep = \".\")\n\nUsing {here}, you can now use here::here(\"data\", \"raw\", file_name). In your code, you only need to change mm and yy to read or save the file. In addition, you can use this to e.g. define variable names, or create character values in variables.\nIf you use paste0(), R puts both strings one after the other without a separator.\n\npaste0(char_a, char_b)\n\n[1] \"Your name isName\"\n\n\nIn other words, you need to include the separator in your strings:\n\nchar_c &lt;- \"Your name is: \"\npaste0(char_c, char_b)\n\n[1] \"Your name is: Name\"\n\n\nRecall that the pipe operators copies what its finds on the left hand side as the first argument of the function on the right hand side. In other words, you can use paste() or paste0() also using that operator:\n\nchar_c |&gt; paste0(char_b)\n\n[1] \"Your name is: Name\"\n\n\n{stringr}’s str_c() function performs the same operation as paste():\n\nstringr::str_c(char_a, char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\n\nHowever, it is designed to be used with other {tidyverse} packages and functions such as {dplyr} ’s mutate() function and you can use the pipe operator:\n\nchar_a |&gt; stringr::str_c(char_b, sep = \": \")\n\n[1] \"Your name is: Name\"\n\n\n{glue} can be used to “insert” a variable value in a string using { }. Suppose that you have a variable\n\nsurname &lt;- \"Alex\"\n\nand you want to add that surname to the string char_a. You could use the previous functions, but those wouldn’t allow you to insert that name in the middle of a string. Glue does.\n\nglue::glue(char_a, \": \", {surname})\n\nYour name is: Alex\n\n\nWe used this functionality in Chapter 2 to fill in the year in the animated plot. Using {glue} you can insert many variables:\n\nname &lt;- \"Anna\" \nage &lt;- 22 \nanniversary &lt;- as.Date(\"2025-10-12\") \nglue::glue('My name is {name}, ',   \n           'my age next year is {age}, ',   \n           'my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.')\n\nMy name is Anna, my age next year is 22, my anniversary is zondag, oktober 12, 2025.\n\n\nNote that my computer’s system language is dutch. So here, it puts the day as “zondag” and not “sunday”, and add “oktober” and not “October”. This {glue} functionality was incorporated in the {stringr} package in the str_glue() function. For instance, the first example using {stringr}’s str_glue() function:\n\nstringr::str_glue(char_a, \": \", {surname})\n\nYour name is: Alex\n\n\nFor the second example, the changes to the {glue} code are similarly small:\n\nname &lt;- \"Anna\" \nage &lt;- 22 \nanniversary &lt;- as.Date(\"2025-10-12\") \nstringr:::str_glue('My name is {name}, ',   \n                   'my age next year is {age}, ',   \n                   'my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.')\n\nMy name is Anna, my age next year is 22, my anniversary is zondag, oktober 12, 2025.\n\n\nIf you don’t need to load {stringr}, you can use {glue}. Although the former includes more functions (as we’ll she shortly), the latter is more focused and doesn’t take up much memory.\nIf you need to duplicate individual strings, str_dub(), can be used to do so. Suppose you need to duplicate the string “abc” 5 times. Let’s first define this string:\n\nchar_d &lt;- \"abc\"\n\nwith str_dub(string = str, times = n) you can duplicate the string “str” n times. Here, there are no default values so you need to add both arguments:\n\nstringr::str_dup(string = char_d, times = 5)\n\n[1] \"abcabcabcabcabc\"\n\n\nNote that the outcome is one character, not 5 character variables.\n\n\n3.4.3.3 Whitespaces\nWhitespaces, or spaces for short, are sometimes included in a string. For instance, suppose you have\n\nchar_space1 &lt;- \" year\"\n\nRecall that for R a space is a separate character. In other words “year” and ” year” are treated as different. You can remove these whitespaces using {stringr}’s str_trim(string, side = c(\"both\", \"left\", \"right\"))and str_squish(string) functions. Using the first, you need to add which spaces you want to remove: those on the left, the right or both. Both is here default. Removing the space in the char_space string:\n\nstringr::str_trim(string = char_space1, side = \"left\")\n\n[1] \"year\"\n\n\nstr_squish() removes all strings to the left and right and replaces internal spaces with a single space. For instance,\n\nchar_space2 &lt;- \" Here, there are    multiple spaces   left, right and   middle  \"\n\nUsing str_squish(), you can remove all those:\n\nstringr::str_squish(char_space2)\n\n[1] \"Here, there are multiple spaces left, right and middle\"\n\n\nYou can eliminate all white spaces using str_remove_all(string, pattern = \" \").\n\nstringr::str_remove_all(string = char_a, pattern = \" \")\n\n[1] \"Yournameis\"\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nSuppose that you have to save a file and you want to generate the name of that file from code. You have the following variables\n\nprodid &lt;- \"AB2503\"\ncountry &lt;- \"SWE\"\nyear &lt;- \"2025\"\n\nUse both paste() and paste0() to create a name for that file: year_country_prodid.csv. Assign this name to a variable datafileand datafile0\n\n\nCode\ndatafile &lt;- paste(paste(year, country, prodid, sep = \"_\"), \"csv\", sep = \".\")\ndatafile\n\n\n[1] \"2025_SWE_AB2503.csv\"\n\n\nCode\ndatafile0 &lt;- paste0(year, \".\", country, \".\", prodid, \".csv\")\ndatafile0\n\n\n[1] \"2025.SWE.AB2503.csv\"\n\n\nUse {glue} to create a string “my filename is” followed by the name of the file you just created in datafile.\n\n\nCode\nglue::glue(\"my filename is \", {datafile})\n\n\nmy filename is 2025_SWE_AB2503.csv\n\n\nNow create the same filename using a {stringr} function and assign this name to a variable datafiles\n\ndatafiles &lt;- stringr::str_c(year, country, prodid, sep = \"_\") |&gt;\n  stringr::str_c(\"csv\", sep = \".\")\ndatafiles\n\n[1] \"2025_SWE_AB2503.csv\"\n\n\nSuppose you have the following variables\n\nvar1 &lt;- \"Per Capita GDP\"\nvar2 &lt;- \"  Inflation rate\"\n\nChange these names and eliminate uppercase letters and remove all whitespaces. To do so for the first, use one line of code and don’t use the pipe operator. For the second, use one line of code with the pipe operator.\n\n\nCode\nvar1 &lt;- stringr::str_remove_all(tolower(var1), pattern = \" \")\nvar1\n\n\n[1] \"percapitagdp\"\n\n\nCode\nvar2 &lt;- tolower(var2) |&gt; stringr::str_remove_all(pattern = \" \")\nvar2\n\n\n[1] \"inflationrate\"\n\n\n\n\n\n\n\n3.4.3.4 Identifying patterns in a string\nLet’s define new strings, a quote by JM Keynes:\n\nchar_a &lt;- \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\nRecall that for R, char_a is an atomic vector even is there are multiple words, spaces, … :\n\nlength(char_a)\n\n[1] 1\n\n\nHere, I used a quote with two sentences but this character variable could be an email with multiple lines, a product or service review. We can calculate the number of letters in this string using base R’s nchar(x, type = c(\"chars\", \"bytes\", \"width\"), allowNW = FALSE, keepNA = NA). You can disregard the latter two options and keep their default values. The option “chars”, which is the default, calculates the number of characters in a string. The other two show the number of bytes needed to store the string and “width” shows the number of columns you need to print the string in monospaced font. Usually, you won’t need those two options. How long is char_a?\n\nnchar(x = char_a, type = \"chars\")\n\n[1] 123\n\n\nchar_a has 123 characters (including white spaces).\nTo detect a pattern in a character variable, you can use base R’s grep() and grepl() functions. grep stand for Global Regular Expression Print. We will cover regular expressions at the end of this section and more in depth in the next chapter. Here, the patterns with match words or parts of words. As we’ll see with regular expressions, this doesn’t have to be the case.\nThe difference between grep() and grepl() is that the first shows either the position of a pattern or its value while the last will evaluate to TRUE if a pattern is detected and FALSE if that is not the case. The most important arguments of grep() include pattern, x, ignore.case = FALSE, value = FALSE. These arguments include the pattern that you are looking for in string x. By default, the function is case sensitive (ignore.case = FALSE) and shows the position of the character (value = FALSE) and not its value. Here, we only have one character variable, so the position will always be 1. Let’s use this function to check if the character variable char_a includes the pattern “pounds”, “pou” or “POU”:\n\ngrep(pattern = \"pounds\", x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\ngrep(pattern = \"pou\", x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\ngrep(pattern = \"POU\", x = char_a, ignore.case = FALSE, value = FALSE)\n\ninteger(0)\n\n\nAs you can see, R detect the patterns “pounds” and “pou” while it can not detect the patter “POU”. Here, the R returns an empty position integer(0). Changing the default ignore.case = FALSE in TRUE, would change that:\n\ngrep(pattern = \"POU\", x = char_a, ignore.case = TRUE, value = FALSE)\n\n[1] 1\n\n\nR detects the pattern “POU” in char_a. Changing the default value = FALSE in TRUE, R shows the character where it detects the pattern. Because we have one character, R returns the full character.\n\ngrep(pattern = \"pou\", x = char_a, ignore.case = FALSE, value = TRUE)\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nYou don’t have to add the pattern within grep(). If you define it outside of the function, you can use that definition within the function.\n\npat1 &lt;- \"pou\"\ngrep(pattern = pat1, x = char_a, ignore.case = FALSE, value = FALSE)\n\n[1] 1\n\n\ngrepl() has the same arguments, except for the value argument. As grepl() returns a logical value TRUE of FALSE in case the pattern was detected or noy, if always shows the same output: TRUE of FALSE. In other words, you don’t need to tell R what the output should be:\n\ngrepl(pattern = \"mana\", x = char_a)\n\n[1] TRUE\n\n\nChanging the ignore.case = FALSE to TRUE, glepl() will show TRUE if the pattern occurs, whether this pattern is in upper or lowercase:\n\ngrepl(pattern = \"MANA\", x = char_a)\n\n[1] FALSE\n\n\nWe can also use {stringr} to detect if a pattern occurs. For instance, does the character variable include a word where “pou” occurs? To check this, you can use the str_detect(string, pattern, negate = FALSE). Here, you need to fill out the string (char_a in this example) and the pattern (pou in this example). You can keep the negate = FALSE default:\n\nstringr::str_detect(string = char_a, pattern = \"pou\")\n\n[1] TRUE\n\n\nAs you can see, the pattern “pou” is included in the string. Note that R is case sensitive. {stringr} will fail to detect “Pou”:\n\nstringr::str_detect(string = char_a, pattern = \"Pou\")\n\n[1] FALSE\n\n\nIf you would change the default negate = FALSE to negate = TRUE, the results of these functions would be FALSE and TRUE. This can be useful if you want to keep strings where a given pattern does not occur. In that case, str_detect() will be FALSE if it detects a pattern and TRUE if it doesn’t detecte a pattern.\nThis function is useful is you want to detect the presence of a string in e.g. an if statement. If string “x” occurs, then do something, else do something else. For instance, suppose you want to import files where the name includes “sales”, e.g.\n\nchar_file &lt;- \"2024_november_sales\"\n\n\nstringr::str_detect(string = char_file, pattern = \"sales\")\n\n[1] TRUE\n\n\nR returns true. You can use this statement to import files. Recall from Chapter 2, that we used is.numeric() and the where() function to select numeric variables in a data frame. In a similar way, you use grepl() or str_detect() to select variables whose name include a pattern.\nWe now know that “pounds” is included in char_a. Let’s now count the number of times “pounds” occurs in this string. To do so, we use the str_count(string, pattern = \" \") function:\n\nstringr::str_count(string = char_a, pattern = \"pounds\")\n\n[1] 2\n\n\nNote that pattern = \"pou\" would show the same result. However, this is due to the fact that “pound” is the only word that includes this pattern. If a string would also include “vapour” or “spouse” that would not be the case. If you know the number of occurrences, you can e.g. create a word cloud or a chart show how many times a specific word of pattern is used in a text.\nLet’s find the location of the word “pounds”. To do so we will use str_locate(string, pattern) of str_locate_all(string, pattern). The former finds the first occurrence and then stops, the second shows all occurrences. So, if you only want to know the first occurrence of “pounds”, you can use str_locate():\n\nstringr::str_locate(string = char_a, pattern = \"pounds\")\n\n     start end\n[1,]    41  46\n\n\nThe pattern “pounds” is located between position 41 (letter p) and position 46 (letter s) in char_a. Where is the second “pounds”? Using str_locate_all() you can find out:\n\nstringr::str_locate_all(string = char_a, pattern = \"pounds\")\n\n[[1]]\n     start end\n[1,]    41  46\n[2,]    96 101\n\n\nNote that in this case pattern = \"pou\" would show a different result: here R would show position 41 to 43 as it wouldn’t include the last letter “nds”. You already know the pattern “pounds” occurred twice and you know the location of the first, now you know the location of the second: between the 96th and 101st position.\nIf you want to extract the word, you can use {stringr} ’s str_extract(string, pattern, group = NULL) or str_extract_all(string, pattern, simplify = FALSE) function. The first function extracts the first occurrence (and only the first occurrence), the second extracts all occurrences:\n\nstringr::str_extract_all(string = char_a, pattern = \"pounds\", simplify = FALSE)\n\n[[1]]\n[1] \"pounds\" \"pounds\"\n\n\nstr_extract() extracts a pattern. Recall that we know where the word “pounds” is. We can use that information to extract that word using its location. To do so, we can use base R’s substr(x, start, stop) function or {stringr}’s str_sub(string, start, end). The first function needs the string and the position where it needs to start extracting and the position where it needs to stop doing so:\n\nsubstr(x = char_a, start = 41, stop = 46)\n\n[1] \"pounds\"\n\n\nstr_sub()needs a string, a start and end position to extract the pattern:\n\nstringr::str_sub(string = char_a, start = 41, end = 46)\n\n[1] \"pounds\"\n\n\nYou also use negative numbers for start and end. In that case, str_sub() counts backwards from the end:\n\nstringr::str_sub(string = char_a, start = -11, end = -1)\n\n[1] \"your mercy.\"\n\n\nThis can be useful if you want to extract the middle part of a string. Suppose you have a character variable\n\nchar_mid1 &lt;- \"1000 Brussels (BEL)\"\n\nand you need to extract the city name “Brussels”. Brussels starts at location 6 (starting from the left) and ends at location -7 (starting from the right):\n\nstringr::str_sub(string = char_mid1, start = 6, end = -7)\n\n[1] \"Brussels\"\n\n\nStarting from the left, removes “1000”, starting from the right removes ” (BEL)“. If all you observations follow a similar pattern, you can extract all city names:\n\nchar_mid2 &lt;- \"2000 Antwerpen (Berchem) (BEL)\"\n\n\nstringr::str_sub(string = char_mid2, start = 6, end = -7)\n\n[1] \"Antwerpen (Berchem)\"\n\n\nThis function can be useful to extract information from e.g. the name of a file. Suppose that you have a file “sales_2025.csv”. The dataset doesn’t include the reference year as one of its variables, but you would like to add that year as a separate variable. You know that the year starts at position -8 and ends at position -5. In other words:\n\nstringr::str_sub(\"sales_2025.csv\", start = -8, end = -5)\n\n[1] \"2025\"\n\n\nextracts the year. If you store this value as.numeric() you can add it to your dataset as a separate variable. If all filenames follow a similar pattern, you can automate that proces.\nIf you want to remove a pattern, you can use str_remove(string, pattern) or str_remove_all(string, pattern). As in the previous case, the former removes only the first occurrence, the second removes all occurrences. We’ll save the outcome in a new string:\n\nchar_b &lt;- stringr::str_remove(string = char_a, pattern = \"pounds\")\nchar_b\n\n[1] \"If you owe your bank manager a thousand , you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\n\nchar_c &lt;- stringr::str_remove_all(string = char_a, pattern = \"pounds\")\nchar_c\n\n[1] \"If you owe your bank manager a thousand , you are at his mercy. If you owe him a million , he is at your mercy.\"\n\n\nThis function is useful is you need to remove, for instance, values from a string. Suppose you have a data frame where observations are recorded as\n\nchar_obs &lt;- \"20 cm\"\n\nIf you need the “20” to do calculations, you’ll need to remove the space and cm from that string. Using stringr::str_remove() as as.numeric() you can do this:\n\nchar_obs &lt;- stringr::str_remove(string = char_obs, pattern = \" cm\")\nchar_obs &lt;- as.numeric(char_obs)\nchar_obs\n\n[1] 20\n\ntypeof(char_obs)\n\n[1] \"double\"\n\n\nCentimeter (cm) is only one example. Often, datasets include values such as “$20”, “30 usd”, “eur 30”, “20l”, “size 30x32”, … . These all prevent you from using the “20”, “30” or “32” in calculations.\nIn addition, you can use this function to remove whitespaces. You’ll find a lot of whitespaces in your work: “sales november”, “long filename.xls”, “stock returns per month”, … . Sometimes it is useful to remove them.\n\nstringr::str_remove_all(string = char_a, pattern = \" \")\n\n[1] \"Ifyouoweyourbankmanagerathousandpounds,youareathismercy.Ifyouowehimamillionpounds,heisatyourmercy.\"\n\n\nYou can remove a pattern, but also replace a pattern. To do so, you can use base R functions sub() and gsub() of {stringr}’s str_replace(string, pattern, replacement) or str_replace_all(string, pattern, replacement).\nsub()and gsub()are both base R function that allow you to replace a pattern in a string. A pattern could be, e.g. a couple of letters, a word, … . We’ll also introduce regular expressions. These too allow you to build patterns. For now, we’ll use recognizable patterns such as a word or a couple of letters. Both base R function’s arguments include pattern, replacement, x, ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE. The first tells R which patterns it needs to identify and replace. The second includes the pattern it needs use to replace the pattern in the character variable x. By default, this search is case sensitive. You can change that is you change the default FALSE to TRUE in ignore.case = FALSE.\nWe will illustrate these two functions using char_a. Suppose you would like to change the word “pounds” in “dollar”. Using sub() you could do that using:\n\nsub(pattern = \"pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nAs you can see, the first “pounds” has been replaced with “dollars”. However, the second wasn’t. This is where sub() and gsub() differ. The first will replace the first occurrence of the pattern, but not the next. gsub() replaces all occurrences:\n\ngsub(pattern = \"pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nTo illustrate the use if for default case sensitive case, Let’s change “pounds” into “Pounds”:\n\ngsub(pattern = \"Pounds\", replacement = \"dollars\", x = char_a, ignore.case = FALSE)\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nAs you can see, gsub() didn’t replace “pounds” as due to R’s case sensitivity. Let’s now change the default ignore.case from FALSE in TRUE\n\ngsub(pattern = \"Pounds\", replacement = \"dollars\", x = char_a, ignore.case = TRUE)\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nHere, you can see that gsub() replaced pounds as it ignores R’s sensitivity.\nYou can also use {stringr} function to replace patterns. Suppose again you want to replace “pounds” with “dollars”. The first function str_replace() replaces the first occurrence and is very similar to sub():\n\nchar_d &lt;- stringr::str_replace(string = char_a, pattern = \"pounds\", replacement = \"dollars\")\nchar_d\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million pounds, he is at your mercy.\"\n\n\nstr_replace_all() is similar to base R’s gsub() and replaces all patterns:\n\nchar_d &lt;- stringr::str_replace_all(string = char_a, pattern = \"pounds\", replacement = \"dollars\")\nchar_d\n\n[1] \"If you owe your bank manager a thousand dollars, you are at his mercy. If you owe him a million dollars, he is at your mercy.\"\n\n\nIf you want replace whitespaces with an underscore “_”, using this function or gsub():\n\nstringr::str_replace_all(string = char_a, pattern = \" \", replacement = \"_\")\n\n[1] \"If_you_owe_your_bank_manager_a_thousand_pounds,_you_are_at_his_mercy._If_you_owe_him_a_million_pounds,_he_is_at_your_mercy.\"\n\n\nIn char_a we have two sentences and many words. Using str_split() and boundary() we can split this string in two sentences or break it in words. Using pattern = boundary(\"sentence\") R will (try to) break up char_a in sentences. Using pattern = boundary(\"words\") breaks char_a up in words. There are two more alternatives: boundary(\"character\") breaks the string up in single characters and boundary(\"lines\") breaks the string up in line. The str_split() also accepts other patterns. The function has two default values: n = Inf and simplify = FALSE. Let’s split up in sentences:\n\nchar_e &lt;- stringr::str_split(string = char_a, pattern = stringr::boundary(\"sentence\"), n = Inf, simplify = FALSE)\nchar_e\n\n[[1]]\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. \"\n[2] \"If you owe him a million pounds, he is at your mercy.\"                 \n\n\nNow you can access the first sentence using\n\nchar_e[[1]][1]\n\n[1] \"If you owe your bank manager a thousand pounds, you are at his mercy. \"\n\n\nand the second sentence\n\nchar_e[[1]][2]\n\n[1] \"If you owe him a million pounds, he is at your mercy.\"\n\n\nIf you split is up in words:\n\nchar_f &lt;- stringr::str_split(string = char_a, pattern = stringr::boundary(\"word\"), n = Inf, simplify = FALSE)\nchar_f\n\n[[1]]\n [1] \"If\"       \"you\"      \"owe\"      \"your\"     \"bank\"     \"manager\" \n [7] \"a\"        \"thousand\" \"pounds\"   \"you\"      \"are\"      \"at\"      \n[13] \"his\"      \"mercy\"    \"If\"       \"you\"      \"owe\"      \"him\"     \n[19] \"a\"        \"million\"  \"pounds\"   \"he\"       \"is\"       \"at\"      \n[25] \"your\"     \"mercy\"   \n\n\nYou can know access the words. For instance, the 20th word is:\n\nchar_f[[1]][20]\n\n[1] \"million\"\n\n\nLet’s see what happens if we include another pattern, e.g. “pounds”:\n\nchar_g &lt;- stringr::str_split(string = char_a, pattern = \"pounds\", n = Inf, simplify = FALSE)\nchar_g\n\n[[1]]\n[1] \"If you owe your bank manager a thousand \"         \n[2] \", you are at his mercy. If you owe him a million \"\n[3] \", he is at your mercy.\"                           \n\n\nAs you can see, char_a is now broken up in 3 pieces: the first equal the part before the first “pounds”, the second, the piece after the first and before the second “pounds” and the last part is the rest of char_a after the second “pounds”.\nThe string in char_a is long. You can shorten this string using str_trunc(string, width, side = c(\"right\", \"left\", \"center\"), ellipsis = \"...\"). Here, you take a string, determine how long it can be (width) and, if the string is longer, which parts has be be shown: the first (right), the last (left) or the middle (center). The last option allows you to show that a part of the string was removed. If you want to reduce the width of char_a to 25, this is how you can do it:\n\nstringr::str_trunc(string = char_a, width = 25, side = \"left\")\n\n[1] \"..., he is at your mercy.\"\n\nstringr::str_trunc(string = char_a, width = 25, side = \"right\")\n\n[1] \"If you owe your bank m...\"\n\nstringr::str_trunc(string = char_a, width = 25, side = \"center\")\n\n[1] \"If you owe ...your mercy.\"\n\n\n\n\n3.4.3.5 Regular expressions: a first look\nIn the previous example, the pattern we looked for was always “pou” or “pounds”, i.e. a word or part of a word. But a word is not the only pattern you can look for. This is what regular expressions do: a short way to look for patterns in a string. The terms “regular expression” is often replaced by “regex”. For instance the world “RStudio” starts with a capital letter R, then a capital letter S and small letters u, d, i and o. This is a pattern. An email adress is written as “someone@somethingelse.xyz”. Again, there is a pattern: someone, then an @ followed by something else, a dot and an abbreviation that looks like “com”, “edu”, or two letters referring to a country, e.g. “ca”, “nl”, “uk”, …; a postal code for Belgium includes 4 digits, a phone number is written as e.g. 0123 45 67 89 and a student number might look like xyz-24-0987654. A regular expression can be used to identify this type of patterns. Regular expressions are widely used. They are also included in e.g. excel.\nLet’s look at how we can use them. Suppose we have a character:\n\nchar_reg &lt;- \"1000 Brussels\"\n\nWe’ll try to extract information from this string. As you can see, the string includes a postal code and a city name. Suppose you want to extract the city name. The city postal code is a number, the name is written letters. With a regular expression, you search for patterns, not words or parts of a words. You don’t extract the city name “Brussels” using the pattern “Brussels” or “Bru” but you define your pattern in terms of “letters”, “numbers”, … in a way that should allow you to extract “Antwerp” from “2000 Antwerp” or “Leuven” from “3000 Leuven” without changing the pattern.\nThis where regular expressions enter. If you want to search for letters, you can use the pattern “[a-z]”. The regular expression “[a-z]” looks in the character variable and identifies all characters that include one of the letters a - z in lowercase. To search for multiple letters (the same or different), you add a plus “[a-z]+”. This regular expressions searches for patterns such as “a”, “b”, … , “z”, “aa”, “cd”, … “ccc”, “ddd”, “qrrt …. zt”. In other words, it moves allong the the character variable and checks if it includes one or a series of lowercase letters. Recall that R is case sensitive: [a-z] refers to all lowercase letters and it excludes uper case letters. To add the capital letter, we need [A-Z]. Without a + after [A-Z], the regular expression is equal to “1 capital letter A - Z”. If you add a plus, the regular expression search for one of more uppercase letters.\nLet’s see how these patterns can be used to extract the city name. “Brussels” is written with one capital B followed by a series of normal letters. This is a pattern that is common to a lot of cities: “Amsterdam”, “Paris”, “London” all start with a capital (“A”, “P” or “L”) and are followed by one of lower case letters; that upper case is at the start and, as there is only one, we don’t need to allow for repetition. Using regular expressions, we can use “[A-Z]” to identify the first letter. All other letters are lower case and usually include one or more lower case letters. How many normal letters depends on the city name. Using regular expressions, this pattern after the uppercase first letter can be written as “[a-z]+”. Adding both: “[A-Z][a-z]+”. This is an example of a regular expression: it searches for all occurrences of the pattern “1 upper case letter + one or more lowercase letters”. In other words, it refers to any pattern in words where the word starts with one upper case letter A, B, C, … Z and continues with any number of normal letters. We can now use this to extract the values from our character. Using {stringr}’s str_extract_all() we can extract the name of the city:\n\nstringr::str_extract_all(string = char_reg, pattern = \"[A-Z][a-z]+\")\n\n[[1]]\n[1] \"Brussels\"\n\n\nNote that this also works for other city names:\n\nchar_reg &lt;- \"75001 Paris\"\nstringr::str_extract_all(string = char_reg, pattern = \"[A-Z][a-z]+\")\n\n[[1]]\n[1] \"Paris\"\n\n\nWhat about the postal code? Here “[0-9]” allows you to search for numbers. If you add a + you can search for multiple numbers:\n\nstringr::str_extract_all(string = char_reg, pattern = \"[0-9]+\")\n\n[[1]]\n[1] \"75001\"\n\n\nIf we know that the postal code includes 4 numbers, we can add this to the regular expression. Replacing + (one or more repetitions) with {n} searches for patterns with exact n repetitions. Using this information, we can refine the regular expression. The pattern includes 4 and only 4 numbers. To do so, we add the number of repetitions between {} after [0-9]:\n\nchar_reg &lt;- \"1000 Brussels\"\nstringr::str_extract_all(string = char_reg, pattern = \"[0-9]{4}\")\n\n[[1]]\n[1] \"1000\"\n\n\nThere is an alternative. If you use \\\\d you search for any digit character:\n\nstringr::str_extract_all(string = char_reg, pattern = \"\\\\d{4}\")\n\n[[1]]\n[1] \"1000\"\n\n\nA couple of words on the double \\\\. Where does it come from? The \\ is an escape sign. If you type “d” for digit, R wouldn’t know that you refer to a digit or the letter d. That is why you use an escape backslash \\. It tells R not to look for a d, but to look for a digit. In other words, it allows you to “escape” the usual meaning of the letter d. However, \\ is a special sign in R. So R could interpret that sign in the wrong way. To avoid that, you add a second escape backslash before the firs bedore d: to tell R that the first backslash isn’t a normal backslash but an escape backslash (used to escape the usual meaning of d). In other words, the second escape sign is used the escape the usual interpretation of the backslash in R. In other words, if you want to add a pattern: any digit: you need \\\\d. Here is used str_extract_all()as an example, but all previous functions that include a patters can be used with regular expressions as well. These regular expressions are very useful if you have multiple strings (e.g. in a vector or a data set).\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nSuppose you have the following files from the earning call from company xyz. These files include the tables, press statement and presentation delivered by company xyz during their 2025-Q1 earnings call where they discuss their sales, profit, … with investors.\n\ndatafile &lt;- \"2025_Q1_earnings_call_xyz_tables.csv, 2025_Q1_earnings_call_xzy_statement.txt, 2025_Q1_earnings_call_xyz_presentation.pptx\"\n\nIn your folder, you also have similar files for the same company for other quarters (Q2, Q3 and Q4) as well as half year results and full year results for 2025. Half year results are denoted with SE1 or SE2 (semester 1 or 2): “2025_SE1_earnings …” and full year results with FY: “2025_FY_earnings…”. All filenames have the same structure: yyyy_Q1/SE1/FY_earnings_call_company_tables/statement/presentation.csv/txt/pptx and are consistent in their use of upper/lowercase.\nExtract the name of the file that includes the tables from the 2025 Q1 earnings call. Do it in such a way that you can apply the same code for another company, e.g. “abcdef” or “klmn. In other words, company names are not fixed to three positions but include more. Use regular expressions to do so. Store the result in a variable”name_file”.\n\n\nCode\nname_file &lt;- stringr::str_extract_all(datafile, pattern = \"\\\\d{4}_Q1_.+_tables\\\\.csv\")\nname_file\n\n\n[[1]]\n[1] \"2025_Q1_earnings_call_xyz_tables.csv\"\n\n\nWhy does the second dot include two backslashes?\n\n\nCode\n# The first dot: you want to allow for parts in the filename such as \n# \"earnings_call_abdcef\", or \"earnings_call_klnmo\". \n# The first dot is the regular expression for \"any single character\". \n# This character can be repeated many times, but you don't how many times. \n# The second dot: you are looking for a true dot. \n# In other words, you need to escape its meaning as \"any single character\". \n# You do that with a backslash.\n\n\nThese files are long. Remove from these files the words “earnings” and “call”. Save the name of the file “tables.csv” without “earnings_call” in a new variable: df_xyz_tab.\n\n\nCode\ndf_xyz_tab &lt;- stringr::str_remove_all(datafile, pattern = \"_earnings_call\") |&gt; stringr::str_extract_all(pattern =\"\\\\d{4}_Q1_.+tables.csv\")\ndf_xyz_tab\n\n\n[[1]]\n[1] \"2025_Q1_xyz_tables.csv\"\n\n\nA university stores its student id’s as 2025-ac-123456 with “2025” the academic year (i.e. can also be 2024), then ac, then six numbers at the end. Using regular expressions check if these variables are student id’s or include a typo:\n\nst_id1 &lt;- \"2024-ac-12365\"\nst_id2 &lt;- \"2023-ac-001223\"\nst_id3 &lt;- \"1997-ac-147225800\"\n\n\n\nCode\nstringr::str_detect(st_id1, pattern = \"\\\\d{4}-ac-[0-9]{6}$\")\n\n\n[1] FALSE\n\n\nCode\nstringr::str_detect(st_id2, pattern = \"\\\\d{4}-ac-[0-9]{6}$\")\n\n\n[1] TRUE\n\n\nCode\nstringr::str_detect(st_id3, pattern = \"\\\\d{4}-ac-[0-9]{6}$\")\n\n\n[1] FALSE\n\n\nSome datasets include such as\n\na1 &lt;- \"usd 20.23\"\na2 &lt;- \"Usd 123.26\"\na3 &lt;- \"Yen 7852.36\"\na4 &lt;- \"Eur 12.36\"\na5 &lt;- \"eur 125.12\"\n\nWrite code that allows you to split this strings in two variables: one variable “cur” with the name of the currency and one variable “price” with the numeric value representing the price. All currencies should be lowercase. Values should be numeric. First write the code for a1 and check if you can use it for all other strings.\n\n\nCode\ncur &lt;- tolower(stringr::str_extract(a1, pattern = \"[A-Za-z]{3}\"))\ncur\n\n\n[1] \"usd\"\n\n\nCode\nprice &lt;- as.numeric(stringr::str_extract(a1, pattern = \"\\\\d+.?\\\\d+\"))\nprice\n\n\n[1] 20.23",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "03_Data_types.html#dates-and-times",
    "href": "03_Data_types.html#dates-and-times",
    "title": "3  Data types",
    "section": "3.5 Dates and times",
    "text": "3.5 Dates and times\nDates are often used in data management: super markets know when - date and time - you made a purchase; during the opening hours, stocks change price every second, profits are often published per quarter and inflation or unemployment are often monthly data. Dates and time are stored in various formats, e.g. March 1, 2021; 2021-03-01; 01/03/2021; 1 march 2021; 1 march 21; 21-03-01 … The same holds for times. In addition, the world is not sharing one common time zone. In other words, if it is 12:00 in one location, in many other locations the time is different. To work with dates and times, we’ll use both base R function as well as functions from the {lubridate} package. This package was specifically designed to work with date/time variables. If a {lubridate} function is used, you’ll see that because that function will be called with lubridate::. As was the case with {stringr}, I’ll also base R functions. Often these functions will do fine. However, like any other tidyverse package, {lubridate} has the advantage that it integrates very will with other tidyverse packages;\n\n3.5.1 Dates\n\n3.5.1.1 Which day is it today?\nTo determine which day it is today, you can use the Sys.Date() function:\n\nSys.Date()\n\n[1] \"2026-01-23\"\n\n\nThis function return the current date in ISO 8601 format: YYYY-MM-DD: first the year, written with 4 digits, followed by the month, always written with 2 digits and the day always written with 2 digits with year, month and day separated by a dash or hyphen (-). As an alternative, the date can also be shown without the hyphen as 20250215. Using this standard avoids a lot of uncertainty. For instance, 2/3/2025 could mean 2nd March 2025 (day, month, year) but also February 3th 2025 (month, day, year). Using the ISO standard avoids this confusion: the first 4 digits represent the year, digits 5-6 the month and digits 7-8 the day. All months and days include two digits, even if they refer to, e.g. January (written as 01) or the third day of the month (written as 03).\nAs an alternative, you can use {lubridate}’s today(tzone = \" \") function. Using this function, you can add the timezone. For instance, if you need the current date in the Greenwhich Meantime timezone (GMT), you can use\n\nlubridate::today(tzone = \"GMT\")\n\n[1] \"2026-01-23\"\n\n\nIf you want to now the current date in Kiribati (time zone UTC + 14) or in Honolulu (time zone UTC - 10), you can use their time zones in the today() function. For Kiribati, this timezone is Pacific/Kiritimati and for Honululu it is Pacific/Honolulu (you can find a list of timezones in the IANA time zone database (tzdatabase) or in the base R file Olsonnames()\n\ndat_k &lt;- lubridate::today(tzone = \"Pacific/Kiritimati\")\ndat_k\n\n[1] \"2026-01-24\"\n\ndat_h &lt;- lubridate::today(tzone = \"Pacific/Honolulu\")\ndat_h\n\n[1] \"2026-01-23\"\n\n\n\n\n3.5.1.2 Creating dates\nSuppose you have a character variable\n\nchar_dat &lt;- \"2025-06-25\"\n\nand you want to coerce it to a date. Here, char_dat is a character variable.\n\nclass(char_dat)\n\n[1] \"character\"\n\n\nThere are various ways to change that into a date. Let’s start with base R’s as.Date(x, format, tryformats = (\"%Y-%m-%d\", \"%Y/%m/%d\"), optional = FALSE, ...) function. The function needs a variable to convert (x) but also a format, e.g. %Y-%m-%d. This format stands for 4 digit year (%Y, e.g. 2025), month as a number (%m, e.g. 06) and day as a number (%d, e.g. 25). If no format is given, as.Date()will try one of the formats (\"%Y-%m-%d\", \"%Y/%m/%d\"). You can check the meaning of these and other symbols in Table 3.1. The optional = FALSE arguments determines what R needs to do in case it can not convert a string into a date. The default, FALSE, means that R will show an NA. If TRUE, R will show an error. Using char_dat and entering the format and accepting the default for optional, we can use as.Date() to coerce the string to a date:\n\ndat_1 &lt;- as.Date(x = char_dat, format = \"%Y-%m-%d\")\n\nLet’s see what the outcome is:\n\ndat_1\n\n[1] \"2025-06-25\"\n\n\nNote that dat_1 is written as a string (it is within ” “) but its class has changed\n\nclass(dat_1)\n\n[1] \"Date\"\n\n\nAs you can see, dat_1 is now a date and it is shown using the ISO standard. In other words, as.Date() converts a string in a date and the output is consistent with the ISO standard.\nYou can check that dat_1 is a date using {lubridate} is.Date() function:\n\nlubridate::is.Date(dat_1)\n\n[1] TRUE\n\n\nIf you check the type of dat_1\n\ntypeof(dat_1)\n\n[1] \"double\"\n\n\nyou’ll see that it is a double. In order words, dat_1’s class is date but it’s type is numeric. The fact that dat_1is a double is caused by the way computers store dates and times. For dates, applications use a reference date or epoch date. Although there are differences, for a lot of software packages, that date is 1 January 1970. A date is recorded as the number of days since Thursday 1 January 1970 00:00:00 UT (where UT stands for Universal Time). This date and, as well see, also time, is also known as Unix epoch or POSIX. To see this, let’s convert dat_1 in a number and show the result:\n\ndat_num &lt;- as.numeric(dat_1)\ndat_num\n\n[1] 20264\n\n\nThis number, 2.0264^{4}, shows the number of days that have passed since 1 January 1970. In converting a data to numeric, R takes into account leap years. As a date is stored as a number, you can convert any number to a date. Here, you use as.Date(x, origin = ) function. With a numeric reference to a date, you can specify the origin. For the latter argument, the default value is 1 January 1970, but you can use other dates. For instance, Microsoft Excel’s origin is 1 january 1900. In other words, if you import data from excel with the date stored in a numeric format, you’ll have to add Excel’s origin. If we accept the default origin, converting a number to a data is straightforward:\n\ndat_2 &lt;- as.Date(25000)\ndat_2\n\n[1] \"2038-06-13\"\n\n\nas.Date() adds the number of days, 25000, to January 1, 1970 and shows that date in ISO format. Doing so, as.Date() takes into account e.g. leap years.\nYou can now verify the origin:\n\ndat_or &lt;- as.Date(0)\ndat_or\n\n[1] \"1970-01-01\"\n\n\nWhat happens with dates before 1 January 1970? These are stored as negative numbers. For instance Neil Armstrong was the first man to set foot on the moon on 20 July 1969 at 20:17 UTC. To see how this day is stored:\n\nchar_moon &lt;- \"1969-07-10\"\ndat_moon &lt;- as.Date(char_moon)\ndat_num_moon &lt;- as.numeric(dat_moon)\ndat_num_moon\n\n[1] -175\n\n\nIn other words, if you enter a negative number in as.Date()and you accept the default origin, you’ll get a date before 1 January 1970. For instance, 25 000 days before 1 January 1970, the date was:\n\nas.Date(-250000)\n\n[1] \"1285-07-10\"\n\n\nNot all dates are stored as 2025-06-30. Some are written as 2025/06/30, as 30-06-2025, as 30 July 2025, July 30, 2025, … . Using as.Date() you can specify the format. The following table shows a variety of symbols that you can use to format dates:\n\n\n\nTable 3.1: Date symbols\n\n\n\n\n\nSymbol\nDefinition\nExample\n\n\n\n\n%d\nDay as a number\n15\n\n\n%a\nAbbreviated weekday\nMon\n\n\n%A\nUnabbreviated weekday\nMonday\n\n\n%m\nMonth as a number\n04\n\n\n%b\nAbbreviated month\nApr\n\n\n%B\nUnabbreviated month\nApril\n\n\n%y\n2-digit year\n25\n\n\n%Y\n4-digit year\n2025\n\n\n\n\n\n\nTo see how this works, let’s create a date for 01/03/2025. The format of the date is “day as number”/“month as number”/“4 digit year”. In R, this is the format %d/%m/%Y (capital Y). To read that string “01/03/2025” as a date, you can use the format option in as.Date():\n\ndat_1 &lt;- \"01/03/2025\"\ndat_1 &lt;- as.Date(dat_1, format = \"%d/%m/%Y\")\ndat_1\n\n[1] \"2025-03-01\"\n\nclass(dat_1)\n\n[1] \"Date\"\n\n\nIf your date, 1 March 2025, is written as 010325, i.e. with format “day as a number”“month as a number”“2 digit year”, you set the format as %d%m%y (note the small y):\n\ndat_2 &lt;- \"01032025\"\ndat_2 &lt;- as.Date(dat_2, format = \"%m%d%y\")\ndat_2\n\n[1] \"2020-01-03\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nIf you are not sure which format it is, you can include various formats that R will test using the tryformats = c() argument in as.Date(). If you don’t specify the format, R will try one of the formats in included in the c() vector. By default, the function includes two formats to try: “%Y-%m-%d” and “%Y/%m/%d” but you can add more. Suppose you have a date “25 April 2025”. Here, the date is written as “day as a number” “unabbreviated month” “4 digit year”. You can add this format “%d %B %Y” to the formats and R will test if that format is used:\n\ndat_3 &lt;- \"25 April 2025\"\ndat_3 &lt;- as.Date(dat_3, tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\", \"%d %B %Y\"))\ndat_3\n\n[1] \"2025-04-25\"\n\nclass(dat_3)\n\n[1] \"Date\"\n\n\nEvery time we used as.Date() R shows the date as %Y-%m-%d. To print dates, that if often not the most convenient way to show the a date as you read your date backward (e.g. year first) relative to how most people refer to a date (e.g. year last). You can change that format using format(x, format = \"%Y-%m-%d). This function allows you to format a date. The default format is %Y-%m-%d. However, suppose you want to write dat_3 including the month as an unabbreviated month (%B), the full name of the day (%A) as well as the day as a number (%d) and a 4 digit year (%Y), you could use:\n\ndat_3\n\n[1] \"2025-04-25\"\n\ndat_4 &lt;- format(dat_3, format = \"%A %d %B %Y\")\ndat_4\n\n[1] \"vrijdag 25 april 2025\"\n\n\nNote the R uses your system language to show the name of the day and the month.\n{lubridate} includes a wide range of function set dates. The “ymd()”-family which includes: ymd(), ydm(), mdy(), myd(), dmy(), dym(), yq(), ym() and my() (where y refers to year, m refers to month, d refers to day and q refers to quarter). The structure of these functions is the same: ymd(..., quiet = FALSE, tz = NULL). For now, you can accept all defaults. This family of functions outputs a date with the traditional %Y-%m-%d format. To illustrate how this family works, we’ll use the dmy() function. This function will try to convert a value (numeric or character) from a format with day first, month second and year last to a date, written in ISO format. Let’s write a date (2025-03-25) in multiple day-month-year formats, e.g. as character “25/03/2025” written %d/%m/%, a numeric written as 25032025, a character “25 March 2025” written as %d %B %Y and a character “25-03-25” written as %d-%m-%y. lubridate::dmy() tries to identify the day, month and year and returns a date with %Y-%m-%d format\n\ndat_11 &lt;- \"25/03/2025\"\ndat_12 &lt;- 25032025\ndat_13 &lt;- \"25 March 2025\"\ndat_14 &lt;- \"25-03-25\"\ndat_21 &lt;- lubridate::dmy(dat_11)\ndat_22 &lt;- lubridate::dmy(dat_12)\ndat_23 &lt;- lubridate::dmy(dat_13)\ndat_24 &lt;- lubridate::dmy(dat_14)\ndat_21\n\n[1] \"2025-03-25\"\n\ndat_22\n\n[1] \"2025-03-25\"\n\ndat_23\n\n[1] \"2025-03-25\"\n\ndat_24\n\n[1] \"2025-03-25\"\n\n\nAnother useful {lubridate} function is make_date(year = , month = , day = ). If a dataset includes a separate variable for the day, the month and the year, this function allows you to collects these into one variable as a date.\n\nyear &lt;- 2025\nmonth &lt;- 3\nday &lt;- 25\ndat_3 &lt;- lubridate::make_date(year = year, month = month, day = day)\ndat_3\n\n[1] \"2025-03-25\"\n\nclass(dat_3)\n\n[1] \"Date\"\n\n\n\n\n3.5.1.3 Extracting information from a date\nOften you need to work with data aggregated per year, semester, quarter, month, week … . If your dataset includes a date, you can extract the relevant period using base R’s weekdays(), months() or quarters() or {lubridate}’s year(), semester(), quarter(), month(), week(), isoweek(), day(), wday(), yday() functions. Each function has its own arguments. Base R’s include the date you will use to extract information as well as abbreviate = which allows you to determine if you need an abbreviated e.g. month or day or not. By default base R’s functions show the full name. Let’s apply these function to a specific date, March 25, 2025:\n\ndat_1 &lt;- as.Date(\"2025-03-25\")\n\nLet’s start with base R’s three functions:\n\nweekdays(x = dat_1, abbreviate = TRUE)\n\n[1] \"di\"\n\nmonths(x = dat_1, abbreviate = FALSE)\n\n[1] \"maart\"\n\nquarters(x = dat_1, abbreviate(TRUE))\n\n[1] \"Q1\"\n\n\n{lubridate} includes similar functions but adds more options. Using year() you can extract the year:\n\nlubridate::year(dat_1)\n\n[1] 2025\n\n\nThe semester(x, with_year = FALSE) allows you to show the semester as well as the year. The default does not show the year. If you change this default to TRUE, the function shows the year.semester, e.g. 2025.1\n\nlubridate::semester(dat_1, with_year = FALSE)\n\n[1] 1\n\nlubridate::semester(dat_1, with_year = TRUE)\n\n[1] 2025.1\n\n\nThe quarter(x, type = \"quarter\", fiscal_start = 1) has more options. The type argument allows you to specify the way the function will show the output. The default is the quarter (1, 2, 3, 4). As an alternative, you can use e.g. “year.quarter” to show the year as well as the quarter, “date_first” or “date_last” to show the first or last day of the quarter or “year_start/end”. In accounting and finance, years do not always coincide with the calender year. If a firm’s fiscal year starts in November, its first quarter will end in January. To accommodate for this, you can specify when the fiscal year starts using fiscal_start = x, with x the month when the fiscal year starts. The default is January (i.e. fiscal year equals calender year). Let’s see what these format options look like:\n\nlubridate::quarter(x = dat_1, type = \"quarter\", fiscal_start = 1)\n\n[1] 1\n\nlubridate::quarter(x = dat_1, type = \"year.quarter\", fiscal_start = 1)\n\n[1] 2025.1\n\nlubridate::quarter(x = dat_1, type = \"date_first\", fiscal_start = 1)\n\n[1] \"2025-01-01\"\n\nlubridate::quarter(x = dat_1, type = \"date_last\", fiscal_start = 1)\n\n[1] \"2025-03-31\"\n\nlubridate::quarter(x = dat_1, type = \"date_first\", fiscal_start = 1)\n\n[1] \"2025-01-01\"\n\nlubridate::quarter(x = dat_1, type = \"year_start/end\", fiscal_start = 1)\n\n[1] \"2024/25 Q1\"\n\n\nIf the start of the fiscal quarter differs from the calender year and starts in e.g. November, you can add this to the function:\n\nlubridate::quarter(x = dat_1, type = \"quarter\", fiscal_start = 11)\n\n[1] 2\n\nlubridate::quarter(x = dat_1, type = \"year_start/end\", fiscal_start = 11)\n\n[1] \"2024/25 Q2\"\n\n\nThe month(x, label = FALSE, abbr = TRUE) function allows you to show the label (e.g. January or Jan). If you specify abbr = TRUE the function shows “Jan” if abbr = FALSE it will show “January”. By default the output shows the month as a number:\n\nlubridate::month(x = dat_1, label = FALSE, abbr = TRUE)\n\n[1] 3\n\nlubridate::month(x = dat_1, label = TRUE, abbr = FALSE)\n\n[1] maart\n12 Levels: januari &lt; februari &lt; maart &lt; april &lt; mei &lt; juni &lt; ... &lt; december\n\nlubridate::month(x = dat_1, label = TRUE, abbr = TRUE)\n\n[1] mrt\n12 Levels: jan &lt; feb &lt; mrt &lt; apr &lt; mei &lt; jun &lt; jul &lt; aug &lt; sep &lt; ... &lt; dec\n\n\nNote that {lubridate} orders the months as they appear in the year and not alphabetically.\nweek()and isoweek() show the number of the week: the fifth week of the year. The former show the complete seven day periods that have occurred between the date and January 1 of the same year, plus one. The latter returns the week using the ISO 8601 standard. The first day of the week in the ISO standard is a Monday and the last day of the week is a Sunday. The first week of the year using the ISO week numbering is the first week with 4 days or more in January. In other words, the first week of the year includes January 4. The first week also includes at least 3 working days (all days except Saturday, Sunday and January 1). In other words, the earliest first week of the year starts Monday December 29 and ends Sunday January 4th. The latest possible week starts on Monday January 4 and lasts to Sunday January 10. If that is the case, January 1 is part of the last week of the previous year. The last week is week 52 or week 53. In 2025, there will be a difference. week() counts 7 day periods since January 1. In 2025, week 1 according to the ISO standard started on December 30, 2024 and ended on Sunday January 5. In other words, as ISO weeks start on Monday, the week() starts counting on Wednesday, there will be a difference in week numbering for Mondays and Tuesdays. As March 25 is on a Tuesday, there will be a difference.\n\nlubridate::week(dat_1)\n\n[1] 12\n\nlubridate::isoweek(dat_1)\n\n[1] 13\n\n\nIf you would use, e.g. March 28, that wouldn’t be the case:\n\nlubridate::week(dat_1 + 3)\n\n[1] 13\n\nlubridate::isoweek(dat_1 + 2)\n\n[1] 13\n\n\nWith respect to the day, there are different ways you can look at the day: a day as part of a week, month, … year. With respect to the day as part of the month, that is how we usually look at the day: March 25 (25th day of March). With respect to the day of the week, there are usually two ways to count, depending on where your week starts: Monday or Sunday. In some countries and in line with the ISO standard, Sunday is the last day of the week. In other words, weekday 1 is Monday. In other countries, the week starts on Sunday and Monday would be weekday 2. If you want to show the day of the week number, you’ll need to take that into account. The function wday(x, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 7)) allows you to show the day of the week using a label, an abbreviated label or a number. In the latter case, you need to specify the start of the week using the argument (\"lubridate.week.start\", 7). As a default, Sunday is the first day of the week. If you want to change that into Monday, you need to change the 7 in 1. If you want to show a label (label  = TRUE) the starting day of the week does not affect the outcome as the function shows Monday or Mon. For instance:\n\nlubridate::wday(x = dat_1, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 7))\n\n[1] 3\n\nlubridate::wday(x = dat_1, label = FALSE, abbr = TRUE, getOption(\"lubridate.week.start\", 1))\n\n[1] 2\n\nlubridate::wday(x = dat_1, label = TRUE, abbr = FALSE)\n\n[1] dinsdag\n7 Levels: zondag &lt; maandag &lt; dinsdag &lt; woensdag &lt; donderdag &lt; ... &lt; zaterdag\n\nlubridate::wday(x = dat_1, label = TRUE, abbr = TRUE)\n\n[1] di\nLevels: zo &lt; ma &lt; di &lt; wo &lt; do &lt; vr &lt; za\n\n\nyday() and qday() show the day as the xth day in the year (yday()) or quarter (qday()). Here, the quarter refers to the calender year. The show the difference let’s add 50 days to March 25 so that we have a date in the second quarter. The result of these functions is:\n\nlubridate::yday(dat_1 + 50)\n\n[1] 134\n\nlubridate::qday(dat_1 + 50)\n\n[1] 44\n\n\n\n\n3.5.1.4 Calculations using dates\nRecall that dates are stored as numbers.\n\ndat_1 &lt;- as.Date(\"2025-03-25\")\ntypeof(dat_1)\n\n[1] \"double\"\n\n\nIf you add 5 to dat_1 the result will be a new date:\n\ndat_2 &lt;- dat_1 + 5\ndat_2\n\n[1] \"2025-03-30\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nAs you can see, R adds 5 days to dat_1. Here, R uses the fact that the type of dat_1 is numeric and is stored as the number of days since 1 January 1970. To calculate the new date, it add 5 to that numeric value and shows that numeric value as a date:\n\ndat_1n &lt;- as.numeric(dat_1)\ndat_1n\n\n[1] 20172\n\ndat_2n &lt;- dat_1n + 5\ndat_2n\n\n[1] 20177\n\ndat_2 &lt;- as.Date(dat_2n)\ndat_2\n\n[1] \"2025-03-30\"\n\nclass(dat_2)\n\n[1] \"Date\"\n\n\nLikewise, if you subtract 250 from dat_1 R will show the date 250 days before dat_1\n\ndat_1 - 250\n\n[1] \"2024-07-18\"\n\n\nSuppose you have a start date and you know that a period will last 3 years, 15 weeks and 4 days. Using {lubridate}’s years(), weeks(), and days() function you can determine the end of that period. Using this approach - as opposed to adding the number of days - {lubridate} will take into account leap years. For instance, suppose that the start is January 1, 2025. The period of 3 years, 15 weeks and 4 days will end\n\nstart_time &lt;- lubridate::ymd(\"2025-01-01\")\nend_time1 &lt;- start_time + lubridate::years(3) + lubridate::weeks(15) + lubridate::days(4)\nend_time1\n\n[1] \"2028-04-19\"\n\n\nIn addition to these more traditional calculation, {lubridate} also includes a number or rounding functions: round_date(), floor_date()and ceiling_date(). These function include three arguments. The first is the date to round; the second is the unit to round to (e.g. day, week, month, quarter, halfyear, season of year) and the last argument specifies the start of the week and is similar to the one we already met in wday(): week_start = getOption(\"lubridate.week.start\", 7). The see what these functions do, let’s round 2025-03-25 using all three of them:\n\ndat_1 &lt;- lubridate::ymd(\"2025-03-25\")\n\nUsing round_date() you can round a date to the nearest unit. Here, the unit is the week, month, quarter, halfyear (semester), season of year.\n\nlubridate::round_date(dat_1, unit = \"year\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-01-01\"\n\nlubridate::round_date(dat_1, unit = \"halfyear\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-01-01\"\n\nlubridate::round_date(dat_1, unit = \"season\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-01\"\n\nlubridate::round_date(dat_1, unit = \"quarter\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::round_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::round_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-24\"\n\nlubridate::round_date(dat_1, unit = \"week\")\n\n[1] \"2025-03-23\"\n\n\nHere, the nearest year is “2025” as 2025-03-25 is closer to the start of 2025 than to the start of 2026. The same holds for the nearest halfyear or semester. For the season, round_date() rounds to the month where the season starts (March 21, June 21, September 21 or December 21). As 2025-03-25 is near the end of the first quarter, round_date() rounds to the start of the second quarter. Likewise, the 25th is the month is closer to the end of the month than it is to the start of the month and the function returns the start of the following month. The nearest week is the current week and round_date will round to the start of the week. For the unit “week” the option (\"lubridate.week.start\", 1) sets the start of the week on Monday. The default would show the week starting on Sunday. This is what you see on the last line of the code block.\nfloor_date() rounds the date down to the unit. Here we’ll use “week” and “month” to illustrate its output:\n\nlubridate::floor_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-01\"\n\nlubridate::floor_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-24\"\n\n\nAs you can see, if the unit if “month”, this function rounds to the start of the month. Likewise, if the unit is week, the function rounds to the start of the current week. For the unit “week” the option (\"lubridate.week.start\", 1) sets the start of the week on Monday. The default would show the week starting on Sunday.\nceiling_date() rounds up to the unit.\n\nlubridate::ceiling_date(dat_1, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-04-01\"\n\nlubridate::ceiling_date(dat_1, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n[1] \"2025-03-31\"\n\n\nHere, the month is rounded to the start of the next month and the week is rounded to the start of the next week (Monday 2025-03-31).\n{lubridate} includes two other functions that you can use to change the date to the last day of the previous month or the first day of the current month (rollbackward(dates, roll_to_first = FALSE, preserve_hms = TRUE)) or to the first day of the next month or the last day of the current month (rollforward(dates, roll_to_first = FALSE, preserve_hms = TRUE)). Both function need a date as their first argument. By default, they roll (back or forward) to the last of the previous month (back) or the last of the current month (forward). If you change roll_to_first = FALSE to TRUE the functions will roll to the first of the current (back) of next (forward) month. The last argument, preserve_hms = TRUE will be covered when we discuss time.\nUsing rollbackward() you can identify the last day of the previous month or the first day of the current month. This is useful is you want to calculate e.g. average for the month or a change since the last day of the previous month or the first day of the current month:\n\nlubridate::rollbackward(dat_1, roll_to_first = FALSE)\n\n[1] \"2025-02-28\"\n\nlubridate::rollbackward(dat_1, roll_to_first = TRUE)\n\n[1] \"2025-03-01\"\n\n\nUsing rollforward() you can identify the last day of the current month or the first day of the next month. This is useful is you want to calculate e.g. the difference between a level end of month and the current level.\n\nlubridate::rollforward(dat_1, roll_to_first = FALSE)\n\n[1] \"2025-03-31\"\n\nlubridate::rollforward(dat_1, roll_to_first = TRUE)\n\n[1] \"2025-04-01\"\n\n\n\n\n3.5.1.5 Time spans\nTime spans can be measured in 3 ways:\n\nintervals set by two dates, a start and end date\nperiods, measured in years, months, days, hours, minutes and seconds\ndurations, measured in seconds\n\nHere, we will focus on the first two. Using {lubridate}’s interval(start, end, tz = \"\") you can create time intervals. For instance, let’s set the start of the academic year (open) at September 18, 2024 and the end of the academic year at September 13, 2025. The time zone is Brussels.\n\naca_open &lt;- \"2024-09-18\"\naca_close &lt;- \"2025-09-13\"\n\naca_year &lt;- lubridate::interval(start = aca_open, \n                                end = aca_close, \n                                tzone = \"Europe/Brussels\")\naca_year\n\n[1] 2024-09-18 CEST--2025-09-13 CEST\n\n\n{lubridate} creates a new “Interval” object whose type is double:\n\nclass(aca_year)\n\n[1] \"Interval\"\nattr(,\"package\")\n[1] \"lubridate\"\n\ntypeof(aca_year)\n\n[1] \"double\"\n\n\nUsing this object, you can now determine e.g. the length of that interval using time_length(x, unit = \" \"). For the unit you can insert e.g. “second”, “day”, “week” or “year”. To measure the length of the academic year in various units, you can use\n\nlubridate::time_length(aca_year, unit = \"month\")\n\n[1] 11.83871\n\nlubridate::time_length(aca_year, unit = \"week\")\n\n[1] 51.42857\n\nlubridate::time_length(aca_year, unit = \"day\")\n\n[1] 360\n\nlubridate::time_length(aca_year, unit = \"hour\")\n\n[1] 8640\n\n\nAs you can see, the academic year as we defined it here, lasts 11.83 months, 51.42 weeks, 360 days or 5640 hours. This function can be useful to calculate e.g. an age. Suppose you have the date of birth, e.g. September 25, 1997. To calculate the age, you can set an interval\n\ndate_birth &lt;- \"1997-09-25\"\nnow &lt;- lubridate::today()\nage &lt;- lubridate::interval(start = date_birth, end = now)\n\nRecall that you can round a number of various ways. Here, we’ll use trunc() which rounds a number by removing the decimals. Measuring the length of the interval in years and rounding by eliminating the decimals, you have a person’s age:\n\ntrunc(lubridate::time_length(age, \"years\"))\n\n[1] 28\n\n\nYou can check if a date is part of an interval using %within%(date, interval). For instance, to check if “2025-03-25” is included in the interval aca_year:\n\nlubridate::`%within%`(dat_1, aca_year)\n\n[1] TRUE\n\n\nAs an alternative, you can define a period using {lubridate}’s period() function. Using this function, you need to specify the number of years, weeks, days, hours, minutes or seconds. Focusing on the date components, you can specify this period as\n\nper &lt;- lubridate::period(year = 3, week = 15, day = 4)\nper\n\n[1] \"3y 0m 109d 0H 0M 0S\"\n\n\nYou can now use that period to e.g. calculate an end date:\n\nend_time2 &lt;- start_time + per\nend_time2\n\n[1] \"2028-04-19\"\n\n\nNote that {lubridate} includes leap years when it calculates end_time1 or end_time2.\n\n\n3.5.1.6 Other useful functions\nThere are a couple of other useful functions on the {lubridate} package. The first, days_in_month(x) shows the number of days on a month for the month includes in x. The output shows the month as well as the number of days in that month. For instance:\n\ndays_in_mar &lt;- lubridate::days_in_month(dat_1)\ndays_in_mar\n\nMar \n 31 \n\n\nTo extract the number of days, you can use\n\ndays_in_mar[[1]]\n\n[1] 31\n\n\nFor most months, you can determine that number in many other ways as the number of days is fixed. However, for the month of February, you need to now if a year is a leap year. ´days_in_month()` takes leap years into account to calculate the number of days in February. As 2024 was a leap year, we can use this a day in February 2024 to show this:\n\nfeb_leap &lt;- lubridate::ymd(\"2024-02-15\")\nlubridate::days_in_month(feb_leap)\n\nFeb \n 29 \n\n\nTo check if a year if a leap year, you can use leap_year(). For a date, the output will be TRUE if the date is part of a leap year and FALSE if that isn’t the case:\n\nlubridate::leap_year(dat_1)\n\n[1] FALSE\n\nlubridate::leap_year(feb_leap)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nHere you have character variables including dates. These all refer to December 18, 2025.\n\ndat1 &lt;- \"2025-12-18\"\ndat2 &lt;- \"25-12-18\"\ndat3 &lt;- \"18/12/2025\"\ndat4 &lt;- \"18 December 2025\"\ndat5 &lt;- \"18Dec2025\"\n\nUsing as.Date() covert all characters to dates in variables dat1_date, dat2_date … and check if the class is “Date”.\n\n\nCode\ndat1_date &lt;- as.Date(dat1, format = \"%Y-%m-%d\")\ndat1_date\n\n\n[1] \"2025-12-18\"\n\n\nCode\nclass(dat1_date)\n\n\n[1] \"Date\"\n\n\nCode\ndat2_date &lt;- as.Date(dat2, format = \"%y-%m-%d\")\ndat2_date\n\n\n[1] \"2025-12-18\"\n\n\nCode\nclass(dat2_date)\n\n\n[1] \"Date\"\n\n\nCode\ndat3_date &lt;- as.Date(dat3, format = \"%d/%m/%Y\")\ndat3_date\n\n\n[1] \"2025-12-18\"\n\n\nCode\nclass(dat3_date)\n\n\n[1] \"Date\"\n\n\nCode\ndat4_date &lt;- as.Date(dat4, format = \"%d %B %Y\")\ndat4_date\n\n\n[1] \"2025-12-18\"\n\n\nCode\nclass(dat4_date)\n\n\n[1] \"Date\"\n\n\nCode\ndat5_date &lt;- as.Date(dat5, format = \"%d%b%Y\")\ndat5_date\n\n\n[1] \"2025-12-18\"\n\n\nCode\nclass(dat5_date)\n\n\n[1] \"Date\"\n\n\nUsing dat1_date, round this data to the nearest week, where the week starts on a Monday, the nearest month and the nearest year\n\n\nCode\nlubridate::round_date(dat1_date, unit = \"week\", week_start = getOption(\"lubridate.week.start\", 1))\n\n\n[1] \"2025-12-15\"\n\n\nCode\nlubridate::round_date(dat1_date, unit = \"month\", week_start = getOption(\"lubridate.week.start\", 1))\n\n\n[1] \"2026-01-01\"\n\n\nCode\nlubridate::round_date(dat1_date, unit = \"year\", week_start = getOption(\"lubridate.week.start\", 1))\n\n\n[1] \"2026-01-01\"\n\n\nIdentify the last day of the previous month for dat1_date.\n\n\nCode\nlubridate::rollbackward(dat1_date, roll_to_first = FALSE)\n\n\n[1] \"2025-11-30\"\n\n\nAdd 7 weeks and 5 days to dat1_date and save the outcome in dat2_date\n\n\nCode\ndat2_date &lt;- dat1_date + lubridate::weeks(7) + lubridate::days(5)\ndat2_date\n\n\n[1] \"2026-02-10\"\n\n\nIn what quarter is dat2_date if a companies fiscal year start in September?\n\n\nCode\nlubridate::quarter(x = dat2_date, type = \"quarter\", fiscal_start = 9)\n\n\n[1] 2\n\n\n\n\n\n\n\n\n3.5.2 Times\nTo a large extent, we’ll be able to use what we know from dates and apply that knowledge to times. A number of functions that you can use with dates, also apply to times. However, time is a bit more complicated. First, there are many different time zones around the world. The time in a time zone is usually as a difference with Greenwhich Mean Time (GMT) or Coordinated Universal Time (UCT). GMT or Coordinated Universal Time (UTC) are the same. However, GMT is a timezone and refers to the local time at the Royal Observatory in London’s Greenwhich. UTC is a time standard and was developed in the late 1960’s. Today, most times zones are expressed in terms of an offset relative to UTC. The offset shows the difference between UTC and the local time, e.g. UTC01:00. UTC divides time into days, hours, minutes and seconds. Each day contains 24 hours and each hour 60 minutes. Usually, every minute counts 60 seconds, although there are leap seconds. If there is a leap second, a minute can count 59 or 61 seconds. Although we mentioned time zones in relation to dates, they were of a minor issue in that case and are only relevant if you look at the current date round the world. Here, for some countries the local date might be “Monday”, while in others it is still “Sunday”. For time however, time zones are relevant. For instance, if you schedule a meeting for 14:00, you need to add the time zone if that meeting includes people from e.g. the UK, US and India. In addition to time zones, a second complicating issue in terms of time is daylight savings time or summer time: advancing the clock by one hour in spring and retarding the clock by one hour in autumn. Not all countries know this practice. In addition, not all countries have used daylight savings time every year since it was introduced.\nThe ISO standard for dates was “%Y-%m-%d”: 4 digit year, 2 digit month and 2 digit day of the month separated by a hyphen (-). The ISO standard for date/time values set in local time is “%Y-%m-%dT%H:%M:%S”, e.g. 2025-03-25T17:30:15. In other words, the date and time are separated by a “T” and time is written as %H (hours (24)), %M (minutes (0-59)) and %S (seconds (0-59)) separated by a colon. These and other time formats are included in Table 3.2.\n\n\n\nTable 3.2: Time symbols\n\n\n\n\n\nSymbol\nDefinition\nExample\n\n\n\n\n%H\nDecimal hours (24 hours)\n15\n\n\n%I\nDecimal hours (12 hours)\n03\n\n\n%M\nDecimal minute\n58\n\n\n%S\nDecimal second\n36\n\n\n%p\nAM/PM\nAM/PM\n\n\n%z\nOffset from UTC\n+0130\n\n\n%Z\nFull time zone name\nAmerica/New York\n\n\n\n\n\n\nIf a date/time refers to UTC time, a Z is added at the end “%Y-%m-%dT%H:%M:%SZ”, e.g. 2025-03-25T17:30:15Z. For other time zones, the ISO standard uses the UTC offset: “%Y-%m-%dT%H:%M:%S+/-hh:mm” (with hh the number of hours and mm the number of minutes a time zone’s time differs from UTC (with + for times zones ahead and - for time zones behind UTC)), e.g. 2025-03-25T17:30:15+01:00 for a timezone 1 hour ahead of UTC.\n\n3.5.2.1 What is the time now?\nTo see the current time, you can use Sys.time()\n\ntime_1 &lt;- Sys.time()\ntime_1\n\n[1] \"2026-01-23 18:26:00 CET\"\n\n\nAs you can see, time is stored using various components: the date (year - month - day), a space followed by the hour, minute and second, separated by a colon, a space and the time zone (CET). This format deviated from the ISO standard as it includes a space between the date and time component and it includes the name of the time zone but not the UTC offset.\nWith respect to the timezone, you can check the current timezone using Sys.timezone()\n\nSys.timezone()\n\n[1] \"Europe/Brussels\"\n\n\nTo check if daylight savings time is on for a specific date, you can use {lubridate}’s dst(). This function returns TRUE if daylight savings time is in force and FALSE otherwise. In August, daylight savings time is in force. If you include this date in dst()the result will be TRUE\n\nlubridate::dst(\"2025-08-25\")\n\n[1] TRUE\n\n\nIn August, daylight savings time is in force.\n{lubridate}’s now(tzone = \" \") shows the current time for the timezone included in tz = \" \". The default timezone is your system’s timezone. To change that into UTC, you can use tz = \"UTC\":\n\nlubridate::now()\n\n[1] \"2026-01-23 18:26:00 CET\"\n\nlubridate::now(tzone = \"UTC\")\n\n[1] \"2026-01-23 17:26:00 UTC\"\n\n\nIf you need to know the time in a different time zone, you can use {lubridate} with_tz(time, tzone = \" \") function. For instance, finding the time in Australia/Brisbane for the local time is possible using\n\nlubridate::with_tz(Sys.time(), tzone = \"Australia/Brisbane\")\n\n[1] \"2026-01-24 03:26:00 AEST\"\n\n\nIf you replace Sys.time() with another time, the function shows the time in the tzone for that specific time.\n\n\n3.5.2.2 Converting strings and numbers to date/time\nThe class of a date/time value is known as POSIX.\n\nclass(time_1)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nNote that the type of time_1 is double:\n\ntypeof(time_1)\n\n[1] \"double\"\n\n\nPOSIX is a portable operating system interface that allows to store date/time values like the time_1 but allows for time zones. It stores values to the nearest second. POSIX does so in two ways: POSIXct (ct for calender time) and POSIXlt (lt for local time). The former stores data/time values as the number of seconds since 1 January 1970, midnight UTC. POSIXlt stores a list of day, month, year, hour, minute, second, … . As the latter requires more memory, base R’s POSIXct is usually more efficient.\nTo change a character into a date/time format, you can use as.POSIXct(x, tz = \"\", format, tryformats = c()). Notice that the overall structure of this function is similar to the as.Date() function for dates. The first argument is the string that you want to convert. If you know the date/time format, you can include that format in the format = \" \" part. To develop the format, you can use the symbols in ?tbl-times. If no format is included, the function will try one of the formats in tryFormats = c(\"%Y-%m-%d %H:%M:%S\", \"%Y/%m/%d, %H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y/%m/%d %H:%M\", \"%Y-%m-%d\", \"%Y/%m/%d\") where you can add your own format. Usually time values start with the hour, then minute and often also the second. In other words, usually, it is safe to assume that the hour is written first, followed by the minute and then, if present, the second.\n\ndat_tim_1 &lt;- \"2025-03-25 17:30:00\"\ndat_tim_1ct &lt;- as.POSIXct(dat_tim_1, format = \" %Y-%m-%d %H:%M:%S\")\ndat_tim_1ct\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\nAs was the case with dates, date/time values are stored as a number:\n\ntime_1_num &lt;- as.numeric(dat_tim_1ct)\ntime_1_num\n\n[1] 1742920200\n\n\nAs times are stored as numeric variable, you can convert numbers to a date/time format. As you had to do with dates, you need to add the origin if that origin differs from the standard January 1, 1970. For instance, if you convert the number 1 to a date/time variable, you’ll see\n\ntime_1 &lt;- as.POSIXct(1, tz = \"UTC\", origin = \"1970-01-01\")\ntime_1\n\n[1] \"1970-01-01 00:00:01 UTC\"\n\n\n{lubridate} includes a number of functions to read date/time variables: ymd_hms(), ymd_hm(), ymd_h(), dmy_hms(), dmy_hm(), dmy_h(), mdy_hms(), mdy_hm(), mdy_h(), ydm_hms(), ydm_hm() and ydm_h() where y refers to year, m to month, d to day, h to hour, m to minute and s to second. For instance, for dat_tim_1, which was written as ymd_hms, you can use ymd_hms(..., tz = \"UTC) function family to convert that string into a date/time variable. Note that the default time zone is UTC:\n\ndat_tim_lub &lt;- lubridate::ymd_hms(dat_tim_1, tz = \"Europe/Brussels\")\ndat_tim_lub\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\n{lubridate}’s make_datetime()function is very similar to make_date(). With make_datetime() you can add the hour, minute and second as well as the time zone:\n\ndat_tim_1 &lt;- lubridate::make_datetime(year = 2025,\n  month = 3, day = 25, hour = 17, min = 30, \n  sec = 0, tz = \"Europe/Brussels\")\ndat_tim_1\n\n[1] \"2025-03-25 17:30:00 CET\"\n\n\nAs with the make_date() function, this is a very useful function if a dataset includes date/time values, but stores each component in a different variable.\nWith respect to formatting a date/time value, the ISO format reads and shows date/time values as “2025-03-25T17:30:15+0100”. The first part before the T is the date in ISO format. The T introduces the time which includes the hour, minute and second. The last part is the UTC offset: the difference between UTC and the time in the timezone. To show data/time in ISO format, you can use {lubridate}’s format_ISO8601() function. The function’s first argument is the date/time value. The second specifies if the output will show the timezone offset. To use UTC, usetz = \"Z\". The default is FALSE. The last argument, the precision allows you to set the precision of the output, e.g. day (show ydm), hour (include ymd as well as the hour). Let’s see what this function shows for dat_tim_1:\n\nlubridate::format_ISO8601(dat_tim_1)\n\n[1] \"2025-03-25T17:30:00\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = TRUE)\n\n[1] \"2025-03-25T17:30:00+0100\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = TRUE, precision = \"ymdh\")\n\n[1] \"2025-03-25T17+0100\"\n\nlubridate::format_ISO8601(dat_tim_1, usetz = FALSE, precision = \"ymdh\")\n\n[1] \"2025-03-25T17\"\n\n\n\n\n3.5.2.3 Extracting information from date/times\nWith dates, you can extract the week, day, month, … . With date/time values, you can also extract the hour using hour(), the minute (minute()) and second (second()) as well as the time zone (tz()). These functions have no other arguments than the date/time value:\n\nlubridate::hour(dat_tim_1)\n\n[1] 17\n\nlubridate::minute(dat_tim_1)\n\n[1] 30\n\nlubridate::second(dat_tim_1)\n\n[1] 0\n\n\n\n\n3.5.2.4 Calculations using times\nAs times are stored as numbers, you can add and subtract seconds. For instance, to determine the time 100000 seconds after 2025-03-25 17:30:15, you can use the add that number to the time variable:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\ntime_2 &lt;- time_1 + 100000\ntime_2\n\n[1] \"2025-03-26 21:16:55 UTC\"\n\n\nLikewise, if you want to subtract one million seconds:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\ntime_3 &lt;- time_1 - 1000000\ntime_3\n\n[1] \"2025-03-14 03:43:35 UTC\"\n\n\nRecall from the dates section that you can add time using the number of years, months and days. For time, you can add the number of hours, minutes, seconds (and even milli-, micro-, nano- and picoseconds). Suppose that you want to add 36 hours, 55 minutes and 30 seconds to time_1 (025-03-25 17:30:15):\n\ntime_4 &lt;- time_1 + lubridate::hours(36) + lubridate::minutes(55) + lubridate::seconds(30)\ntime_4\n\n[1] \"2025-03-27 06:25:45 UTC\"\n\n\nIn addition, you can round times. We already covered these function for dates: round_date(), floor_date()and ceiling_date(). These functions also allow you to use “second”, ’minute” or “hour” as unit. Note that in this case, you don’t need to add the week option. To illustrate, we’ll use time_1 (2025-03-25 17:30:15). This value doesn’t have any sub-second units, so we”ll round to the nearest hour and minute:\n\nlubridate::round_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 18:00:00 UTC\"\n\nlubridate::round_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:30:00 UTC\"\n\n\nAs you can see from the output, the nearest hour is 18:00:00, the nearest minuut is 17:30. Using floor_date()and ceiling_date() rounds down and up to the nearest hour or minute:\n\nlubridate::floor_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 17:00:00 UTC\"\n\nlubridate::floor_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:30:00 UTC\"\n\nlubridate::ceiling_date(time_1, unit = \"hour\")\n\n[1] \"2025-03-25 18:00:00 UTC\"\n\nlubridate::ceiling_date(time_1, unit = \"minute\")\n\n[1] \"2025-03-25 17:31:00 UTC\"\n\n\nRounding down to the hour rounds to 17:00:00 and down to the minute to 17:30:00 while rounding up to the hour shows 18:00:00 and to the minute shows 17:31:00.\n\n\n3.5.2.5 Time spans\nWe met time spans at the date level. You can also work with intervals and periods at the time level. For instance, let’s define interval of one lecture:\n\nlec_start &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\", tz = \"Europe/Brussels\")\nlec_ends &lt;- lubridate::ymd_hms(\"2025-03-25T19:30:15\", tz = \"Europe/Brussels\")\nlec_int &lt;- lubridate::interval(start = lec_start, end = lec_ends, tzone = \"Europe/Brussels\")\nlec_int\n\n[1] 2025-03-25 17:30:15 CET--2025-03-25 19:30:15 CET\n\n\nAs you could with dates, you can now calculate the length\n\nlubridate::time_length(lec_int, unit = \"hour\")\n\n[1] 2\n\nlubridate::time_length(lec_int, unit = \"minute\")\n\n[1] 120\n\nlubridate::time_length(lec_int, unit = \"second\")\n\n[1] 7200\n\n\nNote that here, the time zone matters. Let’s see what happens if we leave the time zone out of our interval when we set the start and end time, but add it when we determine the interval:\n\nlec_starttz &lt;- lubridate::ymd_hms(\"2025-03-25T17:30:15\")\nlec_endstz &lt;- lubridate::ymd_hms(\"2025-03-25T19:30:15\")\nlec_inttz &lt;- lubridate::interval(start = lec_starttz, end = lec_endstz, tzone = \"Europe/Brussels\")\nlec_inttz\n\n[1] 2025-03-25 18:30:15 CET--2025-03-25 20:30:15 CET\n\n\nAs you can see, the times are pushed forward one hour. This is because we set the start and end time without a reference to a time zone and time is measured in UTC. As we define the interval including the time zone, {lubridate} updates the time to Europe/Brussels, which is UTC+01:00 and shows the correct times in CET.\nDefining a period with times follows a similar approach as a period with dates. Using {lubridate}’s period() function, we can set a period equal to, e.g. 3 hours, 30 minutes and 15 seconds:\n\nper &lt;- lubridate::period(hours = 3, minutes = 30, seconds = 15)\nper\n\n[1] \"3H 30M 15S\"\n\n\nWe can now use that period to determine an end time for given start time, e.g;\n\nlec_endspe &lt;- lec_start + per\nlec_endspe\n\n[1] \"2025-03-25 21:00:30 CET\"\n\n\nAgain note that it is important that you specify the time zone. Here we used lec_start which was defined referring to the Europe/Brussels time zone. If you wouldn’t include that reference, the end result would also by in UTC:\n\nlec_endstz &lt;- lec_starttz + per\nlec_endstz\n\n[1] \"2025-03-25 21:00:30 UTC\"\n\n\n{lubridate}’s duration() function calculate the number of seconds in a given time span. You can define the time span using second, minute, hour, day, week, month, year). For instance, how many seconds are there in a time span of 2 weeks, 10 days, 21 hours, 6 minutes and 25 seconds?\n\ndur_1 &lt;- lubridate::duration(week = 2, day = 10, hour = 21, minute = 6, second = 25)\ndur_1\n\n[1] \"2149585s (~3.55 weeks)\"\n\n\nYou can now use that duration to, e.g. determine when an activity started if it ended on 2025-03-25 17:30:15:\n\ntime_1 &lt;- lubridate::ymd_hms(\"2025-03-25 17:30:15\")\ntime_2 &lt;- time_1 - dur_1\ntime_2\n\n[1] \"2025-02-28 20:23:50 UTC\"\n\n\nHere you see when an activity started given its end date and duration.\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nWhat is the time now? Save that time in a variable time1\n\n\nCode\ntime1 &lt;- Sys.time()\ntime1\n\n\n[1] \"2026-01-23 18:26:01 CET\"\n\n\nHere you have character with a date/time.\n\ntime2 &lt;- \"2025-03-14T14:36:05\"\n\nConvert it into a POSIX format. The timezone is UTC.\n\ntime3 &lt;- as.POSIXct(time2, format = \"%Y-%m-%dT%H:%M:%S\", tz = \"UTC\")\ntime3\n\n[1] \"2025-03-14 14:36:05 UTC\"\n\n\nUse {lubridate} to change time3 in ISO86010 format with an hourly precision.\n\n\nCode\ntime4 &lt;- lubridate::format_ISO8601(time3, usetz = FALSE, precision = \"ymdh\")\ntime4\n\n\n[1] \"2025-03-14T14\"",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data types</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html",
    "href": "04_Data_structures.html",
    "title": "4  Data structures",
    "section": "",
    "text": "4.1 Vectors\nA vector is a one dimensional data structure: it has one row and one or more columns. In case there is only one column, an R vector holds one number, one character variable, one logical or one data/time value. In other words, in the previous chapter, we actually used vectors. A vector, like a matrix or an array, is homogeneous: is allows you to store one type of variable (e.g. numeric, character, …).",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#vectors",
    "href": "04_Data_structures.html#vectors",
    "title": "4  Data structures",
    "section": "",
    "text": "4.1.1 Creating a vector: basics\nTo create a vector we use the c() function to combine the elements within this function in one data structure. Let’s create a vector with numbers:\n\nvec_num &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\nvec_num\n\n [1]  0  1  1  2  3  5  8 13 21 34\n\n\nYou can see the total number of elements in this vector using the length() function:\n\nlength(vec_num)\n\n[1] 10\n\n\nHere, we nave a total of 10 columns. You can see in the environment pane that the vector is a 1x10 vector: 1 row and 10 columns.\nIf you check the type of this vector, you’ll see that its type is “double”. In other words, it is a numeric vector.\n\ntypeof(vec_num)\n\n[1] \"double\"\n\n\nYou can check if an object is a vector using\n\nis.vector(vec_num)\n\n[1] TRUE\n\n\nIn addition, you can check if a vector is of a given type by including the mode (numeric, logical, …) in is.vector():\n\nis.vector(vec_num, mode = \"numeric\")\n\n[1] TRUE\n\n\nThe type of vec_num is “double”. You can create a vector with other data types:\n\nvec_char &lt;- c(\"cat\", \"mouse\", \"dog\", \"bird\")\nvec_log &lt;- c(TRUE, FALSE, TRUE, TRUE)\nvec_int &lt;- c(1L, 10L, 50L)\nvec_dat &lt;- c(as.POSIXct(\"2025-03-25\"), as.POSIXct(\"2025-04-25\"))\n\nYou can check the type of the data stored in all these vectors:\n\ntypeof(vec_char)\n\n[1] \"character\"\n\ntypeof(vec_log)\n\n[1] \"logical\"\n\ntypeof(vec_int)\n\n[1] \"integer\"\n\ntypeof(vec_dat)\n\n[1] \"double\"\n\n\nThe type of a vector is the type common to all individual elements. In other words, a vector only holds elements of the same type. If this is not the case, R will change the type of all elements in the vector to a type that fits all. This is also called implicit coercion: R chooses the type for that data that fits all components of the data structure. For a vector, this means that all values in the columns will have the same type.\nFor instance, suppose that we have a vector\n\nvec_1 &lt;- c(1, \"2\", 3)\n\nHere, we mix two numeric values with 1 character value “2”. If you take a look at this vector, you’ll see that R changes all elements in characters:\n\nvec_1\n\n[1] \"1\" \"2\" \"3\"\n\n\nYou can verify this by checking the type\n\ntypeof(vec_1)\n\n[1] \"character\"\n\n\nAs you can see, vec_1 is not a numeric vector, but a character vector. Let’s take another example:\n\nvec_2 &lt;- c(TRUE, FALSE, 5, as.POSIXct(\"2025-03-25\"))\nvec_2\n\n[1]          1          0          5 1742857200\n\n\nIn this example, we have a mix of logical values (TRUE, FALSE), a numeric value and a date/time value. R uses a common type and sets TRUE equal to 1, FALSE equal to 0 and show the number of seconds since January 1, 1970. In other words, R implicitly coerces the vector into a double vector.\n\ntypeof(vec_2)\n\n[1] \"double\"\n\n\nLet’s see what happens if we mix logical, character and numeric values:\n\nvec_3 &lt;- c(TRUE, FALSE, \"a\", 5)\nvec_3\n\n[1] \"TRUE\"  \"FALSE\" \"a\"     \"5\"    \n\n\nHere, from the quotation marks, you can see that R changes the type of all individual elements into character values. These three examples are examples of implicit coercion: R tries to find a way to represent the elements in a vector using a common type. Sometimes, this implicit coercion makes sense, sometimes it doesn’t. For instance, combining a numeric value and a character representation of a numeric value creates a character vector. The reason why R changes numbers into characters is that usually, you can represent a number as a character, while you can not always represent a character as a number. In a similar way, because you can represent a logical value in a number, but a number not always in a logical value - unless that number happens to be 0 or 1, R will set the type of a vector that includes both logical and numeric values in numeric. The same holds for the mixture of date/time and logical, data/time and numeric and data/time, logical and numeric.\nYou can coerce the type of a vector using an as. function: as.numeric(), as.integer(), as.character(), as.logical() or as.Date() or as.POSIXct(). Here, the coercion is explicit. In that case, R will try to change all elements into the same type. In case this is impossible, R produces NA’s. For instance, let’s try to change the three vectors vec_1, vec_2 and vec_3 in numeric:\n\nas.numeric(vec_1)\n\n[1] 1 2 3\n\nas.numeric(vec_2)\n\n[1]          1          0          5 1742857200\n\nas.numeric(vec_3)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA  5\n\n\nFor vec_1and vec_2 R could change all the elements in type numeric: in vec_1 R managed to change the character “2” in a number 2. The same holds for vec_2. Here R could change the type of TRUE, FALSE in 1 and 0 and set the date/time variable in numeric format. For vec_3, changing all elements in numeric was impossible. As a matter of fact, with the exception of the number 5, R didn’t manage to change the type at all. Why couldn’t R change “TRUE” or “FALSE” in 1 and 0 as it could in vec_2. Here, TRUE and FALSE were character values, not boolean. When vec_3 was created, R changed the type of all its values in “character”. In other words, as far as R is concerned, TRUE became “TRUE” and R doesn’t keep track of the path that led it to “TRUE”. In other words, R doesn’t recall changing TRUE into TRUE. Because of this, R didn’t manage to change the “TRUE” (back )into a boolean TRUE from there into a number. As this was not possible, it replaced that value with an NA.\nYou can change an object (e.g. a column in a data frame) into a vector using the as.vector() function. This function takes two arguments: the object that you want to convert into a vector and the vector type. For instance\n\nvec_4 &lt;- as.vector(vec_1, mode = \"numeric\")\nvec_4\n\n[1] 1 2 3\n\n\ncreates a numeric vector from vec_1. Note that here this operation was not as useful as vec_1 is a numeric vector. However, in later chapters we will convert variables or column in a date frame in vectors. To do so, we will often have to be explicit in the mode. Leaving out the mode, R will copy the type of e.g. the column in a data frame into the mode.\nSo far, all vectors were created using c() including all elements one for one in this function. Using the vector(type, length = ) function, you can create an empty vector of a given length and type. For instance, to create an empty numeric vector of length 10:\n\nvec_1 &lt;- vector(\"numeric\", length = 10)\nvec_1\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nAs you can see, this vector is filled with 0. Note that this is a numeric vector but only for now. If you would change one of its elements in a character, the full vector would change from numeric into character. If you want to create an empty character vector:\n\nvec_2 &lt;- vector(\"character\", length = 10)\nvec_2\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nHere, you can see that empty is a space (recall that a space of a character).\nCreating a vector with “0” values can be very useful before a for loop. Suppose that you have a for loop where each ‘loop’ adds the result of a calculation to a vector. Here, you have two option. First, you allow the vector to ‘grow’ in every loop. Second, you define an empty vector with the same length as the number of loops and you fill each element as you run through the loop. The first option is not very efficient as R will copy the entire vector you have each time you expand it with one element. This is not the case if you create the vector before the loop. Here, R fills one element after the other but doesn’t need to grow the vector.\nRecall that NA are missing observations. If a vector includes NA values, that will not change the vector’s type. To see this, let’s create two vectors, one numeric and one character, which both include NA and show their type:\n\nvec_1 &lt;- c(10, 30, NA, 40)\nvec_2 &lt;- c(\"dog\", NA, \"cat\")\ntypeof(vec_1)\n\n[1] \"double\"\n\ntypeof(vec_2)\n\n[1] \"character\"\n\n\nThe same hold for NaN (not a number) and Inf (infinity). If these and NA are part of a character vector, they will become character values “NA”, “Nan” of “Inf”. In other words, they’ll be considered characters and not special values.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate a numeric vector with 5 columns, 1, 2, 3, 4 and 5. Assign this vector to vec_yt1\n\n\nCode\nvec_yt1 &lt;- c(1, 2, 3, 4, 5)\n\n\nCheck the type of this vector\n\n\nCode\ntypeof(vec_yt1)\n\n\n[1] \"double\"\n\n\nCreate a new vector, vec_yt2 with values TRUE, FALSE, TRUE, TRUE, FALSE and check the class and type of this vector\n\n\nCode\nvec_yt2 &lt;- c(T, F, T, T, F)\nclass(vec_yt2)\n\n\n[1] \"logical\"\n\n\nCode\ntypeof(vec_yt2)\n\n\n[1] \"logical\"\n\n\nDetermine the length of the vector vec_yt1.\n\n\nCode\nlength(vec_yt1)\n\n\n[1] 5\n\n\nCreate a character vector vec_yt3 whose elements include: south, west, east, north.\n\n\nCode\nvec_yt3 &lt;- c(\"south\", \"west\", \"east\", \"north\")\n\n\nDetermine the length of this vector and the number of characters\n\n\nCode\nlength(vec_yt3)\n\n\n[1] 4\n\n\nCode\nnchar(vec_yt3)\n\n\n[1] 5 4 4 5\n\n\nCan you store the number of characters in a new vector vec_yt3n?\n\n\nCode\nvec_yt3n &lt;- nchar(vec_yt3)\n\n\n\n\n\n\n\n4.1.2 Named vectors\nYou can define names for the columns of a vector. You can do so when you create the vector using the c() function or the setNames() function, or, at a later sage, using the names() functions. Suppose that you have a vector with exam results for three courses, A, B and C. Using a named vector, allows you to identify the columns:\n\nvec_1 &lt;- c(A = 15, B = 13, C = 17)\n\nThe vector now includes column names. You can see that this is the case in the environment pane where vec_1 is now identified as a Named num [1:3]. These columns are also included if you ask R to show the vector:\n\nvec_1\n\n A  B  C \n15 13 17 \n\n\nThere are other ways to add names. Using setNames() you can define both the vector as well as the names. Using the previous example:\n\nvec_2 &lt;- setNames(c(15, 13, 17), c(\"A\", \"B\", \"C\"))\n\nIn a final example, we’ll use the names() function to add names after the vector was created. Let’s first create a vector:\n\nvec_3 &lt;- c(15, 13, 17)\n\nTo add names, we include them in a another vector and use names() to assign names to vec_3:\n\nnames(vec_3) &lt;- c(\"A\", \"B\", \"C\")\nvec_3\n\n A  B  C \n15 13 17 \n\n\nThe names function adds an attribute to the vector. To see this, let’s check the attributes of vec_3:\n\nattributes(vec_3)\n\n$names\n[1] \"A\" \"B\" \"C\"\n\n\nYou can also use the names() function to extract the names of a vector:\n\nvar_names &lt;- names(vec_1)\nvar_names\n\n[1] \"A\" \"B\" \"C\"\n\n\nHere, R checks the attributes of the vector vec_1 and copies the names of the variables to var_names. As an alternative, you could have done the same using\n\nattributes(vec_1)$names\n\n[1] \"A\" \"B\" \"C\"\n\n\nHere, R reads the attributes of vec_1 and extracts the names of the columns.\nExtracting the names allows you to store these names in a character vector that you can use in your work flow. With many columns, you can see the names using e.g. str():\n\nstr(vec_1)\n\n Named num [1:3] 15 13 17\n - attr(*, \"names\")= chr [1:3] \"A\" \"B\" \"C\"\n\n\nHere, too, you can see that names are defined as an attribute.\nTo remove the names of columns, you can use unname(obj, force = FALSE). The first arguments is the object (e.g. vector) whose names you want to remove; the second is a specific option to remove names even if the object is a data frame. You can usually keep the default value FALSE.\n\nvec_3 &lt;- unname(vec_3)\nvec_3\n\n[1] 15 13 17\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nFor the vector vec_yt1 with elements 1, 2, 3: add names A, B and C to this vector. To this in three ways.\n\nAdd the names as you create the vector: option 1\n\n\n\nCode\nvec_yt1 &lt;- c(A = 1, B = 2, C = 3)\nvec_yt1\n\n\nA B C \n1 2 3 \n\n\n\nAdd the names as you create the vector: option 2:\n\n\n\nCode\nvec_yt1 &lt;- setNames(c(1, 2, 3), c(\"A\", \"B\", \"C\"))\nvec_yt1\n\n\nA B C \n1 2 3 \n\n\n\nAdd the names after you have create the vector:\n\n\n\nCode\nunname(vec_yt1 &lt;- c(1, 2, 3))\n\n\n[1] 1 2 3\n\n\nCode\nnames(vec_yt1) &lt;- c(\"A\", \"B\", \"C\")\n\n# Note that you can use setNames() as well\n\nsetNames(vec_yt1, c(\"A\", \"B\", \"C\"))\n\n\nA B C \n1 2 3 \n\n\nCode\nvec_yt1\n\n\nA B C \n1 2 3 \n\n\nCheck the attributes of vec_yt1:\n\n\nCode\nattributes(vec_yt1)\n\n\n$names\n[1] \"A\" \"B\" \"C\"\n\n\nExtract the names of vec_yt1 and store them in a vector vec_yt1_names\n\n\nCode\n# Option 1\nvec_yt1_names &lt;- names(vec_yt1)\n\n# Option 2\nvec_yt1_names &lt;- attributes(vec_yt1)$names\n\n\nWould the following code work to remove the names from vec_yt1? If not, how can you remove the names?\n\nunname(vec_yt1)\nattributes(vec_yt1)\n\nDoes it work? I you don’t think so, check:\n\n\nCode\nvec_yt1 &lt;- unname(vec_yt1)\nattributes(vec_yt1)\n\n\nNULL\n\n\n\n\n\n\n\n4.1.3 Creating a vector: replicating elements of another vector\nUsing rep(x, times, length.out, each) you can replicate the values in a vector x. Suppose you want a vector where all elements repeat a value 10 times. The first argument is the values yo want to replicate. This can be any value: number, character, a vector … . The second to last arguments determine how many times or how x needs to be replicated. To create a vector with 10 columns and all values equal to 25\n\nvec_rep &lt;- rep(x = 25, times = 10)\n\nHere, x was a number, but you can also replicate characters or other vectors:\n\nvec_rep_char &lt;- rep(x = \"ABC\", times = 5)\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), times = 5)\nvec_rep_char\n\n[1] \"ABC\" \"ABC\" \"ABC\" \"ABC\" \"ABC\"\n\nvec_rep_vec\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3\n\n\nlength.out sets the length of the vector. If x is a single numeric, character, date/time using length.out and times is equivalent. If x is a vector, this is not the case. In the previous example, c(1, 2, 3) was replicated 5 times. In other words, the length of the output vector was 15. Using length.out you can set the total length. In doing so, R will replicate the vector, but will do so only partially on the last replication. For instance, if you set the length.out = 10, the length of the output vector is 10:\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), length.out = 10)\nvec_rep_vec\n\n [1] 1 2 3 1 2 3 1 2 3 1\n\n\nWith times and length.out you replicate the full vector on every replication. Using each you replicate each element of the vector each times. In other words, the output vector will show the first element of the input vector each times before it changes to the second element of the input vector.\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), each = 3)\nvec_rep_vec\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nAdding length.out sets a limit on the total length of the output vector. It does so by reducing the number of replications of the last element in the input vector:\n\nvec_rep_vec &lt;- rep(x = c(1, 2, 3), length.out = 7, each = 3)\nvec_rep_vec\n\n[1] 1 1 1 2 2 2 3\n\n\n\n\n4.1.4 Functions that generate a vector\nThere are a number of functions that produce a vector. These can be grouped into functions that generate a sequence, function that generate a vector with random numbers, vectors that are created by sampling another vector and vectors as a result of set operations.\n\n4.1.4.1 Generating a sequence\nTo create a vector, we used c() and included all its values. Some functions allow you to create a special vector. seq() allows you to fill a vector with a sequence of numbers. To do so, this function requires a start point (from), and endpoint (to) and either the increment of the sequence (by) or the length of the sequence (length.out). If length.out is specified, then R calculates the increments of the sequence. To see how this works, let’s create a vector which holds a sequence starting at 1, ending at 10 in steps of 1:\n\nvec_1 &lt;- seq(from = 1, to = 10, by = 1)\nvec_1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nAs an alternative, we can create the same sequence using the length.out argument:\n\nvec_2 &lt;- seq(1, 10, length.out = 10)\nvec_2\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhat happens if the last increment of the sequence, starting from the starting position, doesn’t end in the value given in the by argument. In that case, seq() stops before the value in to is reached. For instance:\n\nvec_3 &lt;- seq(1, 10, by = 8)\nvec_3\n\n[1] 1 9\n\n\nUsing the length.out = argument, the sequence always end in the value in to. That is so because R determines the increment using equally spaced intervals between from and to using length.out. You can use len or length as an alternative for length.out. As an example:\n\nvec_3 &lt;- seq(1, 10, length.out = 25)\nvec_3\n\n [1]  1.000  1.375  1.750  2.125  2.500  2.875  3.250  3.625  4.000  4.375\n[11]  4.750  5.125  5.500  5.875  6.250  6.625  7.000  7.375  7.750  8.125\n[21]  8.500  8.875  9.250  9.625 10.000\n\n\nYou can also use seq() with from, by and length.out. Here, you don’t specify the last value of the sequence. R will generate a sequence starting from the value in from and it will add the value in by length.out times. Note that in this case, you need to add the arguments of the function as you skip the second argument.\n\nvec_4 &lt;- seq(from = 10, by = 10, length.out = 25)\nvec_4\n\n [1]  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190\n[20] 200 210 220 230 240 250\n\n\nNote that the increment can be negative. In that case, R will reduce the start value with the value of the increment until it reaches the end value or until is reaches the number of increments in length.out:\n\nvec_5 &lt;- seq(from = 100, to = 50, by = -10)\nvec_5\n\n[1] 100  90  80  70  60  50\n\n\nor, as an alternative\n\nvec_6 &lt;- seq(from  = 100, by = -10, length.out = 5)\nvec_6\n\n[1] 100  90  80  70  60\n\n\nIn specific cases, you can create a sequence using shorter notation. For instance, suppose you want a vector of integers, where each increment is exactly 1. To generate this sequence, you can use\n\nvec_7 &lt;- 21:30\nvec_7\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nWe will use this short way to writing a sequence often in a for loop:\n\ni &lt;- 1\nfor (i in 1:5) {\n  print(\"Hello World\")\n}\n\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n[1] \"Hello World\"\n\n\nHere, i will adopt each value in 1:5, i.e. 1, 2, 3, 4 and 5 and print Hello World as long as i is smaller than or equal to 5. The counter i starts with a value 1 and the counter increases by 1 after every print of Hello World.\nIf the starting position is 1, sec.len() can be used as well:\n\nvec_8 &lt;- seq_len(10)\nvec_8\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou can use seq.Date() to generate a sequence of dates. The arguments of this function are very similar to those for the seq() function. As a matter of fact, if you would use seq() and not seq.Date() R would recognize that you are using seq() to generate a sequence of dates and would use sec.Date() without problem. The from argument is the start date, the to the end date. If you use to, you need to specify the increment. Here, you can use “day”, “week”, “month”, “quarter” or “year”. Note that “days”, “weeks”, “months”, “quarters” or “years” is also accepted. If you add an integer, R will increment with a a multiple of “days”, … . To illustrate, let’s create three vectors, all start on January 1, 2025 and end on December 31, 2025. The first increments in days, the second in 3 weeks and the last in quarters:\n\nstart_d &lt;- as.Date(\"2025-01-01\")\nend_d &lt;- as.Date(\"2025-12-31\")\n\nvec_d &lt;- seq.Date(from = start_d, to = end_d, by = \"day\")\nvec_w &lt;- seq.Date(from = start_d, to = end_d, by = \"3 weeks\")\nvec_q &lt;- seq.Date(from = start_d, to = end_d, by = \"quarter\")\n\nR generates a sequence and ends the sequence before the date in to. To see this, let’s ask the maximum value in each of these vectors:\n\nmax(vec_d)\n\n[1] \"2025-12-31\"\n\nmax(vec_w)\n\n[1] \"2025-12-24\"\n\nmax(vec_q)\n\n[1] \"2025-10-01\"\n\n\nIf you increment with “day”, the last date is 2025-12-31. However, in both other cases, the last value of the sequence is before 2025-31-12. Using length_out, you determine the length of the sequence, but you allow R to determine the size of the increment if you include a value for to for the end point:\n\nvec_d10 &lt;- seq.Date(from = start_d, to  = end_d, length.out = 10)\nvec_d10\n\n [1] \"2025-01-01\" \"2025-02-10\" \"2025-03-22\" \"2025-05-02\" \"2025-06-11\"\n [6] \"2025-07-22\" \"2025-08-31\" \"2025-10-11\" \"2025-11-20\" \"2025-12-31\"\n\n\nIf you combine a value for both by and length.out R will determine the end date. For instance, if you use 2025-01-01 as your start day, and increment 10 times with 1 week, R will produce:\n\nvec_w10 &lt;- seq.Date(from = start_d, by = \"weeks\", length.out = 10)\nvec_w10\n\n [1] \"2025-01-01\" \"2025-01-08\" \"2025-01-15\" \"2025-01-22\" \"2025-01-29\"\n [6] \"2025-02-05\" \"2025-02-12\" \"2025-02-19\" \"2025-02-26\" \"2025-03-05\"\n\n\nAs you would with seq() you can also use negative increments. In that case, R will count backwards in time. For instance, the generate a sequence starting on 2025-31-12 and ending at or before 2025-01-01 and steps of 5 weeks:\n\nvec_db &lt;- seq.Date(end_d, start_d, by = \"-5 weeks\")\nvec_db\n\n [1] \"2025-12-31\" \"2025-11-26\" \"2025-10-22\" \"2025-09-17\" \"2025-08-13\"\n [6] \"2025-07-09\" \"2025-06-04\" \"2025-04-30\" \"2025-03-26\" \"2025-02-19\"\n[11] \"2025-01-15\"\n\n\nUsing seq.POSIXt you can generate date/time values. As was the case with seq.Date(), you can enter a starting date/time in the from argument, and end date/time in the to argument and supply the function with an increment “sec”, “min”, “hour”, “day”, “DSTday”, “week”, “month”, “quarter” of “year”. If you add an “s” that will not cause an error. In other words, R know the day is equal to days. In addition, you can add an integer to increment in multiples of “sec”. The difference between “day” and “DSTday” has to to be daylight savings time. DSTday takes daylight savings time into account. Is you include from, to and length.out, R determines the increment. With from, by and length.out R generates a sequence by adding the increment in by as many times and determined in length_out. If the time zone is not UTC, it has to be specified in from. Here are a couple of examples:\n\nstart_d &lt;- as.POSIXct(\"2025-01-01 12:00:00\")\nend_d &lt;- as.POSIXct(\"2025-01-05 12:00:00\")\n\nvec_dt_hour &lt;- seq.POSIXt(from =  start_d, to = end_d, by = \"6 hours\")\nvec_dt_10 &lt;- seq.POSIXt(from =  start_d, to = end_d, length.out = 10)\nvec_dt_20 &lt;- seq.POSIXt(from = start_d, by = \"5 mins\", length.out = 20)\n\nOne can now look at the examples:\n\nincrements per 6 hours:\n\n\nvec_dt_hour\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 18:00:00 CET\"\n [3] \"2025-01-02 00:00:00 CET\" \"2025-01-02 06:00:00 CET\"\n [5] \"2025-01-02 12:00:00 CET\" \"2025-01-02 18:00:00 CET\"\n [7] \"2025-01-03 00:00:00 CET\" \"2025-01-03 06:00:00 CET\"\n [9] \"2025-01-03 12:00:00 CET\" \"2025-01-03 18:00:00 CET\"\n[11] \"2025-01-04 00:00:00 CET\" \"2025-01-04 06:00:00 CET\"\n[13] \"2025-01-04 12:00:00 CET\" \"2025-01-04 18:00:00 CET\"\n[15] \"2025-01-05 00:00:00 CET\" \"2025-01-05 06:00:00 CET\"\n[17] \"2025-01-05 12:00:00 CET\"\n\n\n\nlength out between the start and end equal to 10:\n\n\nvec_dt_10\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 22:40:00 CET\"\n [3] \"2025-01-02 09:20:00 CET\" \"2025-01-02 20:00:00 CET\"\n [5] \"2025-01-03 06:40:00 CET\" \"2025-01-03 17:20:00 CET\"\n [7] \"2025-01-04 04:00:00 CET\" \"2025-01-04 14:40:00 CET\"\n [9] \"2025-01-05 01:20:00 CET\" \"2025-01-05 12:00:00 CET\"\n\n\n\nstarting from the start and incrementing 20 times by 5 minutes:\n\n\nvec_dt_20\n\n [1] \"2025-01-01 12:00:00 CET\" \"2025-01-01 12:05:00 CET\"\n [3] \"2025-01-01 12:10:00 CET\" \"2025-01-01 12:15:00 CET\"\n [5] \"2025-01-01 12:20:00 CET\" \"2025-01-01 12:25:00 CET\"\n [7] \"2025-01-01 12:30:00 CET\" \"2025-01-01 12:35:00 CET\"\n [9] \"2025-01-01 12:40:00 CET\" \"2025-01-01 12:45:00 CET\"\n[11] \"2025-01-01 12:50:00 CET\" \"2025-01-01 12:55:00 CET\"\n[13] \"2025-01-01 13:00:00 CET\" \"2025-01-01 13:05:00 CET\"\n[15] \"2025-01-01 13:10:00 CET\" \"2025-01-01 13:15:00 CET\"\n[17] \"2025-01-01 13:20:00 CET\" \"2025-01-01 13:25:00 CET\"\n[19] \"2025-01-01 13:30:00 CET\" \"2025-01-01 13:35:00 CET\"\n\n\nAs you can see from these examples, the way to use as.POSIXt() is very similar to the way you use seq.Date() or seq().\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nGenerate a vector, vec_yt1 as a sequence\n\nstarting at 2 and ending at 12 in steps of 2\n\n\n #| code-fold: true\n\nvec_yt1 &lt;- seq(from = 2, to = 12, by = 2)\n\n\nstarting at 10 and ending at 0 in steps of -1\n\n\n #| code-fold: true\n\nvec_yt1 &lt;- seq(from = 10, to = 0, by = -1)\n\n\nstarting at 0, in steps of 5 with a length of 5\n\n\n #| code-fold: true\n\nvec_yt1 &lt;- seq(from = 0, by = 5, length.out = 5)\n\n\nstarting at O, ending at 14 in steps of 3. Before you unfold the code: what will this vector look like?\n\n\n #| code-fold: true\n\nvec_yt1 &lt;- seq(from = 0, to = 14, by = 3)\n\n\nusing the shorted possible code, write a sequence starting at 5, ending at 50 in steps of 1.\n\n\n #| code-fold: true\n\nvec_yt1 &lt;- 5:50\n\nSuppose you have a date 2025-03-25 and you need a sequence of 6 dates by week. Write the do to create this sequence and store in a vector vec_ytd:\n\n\nCode\nvec_ytd &lt;- seq.Date(from = as.Date(\"2025-03-25\"), by = \"weeks\", length.out = 6)\nvec_ytd\n\n\n[1] \"2025-03-25\" \"2025-04-01\" \"2025-04-08\" \"2025-04-15\" \"2025-04-22\"\n[6] \"2025-04-29\"\n\n\nCode\nclass(vec_ytd)\n\n\n[1] \"Date\"\n\n\nGenerate a vector, vec_yty that starts at 2000-01-01 and end 2024-12-31 by year. Format the dates so that they only show the year (hint use: ?format()) and use the pipe operator in your code.\n\n\nCode\nvec_yty &lt;- seq.Date(from = as.Date(\"2000-01-01\", format = \"%Y-%m-%d\") , to = as.Date(\"2024-12-31\", format = \"%Y-%m-%d\"), by = \"year\") |&gt;\nformat(format = \"%Y\")\n\n\n\n\n\n\n\n4.1.4.2 Random numbers\nWe already covered statistical functions when we discussed numeric data. In that section, we showed how you can use pnorm(), dnorm(), qnorm() and rnorm(). However, with respect to the latter, rnorm(), we didn’t add too much detail. The same holds for the other function to generate random numbers from e.g. the t-distribution rt(), the uniform distribution runif(), the F-distribution rf() or rchisq()for the Chi-square distribution. In simulations, these random number generators are widely used. Before we move into these random number generates, a few words about the way software generates these numbers. Random number generators are not “random” but they follow an algorithm to generate a sequence of numbers whose properties approximate a random sequence. In other words, random numbers are not random, but their value is determined by and initial value that is used by the algorithm that generates this sequence. This is why random number generators are called pseudo random number generators. They generate a sequence that mimics the properties of a random sequence, but the sequence is fully determined by and initial value. That initial value is called the seed. There are many pseudo random number generators, but the same pseudo random number generator will produce the same sequence of random numbers if the seed it the same. In R, you can select the pseudo random number generator. The default is “Mersenne-Twister”. You can see all other pseudo random number generators that are available if you use ?Random in the console. Using set.seed, you can make sure that R generates the same sequence of random numbers, every time you ask R to generate a series. This function sets the initial value for the pseudo random number generator. Each time you use this value, you’ll get the same results. This is useful is you want to replicate your results. In addition, if you build a simulation, it is often useful to have the same sequence every time to add components to the simulation’s model.\nEvery statistical distribution is characterized by its parameters. For the normal distribution, these are the mean and the standard deviation, for Student’s t-distribution as well as the Chi square distribution this parameter is the degrees of freedom, for the uniform distribution you need the minimum and the maximum and for the F-distribution, the ratio of two independent chi square distributed variables, you need two degrees of freedom. If you supply these parameters, you can generate random numbers of these distributions:\n\nset.seed(1000)\nv_norm &lt;- rnorm(n = 100, mean = 0, sd = 1)\nv_t &lt;- rt(n = 100, df = 5)\nv_unif &lt;- runif(n = 100, min = 0, max = 100)\nv_chi &lt;- rchisq(n = 100, df = 5)\nv_f &lt;- rf(n = 100, df1 = 10, df2 = 2)\n\nWith 100 random draws each, we can show the probability density distribution of each of these 5 randomly generated values using base R’s hist() function:\n\nhist(v_norm, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Normal\")\nlines(density(v_norm), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_t, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Student's t\")\nlines(density(v_t), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_unif, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Uniform\")\nlines(density(v_unif), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_chi, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"Chi squared\")\nlines(density(v_chi), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nhist(v_f, probability = TRUE, col = \"lightblue\", border = \"white\", xlab = \"Value\", main = \"F distribution\")\nlines(density(v_f), lwd = 3, col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\n\n4.1.4.3 Sampling\nA sample refers to a subset of values from a vector that are drawn random. sample(x, size, replace = FALSE, prob = NULL) allows you to draw a random sample of size n, from a vector x . By default, sampling is done without replacement. In other words, an element can not appears twice in the sample unless it is included more than once in the vector x. In addition, all elements are equally likely to be drawn (prop = NULL). To illustrate this function, let’s use\n\nvec_1 &lt;- seq(1:48)\n\nand draw a sample, without replacement, of size = 10:\n\nsample(x = vec_1, size = 10)\n\n [1] 16 43 44 48 35 33 25 47 17 12\n\n\nIf you draw a sample with replacement (replace = TRUE), each draw is returned to the vector and could be drawn again.\n\nsample(x = vec_1, size = 10, replace = TRUE)\n\n [1] 26 10 47 10 37 42 46 27 23 33\n\n\nSampling is not limited to numeric vectors\n\nsample(x = c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"), size = 10, replace = TRUE)\n\n [1] \"g\" \"d\" \"f\" \"e\" \"b\" \"c\" \"c\" \"g\" \"f\" \"d\"\n\n\nWithout replacement, the sample size must be smaller than the length of the vector. With replacement, that is not the case. In the previous example for instance, the length of the vector was 8, while the size was 10. Without replacement, size = 8 would be equal to the vector and any size &gt; 8 would not leave sufficient values to sample from. If some values in the sample need a higher probability of being drawn, you need to add a vector with probability weights.\nAs a special case, if you only include the vector x, R returns a random permutation of the vector’s values:\n\nx &lt;- 1:10\nsample(x)\n\n [1]  6  2  9  5  7  1  3 10  8  4\n\n\n\n\n4.1.4.4 Set operations\nUsing set operators, you determine is an element in one vector is also an element in another, if that is not the case or you merge the elements of both in one new vector.\nSuppose you have two vectors,\n\nvec_1 &lt;- c(10, 20, 30, 40)\nvec_2 &lt;- c(20, 30, 40, 40)\n\nand you want to know if both share common elements. There are various ways to check if that is the case. The first uses the intersect() function. This function has two arguments: the vectors you want to compare. Note that if you load {dplyr}, the package masks this function. To instruct R to use base R’s intersect, you need to add ´base::`. The same holds for some other functions in this section.\n\nbase::intersect(vec_1, vec_2)\n\n[1] 20 30 40\n\n\nThe output shows the values that these two vectors have in common. If you want to store these values, out assign them to a new vector. Note that this also allows to see how many values both vectors have in common. Using the length() function, you can verify how many (unique) values are common to both vectors:\n\nlength(base::intersect(vec_1, vec_2))\n\n[1] 3\n\n\nHere, we used numeric values, but it you can finds common strings in character vectors in a similar way:\n\nfriends &lt;- c(\"Monica\", \"Phoebe\", \"Joey\", \"Chandler\", \"Ross\", \"Rachel\") \ncollegues &lt;- c(\"Taylor\", \"David\", \"Joey\", \"Sandra\")\nbase::intersect(friends, collegues)\n\n[1] \"Joey\"\n\n\nThis example also shows that the vectors don’t have to have the same length. If there are no common values, R will output the null vector:\n\nbase::intersect(c(10, 20), c(50, 60))\n\nnumeric(0)\n\n\nis.element(x, y) allows you to determine if elements of one vector, x, are included in the other y. The outcome is be a boolean vector whose values are TRUE if an element from x occurs in y and FALSE otherwise.\n\nis.element(vec_1, vec_2)\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\nThe values in the last three columns in vec_1 are also included in vec_2. Using the %in% operator has the same outcome as it checks which values on its left hand side vector are include in its right hand side vector:\n\nvec_1 %in% vec_2\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\nYou can also use this result to see how many elements from the first vector are also in the second. Here, you use the fact that TRUE is also 1 and FALSE is 0:\n\nsum(is.element(vec_1, vec_2))\n\n[1] 3\n\n\nNote that the order of the vectors matters. If you use is.element(x, y) you check if the elements from x are included in y. With is.element(y, x) you determine the elements in y that are also in x. In the example, you can see that changing the order in the is.element() function shows a different output as 40 is includes in vec_2 twice, but is only once included in vec_1\n\nvec_1[is.element(vec_1, vec_2)]\n\n[1] 20 30 40\n\nvec_2[is.element(vec_2, vec_1)]\n\n[1] 20 30 40 40\n\n\nRecall that using the “!” you can check if a condition is not met. Here, you can use this to see which elements of x are not in y\n\n!is.element(vec_1, vec_2)\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nbase::setdiff(x y) allows you to look for elements that are different, in other words, which elements from x are not included in y. While !is.element(x, y)’s output is a boolean vector, base::setdiff() shows the values of x that are not included in y.\n\nbase::setdiff(vec_1, vec_2)\n\n[1] 10\n\n\nNote again that the order of the vectors matters.\nTo create a union of x and y, there is the base::union(x, y) function. This function shows the unique values after merging the values in x and y:\n\nbase::union(vec_1, vec_2)\n\n[1] 10 20 30 40\n\n\nIf you want to know positions of these common elements, you can use the which() function:\n\nwhich(is.element(vec_1, vec_2))\n\n[1] 2 3 4\n\n\nThe unique(x, incomparables = FALSE) function determines the unique values in a vector. Suppose that you have a vector\n\nvec_char &lt;- c(\"jan\", \"jan\", \"feb\", \"mar\", \"mar\", \"apr\")\n\nThis vector has 4 unique values: “jan”, “feb”, “mar” and “apr”. Using the unique() function, you can select the unique values:\n\nunique(x = vec_char)\n\n[1] \"jan\" \"feb\" \"mar\" \"apr\"\n\n\nIf you want to exclude one value, you can add it to the incomparables = argument. For instance, suppose that you want to see all unique values, except January, you can add incomparables = c(\"jan\"):\n\nunique(vec_char, incomparables = c(\"jan\"))\n\n[1] \"jan\" \"jan\" \"feb\" \"mar\" \"apr\"\n\n\nR will now show all occurrences of “jan” as well as the unique values of all others.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nGenerate a vector, vec_rn with 20 draws from a normal distribution with mean 5 and standard deviation 10\n\n #| code-fold: true\n\nvec_rn &lt;- rnorm(20, mean = 5, sd = 10)\n\nGenerate a vector, vec_ru with 20 draws from a uniform distribution with minimum 5 and maximum 10. Write this code without naming the arguments.\n\n\nCode\nvec_ru &lt;- runif(20, 5, 10)\n\n\nUsing vec_rn draw a sample of 6 observations with replacement and assign these to a vector vec_rns\n\n\nCode\nvec_rns &lt;- sample(vec_rn, size = 6, replace = TRUE)\n\n\nA lottery includes a weekly draw of 6 numbers, without replacement, from a bowl with all numbers from 1 to 40. To play, you buy a ticket with 6 numbers, from 1 to 40. You win something if at least two numbers on your ticket are drawn. Your numbers are 3, 9, 25, 36, 37, 39. Simulate this lottery. To do so, first sample the weekly draw. Second, determine how many of your numbers match the numbers of the draw. Use 3 ways to calculate the number of winning numbers.\n\n\nCode\ndraw &lt;- sample(1:40, 6, replace = FALSE)\nticket &lt;- c(3, 9, 25, 36, 37, 39)\n\n# Option 1: use intersect\nwin &lt;- length(intersect(draw, ticket))\nwin\n\n\n[1] 1\n\n\nCode\n# Option 2: use is.element\nwin &lt;- sum(is.element(ticket, draw))\nwin\n\n\n[1] 1\n\n\nCode\n# Option 3: use %in%\nwin &lt;- sum(ticket %in% draw)\nwin\n\n\n[1] 1\n\n\n\n\n\n\n\n\n4.1.5 Special vectors\nR includes a number of special vectors. For instance, the vectors “letters” and “LETTERS” include the letters of the alphabet. The first lowercase, the second uppercase\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nIn addition to letters, the vectors “month.abb” and ’month.name” include the names of the month:\n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n4.1.6 Subsetting a vector\nIf you subset a vector, you select one or more columns of that vector (to possibly store them in a new one). We first start with the general case: an unnamed vector. We then continue with the special case of a named vector. Note that all methods for an unnamed vector can be used for named vectors.\n\n4.1.6.1 Subsetting an unnamed vector\nHere, we will use the numeric vector vec_num:\n\nvec_num &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\nNote that you can apply most of these ways to subset to other vector types: character, data/time or logical vectors. The approach wouldn’t differ in case you would use any of these other vector types. There are two subset operators: [] and [[ ]].\n\n4.1.6.1.1 Subsetting by position\nTo access an individual element of a vector, you include its position (or index number) between square brackets of the subscript operator [] after the name of the vector. In R, vector indexing starts at 1. In other words, the first element of a 1x10 vector is at position 1, the second at position 2, … This is not always the case. In Python for instance, the first element of a vector is at position 0, the second at position 1, …\nLet’s look at the element 5 of the element in the fifth column of vec_num:\n\nvec_num[5]\n\n[1] 3\n\n\nIf you want to extract that element to use it in part of your code, you would assign it to a different vector using the &lt;- operator:\n\na &lt;- vec_num[5]\na\n\n[1] 3\n\n\nNote that subsetting leaves the original vector intact. If you subset a vector, you copy the value in a new vector, but that value stays in the original vector.\nYou can subset more than one column. Suppose that you want to subset columns 1 to 4. To do so, you can use 1:4 within the subscript operator:\n\nvec_num[1:4]\n\n[1] 0 1 1 2\n\n\nAgain, you could assign this new vector. Here, this new vector would have 1 row and 4 columns. These 4 columns would be equal to the first 4 columns of the original vector.\nThe third way to access elements in a vector using their position is to combine these position via the c() function within the subsetting operator. The c() function allows you to define the columns you need. The subscript operator will then access these columns and extract their value. Suppose that you want to extract the elements in columns 1 and 4. Note that here, you will extract to columns: 1 and 4. In the previous example you extracted 4 columns: 1 to 4 or column 1, 2, 3, and 4. To extract columns 1 and 4 you need to include those position in the c() function: c(1, 4) and use:\n\nvec_num[c(1, 4)]\n\n[1] 0 2\n\n\nNote that you can mix various ways to subset a vector. For instance, if you need the first to third, fifth and seventh to last element, you can combine the various way to subsetting the vector:\n\nvec_num[c(1:3, 5, 7:10)]\n\n[1]  0  1  1  3  8 13 21 34\n\n\n\n\n4.1.6.1.2 Subsetting using negative positions\nYou can also use negative numbers for the index elements. In that case, R will show all elements, except those in the negative index (negative index range). For instance,\n\nAccessing all elements except for the first:\n\n\nvec_num[-1]\n\n[1]  1  1  2  3  5  8 13 21 34\n\n\n\nAccessing all elements except for the first, second, third and fourth (all in that range):\n\n\nvec_num[-1:-4]\n\n[1]  3  5  8 13 21 34\n\n\n\nAccessing all elements except for the second and fourth:\n\n\nvec_num[(c(-2, -4))]\n\n[1]  0  1  3  5  8 13 21 34\n\nvec_num[-c(2, 4)]\n\n[1]  0  1  3  5  8 13 21 34\n\n\n\n\n4.1.6.1.3 Subsetting by using a logical vector\nThe fourth way to subset columns in a vector uses a logical vector of the same length as the vector to subset. To see how this works, let’s first define two vectors: one numeric and one logical:\n\nvec_1 &lt;- c(1, 2, 3, 4, 5)\nvec_log &lt;- c(TRUE, FALSE, FALSE, FALSE, TRUE)\n\nYou can now subset vec_1 using vec_log:\n\nvec_1[vec_log]\n\n[1] 1 5\n\n\nIf the value on position x in vec_log is “TRUE”, the result of vec_1[vec_log] is equal to the value in the xth column of vec_1. This is the case for the first and last value. If vec_log’s yth element is false, vec_1’s yth element is not extracted.\nIn the example, we defined the logical vector ourselfs. However, there are many other ways to create such a vector. Recall that the outcome of any boolean operation is either TRUE or FALSE. Applying a boolean operation to every column of a vector creates a logical vector of the same length as the vector where the operation was applied to. You can now select those columns that meet that condition. For instance, suppose you want to work with the elements of vec_num that are larger than 5. There are two ways to do so. First, you create a logical vector of the same length as vec_num where an element is TRUE is the element in vec_num on the same position meets the condition and false otherwise. To create that vector, you use logical vector &lt;- original vector + condition. As we will see shortly, boolean operators applied to a vector are applied to every element of that vector. In other words, the logical vector will have the same length as the vector whose elements you want to extract.\n\ncond &lt;- vec_num &gt; 5\ncond\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNow we have a logical vector cond whose values are TRUE if the element in the same position in vec_num meets the condition (&gt; 5) and FALSE otherwise. We can now use this vector to subset vec_num:\n\nvec_num[cond]\n\n[1]  8 13 21 34\n\n\nHere, the TRUE-FALSE elements of cond are used to subset vec_num. Is an element in cond is TRUE, vec_num[cond] extracts that element from vec_num. If the element in cond is FALSE, the element in the same position in vec_num is not extracted.\nThe second option to use a condition is shorter and uses the condition within the subscript operator:\n\nvec_num[vec_num &gt; 5]\n\n[1]  8 13 21 34\n\n\nNote that you can use more than one boolean operator. For instance extracting all elements larger than 3 and not equal to 13 can be done using:\n\ncond &lt;- vec_num &gt; 3 & !(vec_num == 13)\nvec_num[cond]\n\n[1]  5  8 21 34\n\n\nor\n\nvec_num[vec_num &gt; 3 & !(vec_num == 13)]\n\n[1]  5  8 21 34\n\n\nNote that you can use these conditions also in the case of character vectors. For instance, to see if “cat” and “dog” are values in vec_char:\n\nvec_char[vec_char == \"dog\" | vec_char == \"cat\"]\n\ncharacter(0)\n\n\nIf you don’t know the exact location and you don’t have an explicit condition that you can use, but you know which values you want to extract, you can use the %in% operator. Here, you first define a vector with values, e.g. 1, 8 and 143 using c(1, 8, 143). Using the %in% operator, you can now subset the vector vec_num:\n\nvec_num %in% c(1, 8, 143)\n\n [1] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\n\nThis code extract all elements from vec_num that are also in c(1, 8, 143). The result of this operation is a logical vector with the same length as vec_num where the elements equal TRUE is an element in vec_num is included in c(1, 8, 143) and FALSE otherwise. This vector now allows you to extract these elements using, e.g.\n\nvec_num[vec_num %in% c(1, 8, 143)]\n\n[1] 1 1 8\n\n\nAs an alternative, you can do so in two steps:\n\ncond &lt;- vec_num %in% c(1, 8, 143)\nvec_num[cond]\n\n[1] 1 1 8\n\n\nHere we included the elements to extract in a c() function. However, you can use any other vector.\n\na &lt;- c(1, 8, 143)\nvec_num[vec_num %in% a]\n\n[1] 1 1 8\n\n\nNote that you can include values in %in% c() which are not part of the vector. If this is the case, R won’t find them. Hence, adding them wouldn’t change the outcome. If no element in the original vector matches the condition, R will output numeric(0) or a vector of length 0.\nTo illustrate this and the use of %in% for a character vector, suppose you want to extract “dog”, “fish” from vec_char:\n\nvec_char &lt;- c(\"dog\", \"cat\", \"rabbit\")\nvec_char[vec_char %in% c(\"dog\", \"fish\")]\n\n[1] \"dog\"\n\n\nAs “fish” is not included in vec_char but “dog” is and #rabbit” is part of vec_char but isn’t in c(\"dog\", \"fish\"), R outputs “dog”.\nUsing is.element(), you can subset a vector (the first) and extract all values that are both the the first and the second vector. To illustrate:\n\nvec_1 &lt;- c(1:10)\nvec_2 &lt;- c(7:15)\nvec_1[is.element(vec_1, vec_2)]\n\n[1]  7  8  9 10\n\n\nBoolean operators allow you define many conditions. For instance, if you have a vector that includes missing values, you can extract all non missing values using !is.na() or is not (!) NA:\n\nvec_1 &lt;- c(1, 20, 20, NA, NA, 50)\nvec_1[!is.na(vec_1)]\n\n[1]  1 20 20 50\n\n\nAs a second example, suppose that you want to extract all even numbers. Recall that a number of even if the modulus after division by 2 is zero:\n\n10 %% 2\n\n[1] 0\n\n11 %% 2\n\n[1] 1\n\n\nYou can use this to create a condition\n\n10 %% 2 == 0\n\n[1] TRUE\n\n11 %% 2 == 0\n\n[1] FALSE\n\n\nthat you can use to subset elements of a vector:\n\ncond &lt;- vec_num %% 2 == 0\nvec_num[cond]\n\n[1]  0  2  8 34\n\n\nAs a third example, recall that you can use grepl() or stringr::string_detect() if a pattern occurs in a string. If the is the case, these function output TRUE. Suppose you have a character vectors\n\nvec_char &lt;- c(\"sales_shoes\", \"sales_trousers\", \"sales_shirts\", \"sales_jackets\")\n\nand you want to extract the column which includes “shoes”. Using grepl() you can identify the elements in the character vector that include the word “shoes”:\n\ngrepl(pattern = \"shoes\", x = vec_char)\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nYou can use this function to extract the element “sales_shoes” from vec_char.\n\nvec_char[grepl(pattern = \"shoes\", x = vec_char)]\n\n[1] \"sales_shoes\"\n\n\n\n\n4.1.6.1.4 Index positions\nIn all examples we either used an exact index position or a logical vector to extract the values of a vector. What if you are not interested in a value but in an index position? To show an index position rather than its value or TRUE or FALSE, you can use the which() function. For instance, suppose you want to know the position of value 1 in vec_num. To find this position, you can use\n\nwhich(vec_num == 1)\n\n[1] 2 3\n\n\nThe result shows the index positions where you can find 1 in vec_num. Note that you can save the output in a new vector with positions. You can now subset that vector to find the first occurrence. As an alternative, as which() outputs a vector, you can find the first occurrence subsetting the which() function. For instance, to find the first 1 in vec_num:\n\nwhich(vec_num == 1)[1]\n\n[1] 2\n\n\nWhat if you want to find multiple values. Here, you can use the %in% operator. Suppose you want to know the position of the values 1, 2, 8 and 55. First you collect these values in a vector using c(1, 2, 8, 55). You can now use that vector in the which() function:\n\nwhich(vec_num %in% c(1, 2, 8, 55))\n\n[1] 2 3 4 7\n\n\nwhich() shows every occurrence. Using match() you can find the first occurrence. For instance, the first occurrence of “1” in vec_num is in position\n\nmatch(1, vec_num)\n\n[1] 2\n\n\nUsing which() allows you to extract the positions of e.g. missing values. Suppose you have a vector vec_1 which includes missing values:\n\nvec_1 &lt;- c(10, 10, 20, NA, 30, 40, NA, 50, 50)\n\nTo locate these missing value, you can use\n\nwhich(is.na(vec_1), vec_1)\n\n[1] 4 7\n\n\nThere are two variants of the which() function that allow you to find the location of the (first) maximum or minimum values: which.max() and which.min:\n\nwhich.max(vec_1)\n\n[1] 8\n\nwhich.min(vec_1)\n\n[1] 1\n\n\nUsing locigal values, you can find the first occurrence of specific value. Here, which.max() uses the fact that TRUE = 1 and FALSE = 0. In other words, this function will show the first occurrence of TRUE:\n\nwhich.max(vec_1 &gt; 30)\n\n[1] 6\n\n\n\n\n\n4.1.6.2 Subsetting a named vector\nWith a named vector, you can also use the column names to subset. Suppose that you have a vector\n\nvec_1 &lt;- c(A = 10, B = 30, C = 50, D = 70)\n\nFirst you can use the ways you would use to subset an unnamed vector, e.g.\n\nvec_1[3]\nvec_1[2:4]\nvec_1[vec_1 &lt; 50]\n\nAs you can see using [] preserves the structure of the vector: the output shows both the column name as well as its value.\nYou can also use the name of the column to subset the vector using vec_1[\"column name\"]:\n\nvec_1[\"A\"]\n\n A \n10 \n\n\nThe output shows both the column name as well as the value. In other words, here too, the structure of the vector is preserved.\nTo subset more than one column, you can use\n\nvec_1[c(\"A\", \"D\")]\n\n A  D \n10 70 \n\n\nTo extract the value, you have to refer to the column using subsetting operator [[]]. You can do so using both the column name or number. These lines extract the value for the second column\n\nvec_1[[\"B\"]]\n\n[1] 30\n\nvec_1[[2]]\n\n[1] 30\n\n\nThe output shows the value without the column. The [[]] operator simplifies the structure of the vector: it returns the simplest possible data structure: here this is the value of the column, i.e. an unnamed vector.\nYou can also subset column whose name includes a pattern. Recall that names() allow you to extract the names of the columns in a named vector. Using grepl() you can check if these names include a pattern. For instance, let’s check if the names of vec_1 include “A”. Using grepl():\n\ngrepl(pattern = \"A\", x = names(vec_1))\n\n[1]  TRUE FALSE FALSE FALSE\n\n\nTo extract that column, you include that statement in vec_1[]:\n\nvec_1[grepl(pattern = \"A\", x = names(vec_1))]\n\n A \n10 \n\n\nThe result shows the name of the column and its value.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nGenerate a vector:\n\nvec &lt;- c(21:30)\n\nExtract the following elements from this vector:\n\nthe 5th element\n\n\n\nCode\nvec[5]\n\n\n[1] 25\n\n\n\nall elements from 1 to 5\n\n\n\nCode\nvec[1:5]\n\n\n[1] 21 22 23 24 25\n\n\n\nelements in columns 1, 3 and 9\n\n\n\nCode\nvec[c(1, 3, 9)]\n\n\n[1] 21 23 29\n\n\n\nall elements except columns 1, 3 and 9\n\n\n\nCode\nvec[-c(1, 3, 9)]\n\n\n[1] 22 24 25 26 27 28 30\n\n\n\nall elements larger than 25\n\n\n\nCode\nvec[vec &gt; 25]\n\n\n[1] 26 27 28 29 30\n\n\nUse this vector\n\nvecchar &lt;- c(\"dog\", \"fish\", \"cat\", \"bird\", \"duck\", \"rabbit\")\n\nto extract all patterns animals that whose name includes an “a”\n\n\nCode\nvecchar[grepl(pattern = \"a\", vecchar)]\n\n\n[1] \"cat\"    \"rabbit\"\n\n\n\n\n\n\n\n\n4.1.7 Adding, removing and changing elements to a vector\n\n4.1.7.1 Adding elements to a vector\nAs in the previous section, I’ll use a numeric vector here, but you can apply the rules also to other types of vectors. Suppose that you have the 1x10 vector vec_num and you want to add a column with the value 55. The first way to do so is to use the c() function to create a new vector\n\nc(vec_num, 55)\n\n [1]  0  1  1  2  3  5  8 13 21 34 55\n\n\nIn this way you can add multiple columns and or multiple vectors:\n\nc(vec_num, c(55, 89, 144), c(233, 377, 610))\n\n [1]   0   1   1   2   3   5   8  13  21  34  55  89 144 233 377 610\n\n\nc() adds all elements in the order in which they appear in the function:\n\nc(c(610, 377, 233), c(144, 89, 55), vec_num)\n\n [1] 610 377 233 144  89  55   0   1   1   2   3   5   8  13  21  34\n\n\nNote that this doesn’t change the vec_num. c()creates a new vector. If you want to change vec_num you have to reassign it to the new vector. As an alternative, you can assign the new vector to a new object:\n\nvec_1 &lt;- c(vec_num, c(55, 89, 144), c(233, 377, 610))\nvec_1\n\n [1]   0   1   1   2   3   5   8  13  21  34  55  89 144 233 377 610\n\n\nIf you have a named vector, you can add a new named vector:\n\nvec_1 &lt;- c(A = 10, B = 30, C = 50, D = 70)\nc(vec_1, c(E = 90))\n\n A  B  C  D  E \n10 30 50 70 90 \n\n\nYou can also use the append() function to add new elements. By default, append will add an element after the last element in the existing vector. In other words, by default, append() is similar to c(). However, the arguments in the append(vector, value, after = length(x)) allow you to change that default position. If you want to add the new element after position 3, you can add this by changing the default length(x)in 3. Note that append()doesn’t change the original vector:\n\nappend(vec_num, 55)\n\n [1]  0  1  1  2  3  5  8 13 21 34 55\n\nvec_num\n\n [1]  0  1  1  2  3  5  8 13 21 34\n\n\nIf you want to change the original vector, you have to reassign it to its new values or assign the outcome to a new object:\n\nvec_1 &lt;- append(vec_num, 144)\nvec_1\n\n [1]   0   1   1   2   3   5   8  13  21  34 144\n\n\nTo add the value 88 as the first element or 143 after column 9, you can change the default location in append()’s after = argument:\n\nappend(vec_num, 88, after = 0)\n\n [1] 88  0  1  1  2  3  5  8 13 21 34\n\nappend(vec_num, 143, after = 9)\n\n [1]   0   1   1   2   3   5   8  13  21 143  34\n\n\nUsing the c() function, you can add multiple elements. For instance, if you want to add 88 and 143 as the first two columns of vec_num you combine these two values within c() and include them in the append statement:\n\nappend(vec_num, c(88, 143), after = 0)\n\n [1]  88 143   0   1   1   2   3   5   8  13  21  34\n\n\nNote that you can change the position where these new values are added. However, all elements are added after the same position and their position follows their position within the c() function. Note also that, if you add an element whose type of different from the vector type, R will change the vector type.\nYou can also add a named vector\n\nappend(vec_1, c(E = 50), after = 0)\n\n  E                                             \n 50   0   1   1   2   3   5   8  13  21  34 144 \n\n\n\n\n4.1.7.2 Removing elements from a vector\nThere are multiple ways to remove elements from a vector. We already covered two. First, if you know the position of the elements you want to remove, you can use a negative index. Recall that a negative index allows you to extract the elements of a vector except those included in the negative index. For instance, if you want to remove the first 4 columns of vec_num you can do this using\n\nvec_num[-1:-4]\n\n[1]  3  5  8 13 21 34\n\n\nTo remove column 1 and 4 (but not 2 and 3):\n\nvec_num[-c(1, length(vec_num))]\n\n[1]  1  1  2  3  5  8 13 21\n\n\nor\n\nvec_num[c(-1, -length(vec_num))]\n\n[1]  1  1  2  3  5  8 13 21\n\n\nYou can use this approach if you know the exact location (i.e. the columns) who want to remove.\nThe second way to remove elements uses a condition. For instance, the code to remove all elements larger than 3 and not equal to 0 is\n\nvec_num[!vec_num &gt; 3 & !(vec_num == 0)]\n\n[1] 1 1 2 3\n\n\nor, using a specific vector including the condition:\n\ncond &lt;- !vec_num &gt; 3 & !(vec_num == 0)\nvec_num[cond]\n\n[1] 1 1 2 3\n\n\nYou can use this approach if you know the condition that elements need to meet.\nIf you want to remove known values from a vector, e.g. 1, 8 and 143, you can use an approach which is very similar to the one you used to subset these elements. First, you collect them in a vector c(1, 8, 143). Second, you use %in% and not (!) to remove these elements:\n\ncond &lt;- vec_num %in% c(1, 8, 143)\nvec_num[!cond]\n\n[1]  0  2  3  5 13 21 34\n\n\nor, in one line of code\n\nvec_num[!vec_num %in% c(1, 8, 143)]\n\n[1]  0  2  3  5 13 21 34\n\n\nIn the last statement, 143 was included in the vector with values to remove but is not in vec_num. R doesn’t check if all values to be removed are also in the vector where they need to be removed.\n\n\n4.1.7.3 Changing elements in a vector\nSuppose that you know which column you want to change in your vector, e.g. you want to change the value in 4th column. To do this, you first subset that element using vec_num[4] and your reassign its value. For instance, changing the fourth element to 250:\n\nvec_num[4] &lt;- 250\nvec_num\n\n [1]   0   1   1 250   3   5   8  13  21  34\n\n\nAs you can see, fourth element is now 250. Note that the new value needs to be of the same type as the vector. If that is not the case, you”ll change the type of all other elements in the vector. For instance\n\nvec_num[4] &lt;- \"250\"\n\nchanges the type of the vector from double to character:\n\ntypeof(vec_num)\n\n[1] \"character\"\n\n\nIn that case, you have to change the vector’s type:\n\nvec_num &lt;- as.numeric(vec_num)\n\nUsing replace() you can change many values in a vector. Suppose you want to change columns 1, 8 and 10 in 50, 100, 150. The first argument in the replace() function is the vector you want to change. Here, this is vec_num. The second argument is a vector with index position. Using c(1, 8, 10) you can fix these position. The last argument is a vector with the values that will be used to replace the values in the index positions. Here you would use c(50, 100, 150). Using these in the replace() function:\n\nreplace(vec_num, c(1, 8, 10), c(50, 100, 150))\n\n [1]  50   1   1 250   3   5   8 100  21 150\n\n\nNote that the length of the index vector and the length of the vector with new values should be equal. If this is not the case, R will show an error:\n\nreplace(vec_num, c(1, 8, 10), c(50, 100, 150, 200))\n\nWarning in x[list] &lt;- values: number of items to replace is not a multiple of\nreplacement length\n\n\n [1]  50   1   1 250   3   5   8 100  21 150\n\n\nIf you want to replace all values that meet a certain condition with one single value, you can use the replace() function as well. Suppose you want to change all values larger than 25 with 50. Using recplace() you could do this with:\n\nreplace(vec_num, vec_num &gt; 25, 50)\n\n [1]  0  1  1 50  3  5  8 13 21 50\n\n\nChanging the vector’s type is another way to change a vector. Suppose you have a vector\n\nvec_dat_char &lt;- c(\"01-01-2025\", \"02-01-2025\", \"03-01-2025\")\n\nThis vector is a character vector:\n\ntypeof(vec_dat_char)\n\n[1] \"character\"\n\n\nYou can change this type to Date or POSIX using as.Date() or as.POSIXct(). Using the first:\n\nas.Date(vec_dat_char, format = \"%d-%m-%Y\")\n\n[1] \"2025-01-01\" \"2025-01-02\" \"2025-01-03\"\n\n\nIn a similar way, you can change the typeof numeric variables in character, dates in numeric, … .\n\n\n4.1.7.4 Sorting vectors\nTo sort a vector, R includes the sort(x, decreasing = FALSE, na.last = NA) function. Here, x is the vector to sort. By default, R sorts in increasing order. The last argument includes the treatement of “NA” values. By default, they are removed. Using TRUE missing values are retained, but added last. FALSE shows these values first.\n\nsort(x = vec_num, decreasing = FALSE)\n\n [1]   0   1   1   3   5   8  13  21  34 250\n\n\nCharacter vectors are sorted alphabetically by default:\n\nsort(x = c(\"zoo\", \"Zoo\", \"coast\", \"coAst\", \"cOAst\", \"lake\"))\n\n[1] \"coast\" \"coAst\" \"cOAst\" \"lake\"  \"zoo\"   \"Zoo\"  \n\n\nAs you can see, if the strings include copies where one includes a uppercase letter and the other one doesn’t, R orders those with the lowest number of uppercase letters first.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nGenerate a vector:\n\nvec &lt;- c(21:30)\n\nChange the this vector\n\nadd a c(31, 32, 33, 34, 34) after the vast position in vec. Use two methods to do so. Store the results in vec_r:\n\n\n\nCode\n# Option 1: use c()\nvec_r &lt;- c(vec, c(31, 32, 33, 34, 34))\nvec_r\n\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 34\n\n\nCode\n# Option 2: use append()\nvec_r &lt;- append(vec, c(31, 32, 33, 34, 34))\nvec_r\n\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 34\n\n\n\nadd c(31, 32, 33, 34, 34) as the first elements of vec. Use two methods to do so. Store the results in vec_r:\n\n\n\nCode\n# Option 1: use c()\nvec_r &lt;- c(c(31, 32, 33, 34), vec)\nvec_r\n\n\n [1] 31 32 33 34 21 22 23 24 25 26 27 28 29 30\n\n\nCode\n# Option 2: use append() and add position\nvec_r &lt;- append(vec, c(31, 32, 33, 34), after = 0)\nvec_r\n\n\n [1] 31 32 33 34 21 22 23 24 25 26 27 28 29 30\n\n\n\nadd c(31, 32, 33, 34, 34) after the fifth element of vec. Store the results in vec_r:\n\n\n\nCode\nvec_r &lt;- append(vec, c(31, 32, 33, 34, 34), after = 5)\nvec_r\n\n\n [1] 21 22 23 24 25 31 32 33 34 34 26 27 28 29 30\n\n\nUsing vec_r you created in the last exercise:\n\nremove the columns 6 to 10\n\n\n\nCode\nvec_r &lt;- vec_r[-6:-10] \nvec_r\n\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\n\nchange the firth column to 250\n\n\n\nCode\nvec_r[5] &lt;- 250\n\n\n\nreplace the values on position 1, 2 and 3 with 210, 220 and 230, store the result in vec_r.\n\n\n\nCode\nvec_r &lt;- replace(vec_r, c(1, 2, 3), c(210, 220, 230))\nvec_r\n\n\n [1] 210 220 230  24 250  26  27  28  29  30\n\n\n\nreplace all elements smaller than 100 with 100\n\n\n\nCode\nreplace(vec_r, vec_r &lt; 100, 100)\n\n\n [1] 210 220 230 100 250 100 100 100 100 100\n\n\nUsing vec, sort this vector in decreasing and increasing order.\n\n\nCode\nsort(vec, decreasing = TRUE)\n\n\n [1] 30 29 28 27 26 25 24 23 22 21\n\n\nCode\nsort(vec)\n\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\n\n\n\n\n\n\n4.1.8 Functions and vectors\nMany operations in R are vectorized. This means that an operator works on a vector’s individual elements. For functions, that means that R, for most of them, applies them to every element of that vector.\n\n4.1.8.1 Numeric vectors\nWe introduced mathematical operators and function, statistical function and e.g. rounding in the previous chapter. Almost all these are vectorized. All operators and function generate output. In you want to store these results you have to assign them to a new object. Here this object is usually a vector. In the examples this assignment is left out to keep code short.\n\n4.1.8.1.1 Mathematical operators and functions\nLet’s first create a vector, vec_num1 and vec_num2\n\nvec_num1 &lt;- c(10, 10, 20, 30, 50, 80, 130, 210, 340, 550)\nvec_num2 &lt;- c(1, 1, 2, 3, 5, 8, 13, 21, 34, 55)\n\nIf you add, subtract a numeric value to or from a vector or if you multiply that numeric vector with of divide it by a numeric value, R applies this operation to every element of the vector. For instance\n\naddition:\n\n\nvec_num1 + 100\n\n [1] 110 110 120 130 150 180 230 310 440 650\n\n\n\nsubtraction:\n\n\nvec_num1 - 100\n\n [1] -90 -90 -80 -70 -50 -20  30 110 240 450\n\n\n\nmultiplication:\n\n\nvec_num1 * 10\n\n [1]  100  100  200  300  500  800 1300 2100 3400 5500\n\n\n\ndivision:\n\n\nvec_num1 / 25\n\n [1]  0.4  0.4  0.8  1.2  2.0  3.2  5.2  8.4 13.6 22.0\n\n\n\ninteger division:\n\n\nvec_num1 %/% 3\n\n [1]   3   3   6  10  16  26  43  70 113 183\n\n\n\nmodulus:\n\n\nvec_num1 %% 3\n\n [1] 1 1 2 0 2 2 1 0 1 1\n\n\nApplied to two vectors of the same length, R add, subtracts, multiplies or divides each element in one vector to/from/with the corresponding element in the other vector:\n\naddition:\n\n\nvec_num1 + vec_num2\n\n [1]  11  11  22  33  55  88 143 231 374 605\n\n\n\nsubtraction:\n\n\nvec_num1 - vec_num2\n\n [1]   9   9  18  27  45  72 117 189 306 495\n\n\n\nmultiplication:\n\n\nvec_num1 * vec_num2\n\n [1]    10    10    40    90   250   640  1690  4410 11560 30250\n\n\n\ndivision:\n\n\nvec_num1 / vec_num2\n\n [1] 10 10 10 10 10 10 10 10 10 10\n\n\n\ninteger division:\n\n\nvec_num1 %/% vec_num2\n\n [1] 10 10 10 10 10 10 10 10 10 10\n\n\n\nmodulus:\n\n\nvec_num1 %% vec_num2\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nNote that this save a lot of work. Without vectorization, to add two vectors, you would have to write some code, e.g.:\n\nif (length(vec_num1) != length(vec_num2)) {               \n  print(\"Can not add vectors of a different length\")      \n} else {\n  vec_num4 &lt;- vector(\"numeric\", length = length(vec_num1))\n  for (i in 1:length(vec_num1)) { \n    vec_num4[i] &lt;- vec_num1[i] + vec_num2[i]\n   }\n}\nvec_num4\n\n [1]  11  11  22  33  55  88 143 231 374 605\n\n\nFor functions, let’s illustrate vectorisation using the of vec_num1. All functions where introduced in previous sections.\n\nabsolute value:\n\n\nabs(-vec_num1)\n\n [1]  10  10  20  30  50  80 130 210 340 550\n\n\n\nlogarithm base e (natural logarithm):\n\n\nlog(vec_num1)\n\n [1] 2.302585 2.302585 2.995732 3.401197 3.912023 4.382027 4.867534 5.347108\n [9] 5.828946 6.309918\n\n\n\nlogarithm base 10:\n\n\nlog10(vec_num1) \n\n [1] 1.000000 1.000000 1.301030 1.477121 1.698970 1.903090 2.113943 2.322219\n [9] 2.531479 2.740363\n\nlog(vec_num1, base = 10)\n\n [1] 1.000000 1.000000 1.301030 1.477121 1.698970 1.903090 2.113943 2.322219\n [9] 2.531479 2.740363\n\n\n\nsquare root:\n\n\nsqrt(vec_num1)\n\n [1]  3.162278  3.162278  4.472136  5.477226  7.071068  8.944272 11.401754\n [8] 14.491377 18.439089 23.452079\n\n\n\npower, e.g. 2:\n\n\nvec_num1^2\n\n [1]    100    100    400    900   2500   6400  16900  44100 115600 302500\n\n\n\nexponent (e to the power n (n = element of the vector):\n\n\nexp(vec_num1)\n\n [1]  2.202647e+04  2.202647e+04  4.851652e+08  1.068647e+13  5.184706e+21\n [6]  5.540622e+34  2.872650e+56  1.591627e+91 4.572186e+147 7.277212e+238\n\n\n\n\n4.1.8.1.2 Other usefull vector functions\nAlthough R has many useful vector functions, I’ll introduce a couple of them here. To illustrate what they do, we’ll use\n\nvec_num1 &lt;- c(1, 2, 3, 4, 3, 2, 1)\n\ncumsum(x) shows the cumulative sum of a vector. It’s first element is the first element of x; its second element is the sum of its first element and the second element of x; the third equals its second element (or the sum of the first two elements in x) plus the third element of x, … . If one of the elements is a missing value (NA), the rest of the sum will be set to NA.\n\ncumsum(vec_num1)\n\n[1]  1  3  6 10 13 15 16\n\n\nAs you can see, the second element is equal to 2 + 1, the first two elements in x. The third element, 6, is equal to the second element in the cumulative sum (3) and the third element in vec_num1 … .\ncumprod(x) is a similar function but calculates the cumulative product.\n\ncumprod(vec_num1)\n\n[1]   1   2   6  24  72 144 144\n\n\ncummax(x) and cummin(x) produce a vector with cumulative maximum and minimum values. The first starts with the first observation in x and use this as their first element. If the second elemen in x is larger than the first, the second element in the output vector for the cummax() function will equal that value; else is will equal its first value. The function then evaluates the third element in x. If that element is larger then the second element in the output vector for cummax() the third element in the cummax() vector will be that third element in the x vector; else the third element in the cummax() vector equals its second element. To see how this works:\n\ncummax(vec_num1)\n\n[1] 1 2 3 4 4 4 4\n\n\nAs you can see, the first element is 1. As the second element in vec_num1 is 2, this is a new maximum and the cummax() vector’s second element in 2? The same holds for the third element in vec_num1: it is larger than the second element in the cummax() vector, so this is a new maximum. The third element in the cummax() vector shows this. After the fourth element, all elements in vec_num1 are smaller then its maximum value. In the cummax() vector, the maximum is now stable.\ncummin() is similar, but sets the minimum:\n\ncummin(vec_num1)\n\n[1] 1 1 1 1 1 1 1\n\n\nThe {purrr} package includes a function reduce() which is very useful with vectors. This function reduces elements of a vector in a single value using a 2-argument function that passes the accumulated value as this functions second argument. The cumsum() and cumprod() function’s last value equal the sum and product of all elements in the vector but also shows all intermediate cumulative sums. You can calculate that final value using purrr::reduce(.x, .f, ..., .init, .dir = c(\"forward\", \"backward\")). The first argument, .x is an atomic vector. The second argument .f is a function that will be used across elements. This function needs to arguments: the first is an element from the vector; the second is the accumulated values from the previous step. The arguments .init and .dir = c(\"forward\", \"backward\") show the initial value and the direction of the reduction with “forward” being the default. The default value for the initial value is the first element of x. To calculate the cumulative sum using this function:\n\npurrr::reduce(vec_num1, .f = sum)\n\n[1] 16\n\n\nor even simpler:\n\npurrr::reduce(vec_num1, `+`)\n\n[1] 16\n\n\nand the cumulative product:\n\npurrr::reduce(vec_num1, `*`)\n\n[1] 144\n\n\nNote that this function is not limited to + or -, but can be used with, e.g. /\n\npurrr::reduce(vec_num1, `/`)\n\n[1] 0.006944444\n\n\nIf you only need the total sum of all vector elements, you can use sum(x, na.rm = FALSE):\n\nsum(vec_num1)\n\n[1] 16\n\n\nLikewise, the product of all elements in a vector can be computed using prod(x, na.rm = FALSE):\n\nprod(vec_num1)\n\n[1] 144\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate a vector vec_1 as a sequence from 1 to 20\n\n\nCode\nvec_1 &lt;- 1:10\n\n\nUse this vector to\n\ntake the log, base 10:\n\n\n\nCode\nlog(vec_1, base = 10)\n\n\n [1] 0.0000000 0.3010300 0.4771213 0.6020600 0.6989700 0.7781513 0.8450980\n [8] 0.9030900 0.9542425 1.0000000\n\n\nCode\nlog10(vec_1)\n\n\n [1] 0.0000000 0.3010300 0.4771213 0.6020600 0.6989700 0.7781513 0.8450980\n [8] 0.9030900 0.9542425 1.0000000\n\n\n\nmultiply all elements with 2 and store the result in vec_2\n\n\n\nCode\nvec_2 &lt;- vec_1 * 2\n\n\n\nsubtract vec_1 from vec_2\n\n\n\nCode\nvec_2 - vec_1\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\ncalculate the cumulative sum and cumumative product of vec_1. Store the results in vec_1s and vec_1p:\n\n\n\nCode\nvec_1s = cumsum(vec_1)\nvec_1p = cumprod(vec_1)\n\n\n\nusing this results, show the total sum and total product (1 value each) of vec_1. To do so, assume that you don’t know the number of columns in this vector.\n\n\n\nCode\nvec_1s[length(vec_1)]\n\n\n[1] 55\n\n\nCode\nvec_1p[length(vec_1)]\n\n\n[1] 3628800\n\n\nCalculate the total sum and total produce of vec_1 in two other ways\n\nsum of vec_1\n\n\n\nCode\n# Option 1\n\nsum(vec_1)\n\n\n[1] 55\n\n\nCode\n# Option 2: \n\npurrr::reduce(vec_1, sum)\n\n\n[1] 55\n\n\n\nproduct of vec_1\n\n\n\nCode\n# Option 1\n\nprod(vec_1)\n\n\n[1] 3628800\n\n\nCode\n# Option 2: \n\npurrr::reduce(vec_1, `*`)\n\n\n[1] 3628800\n\n\n\n\n\n\n\n4.1.8.1.3 Statistical functions\n\n4.1.8.1.3.1 Distributions\nThe “r”-variants of the distribution functions such as rnorm were covered in a previous section. Here, we will (re-) introduce the other variants. Recall that we covered three. Applied to the normal distribution, these where pnorm(), dnorm() and qnorm(). We’ll use the vector vec_stat to illustrate these functions\n\nvec_stat &lt;- c(-1.959964, -1.64448, -1.281552, 0, 1.281552, 1.64448, 1.959964)\n\n\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) shows the probability that a value is smaller then or equal to q, by default for a standard normal distribution. Changing the default value from lower.tail = TRUE to FALSE shows the probability that a value of larger then q. For vec_stat, these values are equal to\n\n\npnorm(q = vec_stat, lower.tail = TRUE)\n\n[1] 0.02500000 0.05003855 0.09999992 0.50000000 0.90000008 0.94996145 0.97500000\n\npnorm(q = vec_stat, lower.tail = FALSE)\n\n[1] 0.97500000 0.94996145 0.90000008 0.50000000 0.09999992 0.05003855 0.02500000\n\n\n\ndnorm(x, mean = 0, sd = 1, log = FALSE) shows the probability of x\n\n\ndnorm(x = vec_stat)\n\n[1] 0.05844507 0.10319904 0.17549823 0.39894228 0.17549823 0.10319904 0.05844507\n\n\n\nqnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) for which value the condition holds that its probability (probability that a value is smaller than or equal to) is equal to the values in p. Applied to c(0.025, 0.05, 0.10, 0.90, 0.95, 0.975). The default lower.tail = TRUE has the same interpretation as in pnorm():\n\n\nqnorm(p = c(0.025, 0.05, 0.10, 0.90, 0.95, 0.975), lower.tail = TRUE)\n\n[1] -1.959964 -1.644854 -1.281552  1.281552  1.644854  1.959964\n\nqnorm(p = c(0.025, 0.05, 0.10, 0.90, 0.95, 0.975), lower.tail = FALSE)\n\n[1]  1.959964  1.644854  1.281552 -1.281552 -1.644854 -1.959964\n\n\nFor all other function, Student’s t, Chi-square, uniform, F, you can apply similar functions.\nIn addition to these probability functions, there are many function that summarize a vector. These include function for central tendency and location (mean, median, …), for the level of dispersion and skewness. To illustrate these functions, we’ll use\n\nvec_norm &lt;- rnorm(100, 5, 10)\n\n\n\n4.1.8.1.3.2 Central tendency and location\nHere we will focus on functions that you can use to summarise the data: mean(x, trim = 0, na.rm = FALSE) calculates the mean of a vector. The second argument, trim = 0 can be used to remove observations at each end before computing the mean. For instance, trim = 0.10 would remove the smallest and largest 10% of all values and calculate the mean with the middle 80%. By default, this is 0. na.rm = FALSE tells are that it shouldn’t remove missing values. If the vector includes missing values, and the default FALSE is left, the result of this function will be NA.\n\nmean(x = vec_norm, na.rm = TRUE)\n\n[1] 4.414747\n\nmean(x = vec_norm, trim = 0.10, na.rm = TRUE)\n\n[1] 4.443764\n\n\nThe median(x = , na.rm = FALSE) function calculates the median. Here again, you need to specify how to handle missing observations.\n\nmedian(vec_norm, na.rm = TRUE)\n\n[1] 3.787437\n\n\nNote that the median is a special case of quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE, names = TRUE). This function allows you to compute the quantiles of a distribution. By default, the function calculates the minimum, the 25th percentile, the median (50th percentile), the 75th percentile and the maximum. You can see this in the probs = seq(0, 1, 0.25). Recall that seq(0, 1, 0.25) produces a vector (0, 0.25, 0.50, 0.75, 1). These probabilities correspond to default values. You can change this the default if you include your own values using, e.g. c(0.10, 0.25, 0.50, 0.75, 0.90). This option would show the 1st, 2nd and 3rd quartile (25th, 50th and 75th percentile) in addition to the 10th and 90th percentile. To see all deciles, you can use seq(0.10, 0.90, 0.10) as the value for probs. The last options tells R it needs to add names to the values (e.g. Min, 1st Qu, Median, 10% …). If you set this value to FALSE, these names are dropped. If you save these results in a new vector, you can subset them using both the subsetting methods for named and unnamed vectors. To see the 10th and 90th percentile as well as the 1st, 2nd and 3th quartile of vec_stat:\n\nvec_quan &lt;- quantile(x = vec_stat, probs = c(0.10, 0.25, 0.50, 0.75, 0.90), na.rm = TRUE, names = TRUE)\nvec_quan\n\n      10%       25%       50%       75%       90% \n-1.770674 -1.463016  0.000000  1.463016  1.770674 \n\n\nYou can now subset vec_quan:\n\nvec_quan[1]\n\n      10% \n-1.770674 \n\nvec_quan[\"75%\"]\n\n     75% \n1.463016 \n\n\nTo see the minimum and maximum values, you can use min() and max(). Other than a vector, these functions allow you to set the default na.rm from false into TRUE:\n\nmin(vec_norm, na.rm = TRUE)\n\n[1] -15.86817\n\nmax(vec_norm, na.rm = TRUE)\n\n[1] 27.46498\n\n\nThe summary() function shows the mean and median as well as the minimum, maximum and the 1st and 3rd quartile. This function returns a table. If you save the results, you can subset this table using the traditional subsetting rules for named and unnamed vectors.\n\ntab_sum &lt;- summary(vec_norm)\ntab_sum\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-15.868  -3.233   3.787   4.415  12.944  27.465 \n\n\nIf you use a name to subset, note that the name of some summary statistics include a ‘.’ at the end:\n\ntab_sum[\"3rd Qu.\"]\n\n 3rd Qu. \n12.94374 \n\n\n\n\n4.1.8.1.3.3 Measures of dispersion\nOften used measure if dispersion include the range, the minimum and maximum values; the interquartile range or the difference between the 3rd and 1st quartile, the variance and standard deviation. To use the range() function, you need to supply it with the a vector. The other argument is na.rm = FALSE by default. The function shows the minmum and maxium value. Note that these statistics are also includes in e.g. summary(), min() and max() and you can also select them using quantiles().\n\nrange(vec_norm, na.rm = TRUE)\n\n[1] -15.86817  27.46498\n\n\nTo calculate the interquartile range of IQR, you can use IQR(). The most important arguments of this function include the vector and na.rm:\n\nIQR(x = vec_norm, na.rm = TRUE)\n\n[1] 16.177\n\n\nTo compute the variance function you can use var(x, na.rm = FALSE). You can calculate the standard deviation either as the square root of the variance or using sd(x, na.rm = FALSE). In both functions, x is the vector whose variance or standard deviation you need to compute;\n\nvar(x = vec_norm, na.rm = TRUE)\n\n[1] 104.2964\n\nsqrt(var(x = vec_norm, na.rm = TRUE))\n\n[1] 10.21256\n\nsd(x = vec_norm, na.rm = TRUE)\n\n[1] 10.21256\n\n\n\n\n4.1.8.1.3.4 Higher order moments: skewness and kurtosis\nTo calculate moments larger then 2, you can use the {moments} package. This package includes functions such as skewness() and kurtosis(). You can use these to calculate the third and fourth moment of the distribution. For higher order moments, you can use moment(x, order = 1, central = FALSE, absolute = FALSE, na.rm = FALSE). The order = argument allows you to set the order (e.g. 2 for variance, 3 for skewness, …). To set moments around the mean (e.g. like you would do to calculate the variance), set central = TRUE. To use this package, you have to install it first.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate a vector, vec_rn with 100 draws from a normal distribution with mean 5 and standard deviation 5 and vec_rt with 100 draws from Student’s t-distribution with 10 degree of freedom\n\n\nCode\nvec_rn &lt;- rnorm(100, 5, 5)\nvec_rt &lt;- rt(100, 10)\n\n\nDetermine the probability that you find values larger than each of the elements in c(1.65, 1.75, 2.10) if these values follow a t-distribution with 5 degrees of freedom.\n\n\nCode\npt(c(1.65, 1.75, 2.10), df = 5, lower.tail = FALSE)\n\n\n[1] 0.07992788 0.07026118 0.04487662\n\n\nFor a Chi-square distribution with 10 degrees of freedom, determine the values for which holds that the probabilities that you find a value smaller than or equal to that value are equal to c(0.025, 0.05, 0.10, 0.90, 0.95, 0.975)\n\n\nCode\nqchisq(c(0.025, 0.05, 0.10, 0.90, 0.95, 0.975), df = 10)\n\n\n[1]  3.246973  3.940299  4.865182 15.987179 18.307038 20.483177\n\n\nUsing vec_rn determine:\n\nthe mean (include the possibility that there are missing values):\n\n\n\nCode\nmean(vec_rn, na.rm = TRUE)\n\n\n[1] 5.310853\n\n\n\nthe mean if you remove the 10% lowest and 10% highest values:\n\n\n\nCode\nmean(vec_rn, trim = 0.10, na.rm = TRUE)\n\n\n[1] 5.312695\n\n\n\nthe median:\n\n\n\nCode\nmedian(vec_rn, na.rm = TRUE)\n\n\n[1] 5.104114\n\n\n\nquantiles:\n\n\n\nCode\nquantile(vec_rn, na.rm = TRUE)\n\n\n       0%       25%       50%       75%      100% \n-5.807568  1.486231  5.104114  8.714391 15.770025 \n\n\n\nminimum and maximum:\n\n\n\nCode\nmin(vec_rn)\n\n\n[1] -5.807568\n\n\nCode\nmax(vec_rn)\n\n\n[1] 15.77002\n\n\n\nrange:\n\n\n\nCode\nrange(vec_rn, na.rm = TRUE)\n\n\n[1] -5.807568 15.770025\n\n\n\nInterquartile distance:\n\n\n\nCode\nIQR(vec_rn, na.rm = TRUE)\n\n\n[1] 7.228161\n\n\n\nstandard deviation and variance:\n\n\n\nCode\nsd(vec_rn, na.rm = TRUE)\n\n\n[1] 5.049125\n\n\nCode\nvar(vec_rn, na.rm = TRUE)\n\n\n[1] 25.49367\n\n\n\n\n\n\n\n\n4.1.8.1.4 Rounding\nRounding numeric values uses round(), floor(), ceiling(), trunc() or signif(). Applied to a numeric vector, these function output a vector with rounded data. To illustrate, let’s first take of natural logarithm of vec_num1 and use this vector to show how these functions work.\n\nvec_num3 &lt;- log(vec_num1)\n\n\nround(x, digits = 0): rounds x to n decimal places. With n = 2\n\n\nround(vec_num3, digits = 2)\n\n[1] 0.00 0.69 1.10 1.39 1.10 0.69 0.00\n\n\n\n`floor(x): rounds to the largest integer, not greater than the value in x:\n\n\nfloor(vec_num3)\n\n[1] 0 0 1 1 1 0 0\n\n\n\nceiling(x): rounds to the smallest integer not less than the value in x:\n\n\nceiling(vec_num3)\n\n[1] 0 1 2 2 2 1 0\n\n\n\ntrunc(x): removes all decimal places:\n\n\ntrunc(vec_num3)\n\n[1] 0 0 1 1 1 0 0\n\n\n\nsignif(x, digits = 6) rounds values in x to the specified number of significant digits. Applied to c(123456, 654321, 147258, 852147):\n\n\nsignif(x = c(123456, 654321, 147258, 852147), digits = 4)\n\n[1] 123500 654300 147300 852100\n\n\n\n\n4.1.8.1.5 Boolean operators\nBoolean operators work element wise. For instance, to check if the values in vec_num1 are larger than 50:\n\nvec_num1 &gt; 50\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nRecall that we used this observation to subset a vector. There are two other useful function to apply to vectors: any()and all(). The first checks if at least one of the values is TRUE. The other is all values are TRUE. For instance, to check is any of the values in vec_num are larger than 450:\n\nany(vec_num1 &gt; 450, na.rm = TRUE)\n\n[1] FALSE\n\n\nYou can use all()to check is a conditions holds for all elements in a vector. For instance, to see if all elements are positive, you can use\n\nall(vec_num1 &gt; 1, na.rm = TRUE)\n\n[1] FALSE\n\n\n\n\n\n\n4.1.9 Functions operating on character vectors\nIn Chapter 3, we introduced character functions. Here, we will see how they can be used with character vectors. Note that most {stringr} function return a list. We will meet lists in this chapter and see how you can use them in your analysis. Here, we will only use their properties if needed. Most base R functions output a vector. Sometimes, this makes them easier to use.\nWe already looked at paste() and paste0(). You can use these functions to generate a series of numbers as characters:\n\npaste(1:5)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nIf you apply these function to vectors including several character values, you can change the collapse = NULL to show these character values in 1 string. To see what these functions do, let’s use:\n\nvec_char1 &lt;- c(\"dog\", \"cat\", \"fish\")\n\nLet’s first use paste()\n\npaste(vec_char1, collapse = \" __ \")\n\n[1] \"dog __ cat __ fish\"\n\n\nAs you can see, the three elements of the character vector are now one character, seperated by “__”. If you use paste0() you have the same result:\n\npaste0(vec_char1, collapse = \"**\")\n\n[1] \"dog**cat**fish\"\n\n\nBoth function also allow you to create variable names such as “var_1”, “var_2”, …\n\npaste(\"var\", 1:5, sep = \"_\")\n\n[1] \"var_1\" \"var_2\" \"var_3\" \"var_4\" \"var_5\"\n\npaste0(\"var\", 1:5)\n\n[1] \"var1\" \"var2\" \"var3\" \"var4\" \"var5\"\n\n\nNote that here you leave the collapse default, as you want to keep the various character values as separate values in a character vector. For instance, because you will use them as names in a dataset.\nWhen we introduced string variables, we introduced regular expressions. Recall that you can include these regular expressions in {stringr}’s function as well as in e.g. grep() or grepl(). These are especially useful if you have a character vector. Let’s define a character vector first. For instance, a list of product codes could look like\n\nvec_char3 &lt;- c(\"25-78T\", \"25-98S\", \"45-97Q\", \"45-74Q\", \"45-72T\", \"55-10T\", \"55-48T\", \"55-69Q\", \"55+178T\", \n               \"173+8W\", \"235+9W\", \"125+1W\", \"274+2Q\", \"274+5Q\", \"751+9Q\", \"274+1W\", \"274+4W\", \"274+5Q\", \"751+6Q\")\n\nSuppose that you need to extract all product codes that start with 55 and end with T. As you can see in the character vector, there are two matches: “55-10T”, “55-48T” and 55+178T. You don’t want to extract e.g. “55-69-Q” or “25-68-T”. This is where regular expressions are very useful. Recall that regular expressions allow you to identify patterns using e.g. “[A-Z][a-z]+”, “\\\\d” or “[0-9]{4}”. The first searches for pattern in characters that start with a capital letter A to Z and are followed by one or more normal letters a to z. The second searches for numbers and the third for numbers with exact 4 repetitions. In the second, we use the backslash “\\”. Recall that is is the escape operator for regular expressions. Using “d” in a regular expression would search for the occurrence of the letter “d”. To tell R it has to look for digits, we need to “escape” the usual meaning of “d”. To do so, we use “\\” to escape the usual interpretation “d”. As “\\” also has a specific meaning in R - it is the escape character - we need to tell R not to use the literal interpretation of “\\” but interpret it as an escape character. This is where the second backslash enters. In other words the first backslash on the left “\\\\d” tells R to escape the literal interpretation of the second backslash from the left. The second backslash is now an escape character. With that escape character, the interpretation of “d” changes: we don’t want the literal interpretation of “d” but the “d” as referring to all digits interpretation. The same holds e.g. if you want to escape the literal interpretation of a dot “.”: you need to use “\\\\ .”. Doing so, R will not look for a “.” but will find any character.\nIf you want to avoid backslashes for special character such as ., *, {, }, +, ^, $, |, ?, (, ), you can include them between square brackets [] . For instance [.] or [ ?] will look for a literal “.” or “?”.\nTo search for a specific word or part of a word, you can use that word or its part. Suppose that you have a character vector\n\nvec_char1 &lt;- c(\"pineapple\", \"strawberry\", \"blackberry\", \"apple\", \"banana\", \"grapefruit\", \"melon\", \"cranberry\", \"kiwi\", \"lemon\")\n\nTo extract all elements including “berry”, you can use e.g. grep(). Recall that this function’s first argument is the pattern, the second the character vector where grep() will look to pattern matches, the option to set ignore.case = FALSE to TRUE and the option to show the value as opposed to the position by changing the default value = FALSE to TRUE. To show the pattern match, you’ll also see the output from {stringr}’s str_view(x, pattern, match = NA) function. This functions shows the matches in the character vector. Changing match = NA in match = TRUE limits the output from this function to the matched strings. Adding html = TRUE shows an html widget in the Viewer tab of the files pane.\n\ngrep(pattern = \"berry\", x = vec_char1, value = TRUE)\n\n[1] \"strawberry\" \"blackberry\" \"cranberry\" \n\nstringr::str_view(vec_char1, \"berry\", match = NA )\n\n [1] │ pineapple\n [2] │ straw&lt;berry&gt;\n [3] │ black&lt;berry&gt;\n [4] │ apple\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cran&lt;berry&gt;\n [9] │ kiwi\n[10] │ lemon\n\n\nTo see the position of these values:\n\ngrep(pattern = \"berry\", x = vec_char1, value = FALSE)\n\n[1] 2 3 8\n\nstringr::str_view(vec_char1, \"berry\", match = NA )\n\n [1] │ pineapple\n [2] │ straw&lt;berry&gt;\n [3] │ black&lt;berry&gt;\n [4] │ apple\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cran&lt;berry&gt;\n [9] │ kiwi\n[10] │ lemon\n\n\nIncluding the pattern “an” shows all matches that include the letter “an”:\n\ngrep(pattern = \"an\", x = vec_char1, value = TRUE)\n\n[1] \"banana\"    \"cranberry\"\n\nstringr::str_view(vec_char1, \"an\", match = NA)\n\n [1] │ pineapple\n [2] │ strawberry\n [3] │ blackberry\n [4] │ apple\n [5] │ b&lt;an&gt;&lt;an&gt;a\n [6] │ grapefruit\n [7] │ melon\n [8] │ cr&lt;an&gt;berry\n [9] │ kiwi\n[10] │ lemon\n\n\nUsing “|” you can add various patterns. For instance, using “an|be” matches all elements in the string that include “an” or “be”.\n\ngrep(pattern = \"an|be\", x = vec_char1, value = TRUE)\n\n[1] \"strawberry\" \"blackberry\" \"banana\"     \"cranberry\" \n\nstringr::str_view(vec_char1, \"an|be\", match = NA)\n\n [1] │ pineapple\n [2] │ straw&lt;be&gt;rry\n [3] │ black&lt;be&gt;rry\n [4] │ apple\n [5] │ b&lt;an&gt;&lt;an&gt;a\n [6] │ grapefruit\n [7] │ melon\n [8] │ cr&lt;an&gt;&lt;be&gt;rry\n [9] │ kiwi\n[10] │ lemon\n\n\nIn these examples, the pattern is fixed: “berry”. However, often the patterns are less clear. Regular expressions allow you to build more complex patterns.\nFirst you can combine one or more characters (letter, numbers, symbols) sets with qualifiers to control for the number of occurrences and anchors that determine where a pattern occurs. Let’s start with the first. A character set is included between []. For instance, [rst] matches lowercase “r”, “s”, or “t”; [aeiou] matches all lowercase vowels “a”, “e”, “i”, “o” or “u”, [IJK] matches all uppercase “I”, “J”, or “K” and [aBcD] matches lowercase “a” or “c” or uppercase “B” or “D”. In a similar way you can match numbers. For instance [0123] matches “0”, “1”, “2” or “3”. You can include letters and numbers in your set: [a1B2c] matches “a”, “1”, “B”, “2” or “c”. Note that you can combine character sets in one regular expression. For instance [aeiou]b[aeiou] searches for a pattern: a vowel, the letter b and another vowel. For instance, to search for the pattern: any letter from “m”, “n”, “o”, “p”, “q” or “r” followed by an “a” followed by any letter from “m”, “n”, “o”, “p”, “q” or “r”:\n\ngrep(\"[mnopqr]a[mnopqr]\", vec_char1, value = TRUE)\n\n[1] \"banana\"     \"grapefruit\" \"cranberry\" \n\nstringr::str_view(vec_char1, \"[mnoprq]a[mnopqr]\", match = NA)\n\n [1] │ pineapple\n [2] │ strawberry\n [3] │ blackberry\n [4] │ apple\n [5] │ ba&lt;nan&gt;a\n [6] │ g&lt;rap&gt;efruit\n [7] │ melon\n [8] │ c&lt;ran&gt;berry\n [9] │ kiwi\n[10] │ lemon\n\n\nTo include special characters in the set, you need the escape character. For instance [a-z\\.] matches “a” to “z” as well as “.”, [$] matches “$” and [{}] matches “{” or “}”.\nIf you use - within your character set, you can define a range: [a-z] matches all lower case letters starting from a and running across the alphabet to z. If you change the “a” or “z” you can restrict the range, e.g. [k-n] matches “k”, “l”, “m”, or “n” Using uppercase allows you to define a range of uppercase letters: [B-E] matches “B”, “C”, “D” or “E”. Adding both, e.g. [a-zA-A] or [A-Za-Z] matches any character “a” to “z” or “A” to “Z”. Using numbers, [0-9] matches all numbers from 0 to 9 while [1-3] matches all numbers from 1 to 3. To see how these ranges work, let’s look for the pattern: any letter from “a” to “m” followed by an “e” followed by any letter from “n” to “z” in vec_char1:\n\ngrep(\"[a-m]e[n-z]\", vec_char1, value = TRUE)\n\n[1] \"strawberry\" \"blackberry\" \"cranberry\" \n\nstringr::str_view(vec_char1, \"[a-m]e[n-z]\", match = NA)\n\n [1] │ pineapple\n [2] │ straw&lt;ber&gt;ry\n [3] │ black&lt;ber&gt;ry\n [4] │ apple\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cran&lt;ber&gt;ry\n [9] │ kiwi\n[10] │ lemon\n\n\nIncluding the carat sign “^” within a character set works as a negation. For instance [^a-k] matches all lowercase letters except “a” to “k”, [^qrt] matches all letters except “q”, “r” or “t”. Used with digits, [^3-9] matches all except “3” to “9”. To see how this works, the use the carat sign in the previous regular expression: “[^a-m]e[^n-z]” a letter not from “a” to “m” followed by an “e” followed by a letter not from “n” to “z”:\n\ngrep(\"[^a-m]e[^n-z]\", vec_char1, value = TRUE)\n\n[1] \"pineapple\"  \"grapefruit\"\n\nstringr::str_view(vec_char1, \"[^a-m]e[^n-z]\", match = NA)\n\n [1] │ pi&lt;nea&gt;pple\n [2] │ strawberry\n [3] │ blackberry\n [4] │ apple\n [5] │ banana\n [6] │ gra&lt;pef&gt;ruit\n [7] │ melon\n [8] │ cranberry\n [9] │ kiwi\n[10] │ lemon\n\n\nIn addition to these character sets, there are meta characters and shortcuts that have their own meaning. With respect to the metacharacters, you can use “.” to refer to any single character. In other words “a..b” matches all patterns that start with a, and with b and have two characters between them. For instance if you want to find matches “a..e” in vec_char1 R will look at all occurrences of “a” followed by 2 other characters and ending with “e”:\n\ngrep(\"a..e\", vec_char1, value = TRUE)\n\n[1] \"strawberry\" \"cranberry\" \n\nstringr::str_view(vec_char1, \"a..e\", match = NA)\n\n [1] │ pineapple\n [2] │ str&lt;awbe&gt;rry\n [3] │ blackberry\n [4] │ apple\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cr&lt;anbe&gt;rry\n [9] │ kiwi\n[10] │ lemon\n\n\nWith respect to the shortcuts, they include:\n\n\\d : any digit character, 0, 1, 2 …\n\\D : any non digit character: letters, question marks, spaces, …\n\\w : any alphanumeric character\n\\W : any non-alphanumeric character (symbols, punctuation, …)\n\\s : a whitespace including space and tab\n\\S : any non-whitespace\n\nNote that there you need the escape character. Using these shortcuts, you can replace [0-9] with \\d, search for whitespaces using \\s, … . As an example:\n\nmatching a any digit:\n\n\ngrep(\"\\\\d\", c(\"125\", \"abc! \", \" \"), value = TRUE)\n\n[1] \"125\"\n\ngrep(\"[0-9]\", c(\"125\", \"abc! \", \" \"), value = TRUE)\n\n[1] \"125\"\n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\d\", match = NA)\n\n[1] │ &lt;1&gt;&lt;2&gt;&lt;5&gt;\n[2] │ abc! \n[3] │  \n\n\n\nmatching any non-digit:\n\n\ngrep(\"\\\\D\", c(\"125\", \"abc! \",  \" \"), value = TRUE)\n\n[1] \"abc! \" \" \"    \n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\D\", match = NA)\n\n[1] │ 125\n[2] │ &lt;a&gt;&lt;b&gt;&lt;c&gt;&lt;!&gt;&lt; &gt;\n[3] │ &lt; &gt;\n\n\n\nmatching any alfanumeric character:\n\n\ngrep(\"\\\\w\", c(\"125\", \"abc! \", \" \"), value = TRUE)\n\n[1] \"125\"   \"abc! \"\n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\w\", match = NA)\n\n[1] │ &lt;1&gt;&lt;2&gt;&lt;5&gt;\n[2] │ &lt;a&gt;&lt;b&gt;&lt;c&gt;! \n[3] │  \n\n\n\nmatching any non-alfanumeric character:\n\n\ngrep(\"\\\\W\", c(\"125\", \"abc! \"), value = TRUE)\n\n[1] \"abc! \"\n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\W\", match = NA)\n\n[1] │ 125\n[2] │ abc&lt;!&gt;&lt; &gt;\n[3] │ &lt; &gt;\n\n\n\nmatching a whitespace:\n\n\ngrep(\"\\\\s\", c(\"125\", \"abc! \",  \" \"), value = TRUE)\n\n[1] \"abc! \" \" \"    \n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\s\", match = NA)\n\n[1] │ 125\n[2] │ abc!&lt; &gt;\n[3] │ &lt; &gt;\n\n\n\nmatching any non-whitespace:\n\n\ngrep(\"\\\\S\", c(\"125\", \"abc! \",  \" \"), value = TRUE)\n\n[1] \"125\"   \"abc! \"\n\nstringr::str_view(c(\"125\", \"abc! \", \" \"), \"\\\\S\", match = NA)\n\n[1] │ &lt;1&gt;&lt;2&gt;&lt;5&gt;\n[2] │ &lt;a&gt;&lt;b&gt;&lt;c&gt;&lt;!&gt; \n[3] │  \n\n\nAnchors determine the location of a pattern. Using the carat \\^ the pattern needs to be located at the start of the string. In other words ^r will match pattern starting with an “r”. Note here the difference in result if \\^ is used withing [ ] and before a string. Within square brackets, it works to exclude the letters or numbers withing the square brackets. Starting a string with the carat sign, works to determine the position of a character. Ending a pattern with a \\$ means that the pattern should be at the end of a string. In other words y$ matches a y at the end of the string. Using \\b, you locate at pattern at the end of a word (e.g. before a space, dash, comma, semi colon, dot, …) while \\B matches any non-word boundary:\n\n^ : the string starts with the expression following ^,\n$ : the string ends with the expression before $\n\n\\b : matches a word boundary (space, dash, comma, semi colon, …)\n\\B : matches a non-word boundary (\\w-\\w or \\W-\\W)\n\nFor example:\n\nmatching a pattern at the start of a string:\n\n\ngrep(\"^b\", vec_char1, value = TRUE)\n\n[1] \"blackberry\" \"banana\"    \n\nstringr::str_view(vec_char1, \"^b\", match = NA)\n\n [1] │ pineapple\n [2] │ strawberry\n [3] │ &lt;b&gt;lackberry\n [4] │ apple\n [5] │ &lt;b&gt;anana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cranberry\n [9] │ kiwi\n[10] │ lemon\n\n\n\nmatching a pattern at the end of a string:\n\n\ngrep(\"y$\", vec_char1, value = TRUE)\n\n[1] \"strawberry\" \"blackberry\" \"cranberry\" \n\nstringr::str_view(vec_char1, \"y$\", match = NA)\n\n [1] │ pineapple\n [2] │ strawberr&lt;y&gt;\n [3] │ blackberr&lt;y&gt;\n [4] │ apple\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cranberr&lt;y&gt;\n [9] │ kiwi\n[10] │ lemon\n\n\n\nmatching a pattern at the end of a word boundary:\n\n\ngrep(\"e\\\\b\", c(\"average costs\", \"total sales\", \"total revenues\"), value = TRUE)\n\n[1] \"average costs\"\n\nstringr::str_view(c(\"average costs\", \"total sales\", \"total revenues\"), \"e\\\\b\", match = NA)\n\n[1] │ averag&lt;e&gt; costs\n[2] │ total sales\n[3] │ total revenues\n\n\n\nmatching a pattern with a non-word boundary:\n\n\ngrep(\"s\\\\B\", c(\"average costs\", \"total sales\"), value = TRUE)\n\n[1] \"average costs\" \"total sales\"  \n\nstringr::str_view(c(\"average cost\", \"total sales\", \"total revenues\"), \"s\\\\B\", match = NA)\n\n[1] │ average co&lt;s&gt;t\n[2] │ total &lt;s&gt;ales\n[3] │ total revenues\n\n\nUsing word boundaries, you can match occurrences of e.g. individual numbers not included in another one. For instance, to identify the number “2” as a number not included in e.g. “125” or “210” you can use\n\ngrep(\"\\\\b2\\\\b\", c(\"125\", \"2\", \"210\"), value = TRUE)\n\n[1] \"2\"\n\n\nQuantifiers control the number of occurrences of a pattern. Using \\+ at the end of a pattern means that this pattern can be repeated once or more times. These repetitions can occur throughout the string if they are not followed by another part of the regular expression. For instance “[a-z]\\+” means that a letter from “a” to “z” can occur once but also multiple times. Using \\* is used when a pattern doesn’t have to occur or could occur with one of multiple repetitions. With \\? you need at most one repetition. In other words, the pattern before \\? is optional. Using \\{x\\} fixed the number of repetitions to x while \\{x, y\\} sets the number of repetition between x or y.\n\n+ : refers to one of more repetitions\n* : refers to zero or more repetitions (i.e. it doesn’t occur but it can also occur multiple times).\n\n? : at most 1 repetitions\n{x} : in case you want to include the number of repetitions\n{x, } : at least x repetitions\n{x, y} : in case there are x to y repetitions.\n\nHere are a couple of examples:\n\nmatching one or more repetitions:\n\n\ngrep(\"p+\", vec_char1, value = TRUE)\n\n[1] \"pineapple\"  \"apple\"      \"grapefruit\"\n\nstringr::str_view(vec_char1, \"p+\", match = NA)\n\n [1] │ &lt;p&gt;inea&lt;pp&gt;le\n [2] │ strawberry\n [3] │ blackberry\n [4] │ a&lt;pp&gt;le\n [5] │ banana\n [6] │ gra&lt;p&gt;efruit\n [7] │ melon\n [8] │ cranberry\n [9] │ kiwi\n[10] │ lemon\n\n\n\nmatching an optional character (here the Q is optional)\n\n\ngrep(\"abcQ?abc\", c(\"abcQabc\", \"abcabc\", \"abc_abc\"), value = TRUE)\n\n[1] \"abcQabc\" \"abcabc\" \n\nstringr::str_view(c(\"abcQabc\", \"abcabc\", \"abc_abc\"), \"abcQ?abc\", match = NA)\n\n[1] │ &lt;abcQabc&gt;\n[2] │ &lt;abcabc&gt;\n[3] │ abc_abc\n\n\n\nmatching exact two repetitions\n\n\ngrep(\"p{2}\", vec_char1, value = TRUE)\n\n[1] \"pineapple\" \"apple\"    \n\nstringr::str_view(vec_char1, \"p{2}\", match = NA)\n\n [1] │ pinea&lt;pp&gt;le\n [2] │ strawberry\n [3] │ blackberry\n [4] │ a&lt;pp&gt;le\n [5] │ banana\n [6] │ grapefruit\n [7] │ melon\n [8] │ cranberry\n [9] │ kiwi\n[10] │ lemon\n\n\nIf you have longer character vectors that include various lines, you can identify every new line or a tab using:\n\n\\n : A new line\n\\t : A tab\n\nAn expression between parentheses () forms a group. This allows you e.g. to apply a quantifiers to that group. For instance, let’s use the pattern “(na)+” to find matches in vec_char1:\n\ngrep(\"(na)+\", vec_char1, value = TRUE)\n\n[1] \"banana\"\n\nstringr::str_view(vec_char1, \"(na)+\", match = NA)\n\n [1] │ pineapple\n [2] │ strawberry\n [3] │ blackberry\n [4] │ apple\n [5] │ ba&lt;nana&gt;\n [6] │ grapefruit\n [7] │ melon\n [8] │ cranberry\n [9] │ kiwi\n[10] │ lemon\n\n\nAs you can see there is one match:  in banana. This is useful because it allows you to shorten some regular expressions. For instance, suppose that you are looking for a pattern: “3 letters, a number, 3 letters, a number” e.g. abc1csb2 there are two ways to write this regular expression. The first “[a-z]{3}\\d[a-z]{3}\\d”. The second, using parenthesis: “([a-z]{3}\\d){2}”. Using parenthesis, you repeat the part within the group twice.\nNote that you can store regular expressions in an object. For instance,\n\npat_1 &lt;- \"abc|def\"\n\nstores a regular expression you can re-use:\n\ngrep(pat_1, c(\"abc\", \"def\", \"ghi\"), value = TRUE)\n\n[1] \"abc\" \"def\"\n\n\nThis allows you to generate patterns from code.\nLet’s now use these regular expressions to extract characters from a character vector. Let’s first start with\n\nvec_char2 &lt;- c(\"usd 25\", \"eur 35\", \"USD 36\", \"EUR 88\", \"Usd 4700\", \"Eur 18723\", \"$25522\", \"€140\")\n\nHere, you can see that all strings in the character vector refer to a currency, the usd or eur, but that these references are written in multiple ways. To work with the numbers, we need to extract the currency and the currency and store each is a separate variable. Let’s stick to regular expressions (you could e.g. tolower() to change of uppercase currency in lowercase and gsub() to replace all occurrences of \\$ and € with “usd” or “eur”. Using {stringr}’s str_extract_all() to extract the currencies.\n\nstringr::str_extract_all(vec_char2, \"[A-Za-z]{3}|€|\\\\$\")\n\n[[1]]\n[1] \"usd\"\n\n[[2]]\n[1] \"eur\"\n\n[[3]]\n[1] \"USD\"\n\n[[4]]\n[1] \"EUR\"\n\n[[5]]\n[1] \"Usd\"\n\n[[6]]\n[1] \"Eur\"\n\n[[7]]\n[1] \"$\"\n\n[[8]]\n[1] \"€\"\n\n\nNow use the same function to extract the numbers.\n\nstringr::str_extract_all(vec_char2, \"\\\\d+\")\n\n[[1]]\n[1] \"25\"\n\n[[2]]\n[1] \"35\"\n\n[[3]]\n[1] \"36\"\n\n[[4]]\n[1] \"88\"\n\n[[5]]\n[1] \"4700\"\n\n[[6]]\n[1] \"18723\"\n\n[[7]]\n[1] \"25522\"\n\n[[8]]\n[1] \"140\"\n\n\nstr_extract_all() returns a list. You can access the elements of that list using the subsetting operators for a list. For instance, to show the value for the second outcome and return a numeric value, you would use:\n\noutcome &lt;- stringr::str_extract_all(vec_char2, \"\\\\d+\")\nas.numeric(outcome[[2]][1])\n\n[1] 35\n\n\nAs an alternative, you can simplify these results. To do so, you add simplify = TRUE as an argument to the str_extract_all() function\n\noutcomes &lt;- stringr::str_extract_all(vec_char2, \"\\\\d+\", simplify = TRUE)\noutcomes |&gt; as.numeric()\n\n[1]    25    35    36    88  4700 18723 25522   140\n\n\nYou can now subset these results using the usual subsetting operators.\nSuppose that student numbers are written as “r2024-000125-B”. Here the pattern is “lowercase r; followed by academic year; followed by -; followed by 6 digits; followed by - and ends with a uppercase which can be any uppercase letter”. Write a regular expression that identifies these numbers in\n\nchar_stud &lt;- c(\"r2024-000125-B\", \"r2024-005524-L\", \"r2024-00014-5\", \"r2024-1000140-C\")\n\nNote that only the first two are correct.\n\ngrep(\"r\\\\d{4}-\\\\d{6}-[A-Z]\", char_stud, value = TRUE)\n\n[1] \"r2024-000125-B\" \"r2024-005524-L\"\n\n\nHow would you change this regular expression if the part in the middle could be 6 or 7 digits? If that is the case, in addition to the first two, the last number should also match.\n\ngrep(\"r\\\\d{4}-\\\\d{6,7}-[A-Z]\", char_stud, value = TRUE)\n\n[1] \"r2024-000125-B\"  \"r2024-005524-L\"  \"r2024-1000140-C\"\n\n\nRecall that dates are written as “yyyy-mm-dd”. Write a regular expression that actual dates in the following character vector.\n\nvec_dat &lt;- c(\"2025-03-20\", \"2025-03-08\", \"1998-11-11\", \"2025-24-33\", \"2025-19-54\")\n\nNote that only the first 3 are correct.\n\ngrep(\"\\\\d{4}-[0-1][0-9]-[0-3][0-9]\", vec_dat, value = TRUE)\n\n[1] \"2025-03-20\" \"2025-03-08\" \"1998-11-11\"\n\n\nHere you have some sentences. Using {stringr}’s str_count() the number of times the letters “the” occur in words but excluding the word “the” (e.g. thesis, these, they)\n\nvec_quote &lt;-c(\"The thesis was written by 2 students.\", \n              \"These students were in the same group for mathematics.\",\n              \"The first part of their work included their theory.\",\n              \"They had to apply statistics to test their hypothesis.\",\n              \"These tests were done in R.\",\n              \"To collect their data, they had to visit a theater.\")\n\nFirst write the regular expression to match these words:\n\nstringr::str_view(vec_quote, \"(([A-Za-z]?)+(T|t)he)[a-z]+\", match = NA)\n\n[1] │ The &lt;thesis&gt; was written by 2 students.\n[2] │ &lt;These&gt; students were in the same group for &lt;mathematics&gt;.\n[3] │ The first part of &lt;their&gt; work included &lt;their&gt; &lt;theory&gt;.\n[4] │ &lt;They&gt; had to apply statistics to test &lt;their&gt; &lt;hypothesis&gt;.\n[5] │ &lt;These&gt; tests were done in R.\n[6] │ To collect &lt;their&gt; data, &lt;they&gt; had to visit a &lt;theater&gt;.\n\n\nUse str_count() to count the number of matches per sentence:\n\nstringr::str_count(vec_quote, \"(([A-Za-z]?)+(T|t)he)[a-z]+\")\n\n[1] 1 2 3 3 1 3\n\n# Check the words: the is included in thesis, these, mathematics, their, theory\n# hypothesis, they and theater. T can be both uppercase and lowercase\n# ((T|t)he): a group of letters allowing for The as well as the\n# part before this group: ([a-z]?)+: optional number of upper or lowercase\n# letters: upper of lowercase: [A-Za-z], optional: ? can be repeated as a group\n# part after ((T|t)he): any series of letters, lowercase: [a-z]+\n\nHere, str_count shows the result in a vector\nLet’s return to vec_char3\n\nvec_char3\n\n [1] \"25-78T\"  \"25-98S\"  \"45-97Q\"  \"45-74Q\"  \"45-72T\"  \"55-10T\"  \"55-48T\" \n [8] \"55-69Q\"  \"55+178T\" \"173+8W\"  \"235+9W\"  \"125+1W\"  \"274+2Q\"  \"274+5Q\" \n[15] \"751+9Q\"  \"274+1W\"  \"274+4W\"  \"274+5Q\"  \"751+6Q\" \n\n\nand try to write a regular expression that matches all product codes that start with 55 and end with T. To define the start, we can use the carat sign: “^55”. As you can see from the product codes, “55” is followed by other characters. Sometimes it 3 “e.g. ”-10” in another occasion it is 4 “+178”. To allows for this repetition of any sign, we will use \\.: any character and allow for one or more repetitions. The last part includes the “T”. In a regular expression, that is “T$”. With the regular expression, we can now extract the product codes:\n\ngrep(pattern = \"^55.+T$\", vec_char3, value = TRUE)\n\n[1] \"55-10T\"  \"55-48T\"  \"55+178T\"\n\nstringr::str_view(vec_char3, \"^55.+T$\", match = NA)\n\n [1] │ 25-78T\n [2] │ 25-98S\n [3] │ 45-97Q\n [4] │ 45-74Q\n [5] │ 45-72T\n [6] │ &lt;55-10T&gt;\n [7] │ &lt;55-48T&gt;\n [8] │ 55-69Q\n [9] │ &lt;55+178T&gt;\n[10] │ 173+8W\n[11] │ 235+9W\n[12] │ 125+1W\n[13] │ 274+2Q\n[14] │ 274+5Q\n[15] │ 751+9Q\n[16] │ 274+1W\n[17] │ 274+4W\n[18] │ 274+5Q\n[19] │ 751+6Q\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUsing the following vector, you will have to write regular expression to extract elements of that vector. You can use stringr::str_view(vec, pattern) to see if your regular expression is successful in matching the required outcome. In the folded code, this function is included to show the pattern matches. The folded code also assigns the patters to pat to use in the function calls.\n\nvec &lt;- c(\"+32 123 456789\", \"0032 123 456798\", \"+32 012345679\", \"rqx_47-87+5\", \"rqx_47-87+6\", \"rqx_47-86+5\", \"rpts_47-86+5\", \"usd 25\", \"eur 36\")\n\n\nusing grep() extract all location where you can find a cell phone number. This numbers starts with +32 or 0032 and is followed by 3 digit a space and 6 digits. Some people forget to include that second space and add 9 digits after 32. In vec, the first three elements are correct numbers, the others aren’t.\n\n\n\nCode\npat &lt;- \".+32\\\\s[0-9]{3}\\\\s?[0-9]{6}\"\nstringr::str_view(vec, pat, match = NA)\n\n\n[1] │ &lt;+32 123 456789&gt;\n[2] │ &lt;0032 123 456798&gt;\n[3] │ &lt;+32 012345679&gt;\n[4] │ rqx_47-87+5\n[5] │ rqx_47-87+6\n[6] │ rqx_47-86+5\n[7] │ rpts_47-86+5\n[8] │ usd 25\n[9] │ eur 36\n\n\nCode\ngrep(pat, vec, value = TRUE)\n\n\n[1] \"+32 123 456789\"  \"0032 123 456798\" \"+32 012345679\"  \n\n\n\nUse this pattern to extract these values from vec and store the result in vec_phone:\n\n\n\nCode\nvec_phone &lt;- vec[grepl(pat, vec)]\nvec_phone\n\n\n[1] \"+32 123 456789\"  \"0032 123 456798\" \"+32 012345679\"  \n\n\n\nextract all values from vec that include a currency. Write your code in such a way that “yen”, “gbp” and “sek” would also be extracted if included.\n\n\n\nCode\npat &lt;- \"usd|eur|yen|gdp|sec\"\nstringr::str_view(vec, pat, match = NA)\n\n\n[1] │ +32 123 456789\n[2] │ 0032 123 456798\n[3] │ +32 012345679\n[4] │ rqx_47-87+5\n[5] │ rqx_47-87+6\n[6] │ rqx_47-86+5\n[7] │ rpts_47-86+5\n[8] │ &lt;usd&gt; 25\n[9] │ &lt;eur&gt; 36\n\n\nCode\ngrep(pat, vec, value = TRUE)\n\n\n[1] \"usd 25\" \"eur 36\"\n\n\n\nuse this outcome to split the currency from the value. Start from the previous result and use the pipe operator in a {stringr} function and simplify the results:\n\n\n\nCode\ngrep(pat, vec, value = TRUE) |&gt; stringr::str_split(pattern = \" \", simplify = TRUE)\n\n\n     [,1]  [,2]\n[1,] \"usd\" \"25\"\n[2,] \"eur\" \"36\"\n\n\nHere, you have a matrix. You can extract the values using matrix subsetting operators. These will be introduced in this chapter.\n\nuse vec and extract all values that include include “47” after the initial letters and end with “+5”. Use str_extract_all() and simplify the results. Write your code using the pipe operator\n\n\n\nCode\npat &lt;- \"([a-z]+)?_47.+\\\\+5\"\nstringr::str_view(vec, pat, match = NA)\n\n\n[1] │ +32 123 456789\n[2] │ 0032 123 456798\n[3] │ +32 012345679\n[4] │ &lt;rqx_47-87+5&gt;\n[5] │ rqx_47-87+6\n[6] │ &lt;rqx_47-86+5&gt;\n[7] │ &lt;rpts_47-86+5&gt;\n[8] │ usd 25\n[9] │ eur 36\n\n\nCode\nvec |&gt; stringr::str_extract_all(pat, simplify = TRUE)\n\n\n      [,1]          \n [1,] \"\"            \n [2,] \"\"            \n [3,] \"\"            \n [4,] \"rqx_47-87+5\" \n [5,] \"\"            \n [6,] \"rqx_47-86+5\" \n [7,] \"rpts_47-86+5\"\n [8,] \"\"            \n [9,] \"\"            \n\n\nUsing the following paragraph from a reuters article\n\nreuters &lt;- \"The pound headed for its worst weekly performance against the euro in over two years on Friday, as a boost to European spending drove a broad rally in the single currency, while against the dollar, sterling rose ahead of U.S. jobs data. The euro has surged across the board this week, logging its best weekly performance against the dollar since March 2009. Against the pound, it was set for a weekly gain of 1.5%, the most since January 2023. It was last up 0.4% at 84.03 pence. The pound was up 0.4% against the dollar at $1.292.\"\n\n\nverify if this article includes references to “pound” or “sterling” (note that both could be with and without uppercase “P” or “S”). Use str_detect() to do so.\n\n\n\nCode\npat &lt;- \"pound|Pound|sterling|Sterling\"\nstringr::str_view(reuters, pat, match = NA)\n\n\n[1] │ The &lt;pound&gt; headed for its worst weekly performance against the euro in over two years on Friday, as a boost to European spending drove a broad rally in the single currency, while against the dollar, &lt;sterling&gt; rose ahead of U.S. jobs data. The euro has surged across the board this week, logging its best weekly performance against the dollar since March 2009. Against the &lt;pound&gt;, it was set for a weekly gain of 1.5%, the most since January 2023. It was last up 0.4% at 84.03 pence. The &lt;pound&gt; was up 0.4% against the dollar at $1.292.\n\n\nCode\nstringr::str_detect(reuters, pat)\n\n\n[1] TRUE\n\n\n\ndetermine the position of the occurrences of “pound” or “sterling” (including uppercase “P” or “S”)\n\n\n\nCode\nstringr::str_locate_all(reuters, pat)\n\n\n[[1]]\n     start end\n[1,]     5   9\n[2,]   199 206\n[3,]   371 375\n[4,]   485 489\n\n\n\nhow many times does the article refer to “pound” or “sterling” (incuding uppercase “P” or “S”)\n\n\n\nCode\nstringr::str_count(reuters, pat)\n\n\n[1] 4\n\n\n\nbreak this article in sentences:\n\n\n\nCode\nstringr::str_split(reuters, stringr::boundary(\"sentence\"))\n\n\n[[1]]\n[1] \"The pound headed for its worst weekly performance against the euro in over two years on Friday, as a boost to European spending drove a broad rally in the single currency, while against the dollar, sterling rose ahead of U.S. jobs data. \"\n[2] \"The euro has surged across the board this week, logging its best weekly performance against the dollar since March 2009. \"                                                                                                                    \n[3] \"Against the pound, it was set for a weekly gain of 1.5%, the most since January 2023. \"                                                                                                                                                       \n[4] \"It was last up 0.4% at 84.03 pence. \"                                                                                                                                                                                                         \n[5] \"The pound was up 0.4% against the dollar at $1.292.\"                                                                                                                                                                                          \n\n\n\nusing your previous code, extract the fourth sentence from the list:\n\n\n\nCode\nstringr::str_split(reuters, stringr::boundary(\"sentence\"))[[1]][4]\n\n\n[1] \"It was last up 0.4% at 84.03 pence. \"\n\n\n\nuse the pipe operator to: extract the fourth sentence from the text and split that sentence in words:\n\n\n\nCode\nstringr::str_split(reuters, stringr::boundary(\"sentence\"))[[1]][4] |&gt;\n  stringr::str_split(stringr::boundary(\"word\"))\n\n\n[[1]]\n[1] \"It\"    \"was\"   \"last\"  \"up\"    \"0.4\"   \"at\"    \"84.03\" \"pence\"\n\n\nCreate a character variable with “var_1”, … “var_5” that you would use to add names to a vector. Save this vector in vec_names.\n\n\nCode\nvec_names &lt;- paste(\"var\", 1:5, sep = \"_\")\nvec_names\n\n\n[1] \"var_1\" \"var_2\" \"var_3\" \"var_4\" \"var_5\"\n\n\nWhat would happen is you use collapse = \"_\" and not sep = \"_\"?\n\n\nCode\nvec_namesc &lt;- paste(\"var\", 1:5, collapse = \"_\")\nvec_namesc\n\n\n[1] \"var 1_var 2_var 3_var 4_var 5\"\n\n\n\n\n\n\n\n4.1.10 Factors\n\n4.1.10.1 Definition\nFactors are a special vector and are used to represent categorical variables. Categorical variables can take a limited number of known values (often referred to as levels). Examples of categorical variables include nominal variables and ordinal variables. The first, nominal variables, have two or more categories but these have no intrinsic ordering. In other words, you can not take one value of a nominal variables and say that it is higher, lower, bigger, smaller … than another value. Hair color, the name of a city, country or continent, a yes/no reply in a questionnaire or the name of a month are examples of nominal variables. You can order them alphabetically, or, for months, as they appear in a year, but any other ordering wouldn’t affect they way you handle them. In other words, if you would recode city names as numeric variables (1 = Amsterdam, 2 = Brussels, 3 = Copenhagen, …) these numeric values wouldn’t have any meaning. Ordinal variables differ from nominal variables as they have an intrinsic ordering. Examples include e.g. educational experience (elementary school, high school, some college, bachelor’s degree, master’s degree, PhD) or price categories measured as “budget” or “premium”. If you would recode these variables as numeric variables, their level would matter. For instance, you would recode “elementary school” as “1”, “high school” as “2”, “some college” as “3”, “bachelor’s degree” as “4”, “master’s degree” as “5” and “PhD” as “6” or, for price categories, “budget” as “1” and “premium” as “2”. However, these categories are not equally spaced. In other words, the difference between the numeric values for “high school” and “elementary school” (2 - 1 = 1) isn’t the same as the difference between “PhD” and “master’s degree” (6 - 5 = 1). In other words, the categories are not equally spaced.\n\n\n4.1.10.2 Creating a factor\nIn addition to base R factor function, {forcats} - a package included in the {tidyverse} - includes a lot of functions to manipulate factor variables. As we did with {stringr} and {lubridate} function, I’ll include forcats:: at the start of a function if that function is part of that package. If forcats:: is not part of the function call, the function is a base R function. Recall that all {stringr} functions start with str_. In a similar way, all {forcats} function start with fct_ and (most) are follewed by a verb.\nSuppose that you have a variable that records months:\n\nvec_month1 &lt;- c(\"Sep\", \"Aug\", \"Oct\", \"Jan\", \"Nov\", \"Mar\", \"Dec\", \"Apr\", \"Jun\", \"May\", \"Feb\", \"Jul\" )\n\nRecall from Chapter 2, that these months don’t sort in a meaningful way:\n\nsort(vec_month1)\n\n [1] \"Apr\" \"Aug\" \"Dec\" \"Feb\" \"Jan\" \"Jul\" \"Jun\" \"Mar\" \"May\" \"Nov\" \"Oct\" \"Sep\"\n\n\nTo fix this, we can create a factor and include a vector of valid levels. These levels are ordered in a meaningful. For instance:\n\nvec_month_levels &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nWe can not encode vec_month1 as a factor, including the levels using factor(x = , levels, labels = levels, exclude = NA, ordered = is.ordered(x),  nmax = NA). Here, x is the vector with the data you want to encode as a factor. The levels are optional, and are included in a vector of unique values that x might take and are included as character. R assumes by default that his vector with levels is sorted in increasing order of x. If these levels are not included, R uses sort(unique(x)) to set levels. The labels = levels allows you to add labels to the levels. These labels allow you to include more descriptive term for every level. This is especially useful if the levels are recorded as numeric. By default, R sets these labels equal to the levels. You can exclude some values. In the case, you include a vector with the values to exclude after exclude =. By default, all unique values in x are treated as a separate factor. For instance, if your data in x includes missing values, exclude = NULL will treat these missing values as a separate level. By default that level is the last level. is.ordered(x) is by default FALSE. If that is set to TRUE, R will treat the factors as ordinal variables. The last argument allows you to restrict the number of factors if x includes a lot of unique values.\nLet’s see what these options do. First, let’s accept all default values:\n\nvec_fac1 &lt;- factor(x = vec_month1)\nvec_fac1\n\n [1] Sep Aug Oct Jan Nov Mar Dec Apr Jun May Feb Jul\nLevels: Apr Aug Dec Feb Jan Jul Jun Mar May Nov Oct Sep\n\n\nThe output shows vec_month1 first and all levels next. As the command didn’t include levels, R ordered the levels using sort(unique()):\n\nsort(unique(vec_month1))\n\n [1] \"Apr\" \"Aug\" \"Dec\" \"Feb\" \"Jan\" \"Jul\" \"Jun\" \"Mar\" \"May\" \"Nov\" \"Oct\" \"Sep\"\n\n\nIf you add levels, R will change the order and follow the order in the levels argument.\n\nvec_fac1 &lt;- factor(x = vec_month1, levels = vec_month_levels)\nvec_fac1\n\n [1] Sep Aug Oct Jan Nov Mar Dec Apr Jun May Feb Jul\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nNow, the months are ordered as they were in vec_month_levels. Note the levels that do not occur in the x are dropped.\nAdding labels allows you to add more descriptive terms. For the months, these labels could be the months written in full:\n\nvec_month_labels &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\")\n\nUsing these labels:\n\nvec_fac1 &lt;- factor(x = vec_month1, levels = vec_month_levels, labels = vec_month_labels)\nvec_fac1\n\n [1] September August    October   January   November  March     December \n [8] April     June      May       February  July     \n12 Levels: January February March April May June July August ... December\n\n\nAll months are now written in full. Note that using these labels to set the levels is not possible. R searches tries to match every value in the vector it has to encode as factor with a level in the levels vector. In other words, if R encounters “Jan” in the vector it had to encode, it searches for “Jan” in the levels vector. If that value is missing as a level, is will report NA in the vector it had to encode. Here R silently converts any values in the vector to encode that it doesn’t find in the levels vector into NA.\nThe {forcats}’s fct(x, levels = NULL, na = character()) function allows to create a vector, but avoids that missing values are silently encoded as NA. The first argument is the vector to encode as factor, the second the vector used for levels (NULL or none by default) and the third optional argument allows you to include the values in x that fct() should treat as NA. Let’s first show how you can use this function:\n\nvec_fac2 &lt;- forcats::fct(x = c(\"Apr\", \"Feb\", \"Jan\"), levels = c(\"Jan\", \"Feb\", \"Apr\"))\nvec_fac2\n\n[1] Apr Feb Jan\nLevels: Jan Feb Apr\n\n\nNew let’s add a typo and write “Apr” as “Arp”.\n\nvec_fac3 &lt;- forcats::fct(x = c(\"Arp\", \"Feb\", \"Jan\"), levels = c(\"Jan\", \"Feb\", \"Apr\"))\n\nError in `forcats::fct()`:\n! All values of `x` must appear in `levels` or `na`\nℹ Missing level: \"Arp\"\n\nvec_fac3\n\nError: object 'vec_fac3' not found\n\n\nHere, R produces an error: ! All values of x must appear in levels or na. Missing level: \"Arp\". As you can see from the error, I warns that a value in the x vector was not included in the levels vector. If also shows its value: “Arp”. Using base R’s factor would add an NA without warning:\n\nvec_fac3 &lt;- factor(c(\"Arp\", \"Feb\", \"Jan\"), levels = c(\"Jan\", \"Feb\", \"Apr\"))\nvec_fac3\n\n[1] &lt;NA&gt; Feb  Jan \nLevels: Jan Feb Apr\n\n\nHere, base R changes “Arp” into “NA”. If the typo is undetected, which is likely in large datasets, this would affect your analysis. In {forcats} you need to include “Arp” in the na = argument if you want to avoid an error. In other words, you have to instruct R to treat “Arp” as a missing value:\n\nvec_fac3 &lt;- forcats::fct(x = c(\"Arp\", \"Feb\", \"Jan\"), levels = c(\"Jan\", \"Feb\", \"Apr\"), na = c(\"Arp\"))\nvec_fac3\n\n[1] &lt;NA&gt; Feb  Jan \nLevels: Jan Feb Apr\n\n\nYou can see from the output that “Arp” is now indeed recored as a missing value. This is how you instructed R to treat “Arp”. There is a second difference between both functions. Base R’s factor() orders using sort(unique(x)) in case the levels argument is missing. {forcats} fct() orders by first appearance. In other words, it uses the character vector to encode an including an implicit order.\nFactors can include numeric values. Suppose you have a yes/no reply to an answer where “No” recorded as 0 and “Yes” as 1. You could encode that vector as a factor using:\n\nvec_fac2 &lt;- factor(x = c(1, 1, 1, 0, 0, 1, 0), levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nvec_fac2\n\n[1] Yes Yes Yes No  No  Yes No \nLevels: No Yes\n\n\nNote that {forcats} needs a character vector to encode as factor. Including a numeric factor causes an error.\n\nforcats::fct(x = c(1, 1, 1, 0, 0, 1, 0), levels = c(0, 1))\n\nError in `forcats::fct()`:\n! `x` must be a character vector, not a double vector.\n\n\nTo created an ordered factor, you need to change ordered = is.ordered(x) to ordered = TRUE. Doing so creates an ordinal factor. Suppose you have income levels from a survey recorded as low = 1, medium = 2 and high = 3. Creating an ordered factor:\n\nvec_ord1 &lt;- factor(c(1, 2, 3, 3, 1, 1, 2, 2, 1), levels = c(1, 2, 3), labels = c(\"Low income\", \"Medium income\", \"High income\"), ordered = TRUE)\nvec_ord1\n\n[1] Low income    Medium income High income   High income   Low income   \n[6] Low income    Medium income Medium income Low income   \nLevels: Low income &lt; Medium income &lt; High income\n\n\nHere you see that the output shows the levels as well as their ordering: low income is lower than medium and medium income is lower than high income.\nYou can check if a vector is a factor using is.factor() and if it is an ordered factor using is.ordered().\n\nis.factor(vec_ord1)\n\n[1] TRUE\n\nis.ordered(vec_ord1)\n\n[1] TRUE\n\n\nYou can coerce a vector into a factor or ordered factor using as.factor() or as.ordered(). Using c(1, 2, 1, 3, 2, 1) as an example\n\nas.factor(c(1, 2, 1, 3, 2, 1))\n\n[1] 1 2 1 3 2 1\nLevels: 1 2 3\n\nas.ordered(c(1, 2, 1, 3, 2, 1))\n\n[1] 1 2 1 3 2 1\nLevels: 1 &lt; 2 &lt; 3\n\n\nyou can see that both functions create a factor. The ordered factor is created from sort(unique(x)). In other words, as.ordered() assumes that the values in the vector to encode as factor are listed in the correct order.\n\n\n4.1.10.3 Usefull factor functions\nIn plots, it is often useful to reorder factor levels. For instance, if you would plot the population of a city where cities are encoded as factors and are alphabetically ordered, that plot would show these cities on the horizontal or vertical axis in that order. To produce a nice plot, it might be more convenient to have these cities ordered in terms of their population. In that way, the smallest city would show up on the left of the horizontal axis and the largest city on the right. To do so, you can use {forcats}’ fct_reorder() function. This function’s first argument is the factor to reorder. The second argument is the variable that R needs to use to reorder. The third argument, fun = median shows the summary function R uses to reorder. For each factor level, R calculates the value of the function in fun and uses this value to reorder the factors. Using the default na_rm = NULL R removes missing values with a warning. Changing that into TRUE will cause R to remove them without a warning and FALSE preserves the NA’s. By default, R orders descending. Adding desc = FALSE changes this default.\n{forcats} fct_recode() and fct_collapse() allow to modify the factor levels. fct_recode() allows you to recode factor levels. To do so, you need to use fct_recode(x, \"new value\" = \"old value\") where x is the factor and the statement “new value” = “old value” is entered every old factor level that you need to change. If an “old value” is not included, R assumes that it remains as is. To illustrate, we first create a factor:\n\nvec_fac1 &lt;- factor(x = c(1, 2, 3, 4), levels = c(1, 2, 3, 4), labels = c(\"small city\", \"large city\", \"small town\", \"large town\"))\nvec_fac1\n\n[1] small city large city small town large town\nLevels: small city large city small town large town\n\n\nLet’s now recode to show levels “city, small”, “city, large”, “town, small” and “town, large”:\n\nforcats::fct_recode(vec_fac1, \n                    \"city, small\" = \"small city\",\n                    \"city, large\" = \"large city\", \n                    \"town, small\" = \"small town\", \n                    \"town, large\" = \"large town\")\n\n[1] city, small city, large town, small town, large\nLevels: city, small city, large town, small town, large\n\n\nNote that you can use this function to reduce the number of levels. For instance, if you want to drop the difference between “small” and “large” and only keep “city” and “town”, you can recode all “small city” and “large city” to “city”:\n\nforcats::fct_recode(vec_fac1, \n                    \"city\" = \"small city\",\n                    \"city\" = \"large city\", \n                    \"town\" = \"small town\", \n                    \"town\" = \"large town\")\n\n[1] city city town town\nLevels: city town\n\n\nNote that here you will loose these 4 factor levels if you recode and assign to the same factor.\n{forcats}’ fct_collapse() function performs a similar task. It allows you to collaps various factor levels. The function’s argument is similar to those for recode. For instance, suppose you want to recode “small” and “large” in one level and you would use fct_collapse():\n\nforcats::fct_collapse(vec_fac1, \n                    \"city\" = c(\"small city\", \"large city\"), \n                    \"town\" = c(\"small town\", \"large town\"))\n\n[1] city city town town\nLevels: city town\n\n\nHere, you use city\" = c(\"small city\", \"large city\") to collapse the levels on the right hand side of the equality sign into the level on the left hand side.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nSuppose that you have a variable with the values “Asi”, “Afr”, “Eur”, “Ame”, “Oce”. These values stand for “Asia”, “Africa”, “Europe”, “Americas” and “Oceania”. Create a factor that will show these contintents in alfabetical order and add labels. Use cont to store this variable.\n\n\nCode\ncont &lt;- factor(c(\"Asi\", \"Afr\", \"Eur\", \"Ame\", \"Oce\"), levels= c(\"Afr\", \"Ame\", \"Asi\", \"Eur\", \"Oce\"), labels = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\", \"Oceania\"))\ncont\n\n\n[1] Asia     Africa   Europe   Americas Oceania \nLevels: Africa Americas Asia Europe Oceania\n\n\nIs cont a factor?\n\n\nCode\nis.ordered(cont)\n\n\n[1] FALSE\n\n\nIs cont an ordered factor?\n\n\nCode\nis.ordered(cont)\n\n\n[1] FALSE\n\n\nTo measure an individual’s education, the following values are used in your dataset: “some high school”, “high school”, “some college”, “bachelor”, “master”, “PhD”. These values are including using numbers: 1 (some high school), 2 (high school), … 6 (PhD). var_school shows such a variable.\n\nvar_school &lt;- sample(1:6, 20, replace = TRUE)\n\nCreated an ordered factor including labels. Assign this factor to school.\n\n\nCode\nschool &lt;- factor(var_school, levels= c(1, 2, 3, 4, 5, 6), labels = c(\"some high school\", \"high school\", \"some college\", \"bachelor\", \"master\", \"PhD\"), ordered = TRUE)\nschool\n\n\n [1] some college     bachelor         bachelor         PhD             \n [5] some high school bachelor         master           master          \n [9] master           PhD              some high school master          \n[13] PhD              high school      master           some college    \n[17] master           some high school PhD              master          \n6 Levels: some high school &lt; high school &lt; some college &lt; ... &lt; PhD\n\n\nUse {forcats} to recode school and reduce the number of levels by merging “high school” and “some college” into “secondary” and merging “bachelor” and “master” into “tertiary”. There are two ways to do this:\n\nOption 1:\n\n\n\nCode\nforcats::fct_recode(school, \n                    \"secondary\" = \"high school\", \n                    \"secondary\" = \"some college\", \n                    \"tertiary\" = \"bachelor\", \n                    \"tertiary\" = \"master\")\n\n\n [1] secondary        tertiary         tertiary         PhD             \n [5] some high school tertiary         tertiary         tertiary        \n [9] tertiary         PhD              some high school tertiary        \n[13] PhD              secondary        tertiary         secondary       \n[17] tertiary         some high school PhD              tertiary        \nLevels: some high school &lt; secondary &lt; tertiary &lt; PhD\n\n\n\nOption 2:\n\n\n\nCode\nforcats::fct_collapse(school, \n                    \"secondary\" = c(\"high school\", \"some college\"),\n                    \"tertiary\" = c(\"bachelor\", \"master\")) \n\n\n [1] secondary        tertiary         tertiary         PhD             \n [5] some high school tertiary         tertiary         tertiary        \n [9] tertiary         PhD              some high school tertiary        \n[13] PhD              secondary        tertiary         secondary       \n[17] tertiary         some high school PhD              tertiary        \nLevels: some high school &lt; secondary &lt; tertiary &lt; PhD",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#matrices",
    "href": "04_Data_structures.html#matrices",
    "title": "4  Data structures",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\nMatrices are two-dimensional object that allow to store data in rows and columns. Recall that vectors stored data in one row and one or more columns. Like vectors, matrices are homogeneous. In other words, they store numeric or character or boolean or data/time values but not a combination of two or more datatypes. Most of what we discusses for vectors also applies to matrices. As a matter of fact, you can think of a vector as a special case of a matrix: it is a matrix with one row and one or more columns. However, if you want to use the vector as a matrix, you need to create a matrix with 1 row and n columns.\nAn “mxn” matrix has m rows and n columns. In general, the value on the ith row and jth column is referred to as matrix-name(i,j). We’ll see in the next section how you subset a matrix. A matrix with the same number of rows as there are columns, i.e. an nxn matrix is also called a square matrix. Here we will focus on numeric matrices. However, as long as all elements in a matrix are the same, a matric would also include characters, logical values, integers or data/time variables. As you will see here, most of what we learned for vectors also applies to matrices.\n\n4.2.1 Creating a matrix\nWe will first show how to create a matrix in general. We then move to a couple of special matrices.\n\n4.2.1.1 The basics\nTo create a matrix, you use the matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE, dimnames = NULL) function. The first argument is optional and allows you to add a vector with data to fill the matrix. The second and third argument, ncol = 1 and nrow = 1 determine the size of the matrix: the desired number of rows and columns. If you add a vector that R needs to use to fill the data, byrow = FALSE instructs R to fill the matrix by column. In other words, if you have 4 rows and 5 columns, R first fills all rows of the first column, the all rows of the second, … to end with all 4 rows of the 5th column. Changing this default into TRUE tells R to first fill the rows. In other words, R will now first fill the 5 columns of the first row, then move to the second row and fill all columns in that row, … . The last argument allow you to add a name to the row and column dimensions. Let’s use a 2x3 matrix mat_0:\n\nmat_0 &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\nmat_0\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nAs you can see, R creates a matrix with two rows [1, ] and [2, ] and three columns [,1], [,2], [,3]. R added the values in c() by column (default: byrow = FALSE). It took the first two values in c() (1 and 2) and used these to fill the first column. The next two values, 3 and 4, were added to the second column. The last two values in c() are shown in the last column.\nThe attributes of mat_0 include the dimenions of the matrix: the number of rows and the number of columns:\n\nattributes(mat_0)\n\n$dim\n[1] 2 3\n\n\nTo create mat_0 we included its elements via c(). The argument data can include vectors or function. For instance, let’s use a 1x6 vector vec_1 to illustrate this. We’ll fill vec_1 with a sequence of 1 to 6 using the shorthand for seq(from = 1, to = 6, by = 1):\n\nvec_1 &lt;- 1:6\n\nWe can know create a matrix mat_1 (accepting the default values for byrow and dimnames):\n\nmat_1 &lt;- matrix(vec_1, nrow = 2, ncol = 3)\nmat_1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nAs you can see, R filled this matrix in the same way as R did for mat_0. Note that we didn’t have to create a separate vector vec_1. We could have included 1:6 as the first argument.\nIn both examples, the length of c() or vec_1 was equal to the number of cells in the matrix: the number of cells equals nrow * ncol = 6 and the length of vec_1 or c() was also 6. If that is not the case, R reports an warning. If the number of cells in the matrix is larger than the length of the vector, R will use some of all values more than once. Suppose that you have a vector with 4 columns that needs to fill a matrix with 2 rows and 3 columns:\n\nmatrix(1:4, nrow = 2, ncol = 3)\n\nWarning in matrix(1:4, nrow = 2, ncol = 3): data length [4] is not a\nsub-multiple or multiple of the number of columns [3]\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    1\n[2,]    2    4    2\n\n\nR will use the first two observations of the vector two times. After having used all 4 columns of 1:4 to fill the first two columns, R uses the same vector again to fill the other cells. In this example, R used 1 and 2 of the sequence 1:4 twice. Note that R shows a warning that the dimensions of the vector and matrix didn’t fit.\nIf the length of the vector is longer than the number of cells in the matrix,\n\nmatrix(1:9, 2, 3)\n\nWarning in matrix(1:9, 2, 3): data length [9] is not a sub-multiple or multiple\nof the number of rows [2]\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nR uses only the first nrow * ncol columns of the vector. Here R used the first 6 columns of the vector 1:9 to fill the matrix, and dropped the others. Again, R shows a warning message that the dimensions didn’t fit.\nYou can also create a matrix using the dim() function to a vector. The next examples shows how this works:\n\nvec_1 &lt;- 1:6\ndim(vec_1) = c(2, 3)\nvec_1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nYou can use all others functions that generate a vector to fill a matrix. Examples include:\n\na matrix with random numbers:\n\n\nmatrix(rnorm(6), nrow = 2, ncol = 3)\n\n           [,1]      [,2]       [,3]\n[1,] 0.02905913  1.288859 -0.1529046\n[2,] 0.46637676 -0.674857  0.2705640\n\n\n\nusing letters:\n\n\nmatrix(letters[1:6], nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"c\"  \"e\" \n[2,] \"b\"  \"d\"  \"f\" \n\n\n\nas a sample:\n\n\nmatrix(sample(1:1000, 6), nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]  817  426  412\n[2,]  573  530  550\n\n\n\nusing a set operator on two vectors generated as 1:12 and 7:18:\n\n\nmatrix(base::intersect(1:12, 7:18), nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n\nLet’s now see what byrow = TRUE changes to the outcome of matrix():\n\nmat_1 &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE)\nmat_1\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n\nRecall that with the default for byrow, R first filled the first column, then the second and then the third. As you can see from the output now, R now filled the first row first putting the first value of vec_1 in the first column, the second value in the second and the third value in the third column. As all columns in the first row were filled, R moved to the second row and used the fourth value of vec_1 to fill the first column on the second row, the fifth value to fill the second column on the second row and the sixth value in the third column on the second row.\nThe last option allows you to specify row and column names. To do so, you need to collect them in a list. We’ll see shortly that lists are yet another data structure in R. Note that there are other ways to set column and row names.\n\nmat_1 &lt;- matrix(vec_1, nrow = 2, ncol = 3, byrow = TRUE, dimnames = list(c(\"row1\", \"row2\"), c(\"var1\", \"var2\", \"var3\")))\nmat_1\n\n     var1 var2 var3\nrow1    1    2    3\nrow2    4    5    6\n\n\nThe output now shows the row and column names. These names are added to the attributes of the matrix as dimnames[[1]] for the rows and dimnames[[2]] for the columns:\n\nattributes(mat_1)\n\n$dim\n[1] 2 3\n\n$dimnames\n$dimnames[[1]]\n[1] \"row1\" \"row2\"\n\n$dimnames[[2]]\n[1] \"var1\" \"var2\" \"var3\"\n\n\nAs alternative to add row and column names are the functions rownames() and colnames(). Let’s first recreate mat_1 without names:\n\nmat_1 &lt;- matrix(1:6, nrow = 2, ncol = 3)\n\nYou can use colnames() in two ways. The argument of this function is a vector with column names. Suppose you want to add the following names to the columns of mat_1: c(\"var1\", \"var2\", \"var3\"). The first way to do so is to use\n\ncolnames(mat_1) &lt;- c(\"var1\", \"var2\", \"var3\")\nmat_1\n\n     var1 var2 var3\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nTo add rownames, you can use rownames(). This function requires a vector with the names: c(\"row1\", \"row2\"). To add these names:\n\nrownames(mat_1) &lt;- c(\"row1\", \"row2\")\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nIf you only want to use names for column and rows that include a prefix and a row or column number, e.g. “col1” or “row1 then there is a shortcut where you don’t have to type all row or column names. Using colnames(x, do.NULL = TRUE, prefix = \"col\") you can specify the matrix in x and the prefix in prefix = \"col\". The argument do.NULL is by default TRUE. This default do.NULL = TRUE adds no names. Changing that into FALSE tells R to add names.\n\ncolnames(mat_1) &lt;- colnames(mat_1, do.NULL = FALSE, prefix = \"var_\")\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nYou can do the same for the rows and add names using a prefix, e.g. “obs” and the row number:\n\nrownames(mat_1) &lt;- rownames(mat_1, do.NULL = FALSE, prefix = \"obs_\")\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nNote that there are many ways you can use the character functions to automate the process of naming rows and columns. As an illustration, let’s rewrite\n\nrownames(mat_1) &lt;- c(\"row1\", \"row2\")\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nusing the paste0() function:\n\nrownames(mat_1) &lt;- paste0(\"row\", 1:2)\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nand\n\ncolnames(mat_1) &lt;- c(\"var1\", \"var2\", \"var3\")\nmat_1\n\n     var1 var2 var3\nrow1    1    3    5\nrow2    2    4    6\n\n\nusing the paste() function:\n\ncolnames(mat_1) &lt;- paste(1:3, c(\"st\", \"nd\", \"rd\"), sep=\"\")\nmat_1\n\n     1st 2nd 3rd\nrow1   1   3   5\nrow2   2   4   6\n\n\nIf your matrix has column or row names, you can show these using the same colnames() or rownames() function. For instance:\n\ncolnames(mat_1)\n\n[1] \"1st\" \"2nd\" \"3rd\"\n\nrownames(mat_1)\n\n[1] \"row1\" \"row2\"\n\n\n\n\n4.2.1.2 Special matrices\nRecall that vectors are data structures with 1 row and one or more columns. If you need to work with matrix algebra and use vectors, it is best to create a vector explicitly as a matrix. To do so, you need a matrix with 1 row and e.g. 3 columns:\n\nmat_vec &lt;- matrix(1:3, 1, 3)\n\nThere are a couple of special matrices. Using the matrix function, we can create a mxn matrix with one constant value.\n\nmat_2 &lt;- matrix(5, nrow = 2, ncol = 3)\nmat_2\n\n     [,1] [,2] [,3]\n[1,]    5    5    5\n[2,]    5    5    5\n\n\nHere there are two special cases: a square matrix filled with ones;\n\nJ &lt;- matrix(1, nrow = 3, ncol = 3)\nJ\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n\nand the zero matrix: a mxn matrix with zero’s:\n\nzeros &lt;- matrix(0, nrow = 2, ncol = 3)\nzeros\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n\nA diagonal matrix is a square matrix where all values are equal to zero except those on the diagonal:\n\ndiag(c(10, 11, 12), nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]   10    0    0\n[2,]    0   11    0\n[3,]    0    0   12\n\n\nA special case of this diagonal matrix is the identity matrix: a diagonal matrix whose diagonal elements are equal to 1:\n\nident &lt;- diag(1, nrow = 3, ncol = 3)\nident\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\nThe last special case, is a vector (a 1xn matrix), whose elements are all 1:\n\nvec_ones &lt;- matrix(1, 1, 3)\nvec_ones\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n\n\nTriangular matrices are square matrices where all elements below the diagonal are 0 (upper triangular) or all elements above the diagonal are 0 (lower triangular). This, in addition to these elements, the elements on the diagonal are also 0, the square matrix is strict triangular. The functions upper.tri(x, diag = FALSE) and lower.tri(x, diag = FALSE) can be used to create those matrices. These function return a logical matrix whose elements are TRUE if it above the diagonal (upper, with diag = FALSE) and FALSE is this is not the case. With diag = TRUE, the logical values on the diagonal will also be TRUE. The interpretation for the lower triangular function are identical, with the exception that TRUE in this case is for elements below or below and on the diagonal. To see how these function work, we’ll use:\n\nmat_1 &lt;- matrix(1:25, 5, 5)\n\nUsing upper.tri() as an example to show the logical matrix:\n\nupper.tri(mat_1, diag = FALSE)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] FALSE  TRUE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE  TRUE\n[4,] FALSE FALSE FALSE FALSE  TRUE\n[5,] FALSE FALSE FALSE FALSE FALSE\n\n\nWe can now use this logical matrix to change mat_1 into a lower triangular matrix whose elements on the diagonal differ from 0:\n\nmat_1[upper.tri(mat_1, diag = FALSE)] &lt;- 0\nmat_1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    2    7    0    0    0\n[3,]    3    8   13    0    0\n[4,]    4    9   14   19    0\n[5,]    5   10   15   20   25\n\n\nNote that here, we use the function upper.tri() to create a lower triangular matrix. To create a strict lower diagonal matrix, you can change the default value diag = FALSE in diag = TRUE. Doing so allows you to create a strict lower triangular matrix:\n\nmat_1 &lt;- matrix(1:25, 5, 5)\nmat_1[upper.tri(mat_1, diag = TRUE)] &lt;- 0\nmat_1\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    2    0    0    0    0\n[3,]    3    8    0    0    0\n[4,]    4    9   14    0    0\n[5,]    5   10   15   20    0\n\n\n\n\n4.2.1.3 Coercing objects to matrix class\nFor R, a vector is not a matrix. You can see that if you ask R what class vec_1 is and compare that result with the class of mat_1:\n\nclass(vec_1)\n\n[1] \"matrix\" \"array\" \n\nclass(mat_1)\n\n[1] \"matrix\" \"array\" \n\n\nThe as.matrix(x) function tries to turn the object x into a matrix. Doing so, as.matrix() keeps the dimensions of x. In other words, it will change the vector vec_1 into a matrix with 6 rows and 1 column.\n\nmat_1 &lt;- as.matrix(vec_1)\nmat_1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nYou can change a data frame into a matrix in a similar way. Recall from Chapter 1 that R includes a dataset mtcars. A data frame is a data structure we will discuss shortly:\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\n\nRecall that this data frame had 32 observations for 11 variables. This data frame includes variable names and identifies every observation:\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWith as.matrix() you can change this data frame into a matrix:\n\nmat_mtcars &lt;- as.matrix(mtcars)\n\nThis matrix has column and row names.\n\ncolnames(mat_mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nrownames(mat_mtcars)\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nRecall that a matrix is homogeneous: all elements must be of the same type. Often data frames are heterogeneous: they include numeric, character, data/time, boolean of factor variables. Using the data.matrix() function, R will change this data frame in a numeric matrix by converting all variables to numeric first. For instance, suppose that you have a data.frame df:\n\ndf &lt;- data.frame(A = 1:3, B = letters[1:3], C = seq.Date(as.Date(\"2025-03-25\"), by = \"day\", length.out = 3))\ndf\n\n  A B          C\n1 1 a 2025-03-25\n2 2 b 2025-03-26\n3 3 c 2025-03-27\n\n\nIf you would use as.matrix() R would convert this data frame into a character matrix:\n\nmat_df &lt;- as.matrix(df)\ntypeof(mat_df)\n\n[1] \"character\"\n\n\nUsing data.matrix() avoids this:\n\nmat_df &lt;- data.matrix(df)\nmat_df\n\n     A B     C\n[1,] 1 1 20172\n[2,] 2 2 20173\n[3,] 3 3 20174\n\n\nAs you can see, the second column, B, has been changed to numeric. R changed the value of “a” into 1, “b” into 2, … . Here, all unique values are given a different numeric value. In addition, R used the fact that dates are numeric to change the date into numeric.\n\n\n\n4.2.2 Matrix type and attributes\n\n4.2.2.1 Matrix type\nHere we first created the matrix using matrix(). However, it is also possible that you create a matrix witin your code. To check is an object is a matrix, you can use the is.matrix() function.\n\nis.matrix(mat_1)\n\n[1] TRUE\n\n\nIf an object is a matrix, this function show TRUE. If this is not the case, the function shows FALSE. If an object is a matrix, you can check its type using typeof():\n\ntypeof(mat_1)\n\n[1] \"integer\"\n\n\nHere, mat_1 is an integer matrix. In other words, its values are of type “integer”. Recall that this matrix was creates from the sequence 1:6. In other words, it was created as an integer value. The type of a matrix is determined is a similar way as a vector. To illustrate, let’s define 3 matrices\n\nmat_n &lt;- matrix(rnorm(6), 2, 3)\nmat_c &lt;- matrix(letters[1:6], 2, 3)\nmat_d &lt;- matrix(seq(as.Date(\"2025-03-25\"), by = \"day\", length.out = 6), 2, 3)\n\nand check there type:\n\ntypeof(mat_n)\n\n[1] \"double\"\n\ntypeof(mat_c)\n\n[1] \"character\"\n\ntypeof(mat_d)\n\n[1] \"double\"\n\n\nAs you can see, R stores the dates in mat_d as numeric variables with class “Matrix” “Array”. Recall that dates are stored as numbers. In other words you can turn these numbers into dates using one of the function we have covered, e.g. as.Date() or {lubridate}’s ymd().\nIt is important to stress that matrices are homogenious in the sense that they can only include values of one type. For instance, the following line changes the value of the element on the second row and second column in mat_d in a character variable.\n\nmat_n[2,2] &lt;- \"10000\"\nmat_n\n\n     [,1]                [,2]               [,3]               \n[1,] \"0.982499218575708\" \"0.42216077988595\" \"0.22435607224835\" \n[2,] \"-1.01112994823057\" \"10000\"            \"0.125027643014595\"\n\n\nThe output suggest that R coerced all other elements into character variables. Indeed, the type of this matrix is\n\ntypeof(mat_n)\n\n[1] \"character\"\n\n\nnow a character matrix. We have seen similar behavior with vectors.\n\n\n4.2.2.2 Matrix attributes\nA matrix has some attributes that you will often use in code. To check the attributes of an object, you can use R’s attributes() function. To illustrate this function, we’ll use\n\nmat_1 &lt;- matrix(vec_1, nrow = 2, ncol = 3, byrow = TRUE, dimnames = list(c(\"row1\", \"row2\"), c(\"var1\", \"var2\", \"var3\")))\nmat_1\n\n     var1 var2 var3\nrow1    1    2    3\nrow2    4    5    6\n\n\nThe function shows which the attributes for the object:\n\natt_mat1 &lt;- attributes(mat_1)\natt_mat1\n\n$dim\n[1] 2 3\n\n$dimnames\n$dimnames[[1]]\n[1] \"row1\" \"row2\"\n\n$dimnames[[2]]\n[1] \"var1\" \"var2\" \"var3\"\n\n\nHere, you see the various attributes: the dimension $dim, the row names dimnames[1] and colum names dimnames[2]. There are multiple ways to access these attributes. For instance, the dimension of the matrix includes the number of rows (2) and the number of columns (3). To extract the number these dimensions you can use dim(). This function shows the number of rows and column in mat_1.\n\ndim(mat_1)\n\n[1] 2 3\n\n\nYou can use these to extract the number of rows and columns. To do so, you assign the result of this function to an object.\n\ndim_mat1 &lt;- dim(mat_1)\ndim_mat1\n\n[1] 2 3\n\n\nYou can now subset this result:\n\nnobs &lt;- dim_mat1[1]\nnvar &lt;- dim_mat1[2]\n\nThese values now store the number of rows (nobs) and the number of columns (nvar).\nIf you are only interested in the number of rows or number of columns, you can extract these using nrow() or ncol():\n\nnrow(mat_1)\n\n[1] 2\n\nncol(mat_1)\n\n[1] 3\n\n\nNote that in many cases, the number of rows will be equal to the number of observations in your dataset while the number of column is equal to the number of variables.\nTo see the total number of values in the matrix, or the product of the number of rows and the number of columns, you can use length():\n\nlength(mat_1) \n\n[1] 6\n\nnrow(mat_1) * ncol(mat_1)\n\n[1] 6\n\n\nIf you need the column or row names, you can use colnames() or rownames():\n\ncolnames(mat_1)\n\n[1] \"var1\" \"var2\" \"var3\"\n\nrownames(mat_1)\n\n[1] \"row1\" \"row2\"\n\n\nIf you store these names, you can use them in your code.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate a 3x3 matrix, mat_0 using a sequence from 21:29\n\n\nCode\nmat_0 &lt;- matrix(21:29, 3, 3)\nmat_0\n\n\n     [,1] [,2] [,3]\n[1,]   21   24   27\n[2,]   22   25   28\n[3,]   23   26   29\n\n\nUsing the same values, fill this matrix by row:\n\n\nCode\nmat_0 &lt;- matrix(21:29, 3, 3, byrow = TRUE)\nmat_0\n\n\n     [,1] [,2] [,3]\n[1,]   21   22   23\n[2,]   24   25   26\n[3,]   27   28   29\n\n\nStore the numbers 21-29 in a vector mat_0 and create a matrix using the dim() function\n\n\nCode\nmat_0 &lt;- 21:29\ndim(mat_0) &lt;- c(3, 3)\nmat_0\n\n\n     [,1] [,2] [,3]\n[1,]   21   24   27\n[2,]   22   25   28\n[3,]   23   26   29\n\n\nWhat happens if you use 1:4 to create a 2x3 matrix mat_0, filled by column? Predict the value in mat_0[2, 3].\n\n\nCode\nmat_0 &lt;- matrix(1:4, 2, 3)\n\n\nWarning in matrix(1:4, 2, 3): data length [4] is not a sub-multiple or multiple\nof the number of columns [3]\n\n\nCode\nmat_0[2, 3]\n\n\n[1] 2\n\n\nWhat is the value in mat_0[2, 3] you fill the matrix with 1:9\n\n\nCode\nmat_0 &lt;- matrix(1:9, 2, 3)\n\n\nWarning in matrix(1:9, 2, 3): data length [9] is not a sub-multiple or multiple\nof the number of rows [2]\n\n\nCode\nmat_0[2 ,3]\n\n\n[1] 6\n\n\nCreate 3x3 a named matrix mat_0 with elements 21-29 with row names “obs_1”, “obs_2”, … and column names “var_1”, “var_2”\n\n\nCode\nmat_0 &lt;- matrix(21:29, 3, 3, dimnames = list(c(\"obs_1\", \"obs_2\", \"obs_3\"), c(\"var_1\",  \"var_2\", \"var_3\")))\nmat_0\n\n\n      var_1 var_2 var_3\nobs_1    21    24    27\nobs_2    22    25    28\nobs_3    23    26    29\n\n\nHere, you had to write down all names. First recreate mat_0 without names and then use the rownames and colnames function to set the names. Using these function, try to avoid writing all names. There are two ways to do so.\n\noption 1:\n\n\n\nCode\nmat_0 &lt;- matrix(21:29, 3, 3)\nrownames(mat_0) &lt;- paste(\"obs\", 1:3, sep = \"_\")\ncolnames(mat_0) &lt;- paste(\"var\", 1:3, sep = \"_\")\nmat_0\n\n\n      var_1 var_2 var_3\nobs_1    21    24    27\nobs_2    22    25    28\nobs_3    23    26    29\n\n\n\nOption 2\n\n\n\nCode\nmat_0 &lt;- matrix(21:29, 3, 3)\nrownames(mat_0) &lt;- rownames(mat_0, do.NULL = FALSE, prefix = \"obs_\")\ncolnames(mat_0) &lt;- colnames(mat_0, do.NULL = FALSE, prefix = \"var_\")\nmat_0\n\n\n      var_1 var_2 var_3\nobs_1    21    24    27\nobs_2    22    25    28\nobs_3    23    26    29\n\n\nCreate a 4x4 identity matrix ident\n\n\nCode\nident &lt;- diag(1, 4, 4)\nident\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nDetermine the number of rows and columns for this matrix:\n\nmat_0 &lt;- matrix(rnorm(1000), 500, 2)\ncolnames(mat_0) &lt;- c(\"var_1\", \"var_2\")\nrownames(mat_0) &lt;- paste(\"obs\", 1:500, sep = \"_\")\n\n\nOption 1:\n\n\n\nCode\nnrow(mat_0)\n\n\n[1] 500\n\n\nCode\nncol(mat_0)\n\n\n[1] 2\n\n\n\nOption 2:\n\n\n\nCode\nattributes(mat_0)$dim[1]\n\n\n[1] 500\n\n\nCode\nattributes(mat_0)$dim[2]\n\n\n[1] 2\n\n\nDetermine the type of mat_0:\n\n\nCode\ntypeof(mat_0)\n\n\n[1] \"double\"\n\n\nFill a 3x3 matrix, mat_1, with the first 9 letters of the alfabet, lowercase\n\n\nCode\nmat_1 &lt;- matrix(letters[1:9], 3, 3)\nmat_1\n\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"d\"  \"g\" \n[2,] \"b\"  \"e\"  \"h\" \n[3,] \"c\"  \"f\"  \"i\" \n\n\nFill a 3x3 matrix, mat_2 with a sequence of dates, starting 2025-04-01 and ending 2025-04-09.\n\n\nCode\nmat_2 &lt;- matrix(seq.Date(from = as.Date(\"2025-04-01\"), to = as.Date(\"2025-04-09\"), by = \"days\"), 3, 3)\nmat_2\n\n\n      [,1]  [,2]  [,3]\n[1,] 20179 20182 20185\n[2,] 20180 20183 20186\n[3,] 20181 20184 20187\n\n\nCreate a 3x3 boolean matrix, mat_3, using random sample from TRUE and FALSE\n\n\nCode\nmat_3 &lt;- matrix(sample(c(TRUE, FALSE), 9, replace = TRUE), 3, 3)\nmat_3\n\n\n      [,1]  [,2]  [,3]\n[1,] FALSE  TRUE FALSE\n[2,]  TRUE FALSE FALSE\n[3,] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\n4.2.3 Subsetting a matrix\n\n4.2.3.1 Subsetting by position\nSubsetting a matrix uses an approach which is very similar to the one used for a vector. However, with a matrix you have both rows as well as columns. This allows you to subset both individual elements, all rows on one of multiple columns, all columns on one or multiple rows or a range of elements spread over some columns and some rows. Matrix mat will be used to illustrate these approaches:\n\nmat &lt;- matrix(c(11, 21, 31, 41, 12, 22, 32, 42, 13, 23, 33, 34, 41, 42, 43, 44), nrow = 4, ncol = 4)\nmat\n\n     [,1] [,2] [,3] [,4]\n[1,]   11   12   13   41\n[2,]   21   22   23   42\n[3,]   31   32   33   43\n[4,]   41   42   34   44\n\n\nAs you can see, the elements of the matrix are equal to their row-column indices.\nTo subset an individual element, you can use mat[m, n] with m the row index and n the column index. For instance, extracting the element in the second row and the third column:\n\nmat[2, 3]\n\n[1] 23\n\n\nIf you assign the outcome to a new variable, you can use it in your code.\nYou can extract an entire column using mat[, n]. For instance, extracting the 4th column of mat:\n\nmat[, 4]\n\n[1] 41 42 43 44\n\n\nSubsetting a specific row using a similar approach. To subset row m, you use mat[m, ]. For instance, subsetting the 3rd row of mat:\n\nmat[3, ]\n\n[1] 31 32 33 43\n\n\nNote that R shows the simplest possible data structure. Subsetting a row or column, results in a numeric vector. To see this, let’s use is.vector() and ask for the class of mat[3, ]:\n\nclass(mat[3, ])\n\n[1] \"numeric\"\n\nis.vector(mat[3, 1])\n\n[1] TRUE\n\n\nTo preserve the structure, you need to add drop = FALSE within the subsetting operations. For instance,\n\nmat[3, , drop = FALSE]\n\n     [,1] [,2] [,3] [,4]\n[1,]   31   32   33   43\n\n\npreserves the structure of the matrix. You can see this from the result, which is now shown as a matrix, as well as from the logical operators\n\nis.vector(mat[3, , drop = FALSE])\n\n[1] FALSE\n\nis.matrix(mat[3, , drop = FALSE])\n\n[1] TRUE\n\n\nIn programming, adding drop = FALSE is usually a good idea as it preserves the data structure. With vectors, the subsetting operator [] preserved the structure of the vector while [[ ]] acted as the simplifying operator. With matrices, [] act as the simplifying operator. To preserve the structure, you need to add drop = FALSE or drop = F.\nYou can subset multiple columns or rows. Suppose you need columns n to k of mat. You can subset these using mat[, n:k]. For instance, subsetting the 2nd to 4th column:\n\nmat[, 2:4]\n\n     [,1] [,2] [,3]\n[1,]   12   13   41\n[2,]   22   23   42\n[3,]   32   33   43\n[4,]   42   34   44\n\n\nNote that in this case, the structure is preserved: the simplest possible data structure to show the result of the subsetting operation is a matrix.\nSimilarly, substting row m to l, is done using mat[m:l, ]. For instance, with m = 2 and l = 4 you subset the 2nd to 4th row:\n\nmat[2:4, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   22   23   42\n[2,]   31   32   33   43\n[3,]   41   42   34   44\n\n\nmat[m:l, n:k] subsets a range: the elements on row m to l and in columns n to k. For instance, if you need the elements in rows 2 to 4 and in columns 1 to 3:\n\nmat[2:4, 1:3]\n\n     [,1] [,2] [,3]\n[1,]   21   22   23\n[2,]   31   32   33\n[3,]   41   42   34\n\n\nIf you need a specific number of rows or columns who are not in a range, you can identify them within vector using c(m, l, ...). For instance\n\nsubsetting row 1 and 3 and column 2 and 4\n\n\nmat[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]   12   41\n[2,]   32   43\n\n\n\nsubsetting all elements on row 1 and 3\n\n\nmat[c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]   11   12   13   41\n[2,]   31   32   33   43\n\n\n\nsubsetting all elements in column 2 and 4\n\n\nmat[, c(2, 4)]\n\n     [,1] [,2]\n[1,]   12   41\n[2,]   22   42\n[3,]   32   43\n[4,]   42   44\n\n\nUsing negative index numbers, you tell R that you don’t want to extract those rows or columns. For instance, to show all elements in mat except those in the first row and first column, mat[-1, -1] shows:\n\nmat[-1, -1]\n\n     [,1] [,2] [,3]\n[1,]   22   23   42\n[2,]   32   33   43\n[3,]   42   34   44\n\n\nYou can use negative indices to extract one or more rows or columns or ranges:\n\nextracting all columns except columns 3 to 4\n\n\nmat[, -3:-4]\n\n     [,1] [,2]\n[1,]   11   12\n[2,]   21   22\n[3,]   31   32\n[4,]   41   42\n\n\n\nextracting all rows except rows 1 to 3\n\n\nmat[-1:-3, ]\n\n[1] 41 42 34 44\n\n\nNote that in this case, R simplifies the output to a vector (mat has 4 rows can you extract all except the first three). Here you have an example where you would change the data structure by subsetting all rows except 1. To avoid that, you can use the preserving operator:\n\nmat[-1:-3, , drop = F]\n\n     [,1] [,2] [,3] [,4]\n[1,]   41   42   34   44\n\n\n\nextracting all elements except those in columns 1 to 2 and rows 1 to 2\n\n\nmat[-1:-2, -1:-2]\n\n     [,1] [,2]\n[1,]   33   43\n[2,]   34   44\n\n\nNote the you can select multiple columns or rows not in a range using -c(k, l), e.g. extracting all columns except 1 and 3:\n\nmat[, -c(1, 3)]\n\n     [,1] [,2]\n[1,]   12   41\n[2,]   22   42\n[3,]   32   43\n[4,]   42   44\n\n\n\n\n4.2.3.2 Subsetting a named matrix\nWith names matrices, you can also refer to the names of the columns and rows. Let’s add row and column names to mat:\n\ncolnames(mat) &lt;- colnames(mat, do.NULL = FALSE, prefix = \"var_\")\nrownames(mat) &lt;- rownames(mat, do.NULL = FALSE, prefix = \"row_\")\nmat\n\n      var_1 var_2 var_3 var_4\nrow_1    11    12    13    41\nrow_2    21    22    23    42\nrow_3    31    32    33    43\nrow_4    41    42    34    44\n\n\nYou can now subset this matrix using `mat[“rowname”, “columnname”]. For instance, extracting the element on row 2 and column 3:\n\nmat[\"row_2\", \"var_3\"]\n\n[1] 23\n\n\nSubsetting all elements in column var_3:\n\nmat[, \"var_3\"]\n\nrow_1 row_2 row_3 row_4 \n   13    23    33    34 \n\n\nNote that R retains the row names in this case, however, you loose the structure of the matrix. To avoid this, add drop = F:\n\nmat[, \"var_3\", drop = F]\n\n      var_3\nrow_1    13\nrow_2    23\nrow_3    33\nrow_4    34\n\n\nR also shows the names if you subset a named matrix using indices:\n\nmat[, 3, drop = F]\n\n      var_3\nrow_1    13\nrow_2    23\nrow_3    33\nrow_4    34\n\n\nTo extract all elements in row 2 and keep the structure:\n\nmat[\"row_2\", , drop = F]\n\n      var_1 var_2 var_3 var_4\nrow_2    21    22    23    42\n\n\nYou can also collect the names a vector and subset multiple rows:\n\nmat[c(\"row_1\", \"row_3\"), ]\n\n      var_1 var_2 var_3 var_4\nrow_1    11    12    13    41\nrow_3    31    32    33    43\n\n\nor multiple columns:\n\nmat[, c(\"var_1\", \"var_3\")]\n\n      var_1 var_3\nrow_1    11    13\nrow_2    21    23\nrow_3    31    33\nrow_4    41    34\n\n\nor both:\n\nmat[c(\"row_1\", \"row_3\"), c(\"var_1\", \"var_3\")]\n\n      var_1 var_3\nrow_1    11    13\nrow_3    31    33\n\n\n\n\n4.2.3.3 Subsetting using a logical matrix\nRecall that you can subset a vector using a logical vector. For a matrix, this is also true. However, in this case, the result is not a matrix but a vector. This vector includes all elements for which the condition returned TRUE. Let’s create a random logical matrix:\n\ncond = matrix(sample(c(TRUE, FALSE), 16, TRUE), nrow = 4, ncol = 4)\ncond\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE  TRUE FALSE FALSE\n[2,]  TRUE FALSE FALSE FALSE\n[3,] FALSE  TRUE  TRUE FALSE\n[4,]  TRUE  TRUE FALSE FALSE\n\n\nHere, we have a matrix whose elements are either TRUE or FALSE. We can use this matrix to extract the values in mat who are in the same position as the value TRUE in the matrix cond. To do so, we can use:\n\nmat[cond]\n\n[1] 21 41 12 32 42 33\n\n\nWithing the [] you can include various conditions, for instance, to extract all elements in mat larger than 25, you can use\n\nmat[mat &gt; 25]\n\n [1] 31 41 32 42 33 34 41 42 43 44\n\n\nYou can further refine the condition and apply it to only one column or one row. For instance to extract all rows whose value in the first row is larger than 25 you can define this condition:\n\ncond &lt;- mat[, 1] &gt; 25\ncond\n\nrow_1 row_2 row_3 row_4 \nFALSE FALSE  TRUE  TRUE \n\n\nIf you include this condition in the subsetting operator for the rows, you’ll see all columns for the rows whose value in the first column in larger than 25:\n\nmat[cond, ]\n\n      var_1 var_2 var_3 var_4\nrow_3    31    32    33    43\nrow_4    41    42    34    44\n\n\nCollecting the elements in a vector, allows you to verify if they are also elements in the matrix. For instance, extracting the elements in mat who are equal to 12, 22, 33, 44 or 55, is done using\n\nmat[mat %in% c(12, 22, 33, 44, 55)]\n\n[1] 12 22 33 44\n\n\nSubsetting using logical conditions also allows you to subset a named matrix using regular expressions. Recall that the grepl() function outputs a logical vector. If a matrix has column or row names, you can use these in grepl() to extract observations (rows) or variables (columns) that match a regular expressions. Suppose that you want to extract all observations on row_1, row_2 and row_3. Here, a simple regular expression would be “[0-3]”. This regular expression matches all rows that include ”” and one digit equal to 0, 1, 2 or 3. grepl() needs to find matches in the row names of mat:\n\ngrepl(pattern = \"_[0-3]\", x = rownames(mat))\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nWe can now use this expression to extract the observations. To do so, you either create a vector cond to store the result of grepl() which you can then use to subset:\n\ncond &lt;- grepl(pattern = \"_[0-3]\", x = rownames(mat))\nmat[cond, ]\n\n      var_1 var_2 var_3 var_4\nrow_1    11    12    13    41\nrow_2    21    22    23    42\nrow_3    31    32    33    43\n\n\nAs an alternative, you use the grepl() in the subsettig operation:\n\nmat[grepl(pattern = \"_[0-3]\", x = rownames(mat)), ]\n\n      var_1 var_2 var_3 var_4\nrow_1    11    12    13    41\nrow_2    21    22    23    42\nrow_3    31    32    33    43\n\n\nUsing this last method is probably less likely to result in code that is easy to read. In other words, if the pattern is complex, it it in general a good idea to use the first method.\nYou can do the same with column names. For instance, extracting all variables var_2, var_3 and var_4 can be done through:\n\nmat[, grepl(pattern = \"_[2-4]\", x = colnames(mat))]\n\n      var_2 var_3 var_4\nrow_1    12    13    41\nrow_2    22    23    42\nrow_3    32    33    43\nrow_4    42    34    44\n\n\nCombining both subsets both rows as well as columns:\n\nmat[grepl(pattern = \"_[0-3]\", x = rownames(mat)), grepl(pattern = \"_[2-4]\", x = colnames(mat))]\n\n      var_2 var_3 var_4\nrow_1    12    13    41\nrow_2    22    23    42\nrow_3    32    33    43\n\n\nThe subset(x, subset, select, drop = FALSE, ...) function allows you to extract columns, defined in select from the matrix x using a logical index defined in subset. Using this function, you can subset rows and select which columns R needs to return. For instance, to selects the rows in columns 1 and 4 of mat if the value in column 2 is larger than 20 mat[, 2] &gt; 20, you would use:\n\nsubset(mat, subset = mat[, 2] &gt; 20, select = c(1, 4))\n\n      var_1 var_4\nrow_2    21    42\nrow_3    31    43\nrow_4    41    44\n\n\nIn the select argument, you can use the usual subsetting methods:\n\nextracting all columns between 2 and 4\n\n\nsubset(mat, subset = mat[, 2] &gt; 20, select = 2:4)\n\n      var_2 var_3 var_4\nrow_2    22    23    42\nrow_3    32    33    43\nrow_4    42    34    44\n\n\n\nextracting all but column 4:\n\n\nsubset(mat, subset = mat[, 2] &gt; 20, select = -4)\n\n      var_1 var_2 var_3\nrow_2    21    22    23\nrow_3    31    32    33\nrow_4    41    42    34\n\n\nUsing the row names of the matrix, you can also use grepl(). For instance, selecting columns 1 and 4 and only rows whose name includes “3” or “4” uses:\n\ncond &lt;- grepl(pattern = \"row_[3-4]\", rownames(mat))\nsubset(mat, cond, c(1, 4))\n\n      var_1 var_4\nrow_3    31    43\nrow_4    41    44\n\n\nIf you don’t use select =, by default, R return all columns. Excluding the subset argument will return all selected columns.\n\n\n4.2.3.4 Diagonal and lower and upper triangular parts\nIf you have a square matrix, you can extract the diagonal elements using diag(x). Extracting the diagonal elements from `mat:\n\ndiag(mat, names = TRUE)\n\n[1] 11 22 33 44\n\n\nTo extract the upper or lower triangular part of a square matrix, there are two functions: upper.tri(x, diag = FALSE) and lower.tri(x, diag = FALSE). The first subsets the upper triangular part, excluding the diagonal. The second the lower triangular part. Both include the square matrix as the first argument. The second argument determines is the diagonal is included or not (default). The outcome is a logical vector that can be used to subset the matrix.\n\nuptri &lt;- upper.tri(mat, diag = FALSE)\nuptri\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE  TRUE  TRUE  TRUE\n[2,] FALSE FALSE  TRUE  TRUE\n[3,] FALSE FALSE FALSE  TRUE\n[4,] FALSE FALSE FALSE FALSE\n\n\nApplying these two function to mat:\n\nmat[uptri]\n\n[1] 12 13 23 41 42 43\n\n\nExtracting the lower triangular part can be done is a similar way. If we add the diagonal,\n\nlotri &lt;- lower.tri(mat, diag = TRUE)\nmat[lotri]\n\n [1] 11 21 31 41 22 32 42 33 34 44\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate 3x3 matrix, mat_0 as a sequence from 101-109.\n\n\nCode\nmat_0 &lt;- matrix(101:109, 3, 3)\n\n\nUsing this matrix, extract\n\nelement in row 2 and column 3:\n\n\n\nCode\nmat_0[2, 3]\n\n\n[1] 108\n\n\n\nall values in column 2 and preserve the structure of the matrix in the result\n\n\n\nCode\nmat_0[, 2, drop = FALSE]\n\n\n     [,1]\n[1,]  104\n[2,]  105\n[3,]  106\n\n\n\nall values in row 1 and preserve the structure of the matrix in the result\n\n\n\nCode\nmat_0[1, , drop = FALSE]\n\n\n     [,1] [,2] [,3]\n[1,]  101  104  107\n\n\n\nall values except those in column 1:\n\n\n\nCode\nmat_0[, -1]\n\n\n     [,1] [,2]\n[1,]  104  107\n[2,]  105  108\n[3,]  106  109\n\n\n\nall values in columns 1 and 3:\n\n\n\nCode\nmat_0[, c(1, 3)]\n\n\n     [,1] [,2]\n[1,]  101  107\n[2,]  102  108\n[3,]  103  109\n\n\n\nalle values in rows 1 and 3:\n\n\n\nCode\nmat_0[c(1, 3), ]\n\n\n     [,1] [,2] [,3]\n[1,]  101  104  107\n[2,]  103  106  109\n\n\nLet’s now add names to mat_0: “obs_1”, … for the rows and “var_1” … for the columns:\n\n\nCode\ncolnames(mat_0) &lt;- colnames(mat_0, do.NULL = FALSE, prefix = \"var_\")\nrownames(mat_0) &lt;- rownames(mat_0, do.NULL = FALSE, prefix = \"row_\")\n\n\nUsing these names, extract\n\nall values for var_1 preserving the matrix structure of the result:\n\n\n\nCode\nmat_0[, \"var_1\", drop = FALSE]\n\n\n      var_1\nrow_1   101\nrow_2   102\nrow_3   103\n\n\n\nall values for row_1 and row_3`\n\n\n\nCode\nmat_0[c(\"row_1\", \"row_3\"), ]\n\n\n      var_1 var_2 var_3\nrow_1   101   104   107\nrow_3   103   106   109\n\n\nExtract all the values larger than 104:\n\nmat_0[mat_0 &gt; 104]\n\n[1] 105 106 107 108 109\n\n\nWhich values are on the diagonal of mat_0?\n\n\nCode\ndiag(mat_0)\n\n\n[1] 101 105 109\n\n\nExtract the lower triangular part of mat_0 excluding the diagonal.\n\n\nCode\nmat_0[upper.tri(mat_0, diag = FALSE)]\n\n\n[1] 104 107 108\n\n\n\n\n\n\n\n\n4.2.4 Changing elements in a matrix\nYou can change individual elements of a matrix by reassigning them a new value. Suppose you want to change the value on row 2 and column 3 of mat from 23 into 123, you can use\n\nmat[2, 3] &lt;- 123\nmat\n\n      var_1 var_2 var_3 var_4\nrow_1    11    12    13    41\nrow_2    21    22   123    42\nrow_3    31    32    33    43\nrow_4    41    42    34    44\n\n\nIf you want to change all values less than 25 into 0, you can subset using this condition and reassign the values of the elements where the condition is TRUE:\n\nmat[mat &lt; 25] &lt;- 0\nmat\n\n      var_1 var_2 var_3 var_4\nrow_1     0     0     0    41\nrow_2     0     0   123    42\nrow_3    31    32    33    43\nrow_4    41    42    34    44\n\n\n\n\n4.2.5 Changing dimensions of a matrix\n\n4.2.5.1 Changing the number of rows or columns\nSuppose that you have a matrix, mat with 6 rows and 20 columns:\n\nmat &lt;- matrix(1:120, 6, 20)\n\nYou can change the dimensions of this matrix using dim(). For instance, if you want to change this matrix into a 3x40 matrix:\n\ndim(mat) &lt;- c(3, 40)\n\nNote that you can do this as long as the length of the matrix is unaffected. In other words, the number of elements in both matrices must be the same.\n\n\n4.2.5.2 Adding row/columns to matrices\nUsing rbind() (row bind) and cbind() (column bind) functions you can combine vectors and matrices. The first, rbind() combines by rows: it stacks on vector or matrix on top of the other. To do so, the vectors and matrices that will be combined need to have the same number of columns. cbind() adds adds the vectors and matrices next to each other. The vectors and matrices in this function need to have the same number of rows.\nSuppose that you have two matrices, mat_1 (filled with 1’s) and mat_2 (filled with 2’s):\n\nmat_1 &lt;- matrix(1, nrow = 3, ncol = 2)\nmat_1\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n\nmat_2 &lt;- matrix(2, nrow = 3, ncol = 1)\nmat_2\n\n     [,1]\n[1,]    2\n[2,]    2\n[3,]    2\n\n\nThey both have the same number of rows. That means that you can bind both and add the columns of mat_2 to those or mat_1\n\nmat_c12 &lt;- cbind(mat_1, mat_2)\nmat_c12\n\n     [,1] [,2] [,3]\n[1,]    1    1    2\n[2,]    1    1    2\n[3,]    1    1    2\n\n\nNote that cbind(mat2, mat1) would add the columns of mat_1 to those of mat_2:\n\nmat_c21 &lt;- cbind(mat_2, mat_1)\nmat_c21\n\n     [,1] [,2] [,3]\n[1,]    2    1    1\n[2,]    2    1    1\n[3,]    2    1    1\n\n\nYou can add a matrix mat_3 (filled with 3’s) with the same number of columns as e.g. mat_1, you can add these rows to those of mat_1 using rbind():\n\nmat_3 = matrix(3, nrow = 2, ncol = 2)\n\nmat_r13 &lt;- rbind(mat_1, mat_3)\nmat_r13\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n[4,]    3    3\n[5,]    3    3\n\n\nIf you reserve the order, you would add the rows of mat_1 to those of mat_3:\n\nmat_r31 &lt;- rbind(mat_3, mat_1)\nmat_r31\n\n     [,1] [,2]\n[1,]    3    3\n[2,]    3    3\n[3,]    1    1\n[4,]    1    1\n[5,]    1    1\n\n\n\n\n4.2.5.3 Removing rows and columns\nWhen we discussed subsetting a matrix, we introduced negative index positions to subset all but the rows/columns with a negative index. This is the first approach if you want to remove a row or a column. Suppose for instance that you want to remove the last two rows of mat_r31, you use their negative index positions and save the matrix as mat_r31. Note that you can specify the negative positions using a range or you can collect them in a vector and add a minus sign c(). Here, we wil use the last appraoch:\n\nmat_r31 &lt;- mat_r31[-c(4, 5), ]\nmat_r31\n\n     [,1] [,2]\n[1,]    3    3\n[2,]    3    3\n[3,]    1    1\n\n\nYou can remove the first two columns from mat_c12 in a similar way:\n\nmat_c12 &lt;- mat_c12[, -1:-2]\nmat_c12\n\n[1] 2 2 2\n\n\nThe second approach uses a logical vector where a value TRUE will keep the row or column and a value FALSE will remove that column of row. To illustrate this approach, we’ll use mat. Suppose you want to remove columns 1 and 3. The logical vector would then be c(FALSE, TRUE, FALSE, TRUE). Using this vector to subset the matrix\n\nmat_keep &lt;- c(FALSE, TRUE, FALSE, TRUE)\nmat[, mat_keep]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    4   10   16   22   28   34   40   46   52    58    64    70    76    82\n[2,]    5   11   17   23   29   35   41   47   53    59    65    71    77    83\n[3,]    6   12   18   24   30   36   42   48   54    60    66    72    78    84\n     [,15] [,16] [,17] [,18] [,19] [,20]\n[1,]    88    94   100   106   112   118\n[2,]    89    95   101   107   113   119\n[3,]    90    96   102   108   114   120\n\n\nIf you reassign this result to mat you have effectively removed columns 1 and 3. You can use a similar approach to keep/remove rows.\nNote you don’t need to write the logical vector by hand. Usually, this vector will be the outcome of a condition.\n\n\n4.2.5.4 Deconstructing a matrix\nDeconstructing a matrix refers to the operation where change the dimension and change the matrix into a vector. To do so, you can use the c() function. This function changes the matrix into a vector. For instance, applying this function to mat results in a vector.\n\nc(mat)\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120\n\n\nAs you can see, the vector starts with the first column, then add the second column, the third and the fourth.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUsing the next three matrices:\n\nmat_1 &lt;- matrix(1:12, 3, 4)\nmat_2 &lt;- matrix(11:22, 3, 4)\nmat_3 &lt;- matrix(31:42, 3, 4)\n\n\nadd columns of mat_3 to those of mat_1:\n\n\n\nCode\ncbind(mat_1, mat_3)\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    4    7   10   31   34   37   40\n[2,]    2    5    8   11   32   35   38   41\n[3,]    3    6    9   12   33   36   39   42\n\n\n\nadd the rows of mat_2 to those of mat_1:\n\n\n\nCode\nrbind(mat_1, mat_2)\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n[4,]   11   14   17   20\n[5,]   12   15   18   21\n[6,]   13   16   19   22\n\n\n\nremove the 2nd row of mat_2\n\n\n\nCode\nmat_2[-2, ]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]   11   14   17   20\n[2,]   13   16   19   22\n\n\n\nremove the 2nd row of mat_2 using a logical vector\n\n\n\nCode\nmat_2[c(T, F, T), ]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]   11   14   17   20\n[2,]   13   16   19   22\n\n\n\nremove column 1 and 3 from mat_1. Do so in three ways:\nOption 1:\n\n\n\nCode\nmat_1[, -c(1, 3)]\n\n\n     [,1] [,2]\n[1,]    4   10\n[2,]    5   11\n[3,]    6   12\n\n\n\nOption 2:\n\n\n\nCode\nmat_1[, c(-1, -3)]\n\n\n     [,1] [,2]\n[1,]    4   10\n[2,]    5   11\n[3,]    6   12\n\n\n\nOption 3:\n\n\n\nCode\nmat_1[, c(F, T, F, T)]\n\n\n     [,1] [,2]\n[1,]    4   10\n[2,]    5   11\n[3,]    6   12\n\n\nChange the values of mat_1 on the upper triangular part, excluding the diagonal, to 0.\n\n\nCode\nmat_1[upper.tri(mat_1, diag = FALSE)] &lt;- 0\nmat_1\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    2    5    0    0\n[3,]    3    6    9    0\n\n\nTurn mat_3 into a vector.\n\n\nCode\nc(mat_3)\n\n\n [1] 31 32 33 34 35 36 37 38 39 40 41 42\n\n\n\n\n\n\n\n\n4.2.6 Applying functions to a matrix\nRecall that many function in R are vectorized. For matrices, that means that they apply to the individual elements of a matrix. This holds for most operators: they work on an element by element basis. However, note that in this case, this requires that the dimensions of both matrices are the same.\n\n4.2.6.1 Operators\nOperators such as addition, subtraction, division, multiplication, integer division or modulus can be used with matrices. Using\n\nmat_1 &lt;- matrix(c(2, 4, 8, 10), 2, 2)\nmat_2 &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nwe’ll illustrate these operators.\n\naddition:\n\n\nmat_1 + mat_2\n\n     [,1] [,2]\n[1,]    3   11\n[2,]    6   14\n\n\n\nsubtraction:\n\n\nmat_1 - mat_2\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n\n\n\nmultiplication:\n\n\nmat_1 * mat_2\n\n     [,1] [,2]\n[1,]    2   24\n[2,]    8   40\n\n\n\ndivision\n\n\nmat_1 / mat_2\n\n     [,1]     [,2]\n[1,]    2 2.666667\n[2,]    2 2.500000\n\n\n\ninteger division\n\n\nmat_1 %/% mat_2\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\nmodulus\n\n\nmat_1 %% mat_2\n\n     [,1] [,2]\n[1,]    0    2\n[2,]    0    2\n\n\n\n\n4.2.6.2 Functions\nMost functions are also vectorized. In other words, they work on an element by element basis if applied to a matrix. For instance\n\nabsolute value\n\n\nabs(-1 * mat_1)\n\n     [,1] [,2]\n[1,]    2    8\n[2,]    4   10\n\n\n\nnatural logarithm:\n\n\nlog(mat_1)\n\n          [,1]     [,2]\n[1,] 0.6931472 2.079442\n[2,] 1.3862944 2.302585\n\n\n\npower, e.g. n\n\n\nmat_1^3\n\n     [,1] [,2]\n[1,]    8  512\n[2,]   64 1000\n\n\n\nsquare root\n\n\nsqrt(mat_1)\n\n         [,1]     [,2]\n[1,] 1.414214 2.828427\n[2,] 2.000000 3.162278\n\n\n\nexponential function\n\n\nexp(mat_1)\n\n          [,1]      [,2]\n[1,]  7.389056  2980.958\n[2,] 54.598150 22026.466\n\n\nHere, the functions are applied to all elements of the matrix mat_1. Note that this is not necessary. If you subset the column or rows of mat_1, R will apply a function only to the extracted rows or columns. This also holds for the mathematical operators. For instance:\n\nadding the first column of mat_1 to the second column of mat_2:\n\n\nmat_1[, 1] + mat_2[, 2]\n\n[1] 5 8\n\n\n\nnatural logarithm of the first row of mat_1\n\n\nlog(mat_1[1, ])\n\n[1] 0.6931472 2.0794415\n\n\n\n\n4.2.6.3 Statistical functions\nWe covered a number of statistical function and discussed how they are applied to vectors. By extension, you can use these for matrices too. Subsetting a column, row or element allows you to apply these function to all elements in one or multiple rows, columns, elements or ranges. For instance,\n\narea under the Student-t density with 5 degrees of freedom for the values in the first column of mat_2:\n\n\npt(mat_2[, 1], df = 5)\n\n[1] 0.8183913 0.9490303\n\n\n\nthe probability that an F-distributed value with 6 and 2 degrees of freedom equals the values in first and the second row of mat_1:\n\n\n\ndf(mat_1[1:2, ], df1 = 6, df2 = 2)\n\n           [,1]        [,2]\n[1,] 0.13494377 0.013271040\n[2,] 0.04537656 0.008770781\n\n\n\n\n4.2.6.4 Functions on multiple columns/rows\nIn the previous section, we applied all functions to all elements of the matrix or a subset of columns and or rows. R includes a number of functions that you can apply to every column or every row. In addition, you can use the apply() function apply a function per row or per column.\n\n4.2.6.4.1 Functions operating per row/column\nR includes function that work per row or column of a matrix. To illustrate some these functions, we’ll use the following random matrix:\n\nn = 1000\nm = 5\nmatr &lt;- matrix(rnorm(n * m), n, m)\nmatu &lt;- matrix(runif(n * m, min = 0, max = 1), n, m)\ncolnames(matr) &lt;- colnames(matr, do.NULL = FALSE, prefix = \"var_\")\ncolnames(matu) &lt;- colnames(matu, do.NULL = FALSE, prefix = \"var_\")\nrownames(matr) &lt;- rownames(matr, do.NULL = FALSE, prefix = \"obs_\")\nrownames(matu) &lt;- rownames(matu, do.NULL = FALSE, prefix = \"obs_\")\n\nThe mean of every column of the first matrix, matr, should be (close to) zero and its standard deviation (close to) one. For the second matrix, where each element is drawn from a uniform distribution with minimum zero and maximum one, the sum of each column should be close to 500 (the number of observations multiplies with the expected value 0.5), the minimum should be close to zero and the maximum should be (close to) one.\nR includes functions to calculate the means per column or per row: colMeans(x, na.rm = FALSE, dims = 1) and rowMeans(x, na.rm = FALSE, dims = 1). In both functions, x refers to the matrix. If you matrix include missing values, you need to change the second argument from FALSE to TRUE. The arguments dims = 1 allows you to specify which dimensions are regarded as row or column. You can lease this on its default value. Using these functions, you can calculate the mean per column:\n\ncolMeans(matr, na.rm = TRUE)\n\n       var_1        var_2        var_3        var_4        var_5 \n 0.005143367 -0.049817391 -0.001946084  0.020798152  0.045079932 \n\n\nor the mean per row: rowMeans(matr, na.rm = TRUE). In this case, given the size of the matrix, the output would be very long.\nNote that you can reduce the number of columns by subsetting the matrix. For instance, to determine the column means of columns var_2, var_3 and var_5, you can use the grepl() function to subset these columns:\n\ncolMeans(matr[, grepl(pattern = \"_[2-4]\", x = colnames(matr))], na.rm = TRUE)\n\n       var_2        var_3        var_4 \n-0.049817391 -0.001946084  0.020798152 \n\n\nTo calculate the sum of all values in a column or row R includes colSums(x, na.rm = FALSE) and rowSums(x, na.rm = FALSE). As with colMeans(), the first argument is the matrix while the second allows you to specify that missing values should be disregarded in the calculation or not. Using this function to calculate the sum of all values per column in matu:\n\ncolSums(matu, na.rm = TRUE )\n\n   var_1    var_2    var_3    var_4    var_5 \n498.8963 494.7191 510.6848 491.3499 488.0277 \n\n\nYou can use scale() to standardize the values per column. Recall that a standardized value is calculated as\n\\[\nx_{stand} = {{(x - \\overline{x})} \\over{s}}\n\\]\nwhere \\(\\overline{x}\\) is the mean and \\(s\\) is the standard deviation.\nTo illustrate this function, we’ll redefine matr and determine its values as draws from a normal distribution with mean 5 and standard deviation 10:\n\nmatr &lt;- matrix(rnorm(n * m, 5, 10), n, m)\n\nIf you check the column means, you’ll see that the are (close to) 5\n\ncolMeans(matr)\n\n[1] 5.300783 5.116847 5.083844 5.348894 4.590922\n\n\nTo standardize these values, you can use scale(x, center = TRUE, scale = TRUE). The first argument equals the matrix you want to scale. The second and third argument determine is you want to subtract the mean (i.e. you want to center the columns in matr) and divide by the standard deviation of every column in matr. By default, both are the case. Applying that function to matr:\n\nmatrs &lt;- scale(matr, center = TRUE, scale = TRUE)\n\nIf you now look at the means per column,\n\ncolMeans(matrs)\n\n[1] 1.811398e-17 6.473988e-18 1.907155e-17 2.709898e-17 4.207051e-17\n\n\nyou can verify that they are (close to) zero. The standard deviation is also (close to) one:\n\nsd(matrs[, 1])\n\n[1] 1\n\nsd(matrs[, 2])\n\n[1] 1\n\nsd(matrs[, 3])\n\n[1] 1\n\nsd(matrs[, 4])\n\n[1] 1\n\nsd(matrs[, 5])\n\n[1] 1\n\n\nIf you set one of the arguments, center or scale to FALSE, R will not center (i.e. will not subtract the means) or will not scale (i.e. will not divide by the standard deviation). In addition, you can supply your down vector that R will use to both center and scale. Suppose that you don’t want to center, but want to scale by the sum of all values in a column, can can use:\n\nmatrsum &lt;- scale(matr, center = FALSE, scale = colSums(matr))\n\nUsing colSums(), you can verify this result:\n\ncolSums(matrsum)\n\n[1] 1 1 1 1 1\n\n\n\n\n4.2.6.4.2 The apply() function\nThe apply() function allows you to apply any function to each separate row or column of a matrix. Recall that we used this function in Chapter 2. The function includes a number of arguments: apply(X, MARGIN, FUN, ..., simplify = TRUE). The first, x refers to the matrix. The second, MARGIN = is used to determine is the function is applied to all rows (MARGIN = 1), all columns (MARGIN = 2) or to a subset of rows or columns. To apply to function to both, you can use MARGIN = c(1, 2). FUN refers to the function you want to apply to every column. The three dots ... refer to optional arguments for FUN. For instance, you can add na.rm = TRUE as an optional argument if FUN = sd. The last argument tells R to simplify the output if possible. For matrices, apply simplifies to a vector or array. In case simplify = FALSE R will return a list. A list is a data structure that we will discuss later in this chapter.\nThis function allows you to avoid for loops. Although it is not always possible to avoid for loops, in general they are slower than the apply() function. In other words, it is generally a good idea to try to use apply() as opposed to writing a for loop. For small datasets, the difference might be small. However, for larger datasets, the difference in efficiency can be quite large.\nLet’s use a couple of examples to illustrate how you can use apply(). Here, we will apply a function to the columns. For rows, the output would be too long. Note that here too, you can subset the matrix you include in the first argument. Let’s start with the two functions we already met: the mean and sum of the columns. Using apply, you can calculate the mean of every column in matr\n\napply(matr, MARGIN = 2, FUN = mean, na.rm = TRUE, simplify = TRUE)\n\n[1] 5.300783 5.116847 5.083844 5.348894 4.590922\n\n\nFor the sum of matu:\n\napply(matu, MARGIN = 2, FUN = sum, simplify = TRUE)\n\n   var_1    var_2    var_3    var_4    var_5 \n498.8963 494.7191 510.6848 491.3499 488.0277 \n\n\nUsing a for loop would require you to write\n\nmat_mean &lt;- matrix(0, 1, 5)\ni = 1\nfor (i in 1:5) {\n  mat_mean[i] &lt;- mean(matr[, i])\n}\nmat_mean\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 5.300783 5.116847 5.083844 5.348894 4.590922\n\n\nThe argument for FUN can include most functions that we have seen so far. A couple of examples to illustrate:\n\nthe quantiles for every column of matr\n\n\napply(matr, MARGIN = 2, FUN = quantile, simplify = TRUE)\n\n           [,1]       [,2]       [,3]       [,4]       [,5]\n0%   -21.249655 -23.997095 -29.775836 -26.053802 -23.360434\n25%   -1.191518  -1.251438  -1.893282  -1.246803  -2.013940\n50%    4.960671   5.078630   5.033694   5.362717   4.899435\n75%   11.814882  11.494527  11.955620  11.887836  10.981581\n100%  37.325787  30.820108  38.755552  38.062730  35.365880\n\n\n\nthe standard deviation for every column of matr:\n\n\napply(matr, MARGIN = 2, FUN = sd, simplify = TRUE)\n\n[1]  9.513057  9.520386 10.414700 10.239027  9.948180\n\n\n\nfinding the median value for every column in matr:\n\n\napply(matr, MARGIN = 2, FUN = median, simplify = TRUE)\n\n[1] 4.960671 5.078630 5.033694 5.362717 4.899435\n\n\n\nfinding the mimimum value for every column of matu (recall, should be (close to) 0)\n\n\napply(matu, MARGIN = 2, FUN = min, simplify = TRUE)\n\n       var_1        var_2        var_3        var_4        var_5 \n3.130340e-04 3.611716e-05 3.060959e-03 3.400710e-04 1.325362e-03 \n\n\n\nor the location of the minimum value in every column:\n\n\napply(matu, MARGIN = 2, FUN = which.min, simplify = TRUE)\n\nvar_1 var_2 var_3 var_4 var_5 \n  926    54   956   283   579 \n\n\n\nfinding the maximum value for every column of matu (recall, should be (close to) 1):\n\n\napply(matu, MARGIN = 2, FUN = max, simplify = TRUE)\n\n    var_1     var_2     var_3     var_4     var_5 \n0.9981666 0.9997119 0.9987890 0.9996086 0.9992079 \n\n\n\nor the location of the maximum value in every column:\n\n\napply(matu, MARGIN = 2, FUN = which.max, simplify = TRUE)\n\nvar_1 var_2 var_3 var_4 var_5 \n   22    46   291   743   947 \n\n\n\ncumulative sum for the first 10 rows of every column of matu\n\n\napply(matu[1:10, ], MARGIN = 2, FUN = cumsum, simplify = TRUE)\n\n          var_1     var_2     var_3     var_4    var_5\nobs_1  0.600226 0.0169425 0.1018651 0.6426288 0.158503\nobs_2  1.265842 0.1729380 0.3665798 1.5988249 0.946418\nobs_3  2.119343 1.1211914 0.4628187 1.9303313 1.093771\nobs_4  2.847117 1.1250811 1.1040666 2.2613488 2.054247\nobs_5  3.028297 2.1034462 1.1882292 2.4386040 2.696421\nobs_6  3.540854 2.2688245 1.7402507 3.3594061 2.948909\nobs_7  3.782852 2.7938194 1.7781946 4.3171831 3.733870\nobs_8  4.271986 3.2511397 2.4019932 4.7598717 3.736695\nobs_9  4.524177 3.9606292 2.7464903 4.8748816 3.979814\nobs_10 4.985903 4.1790758 3.7290677 5.4866638 4.220611\n\n\n\ncumulative product for the first 10 rows of every column of matu\n\n\napply(matu[1:10, ], MARGIN = 2, FUN = cumprod, simplify = TRUE)\n\n              var_1        var_2        var_3        var_4        var_5\nobs_1  0.6002259518 1.694250e-02 1.018651e-01 0.6426288278 1.585030e-01\nobs_2  0.3995199498 2.642955e-03 2.696519e-02 0.6144791858 1.248869e-01\nobs_3  0.3409906601 2.506191e-03 2.595101e-03 0.2037037471 1.840246e-02\nobs_4  0.2481642950 9.748173e-06 1.664103e-03 0.0674295104 1.767512e-02\nobs_5  0.0449622375 9.537273e-06 1.400551e-04 0.0119522310 1.135051e-02\nobs_6  0.0230457362 1.577258e-06 7.731343e-05 0.0110056391 2.865859e-03\nobs_7  0.0055770128 8.280525e-07 2.933578e-06 0.0105409480 2.249587e-03\nobs_8  0.0027279104 3.786852e-07 1.829962e-06 0.0046663574 6.355042e-06\nobs_9  0.0006879535 2.686732e-07 6.304165e-07 0.0005366777 1.545037e-06\nobs_10 0.0003176458 5.869076e-08 6.194330e-07 0.0003283298 3.720400e-07\n\n\n\ndraw a sample from every column with size 100 and without replacement:\n\n\napply(matu, MARGIN = 2, FUN = sample, size = 100, replace = FALSE, simplify = TRUE)\n\n             var_1        var_2       var_3      var_4       var_5\n  [1,] 0.427874653 0.2962461817 0.245244351 0.71371006 0.614320555\n  [2,] 0.619837658 0.9732767020 0.005746100 0.69770648 0.104410692\n  [3,] 0.211551783 0.7025180887 0.634824334 0.41890317 0.751804105\n  [4,] 0.937301772 0.8625655503 0.313025048 0.25229257 0.727244976\n  [5,] 0.468054898 0.1333571775 0.412062724 0.14209116 0.205491589\n  [6,] 0.652220312 0.1343263080 0.982577400 0.88488528 0.545954252\n  [7,] 0.094731264 0.4035322617 0.792527967 0.20194269 0.846915320\n  [8,] 0.963287048 0.6726470231 0.396185544 0.03091596 0.379442458\n  [9,] 0.829897380 0.1997247315 0.415923977 0.20976465 0.203501021\n [10,] 0.395167737 0.0038896375 0.377768022 0.08304928 0.522725631\n [11,] 0.663212207 0.8908440617 0.932076235 0.68903437 0.069070730\n [12,] 0.010062481 0.5519522047 0.340680641 0.45924904 0.499460750\n [13,] 0.163756774 0.4289824814 0.263133530 0.51283482 0.205762443\n [14,] 0.312948626 0.6088482901 0.699145186 0.18986819 0.818035963\n [15,] 0.569327792 0.4906537151 0.495169261 0.37241806 0.360954494\n [16,] 0.688230235 0.3483616519 0.923396978 0.67985158 0.527013391\n [17,] 0.304477348 0.7016855879 0.738518027 0.22485392 0.699525842\n [18,] 0.914051189 0.0028812722 0.949652195 0.21667207 0.184232822\n [19,] 0.015076805 0.7671682036 0.071696028 0.84160925 0.434728433\n [20,] 0.680477128 0.7291652344 0.789702306 0.04536275 0.132914925\n [21,] 0.483864088 0.4406194480 0.028702055 0.01458980 0.045473437\n [22,] 0.090609562 0.3710276980 0.862361548 0.40796349 0.126826719\n [23,] 0.535610609 0.8934164054 0.939425986 0.86611405 0.031051245\n [24,] 0.406181956 0.3546411851 0.343321695 0.74156995 0.401845718\n [25,] 0.650372807 0.2254920613 0.007305878 0.96089933 0.294542913\n [26,] 0.523949534 0.0001744141 0.447297217 0.40851562 0.764654150\n [27,] 0.744043353 0.7912811625 0.052715830 0.96053204 0.409537329\n [28,] 0.976969536 0.1195863134 0.982976033 0.74644039 0.110714359\n [29,] 0.133260578 0.2870601334 0.115480395 0.13382330 0.122441713\n [30,] 0.214659605 0.9601216391 0.788732514 0.11746025 0.800846495\n [31,] 0.606783367 0.6674754317 0.860529469 0.04416643 0.575592778\n [32,] 0.701639672 0.8425107629 0.600073412 0.05594724 0.788355635\n [33,] 0.395247573 0.4100571568 0.167690868 0.70724876 0.945276574\n [34,] 0.997906438 0.2519476619 0.735430416 0.48066398 0.747088682\n [35,] 0.196408861 0.4243587430 0.596180666 0.51482828 0.957349570\n [36,] 0.727074796 0.9684854858 0.652410179 0.96595233 0.722289495\n [37,] 0.986973475 0.0895374368 0.845779282 0.41230179 0.293967669\n [38,] 0.773164391 0.9638337884 0.552748314 0.06159046 0.458007812\n [39,] 0.796141559 0.7765745323 0.846832885 0.20770468 0.298801046\n [40,] 0.624487828 0.3564885638 0.660334249 0.17683940 0.057621229\n [41,] 0.817350582 0.5913988259 0.220299744 0.86317435 0.299523436\n [42,] 0.967824784 0.4424968567 0.921304168 0.26098331 0.489219855\n [43,] 0.914439068 0.3942986336 0.361325487 0.60169566 0.616653974\n [44,] 0.768421780 0.4217687396 0.295858546 0.37798105 0.077182890\n [45,] 0.016625161 0.6658115482 0.165320277 0.77455860 0.158503026\n [46,] 0.440597997 0.9446910012 0.243012169 0.06447767 0.420961288\n [47,] 0.719860912 0.1772992392 0.560751369 0.44902747 0.849164016\n [48,] 0.839452247 0.8715715578 0.214128093 0.23395167 0.642936618\n [49,] 0.833640202 0.8292887262 0.674816251 0.66711147 0.277001954\n [50,] 0.338264248 0.3612342407 0.238634258 0.94576723 0.542285567\n [51,] 0.542008152 0.0192937264 0.760847150 0.01364760 0.358393266\n [52,] 0.392433255 0.1112235764 0.775153869 0.87165202 0.237003826\n [53,] 0.164499223 0.4162324592 0.275682220 0.19663832 0.074004117\n [54,] 0.369945183 0.4217562771 0.262902491 0.78702023 0.042206783\n [55,] 0.240512830 0.2073696717 0.976909903 0.05358947 0.167880482\n [56,] 0.414119643 0.8492104961 0.605697764 0.41609591 0.872418524\n [57,] 0.662489830 0.1843502908 0.716483586 0.26179877 0.819733626\n [58,] 0.758360146 0.3945711800 0.928629791 0.01249312 0.650508635\n [59,] 0.599016845 0.3895975223 0.367571383 0.91714007 0.002824981\n [60,] 0.336946693 0.1554408779 0.450853944 0.76029389 0.491443093\n [61,] 0.716182998 0.9103479120 0.725621383 0.01429185 0.686092847\n [62,] 0.061481926 0.0163614631 0.989081329 0.52692042 0.761924360\n [63,] 0.286311698 0.6886869357 0.764402661 0.19420963 0.645052908\n [64,] 0.743579587 0.6328592277 0.015155191 0.06716482 0.375276764\n [65,] 0.338893080 0.8474844724 0.048234599 0.92080208 0.868130674\n [66,] 0.955673186 0.1031871077 0.428569158 0.28122884 0.376567396\n [67,] 0.624607087 0.4536155986 0.790632120 0.38309418 0.860546991\n [68,] 0.575755496 0.4721420540 0.053433392 0.47605170 0.634406123\n [69,] 0.755386295 0.0770887041 0.696568140 0.17478258 0.801871024\n [70,] 0.254544970 0.6029461438 0.617348765 0.71324939 0.334514807\n [71,] 0.492098489 0.3417994273 0.806618107 0.64777243 0.158112442\n [72,] 0.981012912 0.7096717325 0.890726835 0.97584609 0.060581106\n [73,] 0.277054097 0.8711021238 0.514938284 0.22167477 0.422439821\n [74,] 0.211788388 0.8441557020 0.972471799 0.47317091 0.444995942\n [75,] 0.573928014 0.3699196551 0.576670796 0.15495689 0.749400497\n [76,] 0.895156737 0.3571989350 0.218822891 0.06512148 0.393396356\n [77,] 0.881618620 0.0430566731 0.635972946 0.93043863 0.766673522\n [78,] 0.682188959 0.7933296857 0.625962837 0.03882977 0.034771726\n [79,] 0.814984214 0.0663515609 0.524825637 0.84765934 0.265665035\n [80,] 0.877733724 0.4871458802 0.025809797 0.82977315 0.754634449\n [81,] 0.637299148 0.5170553513 0.833798896 0.46606657 0.207820970\n [82,] 0.801481901 0.2931252166 0.136746070 0.24331348 0.826765755\n [83,] 0.871730286 0.9466384773 0.064129959 0.64118513 0.762469654\n [84,] 0.159506244 0.6049182811 0.029450050 0.56676077 0.697369893\n [85,] 0.731072485 0.0996799695 0.164878438 0.58723558 0.221791439\n [86,] 0.094668780 0.7252484844 0.710831779 0.93143246 0.950283931\n [87,] 0.469556776 0.4755556541 0.319298754 0.60529539 0.649953627\n [88,] 0.508050772 0.0509314602 0.015070460 0.17068511 0.869120294\n [89,] 0.086548502 0.5792983784 0.353958581 0.61178214 0.111286720\n [90,] 0.997671658 0.5400455620 0.197374821 0.05923918 0.284796121\n [91,] 0.107591556 0.3836067128 0.052368591 0.14104294 0.417281280\n [92,] 0.358777107 0.7570024268 0.086379471 0.13378116 0.746380477\n [93,] 0.490966906 0.0319006587 0.891686819 0.50759548 0.242711229\n [94,] 0.459255106 0.5008176900 0.482638942 0.87273702 0.338060063\n [95,] 0.794867382 0.7031364362 0.456189326 0.09008759 0.870708651\n [96,] 0.007536766 0.5311022992 0.284598397 0.92246731 0.065699746\n [97,] 0.983863101 0.5435822248 0.992995270 0.76408524 0.590230260\n [98,] 0.483696888 0.5903762605 0.520435375 0.91285026 0.627955907\n [99,] 0.202509591 0.3787623248 0.370204954 0.84725329 0.556368871\n[100,] 0.839834335 0.4521930083 0.804261910 0.18736976 0.368933453\n\n\nNote that in this case, the ... in apply are used to include specify the sample size and replacement method: size = 100, replace = FALSE.\n\norder each column in matr in decreasing order (here the first 10 rows):\n\n\napply(matr[1:10, ], 2, sort, decreasing = TRUE, TRUE )\n\n           [,1]        [,2]        [,3]        [,4]        [,5]\n [1,] 15.460149  16.2546301 15.91465424  21.2399511 20.27354208\n [2,] 15.020452  13.5497243 12.07519698  18.7872667  6.64172071\n [3,]  8.922464  10.4209857  6.55576309  17.0451525  6.26640391\n [4,]  6.488385   9.3100825  5.60532134  10.5860546  4.51004705\n [5,]  4.711979   4.5994621  0.09526772   2.4596832  2.70702889\n [6,]  4.606092   2.4900037 -0.91912989   1.2480615  1.37693694\n [7,] -1.793389   1.0875137 -0.99093526   0.8535719 -0.01652879\n [8,] -3.080375  -0.3370674 -1.47375072  -2.9420104 -0.63829143\n [9,] -6.356346 -10.4651753 -3.40353956  -3.0203727 -2.86726235\n[10,] -6.714175 -16.3735425 -8.44512193 -22.1994413 -4.65856598\n\n\nNote that in this case, the ... in apply are used to include specify the order decreasing = TRUE.\nYou are not limited to these predefined functions. In the FUN argument, you can define your own function. As we will see in Chapter 14, R allows you to build your own function. In addition, you can add so called anonymous functions or lambda function in the FUN argument of apply. To do to, you first write function(x) or use the shorthand \\(x) and add the body of your function, e.g. mean(x)/sd(x). With apply(), x refers to a column is MARGIN = 2 and to the row if MARGIN = 1. Note that there is no comma between function(x) and the body of your function. Adding all these into apply(): apply(mat, 1/2, function(x) mean(x)/sd(x)). This statement could be read as: for each row (if MARGIN = 1) or each column (is MARGIN = 2), substitute that row/column for x in the function function(x). In other words, and assuming that MARGIN = 2, R applies the function function(x) to mat[, 1], then to mat[, 3], … until is reaches the last column. Each time, R stores the outcome in a vector, matrix or list and adds, where possible, the name of that column to that vector, matrix or list. In the previous examples, FUN = mean was actually shorthand for function(x) mean(x). As mean is a known function in R, you don’t need to use function(x). As this function is the third argument after mat and MARGIN, you can further shorten the apply() code to apply(mat, 2, mean)\nFor instance, to standardize all columns in a matrix, you can define function(x) (x - mean(x))/sd(x) or \\(x) (x - mean(x))/sd(x). These functions are anonymous because they don’t have a name. Other functions, such as mean() or functions that you will write yourself have a name. This allows you to use these function throughout your code. Anonymous functions or lambda function only exist when used within code, but can not be called in subsequent parts of your code.\nTo illustrate, let’s standardize the first 10 rows of all columns in matr after adding 5 and multiplying with 10. Although we could include this restriction in the apply() function, we will first create a matrix with the first 10 rows:\n\nmatr10 &lt;- 5 + 10 * matr[1:10, ]\nmatr10\n\n           [,1]        [,2]       [,3]       [,4]       [,5]\n [1,] -12.93389    1.629326 125.751970 -216.99441 -23.672623\n [2,] -58.56346   15.875137 164.146542   13.53572   4.834712\n [3,]  94.22464  -99.651753   5.952677  175.45152 -41.585660\n [4,] -25.80375  109.209857 -79.451219  217.39951 207.735421\n [5,] 159.60149  140.497243  70.557631  -25.20373  -1.382914\n [6,] -62.14175   50.994621  -9.737507   29.59683  32.070289\n [7,]  52.11979  167.546301 -29.035396   17.48061  50.100470\n [8,]  51.06092 -158.735425  61.053213  110.86055  67.664039\n [9,] 155.20452   98.100825  -4.191299  192.87267  71.417207\n[10,]  69.88385   29.900037  -4.909353  -24.42010  18.769369\n\n\nUsing matr10, we can now write the apply() command to standardize:\n\napply(matr10, 2, function(x) (x - mean(x))/sd(x))\n\n            [,1]        [,2]       [,3]       [,4]        [,5]\n [1,] -0.6822876 -0.32897513  1.2872629 -2.0395166 -0.88910479\n [2,] -1.2462908 -0.19075950  1.8035029 -0.2723077 -0.48205544\n [3,]  0.6422431 -1.31162383 -0.3235164  0.9689141 -1.14488071\n [4,] -0.8413651  0.71479209 -1.4718273  1.2904810  2.41511475\n [5,]  1.4503320  1.01834838  0.5451391 -0.5692784 -0.57083542\n [6,] -1.2905202  0.14997656 -0.5344812 -0.1491857 -0.09316522\n [7,]  0.1218070  1.28078360 -0.7939538 -0.2420668  0.16428339\n [8,]  0.1087188 -1.88486509  0.4173461  0.4737695  0.41506934\n [9,]  1.3959834  0.60701010 -0.4599088  1.1024619  0.46865992\n[10,]  0.3413793 -0.05468719 -0.4695635 -0.5632713 -0.28308583\n\n\nor\n\napply(matr10, 2, \\(x) (x - mean(x))/sd(x))\n\n            [,1]        [,2]       [,3]       [,4]        [,5]\n [1,] -0.6822876 -0.32897513  1.2872629 -2.0395166 -0.88910479\n [2,] -1.2462908 -0.19075950  1.8035029 -0.2723077 -0.48205544\n [3,]  0.6422431 -1.31162383 -0.3235164  0.9689141 -1.14488071\n [4,] -0.8413651  0.71479209 -1.4718273  1.2904810  2.41511475\n [5,]  1.4503320  1.01834838  0.5451391 -0.5692784 -0.57083542\n [6,] -1.2905202  0.14997656 -0.5344812 -0.1491857 -0.09316522\n [7,]  0.1218070  1.28078360 -0.7939538 -0.2420668  0.16428339\n [8,]  0.1087188 -1.88486509  0.4173461  0.4737695  0.41506934\n [9,]  1.3959834  0.60701010 -0.4599088  1.1024619  0.46865992\n[10,]  0.3413793 -0.05468719 -0.4695635 -0.5632713 -0.28308583\n\n\nHere, we used “(x)” as shorthand for “function(x)”.\nLet’s now use the apply() function to\n\ntransform the values in mart10 using the min-max transformation (x - min(x))/(max(x) - min(x)). The outcome of this function rescales the values in every column to a 0-1 range:\n\n\napply(matr10, 2, \\(x) (x - min(x))/(max(x) - min(x)))\n\n            [,1]      [,2]      [,3]      [,4]       [,5]\n [1,] 0.22191368 0.4914917 0.8423854 0.0000000 0.07184726\n [2,] 0.01613708 0.5351527 1.0000000 0.5306937 0.18618711\n [3,] 0.70516872 0.1810818 0.3505939 0.9034333 0.00000000\n [4,] 0.16387424 0.8212084 0.0000000 1.0000000 1.00000000\n [5,] 1.00000000 0.9170991 0.6158055 0.4415133 0.16124888\n [6,] 0.00000000 0.6427882 0.2861837 0.5676673 0.29542608\n [7,] 0.51528761 1.0000000 0.2069634 0.5397751 0.36774319\n [8,] 0.51051238 0.0000000 0.5767887 0.7547411 0.43818878\n [9,] 0.98017090 0.7871610 0.3089516 0.9435378 0.45324233\n[10,] 0.59539855 0.5781368 0.3060039 0.4433172 0.24207752\n\n\n\ndetermine the number of values that are positive:\n\n\napply(matr10, 2, function(x) sum(x &gt; 0))\n\n[1] 6 8 5 7 7\n\n\n\nadd to every value in every column the absolute value of the difference between 0 and the minimum value (i.e. make every value at least equal to 0 or larger than 0)\n\n\napply(matr10, 2, \\(x) x + abs(min(x) - 0))\n\n            [,1]      [,2]      [,3]     [,4]      [,5]\n [1,]  49.207857 160.36475 205.20319   0.0000  17.91304\n [2,]   3.578288 174.61056 243.59776 230.5301  46.42037\n [3,] 156.366395  59.08367  85.40390 392.4459   0.00000\n [4,]  36.338003 267.94528   0.00000 434.3939 249.32108\n [5,] 221.743237 299.23267 150.00885 191.7907  40.20275\n [6,]   0.000000 209.73005  69.71371 246.5912  73.65595\n [7,] 114.261542 326.28173  50.41582 234.4750  91.68613\n [8,] 113.202667   0.00000 140.50443 327.8550 109.24970\n [9,] 217.346269 256.83625  75.25992 409.8671 113.00287\n[10,] 132.025602 188.63546  74.54187 192.5743  60.35503\n\n\n\ncalculate the probability that you find a value for a mean for each column in matr10 which is larger 5 using a t-distribution with 9 degrees of freedom:\n\n\napply(matr10, 2, function(x) pt(mean(x)-5, df = 9, lower.tail = FALSE))\n\n[1] 1.789074e-11 1.060196e-10 6.264105e-10 3.993985e-12 4.521195e-11\n\n\n\n\n\n\n4.2.7 Matrix algebra\nUsing * R caculates the product of two matrices element-wise. You can also calculate the product of two matrices. In addition, R allows you to calculate e.g. the determinant of a (square) matrix. To illustrate we’ll use one square matrix A\n\nA &lt;- matrix(c(147, 258, 369, 123, 456, 789, 159, 483, 267), 3, 3)\nA\n\n     [,1] [,2] [,3]\n[1,]  147  123  159\n[2,]  258  456  483\n[3,]  369  789  267\n\n\nand a column vectors with 3 rows x:\n\nx &lt;- matrix(c(5, 10, 15), 3, 1)\nx\n\n     [,1]\n[1,]    5\n[2,]   10\n[3,]   15\n\n\nUsing these two matrices, you can now do matrix algebra. For instance:\n\ntranspose of a matrix (change rows into columns), you can use t(). For the square matrix A the element in position (i, j) changes position to (j, i).\n\n\nt(A)\n\n     [,1] [,2] [,3]\n[1,]  147  258  369\n[2,]  123  456  789\n[3,]  159  483  267\n\n\nAs you can see, 258, which is in position (2, 1) in A is now located in position (1, 2). Applying the transpose to x changes this vector from a column vector into a row vector:\n\nt(x)\n\n     [,1] [,2] [,3]\n[1,]    5   10   15\n\n\n\nmatrix multiplication. Recall that matrix multiplication requires that the number of columns in the first matrix equals the number of rows in the second and that the outcome is a matrix with dimension (nrow(first), ncol(second)). Here, A is a 3x3 matrix and x is a 3x1. Using %*% you can multiply both. The outcome is a 3x1 matrix.\n\n\nA %*% x\n\n      [,1]\n[1,]  4350\n[2,] 13095\n[3,] 13740\n\n\nThe element in position (1, 1) is equal to 147 * 5 + 123 * 10 + 159 * 15, the element in position (2, 1) is equal to 285 * 5 + 456 * 10 + 483 * 15 and the element in (3, 1) is equal to 369 * 5 + 789 * 10 + 267 * 15.\n\ndeterminant of a matrix is only defined for square matrices. Here, A is a square matrix so we can use det(A) to calculate the determinant;\n\n\ndet(A)\n\n[1] -19060920\n\n\nHere, det(A) = 147 * 456 * 267 + 123 * 483 * 369 + 258 * 789 * 159 - 159 * 456 * 369 - 123 * 258 * 267 - 147 * 789 * 483. Here, the determinant is different from zero. In other words, the columns in A are linearly independent.\n\ntrace if a matrix is only defined for square matrices. The trace equals the sum of the elements on the diagonal of the matrix:\n\n\nsum(diag(A))\n\n[1] 870\n\n\n\ninverse of a matrix. To determine the inverse of a matrix, you can use the solve(a, b) function. In general, this function solves a system of equations ax = b. If there is no value, b is set equal to the identity matrix. In that case, solve calculates the inverse:\n\n\nsolve(A)\n\n             [,1]         [,2]          [,3]\n[1,]  0.013605587 -0.004858632  0.0006870078\n[2,] -0.005736397  0.001018943  0.0015727992\n[3,] -0.001851852  0.003703704 -0.0018518519\n\n\n\neigenvalues of a matrix\n\n\neigen(A)$values\n\n[1] 1079.22223 -273.74185   64.51962\n\n\n\neigenvectors of a matrix\n\n\neigen(A)$vectors\n\n           [,1]       [,2]       [,3]\n[1,] -0.2102804 -0.1744814 -0.9081183\n[2,] -0.6517779 -0.4998281  0.3790710\n[3,] -0.7286753  0.8483679  0.1778379\n\n\nFrom you mathematics class, you may recall that the solution of a system of equations\n\\[\nAx = B\n\\]\nequals\n\\[\nx = BA^{-1}.\n\\]\nLet’s first define B:\n\nB &lt;- A %*% x\n\nIn R, you can find the solution as solve(A, B):\n\nsolve(A, B)\n\n     [,1]\n[1,]    5\n[2,]   10\n[3,]   15\n\n\nRecall that we used this function to calculate the inverse. There, this function sets B equal to the identity matrix, in other words\n\\[\nx = BA^{-1} = IA^{-1} = A^{-1}\n\\]\n\n\n4.2.8 Other matrix functions\nThere are many other packages available that you can install and use to do matrix calculations. For instance, {matrixStats} is a package that includes many functions that apply to rows and columns of a matrix. To use the package, you need to install if first. To do so, use install.packages(\"matrixStats\"). The functions in this package are faster and more memory efficient than using apply. You can find all the functions in that package in Bengtsson (2025) .\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUsing matA and matB,\n\nmatA &lt;- matrix(1:16, 4, 4)\nmatB &lt;- matrix(101:116, 4, 4)\n\n\nadd matA to matB:\n\n\n\nCode\nmatA + matB\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  102  110  118  126\n[2,]  104  112  120  128\n[3,]  106  114  122  130\n[4,]  108  116  124  132\n\n\n\nmultiply matA with matB:\n\n\n\nCode\nmatA * matB\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  101  525  981 1469\n[2,]  204  636 1100 1596\n[3,]  309  749 1221 1725\n[4,]  416  864 1344 1856\n\n\n\ntake the natural logarithm of the 2nd column of matB:\n\n\n\nCode\nlog(matB[, 2])\n\n\n[1] 4.653960 4.663439 4.672829 4.682131\n\n\n\ndetermine the probability that the you will have a value less than or equal to those in the 1st column of matA if these values follow a normal distribution with mean 2 and standard deviation 1.5. What do you expect for the value matA[2, 1] = 2?\n\n\n\nCode\npnorm(matA[, 1], 2, 1.5)\n\n\n[1] 0.2524925 0.5000000 0.7475075 0.9087888\n\n\n\ndetermine the means and the sum of every column in matB:\n\n\n\nCode\ncolMeans(matB)\n\n\n[1] 102.5 106.5 110.5 114.5\n\n\nCode\ncolSums(matB)\n\n\n[1] 410 426 442 458\n\n\n\ndetermine the mean and the sum of every row in matA\n\n\n\nCode\nrowMeans(matA)\n\n\n[1]  7  8  9 10\n\n\nCode\nrowSums(matA)\n\n\n[1] 28 32 36 40\n\n\n\nstandardize the values of matB:\n\n\n\nCode\nscale(matB, center = TRUE, scale = TRUE)\n\n\n           [,1]       [,2]       [,3]       [,4]\n[1,] -1.1618950 -1.1618950 -1.1618950 -1.1618950\n[2,] -0.3872983 -0.3872983 -0.3872983 -0.3872983\n[3,]  0.3872983  0.3872983  0.3872983  0.3872983\n[4,]  1.1618950  1.1618950  1.1618950  1.1618950\nattr(,\"scaled:center\")\n[1] 102.5 106.5 110.5 114.5\nattr(,\"scaled:scale\")\n[1] 1.290994 1.290994 1.290994 1.290994\n\n\n\nsubtract from the mean of column i from the values in that column of matA:\n\n\n\nCode\nscale(matA, center = TRUE, scale = FALSE)\n\n\n     [,1] [,2] [,3] [,4]\n[1,] -1.5 -1.5 -1.5 -1.5\n[2,] -0.5 -0.5 -0.5 -0.5\n[3,]  0.5  0.5  0.5  0.5\n[4,]  1.5  1.5  1.5  1.5\nattr(,\"scaled:center\")\n[1]  2.5  6.5 10.5 14.5\n\n\nUsing the apply() function and simplifying your results:\n\ndetermine the quantiles for every column of matB:\n\n\n\nCode\napply(matB, MARGIN = 2, FUN = quantile, simplify = TRUE)\n\n\n       [,1]   [,2]   [,3]   [,4]\n0%   101.00 105.00 109.00 113.00\n25%  101.75 105.75 109.75 113.75\n50%  102.50 106.50 110.50 114.50\n75%  103.25 107.25 111.25 115.25\n100% 104.00 108.00 112.00 116.00\n\n\n\ndetermine the quantiles for every row of matB:\n\n\n\nCode\napply(matB, MARGIN = 1, FUN = quantile, simplify = TRUE)\n\n\n     [,1] [,2] [,3] [,4]\n0%    101  102  103  104\n25%   104  105  106  107\n50%   107  108  109  110\n75%   110  111  112  113\n100%  113  114  115  116\n\n\n\nfind the location of the minimum for every column of matA (what do you expect?)\n\n\n\nCode\napply(matA, MARGIN = 2, FUN = which.min, simplify = TRUE)\n\n\n[1] 1 1 1 1\n\n\n\nrescale the values every row in matA by subtracting the minimum of that row and dividing the the difference between the maximum and mimimum for that row\n\n\n\nCode\napply(matA, 1, function(x) (x - mean(x))/(max(x) - min(x)), simplify = TRUE)\n\n\n           [,1]       [,2]       [,3]       [,4]\n[1,] -0.5000000 -0.5000000 -0.5000000 -0.5000000\n[2,] -0.1666667 -0.1666667 -0.1666667 -0.1666667\n[3,]  0.1666667  0.1666667  0.1666667  0.1666667\n[4,]  0.5000000  0.5000000  0.5000000  0.5000000\n\n\n\nsubtract, from every value in every column in matB the median value for the column and divide by the standard deviation of the column:\n\n\n\nCode\napply(matB, 2, \\(x) (x - median(x)/sd(x)), simplify = TRUE)\n\n\n         [,1]     [,2]     [,3]     [,4]\n[1,] 21.60384 22.50545 23.40707 24.30868\n[2,] 22.60384 23.50545 24.40707 25.30868\n[3,] 23.60384 24.50545 25.40707 26.30868\n[4,] 24.60384 25.50545 26.40707 27.30868\n\n\n\ndetermine for every column of matB if its mean is different from 101 at the 5% level using Student’s t-distribution with 3 degree of freedom. Show the resuls as TRUE is the mean is different and FALSE otherwise. Do so in one line of code within the apply() function.\n\n\n\nCode\napply(matB, 2, function(x) (pt(mean(x)-101, df = 3, lower.tail = FALSE)) &lt;= 0.05)\n\n\n[1] FALSE  TRUE  TRUE  TRUE\n\n\nUsing matC\n\nvec1 &lt;- sample(c(letters, LETTERS), 16)\nvec2 &lt;- sample(c(letters, LETTERS), 16)\nmatC &lt;- matrix(paste(vec1, vec2, sep = \"_\"), 8, 2)\nmatC\n\n     [,1]  [,2] \n[1,] \"V_O\" \"d_i\"\n[2,] \"W_u\" \"M_R\"\n[3,] \"R_c\" \"t_L\"\n[4,] \"q_W\" \"s_M\"\n[5,] \"S_F\" \"i_P\"\n[6,] \"k_r\" \"Y_Q\"\n[7,] \"c_t\" \"f_K\"\n[8,] \"e_G\" \"H_s\"\n\n\n\ndetermine for each column how many times the pattern “uppercase.lowercase” (e.g. A.b) occurs:\n\n\n\nCode\napply(matC, 2, function(x) sum(grepl(pattern = \"[A-Z]_[a-z]\", x)), simplify = TRUE)\n\n\n[1] 2 1\n\n\n\ndetermine for each row how many times the patterns “lowercase.lowercase” or “lowercase.uppercase” (e.g. n_k or n_K) occurs:\n\n\n\nCode\napply(matC, 1, function(x) sum(grepl(pattern = \"[a-z]_[a-zA-Z]\", x)), simplify = TRUE)\n\n\n[1] 1 0 1 2 1 1 2 1\n\n\nUsing matD and matE\n\nmatD &lt;- matrix(rnorm(5, 5, 10), 5, 1)\nmatE &lt;- matrix(rnorm(5, 5, 10), 5, 1)\n\n\ncalculate the transpose of matD:\n\n\n\nCode\nt(matD)\n\n\n         [,1]       [,2]    [,3]      [,4]      [,5]\n[1,] 13.75133 -0.1315757 1.75805 -8.878751 -1.445147\n\n\n\ncalculate the matrix product of the transpose of matD and matE\n\n\n\nCode\nt(matD) %*% matE\n\n\n          [,1]\n[1,] -205.5606\n\n\n\nsubtract from matD, multiply the transpose of this matrix with itself, divide by the number of rows - 1 and take the square root:\n\n\n\nCode\nmatDscale &lt;- scale(matD, center = TRUE, scale = FALSE)\nsqrt((t(matDscale) %*% matDscale)/(nrow(matD) - 1))\n\n\n         [,1]\n[1,] 8.185649\n\n\n\ncalculate the standard deviation of matD:\n\n\n\nCode\nsd(matD)\n\n\n[1] 8.185649\n\n\nWhat do you see if you compare the outcome of the last two calculation? Do you know why that is the case?",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#arrays",
    "href": "04_Data_structures.html#arrays",
    "title": "4  Data structures",
    "section": "4.3 Arrays",
    "text": "4.3 Arrays\nVectors are uni-dimensional. Matrices are two-dimensional. Arrays allow you to store data in more than two dimensions. You can think of arrays as a series of matrices of the same dimensions. Like matrices, they are homogeneous: all values in an array have the same type. In other words, matrices are a special case of arrays: they are arrays with 1 matrix.\n\n4.3.1 Creating a array\nTo create an array, you can use the array() function. This function needs the data to be stored in a array and the dimensions of the array stored in a vector. Here, you need three: nrow, ncol and nmat. The array() function read the data and determines the dimension from c(nrow, ncol, nmat). For stance, to create an array with 2 3x3 matrices, you can use\n\narr &lt;- array(1:18, c(3, 3, 2))\narr\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n\nThe array arr included 2 matrices. Both are 3x3 matrices. The first includes the values 1 to 9 and the second the values 10 to 18. Note that R stores the values first by matrix and then by column within a matrix.\nTo see if arr is an array, you check its class\n\nclass(arr)\n\n[1] \"array\"\n\n\nIn addition, to see the type of values of an array, you can use\n\ntypeof(arr)\n\n[1] \"integer\"\n\n\nIn this case, R read the values as integers. As an alternative, you can verify if arr is an array using:\n\nis.array(arr)\n\n[1] TRUE\n\n\nYou can check the dimensions of an array using\n\ndim(arr)\n\n[1] 3 3 2\n\n\nLet’s see what happens if the data in the array() function has less values than the number of elements in the array:\n\narr &lt;- array(1:6, c(3, 3, 2))\narr\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    1\n[2,]    2    5    2\n[3,]    3    6    3\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    4    1    4\n[2,]    5    2    5\n[3,]    6    3    6\n\n\nAs was the case with matrices, R uses some values in the data more than once. Here, the first matrix is filled by column. As the data only includes numeric values from 1 to 6, uses the first three values of the data, 1 to 3, again to fill the last column of the first matrix. To fill the second matrix, R continues with the values 4 to 6 to store the first column of the second matrix. To fill the last 2 columns of the second matrix, R uses the data a third time.\nIf the data include more values then there are elements in the array, e.g.\n\narr &lt;- array(1:24, c(3, 3, 2))\narr\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n\nR uses the values it needs to store the array and leave all other out. Here, there are 24 values to store in an 18 element array. R uses only the first 18.\nSuppose you have two matrices, matc1 and matc2\n\nmatc1 &lt;- matrix(1:9, 3, 3)\nmatc2 &lt;- matrix(11:19, 3, 3)\n\nYou can collect these into an array using cbind(). The function creates a new matrix adding the columns of matc2 to those of matc1. To fill the array when the data is a matrix, R starts to fill the array with the elements in all rows on the first column, then moves to all rows of the second column, … .\n\ncbind(matc1, matc2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    4    7   11   14   17\n[2,]    2    5    8   12   15   18\n[3,]    3    6    9   13   16   19\n\n\nIn other words, to fill array from 2 3x3 matrices, you can use:\n\narrc &lt;- array(cbind(matc1, matc2), c(3, 3, 2))\narrc\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   11   14   17\n[2,]   12   15   18\n[3,]   13   16   19\n\n\nYou can add names to the rows, columns and matrices in an array. You can do so in a number of ways. First you can include the names in a list in the the array() function. This lists first show the row names, than the column names followed by the matrix names:\n\narr &lt;- array(1:18, c(3, 3, 2), \n             dimnames = list(c(\"year_1\", \"year_2\", \"year_3\"), \n                             c(\"var_1\", \"var_2\", \"var_3\"), \n                             c(\"firm_1\", \"firm_2\")))\narr\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1     1     4     7\nyear_2     2     5     8\nyear_3     3     6     9\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1    10    13    16\nyear_2    11    14    17\nyear_3    12    15    18\n\n\nTo verify the names of an array, you can use dimnames(arr):\n\ndimnames(arr)\n\n[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n[[2]]\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n[[3]]\n[1] \"firm_1\" \"firm_2\"\n\n\nHere, R returns a list. To extract the names for the rows, columns or matrices, you add [1], [2] or [3]. Doing so, you extract list with their names. For instance, for the rows:\n\ndimnames(arr)[1]\n\n[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n\nSecond, you can add the names to an existing array. To do so for arrc, you can use\n\ndimnames(arrc) &lt;- list(c(\"year_1\", \"year_2\", \"year_3\"), \n                             c(\"var_1\", \"var_2\", \"var_3\"), \n                             c(\"firm_1\", \"firm_2\"))\n\nYou can verify these names in a similar way\n\ndimnames(arrc)\n\n[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n[[2]]\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n[[3]]\n[1] \"firm_1\" \"firm_2\"\n\n\nThe dimensions of an array and the dimnames are attributes of an array. To see this, we can ask R to show the attributes of arrc:\n\nattributes(arrc)\n\n$dim\n[1] 3 3 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n$dimnames[[2]]\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n$dimnames[[3]]\n[1] \"firm_1\" \"firm_2\"\n\n\nR returns a list. To access these values, you can use, e.g.\n\nattributes(arrc)$dim\n\n[1] 3 3 2\n\nattributes(arrc)$dimnames[1]\n\n[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n\n\n\n4.3.2 Subsetting an array\nTo subset and array, you can use an approach which is very similar to the approach for matrices and vectors: subsetting by position, by name of by logical condition.\n\n4.3.2.1 Subsetting by position\nTo illustrate, we will use the following array:\n\nvec1 &lt;- c(111, 211, 311, 121, 221, 321, 131, 231, 331, 112, 212, 312, 122, 222, 322, 132, 232, 332)\nrown &lt;- paste(\"year\", 1:3, sep = \"_\")\ncoln &lt;- paste(\"var\", 1:3, sep = \"_\")\nmatn &lt;- paste(\"firm\", 1:2, sep = \"_\")\narr &lt;- array(vec1, c(3, 3, 2), dimnames = list(rown, coln, matn))\narr\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111   121   131\nyear_2   211   221   231\nyear_3   311   321   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\nThe values are equal to the row number, column number and matrix number. To subset an array using index positions, you include them in [i, j, k]. The first index position refers to the row, the second to the column and the third to the matrix. For instance, to extract the value in the third row of the second column in the first matrix:\n\narr[3, 2, 1]\n\n[1] 321\n\n\nNote that R simplifies the result. In other words, [] acts as a simplifying subsetting operator. To preserve the structure, you need to add drop = FALSE. Doing so, R will keep the structure of the data:\n\narr[3, 2, 1, drop = FALSE]\n\n, , firm_1\n\n       var_2\nyear_3   321\n\n\nTo subset one value from both matrices, you can use [i, j, ]. Here, you leave the third dimension (the matrix) open. R will show the results in a simplified way unless you add drop = FALSE. For instance, the element on the first row and first column of both matrices equals:\n\narr[1, 1, ]\n\nfirm_1 firm_2 \n   111    112 \n\n\nAs you can see, R simplifies to result to a vector. Adding drop = FALSE preserves the structure of the data:\n\narr[1, 1, , drop = FALSE]\n\n, , firm_1\n\n       var_1\nyear_1   111\n\n, , firm_2\n\n       var_1\nyear_1   112\n\n\nYou can extract the values on all rows iof one column in one matrix k using `[i, , k]. For instance to subset the all values on the first row of the first matrix:\n\narr[1, , 1]\n\nvar_1 var_2 var_3 \n  111   121   131 \n\n\n[, j, k] subsets the values on all rows in column j of matrix k. For instance, to see the values for the second column of the second matrix:\n\narr[, 2, 2]\n\nyear_1 year_2 year_3 \n   122    222    322 \n\n\nIf you leave two positions open, you extract\n\none matrix (e.g. the second matrix)\n\n\narr[, , 2, drop = FALSE]\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\n\none column in all matrices (e.g. he second column)\n\n\narr[, 2, , drop = FALSE]\n\n, , firm_1\n\n       var_2\nyear_1   121\nyear_2   221\nyear_3   321\n\n, , firm_2\n\n       var_2\nyear_1   122\nyear_2   222\nyear_3   322\n\n\n\none row in all matrices (e.g. the thrid row)\n\n\narr[3, , ]\n\n      firm_1 firm_2\nvar_1    311    312\nvar_2    321    322\nvar_3    331    332\n\n\nThere are two ways to subset multiple row, columns or matrices from an array. The first uses the colon and subsets a range from x to y: x:y. For instance, rows 1 to 2 from column 1 and matrix 1 to 2:\n\narr[1:2, 1, 1:2]\n\n       firm_1 firm_2\nyear_1    111    112\nyear_2    211    212\n\n\nCollecting all rows, columns or matrixes you want to subset in a vector using c() allows you to subset these row, columns and matrices individually. For instance, subsetting rows 1 and 3 and columns 1 and 3 from matrices 1 and 2:\n\narr[c(1, 3), c(1, 3), c(1, 2)]\n\n, , firm_1\n\n       var_1 var_3\nyear_1   111   131\nyear_3   311   331\n\n, , firm_2\n\n       var_1 var_3\nyear_1   112   132\nyear_3   312   332\n\n\nUsing negative indices, you can subset all rows/columns/matrices except those with a negative index number. In the previous example, we extracted all values except row and column 2 from all matrices. You would do the same using negative index positions using:\n\narr[-2, -2, ]\n\n, , firm_1\n\n       var_1 var_3\nyear_1   111   131\nyear_3   311   331\n\n, , firm_2\n\n       var_1 var_3\nyear_1   112   132\nyear_3   312   332\n\n\n\n\n4.3.2.2 Subsetting by name\nYou can also use the names of the row, columns and matrices to subset. To do so, you include the names in quotation marks within the subsetting operator. For instance:\n\nsubset one element:\n\n\narr[\"year_1\", \"var_1\", \"firm_1\"]\n\n[1] 111\n\n\n\nsubset one row in one matrix\n\n\narr[, \"var_1\", \"firm_1\"]\n\nyear_1 year_2 year_3 \n   111    211    311 \n\n\n\nsubset one column in one matrix\n\n\narr[\"year_3\", , \"firm_2\"]\n\nvar_1 var_2 var_3 \n  312   322   332 \n\n\n\nsubset one row and one column in all matrices\n\n\narr[\"year_3\", \"var_2\", ]\n\nfirm_1 firm_2 \n   321    322 \n\n\n\nsubset one row for all columns and matrices\n\n\narr[\"year_3\", , ]\n\n      firm_1 firm_2\nvar_1    311    312\nvar_2    321    322\nvar_3    331    332\n\n\n\nsubset one column for all rows and matrices\n\n\narr[, \"var_3\", ]\n\n       firm_1 firm_2\nyear_1    131    132\nyear_2    231    232\nyear_3    331    332\n\n\n\nsubset one matrix\n\n\narr[, , \"firm_2\"]\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\n\n\n4.3.2.3 Subsetting by logical condition\nAs you could with matrices, you can subset an array with a logical array. Let’s first create a random logical array:\n\ncond &lt;- array(sample(c(TRUE, FALSE), size = 18, replace = TRUE), c(3, 3, 2))\ncond\n\n, , 1\n\n      [,1]  [,2]  [,3]\n[1,]  TRUE FALSE FALSE\n[2,] FALSE  TRUE  TRUE\n[3,] FALSE FALSE FALSE\n\n, , 2\n\n      [,1]  [,2] [,3]\n[1,] FALSE  TRUE TRUE\n[2,] FALSE FALSE TRUE\n[3,] FALSE FALSE TRUE\n\n\n\narr[cond]\n\n[1] 111 221 231 122 132 232 332\n\n\nYou can create these logical conditions in many ways. For instance, if you want to extract all values larger then 200, you can use this condition in the subsetting operator:\n\narr[arr &gt; 200]\n\n [1] 211 311 221 321 231 331 212 312 222 322 232 332\n\n\nYou can refine this condition. For instance, if you want to extract all values for the rows and matrices where the first column of the first matrix is larger than 200, you can define the following condition:\n\ncond &lt;- arr[, 1, 1] &gt; 200\ncond\n\nyear_1 year_2 year_3 \n FALSE   TRUE   TRUE \n\n\nAs you can see, there are two values in the first column of the first matrix who are larger than 200. These values are in row 2 and 3. You can now use this condition to extract the values for rows 2 and 3 for all columns and in both matrices:\n\narr[cond, , ]\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_2   211   221   231\nyear_3   311   321   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_2   212   222   232\nyear_3   312   322   332\n\n\nRecall that for a matrix, you could use grepl() to subset row or column names. With arrays, you can also subset matrix names. For instance, to extract all data (full matrix) for the matrix whose name include a digit, you can use the pattern “_[2-3]” to extract all matrices whose name end with a 2 or 3. To do so, you need the matrix names. You can extract these names using\n\ndimnames(arr)\n\n[[1]]\n[1] \"year_1\" \"year_2\" \"year_3\"\n\n[[2]]\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n[[3]]\n[1] \"firm_1\" \"firm_2\"\n\n\nThe output of this function is a list. To extract the values of the third variable in this list, you can use the double subsetting operator [[ ]]: dimnames[[3]]. We”ll cover that operator more in depth when we discuss lists. Now you have all the information you need to extract the values:\n\narr[, , grepl(pattern = \"_[2-3]\", x = dimnames(arr)[[3]])]\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\n\n\n\n4.3.3 Changing an array\n\n4.3.3.1 Changing elements of an array\nAs you could with matrices, you can change an individual value or a range of values by subsetting that value or range and reassigning a different value. For instance, to multiply all values in the second column of the first matrix with 10:\n\narr[, 2, 1] &lt;- arr[, 2, 1] * 10\narr[, , 1]\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\n\n\n\n\n\n4.3.4 Changing an array’s dimensions\n\n4.3.4.1 Adding matrices to an array\nTo add a matrix to an array, you can use the abind() function of the {abind} package. This package is usually installed. Let’s first define a new array, arr1. We know that we will add it to arr. In other words, we can use the names of the rows and columns in arr to create the names for the rows and columns in the new array arr1. To do so, we use dimnames(arr)[[1]] for the row names and dimnames(arr)[[2]] for the column names. To be consistent with the naming of matrices, I”ll use “firm_3” for the matrix name. Using array():\n\narr1 &lt;- array(c(113, 213, 313, 123, 223, 323, 133, 233, 333), c(3, 3, 1), dimnames = list(dimnames(arr)[[1]],dimnames(arr)[[2]], c(\"firm_3\")))\narr1\n\n, , firm_3\n\n       var_1 var_2 var_3\nyear_1   113   123   133\nyear_2   213   223   233\nyear_3   313   323   333\n\n\nThe abind() function has many options. Here, we will keep all default values and add the matrix as the last matrix in the array. To do so with the abind function uses:\n\nabind::abind(arr, arr1)\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n, , firm_3\n\n       var_1 var_2 var_3\nyear_1   113   123   133\nyear_2   213   223   233\nyear_3   313   323   333\n\n\nAs you can see, the array has 3 matrices: firm_1, firm_2 and firm_3. Here, I used an array, but you can also add a matrix.\nA second way starts from the deconstruction of the array. Recall that c() applies to a matrix turns the matrix into a vector. The same holds for an array. After deconstruction, you can append that vector with your new values for your matrix. Doing so, you have all the elements that you need to rebuild an array. For instance,\n\narr_new &lt;- array(cbind(c(arr), c(113, 213, 313, 123, 223, 323, 133, 233, 333)), c(3, 3, 3), dimnames = list(dimnames(arr)[[1]],dimnames(arr)[[2]], c(dimnames(arr)[[3]], \"firm_3\")))\narr_new\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n, , firm_3\n\n       var_1 var_2 var_3\nyear_1   113   123   133\nyear_2   213   223   233\nyear_3   313   323   333\n\n\nTo add rows or columns to the matrices, you can first collect them in a separate matrix:\n\nmat_1 &lt;- arr[, , 1]\nmat_2 &lt;- arr[, , 2]\n\nUsing cbind() or rbind() you can now add new rows or columns. For instance, let’s add c(411, 4210, 431) to the first matrix and c(412, 422, 432) to the second:\n\nmat_1 &lt;- rbind(mat_1, c(411, 4210, 431))\nmat_2 &lt;- rbind(mat_2, c(412, 422, 432))\n\nYou can now change the array arr:\n\narr &lt;- array(cbind(mat_1, mat_2), c(4, 3, 2),dimnames = list(c(dimnames(arr)[[1]], \"year_4\"),dimnames(arr)[[2]], c(dimnames(arr)[[3]])))\narr\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\nyear_4   411  4210   431\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\nyear_4   412   422   432\n\n\nThe fourth row is now added to both matrices.\n\n\n4.3.4.2 Removing elements from an array\nRemoving parts of an array can be done using negative indices. However, in that case, you need to make sure that the dimensions of the various matrices stay equal. For instance, to remove the fourth row from all matrices in arr:\n\narr[-4, , ]\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\nTo remove a matrix (e.g. the third) from arr_new:\n\narr_new[, , -3]\n\n, , firm_1\n\n       var_1 var_2 var_3\nyear_1   111  1210   131\nyear_2   211  2210   231\nyear_3   311  3210   331\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\n\n\n\n\n\n4.3.5 Applying functions to an array\nAs most functions are vectorized, most will apply to each element of an array. For instance\n\nnatural logarithm:\n\n\nlog(arr)\n\n, , firm_1\n\n          var_1    var_2    var_3\nyear_1 4.709530 7.098376 4.875197\nyear_2 5.351858 7.700748 5.442418\nyear_3 5.739793 8.074026 5.802118\nyear_4 6.018593 8.345218 6.066108\n\n, , firm_2\n\n          var_1    var_2    var_3\nyear_1 4.718499 4.804021 4.882802\nyear_2 5.356586 5.402677 5.446737\nyear_3 5.743003 5.774552 5.805135\nyear_4 6.021023 6.045005 6.068426\n\n\n\npower (e.g. 2)\n\n\narr^2\n\n, , firm_1\n\n        var_1    var_2  var_3\nyear_1  12321  1464100  17161\nyear_2  44521  4884100  53361\nyear_3  96721 10304100 109561\nyear_4 168921 17724100 185761\n\n, , firm_2\n\n        var_1  var_2  var_3\nyear_1  12544  14884  17424\nyear_2  44944  49284  53824\nyear_3  97344 103684 110224\nyear_4 169744 178084 186624\n\n\n\nsquare root\n\n\nsqrt(arr)\n\n, , firm_1\n\n          var_1    var_2    var_3\nyear_1 10.53565 34.78505 11.44552\nyear_2 14.52584 47.01064 15.19868\nyear_3 17.63519 56.65686 18.19341\nyear_4 20.27313 64.88451 20.76054\n\n, , firm_2\n\n          var_1    var_2    var_3\nyear_1 10.58301 11.04536 11.48913\nyear_2 14.56022 14.89966 15.23155\nyear_3 17.66352 17.94436 18.22087\nyear_4 20.29778 20.54264 20.78461\n\n\n\nexpontential:\n\n\nexp(arr)\n\n, , firm_1\n\n               var_1 var_2         var_3\nyear_1  1.609487e+48   Inf  7.808671e+56\nyear_2  4.326490e+91   Inf 2.099062e+100\nyear_3 1.163011e+135   Inf 5.642525e+143\nyear_4 3.126310e+178   Inf 1.516777e+187\n\n, , firm_2\n\n               var_1         var_2         var_3\nyear_1  4.375039e+48  9.636666e+52  2.122617e+57\nyear_2  1.176062e+92  2.590449e+96 5.705843e+100\nyear_3 3.161392e+135 6.963429e+139 1.533797e+144\nyear_4 8.498192e+178 1.871851e+183 4.123027e+187\n\n\nAfter subsetting the appropriate matrix, you can apply these function to one or more matrices. If you reassign their values, these matrices will also change in the array:\n\narr[, , 1] &lt;- log(arr[, , 1])\narr\n\n, , firm_1\n\n          var_1    var_2    var_3\nyear_1 4.709530 7.098376 4.875197\nyear_2 5.351858 7.700748 5.442418\nyear_3 5.739793 8.074026 5.802118\nyear_4 6.018593 8.345218 6.066108\n\n, , firm_2\n\n       var_1 var_2 var_3\nyear_1   112   122   132\nyear_2   212   222   232\nyear_3   312   322   332\nyear_4   412   422   432\n\n\nYou can calculate the column means and column sums (or their equivalent row function) using colMeans(). When we introduced this function for matrices, we disregarded the dims argument. Here this argument plays a role. dims = 1 shows the means per column and per matrix:\n\ncolMeans(arr_new, dims = 1)\n\n      firm_1 firm_2 firm_3\nvar_1    211    212    213\nvar_2   2210    222    223\nvar_3    231    232    233\n\n\nChanging this into dims = 2 calculated means for all values per matrix:\n\ncolMeans(arr_new, dims = 2)\n\nfirm_1 firm_2 firm_3 \n   884    222    223 \n\n\nWhether you need the first or the second option, depends on the data in the matrices. Here, if matrices refer to firms, variables to e.g. revenue, profit or market capitalization and the rows to years, an average across all variables per firm doesn’t make sense. However, if you data refers to measurements (e.g. temperature) per hour and location where each matrix is a day, an average across all measurements per day does make sense: it is the average daily temperature in e.g. a country.\ncolSums, rowSums and rowMeans work in a similar way.\nThe apply() function with MARGIN = 2 applies a function FUN to all columns of an array. For instance, the average for the all the columns across all matrices in arr_new can be calculated as\n\napply(arr_new, 2, mean)\n\nvar_1 var_2 var_3 \n  212   885   232 \n\n\nTo use the apply function per matrix, you’ll have to write a for loop. We will discuss loops more in depth in Chapter 13, but the overall setup of a loop is straightforward. The first part if for (i in c(1, 2, 3)). Here i will first take the first value in c(1, 2, 3) i.e. i will be 1? The second part of the loop includes the statement that R needs to execute. For instance: k &lt;- i^2. R will calculate the square of k and assign it to k. If R finishes with the code, it moves back to i in c(1, 2, 3) and changes to value from 1 in 2. It now executes the code with i = 2. Here, we use the fact that we can determine the number of matrices from dim(arr) The third position in that vector shows the number of matrices. This allows us to determine how many loops the for loop will make. The code R needs to execute is the apply() function. All we need to do is store the results in a separate matrix. With respect to the dimensions: the apply functio will generate a mean for every variable and for every matrix. If you store the means per matrix in a separate row, we need as many columns in the matrix as we have columns in the array and as many rows as there are matrices in the array. We are now in a position to write the loop. First we create the matrix for the results:\n\nnc &lt;- dim(arr_new)[2]\nnr &lt;- dim(arr_new)[3]\nmatrix_mean &lt;- matrix(0, nr, nc)\n\n# add column names and row names\n# column names are the names in the array\n# row names are the names of the matrices in the array\n\ncolnames(matrix_mean) &lt;- dimnames(arr_new)[[2]]\nrownames(matrix_mean) &lt;- dimnames(arr_new)[[3]]\n\nWe can use this matrix to store the results as we apply the apply() function across all matrices in the array:\n\nfor (i in 1:dim(arr_new)[3]) {\n  matrix_mean[i, ] &lt;- apply(arr_new[, , i], 2, mean)\n}\n\nTo see the results for the mean per variable and per matrix, you can check:\n\nmatrix_mean\n\n       var_1 var_2 var_3\nfirm_1   211  2210   231\nfirm_2   212   222   232\nfirm_3   213   223   233\n\n\nIn a similar way, you can use the apply() function for all other functions, including your own.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nFirst create an 4x3x2 array (24 values) arr1 filles with c(1:24)\n\n\nCode\narr1 &lt;- array(1:24, c(4, 3, 2))\narr1\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n\nUsing 2 4x3 matrices, mat1 and mat2, the first including c(1:12) and the second including c(13:24), create an array arr2 with these two matrices.\n\n\nCode\nmat1 &lt;- matrix(1:12, 4, 3)\nmat2 &lt;- matrix(13:24, 4, 3)\n\narr2 &lt;- array(cbind(mat1, mat2), c(4, 3, 2))\narr2\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n\nSet names for the rows (obs_1, obs_2, …), the columns (var_1, var_2, …) and the matrices (mat_1, mat_2) of arr1.\n\n\nCode\ndimnames(arr1) &lt;- list(c(\"obs_1\", \"obs_2\", \"obs_3\", \"obs_4\"), \n                        c(\"var_1\", \"var_2\", \"var_3\"), \n                        c(\"mat_1\", \"mat_2\"))\n\n\nCheck the attributes of arr1.\n\n\nCode\nattributes(arr1)\n\n\n$dim\n[1] 4 3 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"obs_1\" \"obs_2\" \"obs_3\" \"obs_4\"\n\n$dimnames[[2]]\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n$dimnames[[3]]\n[1] \"mat_1\" \"mat_2\"\n\n\nUsing arr2, extract\n\nthe value on the second row of the second column in the second matrix:\n\n\n\nCode\narr2[2, 2, 2]\n\n\n[1] 18\n\n\n\nall values of on the first row of the first matrix:\n\n\n\nCode\narr2[1, , 1]\n\n\n[1] 1 5 9\n\n\n\nall values in the third column of the second matrix:\n\n\n\nCode\narr2[, 3, 2]\n\n\n[1] 21 22 23 24\n\n\n\nall values in the first row and the second column of both matrices:\n\n\n\nCode\narr2[1, 2, ]\n\n\n[1]  5 17\n\n\n\nall values in the second and third column of the first matrix.\n\n\n\nCode\narr2[, 1:2, 1]\n\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\n\nall values but those in the first row of both matrices:\n\n\n\nCode\narr2[-1, , ]\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    3    7   11\n[3,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   14   18   22\n[2,]   15   19   23\n[3,]   16   20   24\n\n\nUsing names, extract the values in arr1\n\nthe first row and second column of both matrices\n\n\n\nCode\narr1[\"obs_1\", \"var_2\", ]\n\n\nmat_1 mat_2 \n    5    17 \n\n\n\nthe values in the second matrix\n\n\n\nCode\narr1[, , \"mat_2\"]\n\n\n      var_1 var_2 var_3\nobs_1    13    17    21\nobs_2    14    18    22\nobs_3    15    19    23\nobs_4    16    20    24\n\n\nExtract all values larger than 15 from arr2\n\n\nCode\narr2[arr2 &gt; 15]\n\n\n[1] 16 17 18 19 20 21 22 23 24\n\n\nCreate a 4x3 matrix, mat_3, filled with c(25:36)\n\n\nCode\nmat_3 &lt;- matrix(25:36, 4, 3)\n\n\nAdd this matrix to arr2\n\n\nCode\narr2 &lt;- abind::abind(arr2, mat_3)\n\n\nRemove the fourth row of each matrix in arr2.\n\n\nCode\narr2 &lt;- arr2[-4, , ]\narr2\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   25   29   33\n[2,]   26   30   34\n[3,]   27   31   35\n\n\nRemove the third matrix from arr2\n\n\nCode\narr2[, , -3]\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n\n\nCalculate the column means for every column in each matrix of arr1.\n\n\nCode\ncolMeans(arr1, dims = 1)\n\n\n      mat_1 mat_2\nvar_1   2.5  14.5\nvar_2   6.5  18.5\nvar_3  10.5  22.5\n\n\nCalculate the column sum for every column in each matrix of arr1.\n\n\nCode\ncolSums(arr1, dims = 1)\n\n\n      mat_1 mat_2\nvar_1    10    58\nvar_2    26    74\nvar_3    42    90\n\n\nUse the apply() function to calculate for each column in arr1 the value: (x - min(x)/(max(x)- min(x))). Write your code in such a way that you can apply it to other arrays with different dimensions. You have to write a for loop. This statement includes for (i in ...) {apply(...)}. Store the results in an array res.\n\n\nCode\nres &lt;- array(0, c(4, 3, 2))\nfor (i in 1:dim(arr1)[3]) {\n  res[, , i] &lt;- apply(arr1[, , i], 2, function(x) (x - min(x))/(max(x) - min(x)))\n}\nres\n\n\n, , 1\n\n          [,1]      [,2]      [,3]\n[1,] 0.0000000 0.0000000 0.0000000\n[2,] 0.3333333 0.3333333 0.3333333\n[3,] 0.6666667 0.6666667 0.6666667\n[4,] 1.0000000 1.0000000 1.0000000\n\n, , 2\n\n          [,1]      [,2]      [,3]\n[1,] 0.0000000 0.0000000 0.0000000\n[2,] 0.3333333 0.3333333 0.3333333\n[3,] 0.6666667 0.6666667 0.6666667\n[4,] 1.0000000 1.0000000 1.0000000\n\n\n\n\n\n\n #| echo: false\n #| error: false\n #| message: false\n #| output: false\n #| warning: false\n\nrm(arr, matc1, matc2, arr_new, arrc, matrix_mean, nc, nr)",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#lists",
    "href": "04_Data_structures.html#lists",
    "title": "4  Data structures",
    "section": "4.4 Lists",
    "text": "4.4 Lists\nLists are widely used in R. In the previous section we referred to lists a couple of times. For instance, str_extract_all returns a list by default. Likewise, the apply() function returns a list unless you add simplify = TRUE. The attributes of a matrix are shown in a list. Here we add more depth. With lists we move from homogeneous data structures to heterogeneous data structures. Heterogeneous datas tructures can be used to store various types of data.\n\n4.4.1 What are lists?\nLike vectors, lists are uni-dimensional. Unlike vectors, matrices or arrays, they can be used to store various data types. In a list, you store vectors, matrices, characters, formulas, plots or other lists or arrays. In other words, every element in a list can have both a different type as well as different dimensions. As a result, lists are a very flexible way of storing a wide variety of data into one data structure and are used to store, e.g. hierarchical data and to organize complete datasets, to store output from formulas or functions. For instance, the dimnames() function for arrays shows a complex data structure including the dimensions of an array as well as the names of the columns, rows and matrices. The first are numeric, the second are character variables. The first include 3 elements: the number of rows, columns and matrices while the names can be as short as one and further take one any size.\n\n\n4.4.2 Non-nested list\nA non-nested list is a list that doesn’t include any other lists. In other words, the element of this list are e.g. matrices, vectors or character variables. Suppose you have the following data per student: the name, student number, a logical indicator for exchange students, the program in which the student is enrolled and information on the student’s courses in his or her individual program including their name, ects and lecture hours. These data are stored in various data structures:\n\nstudent &lt;- \"Alice Wonderland\"\nstudentnr &lt;- \"r00369258\"\nprogram &lt;- \"Bachelor business adminstration\"\nexchange = F\ncourse &lt;- c(\"Data and programming skills\", \"Strategic management\", \"Macro-economics and economic policy\", \"Economic sociology\", \"Introduction to methods for operational research\")\nects &lt;- c(6, 3, 6, 3, 3)\nhours &lt;- c(52, 26, 52, 26, 26)\n\nFrom the previous section, you should recognize these structures as a character variable, a character vector, a logical value and numeric vectors.\nTo create a list, you can use the list() function. This functions main arguments are the objects to store in the list. These objects could be named, but for now, we’ll add no names. We can add all these structures to a list using:\n\nstud1 &lt;- list(student, studentnr, program, exchange, course, ects, hours)\n\nLet’s first inspect the structure of this list using str():\n\nstr(stud1)\n\nList of 7\n $ : chr \"Alice Wonderland\"\n $ : chr \"r00369258\"\n $ : chr \"Bachelor business adminstration\"\n $ : logi FALSE\n $ : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Economic sociology\" ...\n $ : num [1:5] 6 3 6 3 3\n $ : num [1:5] 52 26 52 26 26\n\n\nHere, you can see that this list as 7 elements: 4 with type character, 2 with type numeric and 1 with type logical. As you can see, lists can store elements with various types. We can also inspect the list by printing it:\n\nstud1\n\n[[1]]\n[1] \"Alice Wonderland\"\n\n[[2]]\n[1] \"r00369258\"\n\n[[3]]\n[1] \"Bachelor business adminstration\"\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n[[6]]\n[1] 6 3 6 3 3\n\n[[7]]\n[1] 52 26 52 26 26\n\n\nHere, you see that stud1 has two levels: the first is the level of the 7 elements in that list, the second level are the individual elements of each of the 7 elements. The highest hierarchy is shown with double square brackets [[ ]]. The second level is shown with one square bracket [ ] You can verify the class and type of stud1 using\n\nclass(stud1)\n\n[1] \"list\"\n\ntypeof(stud1)\n\n[1] \"list\"\n\n\nAs you can see, both show “list”. You can determine the number of components in a list using the length() function. Here, stud1 has 7 components. To check this, you can use\n\nlength(stud1)\n\n[1] 7\n\n\nA lot of the functions that we saw in the previous sections that return a list, return a non named list. For instance ’str_extract_all() returns\n\nchar &lt;- c(\"Fair if foul and foul is fair.\",  \"Hover through the fog and filthy air.\")\nstringr::str_extract_all(char, pattern = \"fair|fog|filthy\")\n\n[[1]]\n[1] \"fair\"\n\n[[2]]\n[1] \"fog\"    \"filthy\"\n\n\nThey do this because the results of these function is often not compatible with a matrix or vector. For instance, here, you have two matches: one with 1 element (fair) and one with 2 elements (fog and filthy both appear in the second element of the character vector). To store these results, you need a list.\nYou can add a name to the elements of a list by adding them in the list() function. For instance:\n\nstud1 &lt;- list(name = student, \n                 number = studentnr,\n                 program = program,\n                 exchange = exchange,\n                 course  = course,\n                 hours = hours, \n                 ects = ects)\n\nIf you check the structure of the list, you can now see the names of that list:\n\nstr(stud1)\n\nList of 7\n $ name    : chr \"Alice Wonderland\"\n $ number  : chr \"r00369258\"\n $ program : chr \"Bachelor business adminstration\"\n $ exchange: logi FALSE\n $ course  : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Economic sociology\" ...\n $ hours   : num [1:5] 52 26 52 26 26\n $ ects    : num [1:5] 6 3 6 3 3\n\n\nPrinting the list also reveals their names\n\nstud1\n\n$name\n[1] \"Alice Wonderland\"\n\n$number\n[1] \"r00369258\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] FALSE\n\n$course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$hours\n[1] 52 26 52 26 26\n\n$ects\n[1] 6 3 6 3 3\n\n\nTo extract the names in the list, you can use names().\n\nnames(stud1)\n\n[1] \"name\"     \"number\"   \"program\"  \"exchange\" \"course\"   \"hours\"    \"ects\"    \n\n\nSome function in R return a named list. For instance the attributes() function shows the attributes of a vector or a matrix as a names list:\n\nattributes(matrix(c(10, 20, 30, 40), 2, 2, dimnames = list(c(\"obs1\", \"obs2\"), c(\"var1\", \"var2\"))))\n\n$dim\n[1] 2 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"obs1\" \"obs2\"\n\n$dimnames[[2]]\n[1] \"var1\" \"var2\"\n\n\nAgain note that attibutes() returns a list as it wouldn’t be possible to show that result otherwise as it mixes characters and numeric values.\n\n\n4.4.3 Nested lists\nInside a list, you can have lists. In that case, lists are nested. let’s add two new students and store their data in lists stud2 and stud3:\n\nstudent &lt;- \"Bart Vader\"\nstudentnr &lt;- \"r00362958\"\nprogram &lt;- \"Bachelor business adminstration\"\nexchange = F\ncourse &lt;- c(\"Data and programming skills\", \"Strategic management\", \"Macro-economics and economic policy\", \"Financial statement analysis\", \"Entrepreneurship and business planning\")\nects &lt;- c(6, 3, 6, 6, 3)\nhours &lt;- c(52, 26, 52, 52, 26)\n\nstud2 &lt;- list(name = student, \n             number = studentnr,\n             program = program,\n             exchange = exchange,\n             course  = course,\n             hours = hours, \n             ects = ects)\n\nstudent &lt;- \"Clark Kent\"\nstudentnr &lt;- \"r00362478\"\nprogram &lt;- \"Bachelor business adminstration\"\nexchange = T\ncourse &lt;- c(\"Macro-economics and economic policy\", \"Economic sociology\", \"Entrepreneurship and business planning\", \"Financial accouing B\", \"Mathematics for business B\")\nects &lt;- c(6, 3, 3, 3, 3)\nhours &lt;- c(52, 26, 26, 26, 26)\n\nstud3 &lt;- list(name = student, \n             number = studentnr,\n             program = program,\n             exchange = exchange,\n             course  = course,\n             hours = hours, \n             ects = ects)\n\nUsing list() we can add these three students in one list and give each list a name\n\nallstud &lt;- list(student1 = stud1, \n                 student2 = stud2, \n                 student3 = stud3)\n\nNote that the three lists here include the same components. However, this is not necessary. A nested list can include lists with various components.\nFrom the structure of the list\n\nstr(allstud)\n\nList of 3\n $ student1:List of 7\n  ..$ name    : chr \"Alice Wonderland\"\n  ..$ number  : chr \"r00369258\"\n  ..$ program : chr \"Bachelor business adminstration\"\n  ..$ exchange: logi FALSE\n  ..$ course  : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Economic sociology\" ...\n  ..$ hours   : num [1:5] 52 26 52 26 26\n  ..$ ects    : num [1:5] 6 3 6 3 3\n $ student2:List of 7\n  ..$ name    : chr \"Bart Vader\"\n  ..$ number  : chr \"r00362958\"\n  ..$ program : chr \"Bachelor business adminstration\"\n  ..$ exchange: logi FALSE\n  ..$ course  : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Financial statement analysis\" ...\n  ..$ hours   : num [1:5] 52 26 52 52 26\n  ..$ ects    : num [1:5] 6 3 6 6 3\n $ student3:List of 7\n  ..$ name    : chr \"Clark Kent\"\n  ..$ number  : chr \"r00362478\"\n  ..$ program : chr \"Bachelor business adminstration\"\n  ..$ exchange: logi TRUE\n  ..$ course  : chr [1:5] \"Macro-economics and economic policy\" \"Economic sociology\" \"Entrepreneurship and business planning\" \"Financial accouing B\" ...\n  ..$ hours   : num [1:5] 52 26 26 26 26\n  ..$ ects    : num [1:5] 6 3 3 3 3\n\n\nyou can now see that this list has 3 levels: the first includes the three lists for every student. The second level shows the list per student and the third level includes the individual values for each list component. These last two levels coincide with the components of stud1, stud2 and stud3. You could add more lists. For instance, you could define a list with course data including the course, the hours and ects vectors and store these vectors in a seperate list. In that case, you would add a hierarchy.\nHere, the function names() returns the names of the highest hierarchy:\n\nnames(allstud)\n\n[1] \"student1\" \"student2\" \"student3\"\n\n\nand length() shows the number of components in the highest hierarchy:\n\nlength(allstud)\n\n[1] 3\n\n\nA special case of lists are plots. Recall the plots with the random draws from various distributions, e.g.\n\nhist(v_norm &lt;- rnorm(n = 100, mean = 0, sd = 1), \n     probability = TRUE, \n     col = \"lightblue\", \n     border = \"white\", \n     xlab = \"Value\", \n     main = \"Normal\")\n\n\n\n\n\n\n\n\nYou can assign this plot to an object, plot_norm:\n\nplot_norm &lt;- hist(v_norm &lt;- rnorm(n = 100, mean = 0, sd = 1), \n                  probability = TRUE, \n                  col = \"lightblue\", \n                  border = \"white\", \n                  xlab = \"Value\", \n                  main = \"Normal\")\n\nNow check the type of this plot\n\ntypeof(plot_norm)\n\n[1] \"list\"\n\n\nAs you can see, this plot is stored as a list. In other words, if you store plots in a list, you are using nested lists.\n\n\n4.4.4 Unlist\nThe function unlist(x, recursive = TRUE, use.names = TRUE) simplifies the list structure the returns all the individual components of the list. The option recurive = TRUE by default will apply this function to all components of the list. With nested lists, this default option unlists all lists within the list. The last option use.names = TRUE by default preserves the names. To see what this function does, let’s apply it to stud1. As we don’t have any lists within stud1 the option recursive is not applicable. Unlisting stud1 returns:\n\nunlist(stud1)\n\n                                              name \n                                \"Alice Wonderland\" \n                                            number \n                                       \"r00369258\" \n                                           program \n                 \"Bachelor business adminstration\" \n                                          exchange \n                                           \"FALSE\" \n                                           course1 \n                     \"Data and programming skills\" \n                                           course2 \n                            \"Strategic management\" \n                                           course3 \n             \"Macro-economics and economic policy\" \n                                           course4 \n                              \"Economic sociology\" \n                                           course5 \n\"Introduction to methods for operational research\" \n                                            hours1 \n                                              \"52\" \n                                            hours2 \n                                              \"26\" \n                                            hours3 \n                                              \"52\" \n                                            hours4 \n                                              \"26\" \n                                            hours5 \n                                              \"26\" \n                                             ects1 \n                                               \"6\" \n                                             ects2 \n                                               \"3\" \n                                             ects3 \n                                               \"6\" \n                                             ects4 \n                                               \"3\" \n                                             ects5 \n                                               \"3\" \n\n\nThe output shows all individual components. Note that e.g. course, which is a character vector, is simplfied to its individual elements. R labels these elements as e.g. course1, course2, … . Likewise, hours, a numeric vector, is shown as individual elements with name hours1, hours2, … .\nApplied to allstud, a nested list and using recursive = FALSE, returns the individual components of the three lists as one long list. The names of the highest hierarchy in addstud is used to construct names. Using unlist(allstud, recursive = TRUE) returns:\n\nunlist(allstud, recursive = FALSE)\n\n$student1.name\n[1] \"Alice Wonderland\"\n\n$student1.number\n[1] \"r00369258\"\n\n$student1.program\n[1] \"Bachelor business adminstration\"\n\n$student1.exchange\n[1] FALSE\n\n$student1.course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$student1.hours\n[1] 52 26 52 26 26\n\n$student1.ects\n[1] 6 3 6 3 3\n\n$student2.name\n[1] \"Bart Vader\"\n\n$student2.number\n[1] \"r00362958\"\n\n$student2.program\n[1] \"Bachelor business adminstration\"\n\n$student2.exchange\n[1] FALSE\n\n$student2.course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$student2.hours\n[1] 52 26 52 52 26\n\n$student2.ects\n[1] 6 3 6 6 3\n\n$student3.name\n[1] \"Clark Kent\"\n\n$student3.number\n[1] \"r00362478\"\n\n$student3.program\n[1] \"Bachelor business adminstration\"\n\n$student3.exchange\n[1] TRUE\n\n$student3.course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n$student3.hours\n[1] 52 26 26 26 26\n\n$student3.ects\n[1] 6 3 3 3 3\n\n\nYou can see that this is a list if you use is.list():\n\nis.list(unlist(allstud, recursive = FALSE))\n\n[1] TRUE\n\n\nNote that all names include a dot “.” to separate the name of the list (e.g. student1) and the name of the component (e.g. name). Using names() you can select the names:\n\nnames(unlist(allstud, recursive = FALSE))\n\n [1] \"student1.name\"     \"student1.number\"   \"student1.program\" \n [4] \"student1.exchange\" \"student1.course\"   \"student1.hours\"   \n [7] \"student1.ects\"     \"student2.name\"     \"student2.number\"  \n[10] \"student2.program\"  \"student2.exchange\" \"student2.course\"  \n[13] \"student2.hours\"    \"student2.ects\"     \"student3.name\"    \n[16] \"student3.number\"   \"student3.program\"  \"student3.exchange\"\n[19] \"student3.course\"   \"student3.hours\"    \"student3.ects\"    \n\n\nIn case of nested lists, the default recursive  = TRUE will simplify every list in the nested list. In other words, the output will be similar to the one for unlisting unnested lists.\n\n\n4.4.5 Subsetting a list\n\n4.4.5.1 Subsetting non-nested lists\nTo subset a list, you can use index positions using both the [] subsetting operator as well as the double square brackets operator [[]]. Let’ start with the first: [] and extract the first element of stud1, the list with the data on the first student Alice Wonderland:\n\nstud1[1]\n\n$name\n[1] \"Alice Wonderland\"\n\n\nAs you can see, this operator returns the first component of stud1 and does so as a list. In other words, [] preserves the structure of the data. You can see this from the output (which refers to the $name) as well as from the class of the output:\n\nclass(stud1[1])\n\n[1] \"list\"\n\n\nThe double square brackets [[]]are a simplifying operator. They simplify the result as much as possible e.g. to a numeric vector, a character vector, a logical value … . For instance, let’s use the [[]] to extract the first element of stud1:\n\nstud1[[1]]\n\n[1] \"Alice Wonderland\"\n\n\nRecall that the preserving subsetting operator returned a list, here R simplifies to a character variable.\n\nclass(stud1[[1]])\n\n[1] \"character\"\n\n\nLet’s now subset the sixth element of stud1, the hours for each course. Using the single square brackets, R returns a list:\n\nstud1[6]\n\n$hours\n[1] 52 26 52 26 26\n\nclass(stud1[6])\n\n[1] \"list\"\n\n\nwhile the the simplifying operator returns a numeric vector:\n\nstud1[[6]]\n\n[1] 52 26 52 26 26\n\nis.vector(stud1[[6]])\n\n[1] TRUE\n\nclass(stud1[[6]])\n\n[1] \"numeric\"\n\n\nTo subset this vector, you start from the simplifying operator. As this operator creates a vector, you can now use the subsetting rules for a vector. Here, the vector you subset is stud1[[6]]. To subset the first element, you add [1]:\n\nstud1[[6]][1]\n\n[1] 52\n\n\nYou can now use all subsetting rules for vectors, e.g.\n\na range:\n\n\nstud1[[6]][1:4]\n\n[1] 52 26 52 26\n\n\n\nall but the first:\n\n\nstud1[[6]][-1]\n\n[1] 26 52 26 26\n\n\n\na logical condition:\n\n\nstud1[[6]][stud1[[6]] &gt; 30]\n\n[1] 52 52\n\n\nIf the list is named, you can also use the names and add them between quotation marks in the preserving subsetting operator [] or the simplifying operator [[]]. The first returns a list, the second simplifies to output. To extract the name of the student in stud2 and return a list, you can use:\n\nstud2[\"name\"]\n\n$name\n[1] \"Bart Vader\"\n\n\nSimplifying this result can be done using the simplifying subsetting operator [[]]:\n\nstud2[[\"name\"]]\n\n[1] \"Bart Vader\"\n\n\nYou can extract the value of a list and simplify the result also in a second way: you add the name of the component after the name of the list separated by the $ subsetting operator: name_of_list$name_of_element. Doing so, R simplifies the results. For instance, to subset the component ects from the list stud2, you can use:\n\nstud2$ects\n\n[1] 6 3 6 6 3\n\n\nHere, the output is simplified to a vector. In other words, stud2$ects returns the same output as stud2[[\"ects\"]]. You can now use all subsetting methods for a vector.\n\nstud2$ects[3]\n\n[1] 6\n\n\nSubsetting within an component of a list is determined by the class of that element. In the examples, R simplified to a numeric vector. If one of the elements of the list would be a matrix, you would use the subsetting rules for a matrix.\nAs was the case with vectors, matrices or arrays, a negative index position extracts all but the element that is in that position. For instance, extracting all element of stud2 except the first can be done using:\n\nstud2[-1]\n\n$number\n[1] \"r00362958\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] FALSE\n\n$course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$hours\n[1] 52 26 52 52 26\n\n$ects\n[1] 6 3 6 6 3\n\n\nTo extract multiple values, you combine them via c(). For instance, to extract the first and third element of the list stud3, you add these to the preserving operator []\n\nstud3[c(1, 3)]\n\n$name\n[1] \"Clark Kent\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n\nNote that in this case the simplifying operator doesn’t work: the output includes heterogeneous variable types. With named elements, you can also include the names of these elements:\n\nstud3[c(\"name\", \"number\")]\n\n$name\n[1] \"Clark Kent\"\n\n$number\n[1] \"r00362478\"\n\n\nUsing negative index position, you can extract all but the elements with the negative index position. For instance, extracting all elements from stud3 except the first and third:\n\nstud3[c(-1, -3)]\n\n$number\n[1] \"r00362478\"\n\n$exchange\n[1] TRUE\n\n$course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n$hours\n[1] 52 26 26 26 26\n\n$ects\n[1] 6 3 3 3 3\n\n\nYou can also use logical values to subset a list. For instance:\n\nstud1[c(TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE)]\n\n$name\n[1] \"Alice Wonderland\"\n\n$exchange\n[1] FALSE\n\n$ects\n[1] 6 3 6 3 3\n\n\nThis allows you to extract e.g. components of a list using patterns in a name. For instance, extracting a component that includes the pattern “ects” can be done using grepl() where this function searches in the vector names(stud1) for a match with the pattern “ects”:\n\nstud1[grepl(pattern = \"ects\", names(stud1))]\n\n$ects\n[1] 6 3 6 3 3\n\n\n\n\n4.4.5.2 Subsetting nested lists\nRecall that nested lists are lists that include other lists as their elements. How do you subset a list with lists? Let’s first use index positions. Using [] returns a list. For instance,\n\nallstud[1]\n\n$student1\n$student1$name\n[1] \"Alice Wonderland\"\n\n$student1$number\n[1] \"r00369258\"\n\n$student1$program\n[1] \"Bachelor business adminstration\"\n\n$student1$exchange\n[1] FALSE\n\n$student1$course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$student1$hours\n[1] 52 26 52 26 26\n\n$student1$ects\n[1] 6 3 6 3 3\n\n\nreturns the first list, stud1 but the output keeps all references to e.g. the name of stud1 within the list allstud. Simplifying using the [[]] operator removes part of the structure of stud1, e.g. the reference to $student1 but the results are still a list.\n\nallstud[[1]]\n\n$name\n[1] \"Alice Wonderland\"\n\n$number\n[1] \"r00369258\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] FALSE\n\n$course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$hours\n[1] 52 26 52 26 26\n\n$ects\n[1] 6 3 6 3 3\n\n\nNote that this shouldn’t be surprising as stud1 is a list and [[]] returns the most simplified version of this list: which is in this case a list nested in another list. As an alternative to the index position, you can also refer to the name of the list you want to extract. Adding that name to the preservering subsetting operator will extract the list while preserving the structure of the list. For instance, extracting the second list:\n\nallstud[\"student2\"]\n\n$student2\n$student2$name\n[1] \"Bart Vader\"\n\n$student2$number\n[1] \"r00362958\"\n\n$student2$program\n[1] \"Bachelor business adminstration\"\n\n$student2$exchange\n[1] FALSE\n\n$student2$course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$student2$hours\n[1] 52 26 52 52 26\n\n$student2$ects\n[1] 6 3 6 6 3\n\n\nDoing so with the simplifying operator returns the original list:\n\nallstud[[\"student2\"]]\n\n$name\n[1] \"Bart Vader\"\n\n$number\n[1] \"r00362958\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] FALSE\n\n$course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$hours\n[1] 52 26 52 52 26\n\n$ects\n[1] 6 3 6 6 3\n\n\nLet’s now move one step lower in the hierarchy. If you want to extract e.g. the name of student1, you first extract the first list using the simplifying operator. Doing so, you extract the list stud1. Adding [1] extract the first index position of the list stud1\n\nallstud[[1]][1]\n\n$name\n[1] \"Alice Wonderland\"\n\n\nwhile using [[1]] simplifies the output\n\nallstud[[1]][[1]]\n\n[1] \"Alice Wonderland\"\n\n\nA second way using the names of the elements. For instance, extracting the name of the student in student1 using the preserving operator to return a list:\n\nallstud[[\"student1\"]][\"name\"]\n\n$name\n[1] \"Alice Wonderland\"\n\n\nor the simplifying operator to return a character variable:\n\nallstud[[\"student1\"]][[\"name\"]]\n\n[1] \"Alice Wonderland\"\n\n\nThird, recall that the $ operator acts as a simplifying operator. In other words, you can extract the first list using allstud$student1. You can now extract the elements of that list using either the presering operator [], the simplifying operator [[]] both with index positions and names as well as the $ operator. For instance to extract the values in ects:\n\npreserving the structure:\n\n\nallstud$student1[7]\n\n$ects\n[1] 6 3 6 3 3\n\nallstud$student1[\"ects\"]\n\n$ects\n[1] 6 3 6 3 3\n\n\n\nsimplifying the structure using [[]]:\n\n\nallstud$student1[[7]]\n\n[1] 6 3 6 3 3\n\nallstud$student1[[\"ects\"]]\n\n[1] 6 3 6 3 3\n\n\n\nsimplifying the structure using $:\n\n\nallstud$student1$ects\n\n[1] 6 3 6 3 3\n\n\nNote that you can mix both index and named subsetting. Recall that the [[]] operator returns a list, but removes all references to the name of that list (e.g. student1). Here, you can For instance\n\nallstud[[1]]$ects\n\n[1] 6 3 6 3 3\n\n\nextracts the number of credits for student1.\n\n\n4.4.5.3 Extracting components across many lists in a nested list.\nallstud includes data for all students, where each student’s data is stored in a separate list. In the previous section, we subsetted data for an individual student. But what if we need similar data for each student in the list. To do that, you can use the Filter()function or use unlist() to remove the highest list level and extract the information from lists in at the second level.\nUsing the Filter(f, x) function (note the uppercase F), you can filter nested lists. The arguments of this function are f, a function that returns a logical vector and x a vector. The function uses f to subset x. Here, x refers to the nested list allstud. In that nested list, there are vectors such as ects, hours or course. We can use these to extract information on all students that meet a condition. This condition is defined by f. For instance, suppose that we want to extract all students whose courses are more than 21 ECTS. To calculate the total number of ECTS, we use sum(x$ects). The x here refers to the allstud. In other words, x$ects is shorthand for allstud$stduenti$ects. The condition can be written as sum(x$exts) &gt; 21). We now also have the function f: function(x) sum(x$ects) &gt; 21. Using this in Filter():\n\nFilter(function(x) sum(x$ects) &gt; 21, allstud)\n\n$student2\n$student2$name\n[1] \"Bart Vader\"\n\n$student2$number\n[1] \"r00362958\"\n\n$student2$program\n[1] \"Bachelor business adminstration\"\n\n$student2$exchange\n[1] FALSE\n\n$student2$course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$student2$hours\n[1] 52 26 52 52 26\n\n$student2$ects\n[1] 6 3 6 6 3\n\n\nThis function returns the list of the second student. This is the only student whose ECTS is higher than 21. Extracting all exchange students (exhange = T) can be done using:\n\nFilter(function(x) x$exchange == T, allstud)\n\n$student3\n$student3$name\n[1] \"Clark Kent\"\n\n$student3$number\n[1] \"r00362478\"\n\n$student3$program\n[1] \"Bachelor business adminstration\"\n\n$student3$exchange\n[1] TRUE\n\n$student3$course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n$student3$hours\n[1] 52 26 26 26 26\n\n$student3$ects\n[1] 6 3 3 3 3\n\n\nYou can also first use unlist to remove the highest level of the nested list. Recall that unlist() removes the upper hierarchy of a nested list and that you can collect the names for each of the components in the remaining list. Using these names, you can now extract components. To see how, let’s first store the output of unlist in a separate list:\n\nunl_allstud &lt;- unlist(allstud, recursive = FALSE)\n\nand extract the names\n\nunl_allstud_names &lt;- names(unl_allstud)\n\nlet’s now try to extract all courses for every student. This is where regular expressions enter. Here you want to extract all courses. These are stored in e.g. student1.course or student2.course, i.e. a pattern “student”“digit”“.”“course”. In terms of a regular expression, this is a pattern \"student\\\\d.course\". Recall that grepl() returns a logical value TRUE is a pattern is matched. In other words, grepl(pattern = \"student\\\\d.course\", unl_allstud_names) will return TRUE is the names vector includes a names such as student1.course or student3.course. We can now use this vector to subset unl_allstud:\n\nunl_allstud[grepl(pattern = \"student\\\\d.course\", unl_allstud_names)]\n\n$student1.course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$student2.course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$student3.course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n\nAs you can see, we now have a list which includes the courses for each student. If you assign this result to a list e.g. courses, you can now subset these courses and find studenten who, e.g. took Economic sociolocy.\nYou can write this code shorter:\n\nunlist(allstud, recursive = FALSE)[grepl(pattern = \"student\\\\d.course\", names(unlist(allstud, recursive = FALSE)))]\n\n$student1.course\n[1] \"Data and programming skills\"                     \n[2] \"Strategic management\"                            \n[3] \"Macro-economics and economic policy\"             \n[4] \"Economic sociology\"                              \n[5] \"Introduction to methods for operational research\"\n\n$student2.course\n[1] \"Data and programming skills\"           \n[2] \"Strategic management\"                  \n[3] \"Macro-economics and economic policy\"   \n[4] \"Financial statement analysis\"          \n[5] \"Entrepreneurship and business planning\"\n\n$student3.course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n\n\n\n\n4.4.6 Changing the elements in a list\nThere are three ways to change the elements in a list: first you change one of a list’s components. Second, you can add a new component and third, you can remove a component.\n\n4.4.6.1 Unnested lists\n\n4.4.6.1.1 Changing a component in a list\nChanging one of the components of a non nested list is not different from changing one of the elements of a vector or matrix. Subsetting this component and reassigning its value will do just that. For instance, changing the value FALSE to TRUE in the exchange component of stud1:\n\nstud1[4] &lt;- TRUE\n\nas an alternative, you can also use the other subsetting operators, [[]] or $. For instance\n\nstud1$exchange &lt;- FALSE\n\nchanges this value back to FALSE.\nTo change a value in a vector, matrix or array, you would use a similar approach. For instance, changing the first element in the hours vector for student 1 from 52 in 26 uses the fact that stud1$hours is a vector. Changing the first element of this vector:\n\nstud1$hours[1] &lt;- 26\n\nNote that here, you can use any approach we have covered for the other data structures. In other words, you can increase the number of elements in a vector (e.g. by adding them via append() or via c()), add columns and rows to a matrix using rbind or cbind or change the number of matrices in an array.\n\n\n4.4.6.1.2 Adding components to a list\nSuppose that you would like to add the total number of hours to list in stud1, stud2 and stud3. The first approach adds a component by assigning its value to stud1[8]. Recall that stud1 includes 7 components. Adding an new components adds one component to the existing ones. This new component will be the eight component. You can define this more in general using length(stud1). Recall that this function shows the number of components in stud1. Adding one will create a new component. This procedure is safer than just using a number such as 8. Especially is you have long and complex lists, you could easily overwrite an existing component. To add to total hours we use the fact that stud1$hours is a vector. Using sum(stud1$hours) allows to add the total number of hours:\n\nstud1[length(stud1) + 1] &lt;- sum(stud1$hours)\nstud1[8]\n\n[[1]]\n[1] 156\n\n\nNote that stud1[8] is not named. To fix this, we can add a name total. names(stud1) is a vector. We can add an eight element to that vector using:\n\nnames(stud1)[8] &lt;- \"total\"\nstud1$total\n\n[1] 156\n\n\nTo name the component, you could again use length(stud1). However, in this case, note that you want to change the last component and not the last plus one.\nThe second way creates a named component. Do do so, we add the name of that component, total to stud2 using the $ operator. We can assign the total number of hours to that names component:\n\nstud2$total &lt;- sum(stud2$hours)\nstud2$total\n\n[1] 208\n\n\nThe third approach uses the c() function. Here, we add the component “total” by combining it with the existing components of stud3 an assigning this new list to stud3. For stud3:\n\nstud3 &lt;- c(stud3, \"total\" = sum(stud3$hours))\nstud3$total\n\n[1] 156\n\n\nThe fourth approach uses append(). Here, you include the list as well as the value you want to add in the function arguments: append(list, value). Using this function, you can also add the position using the after = option.\nIf you want to add a vector, matrix or array as a new component of the list, using the first, third and fourth approach you need to tell R you want to include the values in that structure as a structure and not as individual components. To add the former, you need to include that structure in a list() statement. For instance, to add a new a new vector semester with values c(1, 2):\n\nstud1[length(stud1) + 1] &lt;- list(c(1, 2))\nnames(stud1)[length(stud1)] &lt;- \"semester\"\n\nstud3 &lt;- c(stud3, \"semester\" = list(c(1, 2)))\n\nYou can now check that this component was added as a vector:\n\nstud1$semester\n\n[1] 1 2\n\nstud3$semester\n\n[1] 1 2\n\n\nLet’s see what would happen is you didn’t include the list() statement. To do so, we’ll use a copy of stud1:\n\nstud1_copy &lt;- stud1\nstud1_copy &lt;- c(stud1_copy, \"test\" = c(200, 300))\nstud1_copy$test1\n\n[1] 200\n\nstud1_copy$test2\n\n[1] 300\n\nrm(stud1_copy)\n\nHere, you can see that R added both values in c(200, 300) as individual elements to components it named test1 and test2. In other words, R didn’t add the vector, it added the values.\nUsing the second approach to add a new component doesn’t require the `list()´ statement:\n\nstud2$semester &lt;- c(1, 2)\nstud2$semester\n\n[1] 1 2\n\n\nHere, you are explicitly telling R that the values c(1, 2) have to be added to one component in the list stud2$semester.\n\n\n4.4.6.1.3 Removing components from a list\nThe first approach to removing components from a list uses negative index numbers. Recall that a negative index subsets all except the negative indices. Using this approach, you assign the value of the subsetted list to the same list name. Doing to will give you a new list with the same name, but without the removed component. For instance, to remove “total” from stud3:\n\nstud3 &lt;- stud3[-8]\nstud3\n\n$name\n[1] \"Clark Kent\"\n\n$number\n[1] \"r00362478\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] TRUE\n\n$course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Mathematics for business B\"            \n\n$hours\n[1] 52 26 26 26 26\n\n$ects\n[1] 6 3 3 3 3\n\n$semester\n[1] 1 2\n\n\nA second way to remove components is to assign them NULL. For instance, to remove the total number of hours for stud1 and stud2:\n\nstud1$total &lt;- NULL\nstud2[8] &lt;- NULL\n\nYou can verify that both these lists lost their component total\n\nstr(stud1)\n\nList of 8\n $ name    : chr \"Alice Wonderland\"\n $ number  : chr \"r00369258\"\n $ program : chr \"Bachelor business adminstration\"\n $ exchange: logi FALSE\n $ course  : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Economic sociology\" ...\n $ hours   : num [1:5] 26 26 52 26 26\n $ ects    : num [1:5] 6 3 6 3 3\n $ semester: num [1:2] 1 2\n\n\n\n\n\n4.4.6.2 Nested lists\nLet’s add a new student to allstud. The data for this student are collected in a list, stud4. This list will then be added to allstud. The fourth student:\n\nstudent &lt;- \"Lois Lane\"\nstudentnr &lt;- \"r00252478\"\nprogram &lt;- \"Bachelor business adminstration\"\nexchange = F\ncourse &lt;- c(\"Macro-economics and economic policy\", \"Economic sociology\", \"Entrepreneurship and business planning\", \"Financial accouing B\", \"Economics of the single market\")\nects &lt;- c(6, 3, 3, 3, 6)\nhours &lt;- c(52, 26, 26, 26, 52)\n\nstud4 &lt;- list(name = student, \n             number = studentnr,\n             program = program,\n             exchange = exchange,\n             course  = course,\n             hours = hours, \n             ects = ects)\n\nWe can now add this student to allstud. To do so, we use the append() function and add stud4to allstud using append(allstud, list(stud4)). We add the name using names(allstud)[4] \\&lt;- \"student4\". Doing so will add the fourth student to this list\n\nallstud &lt;- append(allstud, list(stud4))\nnames(allstud)[4] &lt;- \"student4\"\nallstud$student4\n\n$name\n[1] \"Lois Lane\"\n\n$number\n[1] \"r00252478\"\n\n$program\n[1] \"Bachelor business adminstration\"\n\n$exchange\n[1] FALSE\n\n$course\n[1] \"Macro-economics and economic policy\"   \n[2] \"Economic sociology\"                    \n[3] \"Entrepreneurship and business planning\"\n[4] \"Financial accouing B\"                  \n[5] \"Economics of the single market\"        \n\n$hours\n[1] 52 26 26 26 52\n\n$ects\n[1] 6 3 3 3 6\n\n\nUsing the append() function also allows you to specify where the new list enters. Using the argument after = 2 for instance would add student4 after the second position.\nAs an alternative, you can use c(allstud, list(stud4)). Doing so will add the fourth student to allstud. Here too, you will have to add names. If you cont want to add names, you can use the $ operator and add the name of the new list, e.g. adding the data for student 4 as allstud$student5:\n\nallstud$student5 &lt;- stud4\n\nRemoving lists from a nested list follows a similar approach to the one to remove components from a list: you subset using a negative index and reassign this new list to the name of the old list or you use NULL to remove the list. For instance, to remove student5 from allstud:\n\nallstud$student5 &lt;- NULL\nlength(allstud)\n\n[1] 4\n\n\n\n\n\n4.4.7 lapply() and sapply().\n\n4.4.7.1 Applying a function to list components\nWe already met the apply() function. This function was used to apply functions the rows or columns of a matrix and allows to avoid for loops. The lapply() and sapply() function are designed to apply a function to a list. lapply() returns a list. Hence, the name “l”apply: the list version of apply. sapply() simplifies the result to a vector or matrix or an array. Hence, the name “s” apply: the simplified version of lapply. Like the apply() function, both allow you to avoid loops. Most of what you do within lapply() or sapply() can be done with a loop as well. However, as with apply(), it is often more efficient to use these function.\nTo see how these work, let’s start from a simple example: a list with 3 numeric vectors as component:\n\nlist1 &lt;- list(vec1 = rnorm(100, 0, 1), \n              vec2 = rnorm(100, 5, 10), \n              vec3 = rnorm(100, 10, 20))\n\nLet’s now use the lapply() function to calculate the mean of each of list1’s components. This function has a couple of arguments. First, the list that will be used to apply a function to. Second, the argument FUN, the function to be applied to each component of the list, including optional arguments, e.g. na.rm = TRUE.\nThe function can be a base R function or a function you include in the lapply() or sapply() call. For instance, to calculate the mean of the components of list1:\n\nlapply(list1, mean, na.rm = TRUE)\n\n$vec1\n[1] 0.0144794\n\n$vec2\n[1] 4.745767\n\n$vec3\n[1] 9.735662\n\n\nHere, lapply() returns a list. Using sapply() in addition to the arguments for lapply() we can set simplify = TRUE (which is TRUE by default) and use.names = TRUE (which is TRUE by default). We will keep these default values. To calculate the mean for every component in the list:\n\nsapply(list1, mean, na.rm = TRUE)\n\n     vec1      vec2      vec3 \n0.0144794 4.7457673 9.7356624 \n\n\nAs you can see, this function returns a vector.\nLet’s see what it would take to write the same code with a loop:\n\nresult_mean &lt;- matrix(0, 1, 3)\nfor (i in 1:3) {\n  result_mean[1, i] &lt;- mean(list1[[i]])\n}\ncolnames(result_mean) &lt;- names(list1)\nresult_mean\n\n          vec1     vec2     vec3\n[1,] 0.0144794 4.745767 9.735662\n\n\nUsing sapply() you write this for loop in one line of code: sapply(list1, mean).\nLike you could with the apply() function, you can define your own functions in both lapply() and sapply(). Recall that we used apply() to calculate a new value as the difference between the element in a column and the minimum to the difference between the minimum and the maximum. Using lapply() and reassigning these new values to list2:\n\nlist2 &lt;- lapply(list1, function(x) (x - min(x))/(max(x) - min(x)))\n\nYou can now verify that all values in list2 are rescaled:\n\nlapply(list2, range)\n\n$vec1\n[1] 0 1\n\n$vec2\n[1] 0 1\n\n$vec3\n[1] 0 1\n\n\nUsing sapply() and storing the values in mat1:\n\nmat1 &lt;- sapply(list1, function(x) (x - min(x))/(max(x) - min(x)))\n\nYou can verify this result (recall mat1 is a matrix):\n\napply(mat1, 2, range)\n\n     vec1 vec2 vec3\n[1,]    0    0    0\n[2,]    1    1    1\n\n\nLet’s revisit the first line lapply(list1, function(x) (x - min(x))/(max(x) - min(x))). Here you call lapply() to apply a function to every component of a list list1. In this case, the list’s components are vectors. The function to apply is function(x) (x - min(x))/(max(x) - min(x)). R will “loop over” every component of list1 and substitute that component for x in function(x). In other words, it applies that function to list1[[1]], then to lists[[2]] … until it reaches the end of the list. lapply() stores the result for every component in a list. sapply() has a similar way of applying a function, but simplifies the result, where possible, to a vector or matrix.\nNote that you can have an apply() function within an lapply() function. If the components of a list are matrices and you would like to apply a function to every column of every matrix, you can use lapply(list, function(x) apply(x, 2, fun)).\nlist1 included only numeric vectors. In stud1 we had a mixture of data types. Most functions such as mean() or toupper() are only defined for a specific type of data. As the list can store many types, it is often convenient to first select the components of a list with the same type. Suppose you want to calculate the totals for all numeric vectors in stud1. First, we need to extract these vectors using a logical subsetting vector. To do so we will use the sapply() function to identify which components meet a condition and define a function that returns TRUE is the condition is met and FALSE otherwise. To select the numeric values, we can use the is.numeric() function within sapply(). This function will then return for every component of the list a value TRUE is that component is numeric and FALSE if that is not the case:\n\ncond &lt;- sapply(stud1, \\(x) is.numeric(x))\ncond\n\n    name   number  program exchange   course    hours     ects semester \n   FALSE    FALSE    FALSE    FALSE    FALSE     TRUE     TRUE     TRUE \n\n\nHere, sapply() checks for very components in stud1 is this components is numeric. In other words, it tests is.numeric(stud1[[1]]), is.numeric(stud1[[2]]) … until it reaches the last component. For every component is.numeric() returns TRUE is the component is numeric and FALSE otherwise. sapply() stores each of these outcomes in a matrix or vector, here cond. In other words, cond is a logical vector whose elements are TRUE is a component of stud1 is numeric and FALSE otherwise. We can now use this logical vector to extract the components of ´stud1` that include numeric data. To do so, you can use\n\nstud1[cond]\n\n$hours\n[1] 26 26 52 26 26\n\n$ects\n[1] 6 3 6 3 3\n\n$semester\n[1] 1 2\n\n\nWe now have the numeric components of stud1. Because we subsetted a list, the output is also a list. We can now use lapply() or sapply() to calculate the totals for all numeric vectors in the list stud1:\n\nsapply(stud1[cond], function(x) sum(x))\n\n   hours     ects semester \n     156       21        3 \n\n\n\n\n4.4.7.2 Adding components to nested lists\nWith nested lists, the second level in the hierarchy is a list. Suppose now that you want to add a component to each list in the nested list. To illustrate, we’ll add the total number of hours for each student as an additional component to that list. You can subset the components of the lists on the second level within the lapply() or sapply() functions. For our example: for each student, the hours are stored in allstud$studenti$hours. lapply() applies a function to all studenti lists in allstud. Using this observation, including function(x) sum(x$hours) as a function in lapply(), R will ‘loop’ over each studenti and replace x with studenti. In doing so, R calculates the total hours for each student. lapply() returns a list:\n\nlapply(allstud, function(x) sum(x$hours))\n\n$student1\n[1] 182\n\n$student2\n[1] 208\n\n$student3\n[1] 156\n\n$student4\n[1] 182\n\n\nIf you want to add these total hours each of the students in allstud, you can use the c() and add the component “totalhours” to each sublist in allstud. Here, I copy the result of this procedure in a new list. In the structure of this new list allstud_1´ you'll see that the component,totalhours` was added to each of the student’s list:\n\nallstud_1 &lt;- lapply(allstud, function(x) c(x, \"totalhours\" = sum(x$hours)))\nstr(allstud_1)\n\nList of 4\n $ student1:List of 8\n  ..$ name      : chr \"Alice Wonderland\"\n  ..$ number    : chr \"r00369258\"\n  ..$ program   : chr \"Bachelor business adminstration\"\n  ..$ exchange  : logi FALSE\n  ..$ course    : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Economic sociology\" ...\n  ..$ hours     : num [1:5] 52 26 52 26 26\n  ..$ ects      : num [1:5] 6 3 6 3 3\n  ..$ totalhours: num 182\n $ student2:List of 8\n  ..$ name      : chr \"Bart Vader\"\n  ..$ number    : chr \"r00362958\"\n  ..$ program   : chr \"Bachelor business adminstration\"\n  ..$ exchange  : logi FALSE\n  ..$ course    : chr [1:5] \"Data and programming skills\" \"Strategic management\" \"Macro-economics and economic policy\" \"Financial statement analysis\" ...\n  ..$ hours     : num [1:5] 52 26 52 52 26\n  ..$ ects      : num [1:5] 6 3 6 6 3\n  ..$ totalhours: num 208\n $ student3:List of 8\n  ..$ name      : chr \"Clark Kent\"\n  ..$ number    : chr \"r00362478\"\n  ..$ program   : chr \"Bachelor business adminstration\"\n  ..$ exchange  : logi TRUE\n  ..$ course    : chr [1:5] \"Macro-economics and economic policy\" \"Economic sociology\" \"Entrepreneurship and business planning\" \"Financial accouing B\" ...\n  ..$ hours     : num [1:5] 52 26 26 26 26\n  ..$ ects      : num [1:5] 6 3 3 3 3\n  ..$ totalhours: num 156\n $ student4:List of 8\n  ..$ name      : chr \"Lois Lane\"\n  ..$ number    : chr \"r00252478\"\n  ..$ program   : chr \"Bachelor business adminstration\"\n  ..$ exchange  : logi FALSE\n  ..$ course    : chr [1:5] \"Macro-economics and economic policy\" \"Economic sociology\" \"Entrepreneurship and business planning\" \"Financial accouing B\" ...\n  ..$ hours     : num [1:5] 52 26 26 26 52\n  ..$ ects      : num [1:5] 6 3 3 3 6\n  ..$ totalhours: num 182\n\nrm(all_stud1)\n\nWarning in rm(all_stud1): object 'all_stud1' not found\n\n\nRecall that in case you add a data structure such as a vector, matrix or array, you need to include that structure in a list() statement, e.g. c(x, \"semester\" = list(c(1, 2))).\nUsing sapply() returns similar results but as a matrix and not as a list:\n\nsapply(allstud, function(x) sum(x$hours))\n\nstudent1 student2 student3 student4 \n     182      208      156      182 \n\n\nNote that you can not use sapply() to add a component to a list: sapply() returns a vector and not a list. In other words, you can not use it to change a list.\nA second way to access the components of the lists in a nested list uses the unlist() function. Recall that we can use unlist( ,recursive = FALSE) to unlist the first level. Doing so, returns the second level as a list. Using this level, you can now use lapply() or sapply(). Let’s extract the numeric vectors from the nested list allstud and calculate their sum. In the first step, we unlist allstud with the option recursive = FALSE and store the results in a list allstud_ul:\n\nallstud_ul &lt;- unlist(allstud, recursive = FALSE)\n\nYou can verify that we removed the first level from the allstud list. We can now proceed along the lines of the previous example:\n\ncond &lt;- sapply(allstud_ul, function(x) is.numeric(x))\nsapply(allstud_ul[cond], function(x) sum(x))\n\nstudent1.hours  student1.ects student2.hours  student2.ects student3.hours \n           182             21            208             24            156 \n student3.ects student4.hours  student4.ects \n            18            182             21 \n\n\nYou can now subset this result using the familiar vector or matrix subetting operations, e.g.\n\nhours_ects &lt;- sapply(allstud_ul[cond], function(x) sum(x))\nhours &lt;- hours_ects[grepl(pattern = \".hours\", names(hours_ects))]\nhours\n\nstudent1.hours student2.hours student3.hours student4.hours \n           182            208            156            182 \n\n\nIf you want to remove the reference to hours in the names, you can use the familiar charachter functions, e.g.\n\nnames(hours) &lt;- stringr::str_extract_all(names(hours), pattern = \"student\\\\d\", simplify = TRUE)\nhours\n\nstudent1 student2 student3 student4 \n     182      208      156      182 \n\n\n\n\n4.4.7.3 Searching for pattern across lists in a nested list\nThe lists that are part of a nested list include data. Sometimes you need to identify patterns that occur in some but not necessarily all sublists. For instance, in the example, the students list that are components of the nested list allstud include data on the courses they took. Suppose that you need to know which student took a specific course, e.g. “Economic sociology”. Visual inspection shows that there are three students who took “Economic sociology”: student1, student3 and student4. To find these students, you look for a pattern in studenti$course. That pattern is \"Economic sociology. We want to subset all studenti$course components in every student list. To do so within lapply(), we use x$course. The function that we apply for every student’s list is to subset x$course using a logical vector that equals TRUE if “Economic sociology” is part of the vector with courses and FALSE otherwise. Here, we can use grepl(). Using this function in lapply() returns a logical vector\n\nlapply(allstud, function(x) grepl(pattern = \"Economic sociology\", x = x$course))\n\n$student1\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n$student2\n[1] FALSE FALSE FALSE FALSE FALSE\n\n$student3\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n$student4\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can now use that vector to subset the course vector for every student. Recall that we can use a logical vector to subset vectors. Here, we do so using x$course[grepl(pattern = \"Economic sociology\", x = x$course)]. Note that the first x in x = x$course refers to grepl()’s argument name, not to the list’s components. Adding all these in lapply():\n\nlapply(allstud, function(x) x$course[grepl(pattern = \"Economic sociology\", x = x$course)])\n\n$student1\n[1] \"Economic sociology\"\n\n$student2\ncharacter(0)\n\n$student3\n[1] \"Economic sociology\"\n\n$student4\n[1] \"Economic sociology\"\n\n\nThe list includes all students and for student2, the component in that list is an empty character vector. In other words, this student doesn’t have this course in the course vector. Without subsetting x$course the function grepl() would show a list with logical indices.\nNote that we can use lapply(allstud, function(x) grepl(pattern = \"Economic sociology\", x = x$course)) to subset other components in every student’s list. For instance, the length of “ects” or “hours” is equal to the length of the components in the logical vector. In other words, we can also extract the ects or hours included in the program of every student who took Economic sociology. Here, both hours and ects are the same as Economic sociology is the same course across students:\n\nlapply(allstud, function(x) x$ects[grepl(pattern = \"Economic sociology\", x = x$course)])\n\n$student1\n[1] 3\n\n$student2\nnumeric(0)\n\n$student3\n[1] 3\n\n$student4\n[1] 3\n\n\nWhat about components such as “name” or “number”? Their length (1) is different from the length of the subsetting logical vectors. Here we can use the fact that TRUE = 1 and FALSE = 0. We looked for one pattern “Economic sociology”. If this pattern occurs in the “course” vector, lapply(allstud, function(x) grepl(pattern = \"Economic sociology\", x = x$course)) shows TRUE for that position and FALSE elsewhere. Summing across TRUE and FALSE will result in 1 if the subject is included and 0 if this is not the case:\n\nlapply(allstud, function(x) sum(grepl(pattern = \"Economic sociology\", x = x$course)))\n\n$student1\n[1] 1\n\n$student2\n[1] 0\n\n$student3\n[1] 1\n\n$student4\n[1] 1\n\n\nWe can now use result to subset, e.g. the name and identify who took Economic sociology and who didn’t:\n\nlapply(allstud, function(x) x$name[sum(grepl(pattern = \"Economic sociology\", x = x$course))])\n\n$student1\n[1] \"Alice Wonderland\"\n\n$student2\ncharacter(0)\n\n$student3\n[1] \"Clark Kent\"\n\n$student4\n[1] \"Lois Lane\"\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nSuppose that you want to know the distribution of the value of a stock market portfolio 30 from now. You would like to answer questions such as: what is the probability that for every euro you invest today, the (nominal) value of your portfolio will rise to e.g. euro 10 in 30 years time, what is the probability that the your portfolio will be worth 5 euro’s in 30 years time. Because you can not predict the future with certainty, you decide to run a simulation to estimate this distribution. Using the simulation, you will generate “a lot of” 30 year periods. Using these results, you try to answer your questions. You assume that stock market returns (i.e. the percentage change in the value of your portfolio) are normally distributed. The parameters of this normal distribution - the mean and the standard deviation - equal the average percentage change and the volatility. For instance, if you assume that the yearly mean is 8% and the yearly volatility if 20%, then you know that in any given year, the return will be between -12% and +28% in 68,2% of all years and will be between -32% and + 48% ion 95.4% of all years. However, you are not sure of the mean will be 8%. Some portfolio’s have a lower expected return. Usually, they also have a lower volatility. On the other hand, some portfolio’s also have a higher expected return. In that case, their volatility is higher. You also want to run look at returns per month. Doing so allows you to have 360 months in year simulation and not 30 years. In other words, you will run simualtions using the following combinations of expected return and volatility:\n\nyearly: 6% and 12.00% - monthly: 0.48676% and 3.46410%\nyearly: 7% and 15.75% - monthly: 0.56541% and 4.54663%\nyearly: 8% and 20.00% - monthly: 0.64340% and 5.77350%\nyearly: 9% and 22.25% - monthly: 0.72073% and 6.42302%\nyearly: 10% and 30.00% - monthly: 0.79741% and 8.66025%\n\nYou store these values in a matrix, mat_data. This matrix is given:\n\nmat_data &lt;- matrix(c(0.48676, 0.56541, 0.64340, 0.72073, 0.79741, 3.46410, 4.54663, 5.77350, 7.14471, 8.66025)/100, nrow = 5, ncol = 2)\ncolnames(mat_data) &lt;- c(\"exp_ret\", \"vol\")\nrownames(mat_data) &lt;- paste(\"sim\", 1:5, sep = \"_\")\nmat_data\n\n        exp_ret       vol\nsim_1 0.0048676 0.0346410\nsim_2 0.0056541 0.0454663\nsim_3 0.0064340 0.0577350\nsim_4 0.0072073 0.0714471\nsim_5 0.0079741 0.0866025\n\n\nHow do you run this simulation? For every monthly return - volatility combination (i.e. for every row in mat_data), you draw 360 random draws from a normal distribution. To see the total value after 360 months, you want 1 and calculate the cumulative product. To see this, not that every euro invested will be worth\n\\[\n(1 + r_1)\n\\] after one month, \\[\n(1+ r_1)(1 + r_2)\n\\] after two months, … . In other words,\n\\[\n(1 + r_1)(1 + r_2) ... (1 + r_{360})\n\\] will be the value after 360 months or 30 years.\nHere you draw the r’s from the normal distribution. If you then add 1, every value will equal \\(1 + r_1\\). The cumulative product will then show your total value after 360 months. Here, you have one simulation but you need a “large number” of these simulation to answer you question. So, for every return - volatility combination, you generate this simulation 100 times.\nTo store the results, we will use a list for every return- volatility pair and call it simi where i refers to the row in mat_data. We will store the expected return and volatility as simi$exp_return and simi$volatility. Because you are not sure you will need these results for other time periods as well, you store the returns matrix in simi$sim_data. After the simulation, you add the 100 results in a matrix and add it to simi$exp_value. Your results allow you to estimate the quantiles of the value distribution. You will store them as simi$quantiles. In addition, you store the values such as the mean in simi$mean and the standard deviation in simi$st_dev. The last thing you want to store is the histogram of the final values in simi$plot. For every expected - return volatility combination, you have a separate list. You store this lists in a list simulations.\nLet’s create the lists first\n\ncreate an empty list:\n\n\n\nCode\nsimulations &lt;- list()\n\n\nLet’s look at the simulation for the first return - volatility pair.\n\ncreate a list, sim1 and add expected return sim1$exp_return and volatility sim1$volatility to the list. Recall that these values are stored in the first row in mat_data:\n\n\n\nCode\nsim1 &lt;- list(exp_return = mat_data[[1, 1]], \n             volatility = mat_data[[1, 2]])\nsim1\n\n\n$exp_return\n[1] 0.0048676\n\n$volatility\n[1] 0.034641\n\n\nCode\n# Note that there are other ways to do to. For instance, you could have \n# created an empty list `sim1 &lt;- list()` and used `sim1$exp_return &lt;- mat_data[1, 1]` \n# started from the empty list and used  `sim1 &lt;- c(sim1, \"exp_return\" = mat_data[1, 1])`. \n\n\n\nadd this list to simulations with the name sim1\n\n\nsimulations[[\"sim1\"]] &lt;- sim1\n\n# Note that there are other ways to do this, e.g. `simulations$sim1 &lt;- sim1`. \n\nLet’s automate this for the other lists. Here the code is given. Try to predict what every line in this code does. Note that sim1 was created. In other words, i can start from 2 and needs to run to 5. Focus on the lines that deal with “lists”.\n\nfor (i in 2:5) {\n  sim_names &lt;- paste0(\"sim\", i)\n  temp_list &lt;- list(exp_return = mat_data[[i, 1]],\n                    volatility = mat_data[[i, 2]])\n  simulations[[sim_names]] &lt;- temp_list\n}\nrm(temp_list)\n\nUse the values in sim1$exp_return and sim1$volatility … sim5$exp_return and sim5$volatility to generate a 360 x 100 matrix with random draws from a normal distribution with mean and standard deviation given by exp_return and volatility, add 1 to every element and add this matrix to sim1 … sim5 Do this so that you can rerun the simulations with another set of parameters for the months and draws. In other words, assign the values for the number of draws, ndraws and the number of months nmonths is separate variables. Use these to determine the dimensions of your matrix. Assign this matrix to simi$sim_data.\nFirst let’s look at an example to generate the matrix. Here, call this matrix mat and use the data stored in sim1 to set the mean and standard deviation:\n\n\nCode\nndraws &lt;- 100\nnmonths &lt;- 360\n\n# we need ndraws per month: total of ndraw * nmonths random draws\n# store in ndraw columns with one row per month\n\nmat &lt;- matrix(rnorm(n = (ndraws * nmonths), \n                    mean = simulations$sim1$exp_return, \n                    sd = simulations$sim1$volatility),\n              nrow = nmonths, \n              ncol = ndraws) + 1\n\n\nLet’s try to automate this process using the lapply() function and add the matrix sim_data to every list witing the simulations list. Recall that you need to wrap the matrix in a list() call. Use function(x) c(list \"name\" = ) in the lapply() function to do so:\n\n\nCode\nsimulations &lt;- lapply(simulations, function(x) c(x, \"sim_data\" = list(matrix(rnorm(ndraws * nmonths, x$exp_return, x$volatility), nmonths, ndraws) + 1)))\n\n\nLet’s see what the alternative would have been is you would have use a for loop. Here, the code is given. Try to see what these steps do with respect to lists in this simulation (how are they subsetted …).\n\n# for (i in 2:5) {\n#   \n#   simulations[[i]]$sim_data &lt;- matrix(rnorm(ndraws * nmonths, simulations[[i]]$exp_return, simulations[[i]]$volatility),\n#                                       nrow = nmonths,\n#                                       ncol = ndraws) + 1\n# }\n\nVerify that your results are from the correct normal distribution. To do so, use the sapply() function to create a est_mean and est_volatility matrix as mean and standard deviation of all elements in the sim_data matrix minus 1 (recall that you added one, so here, for this purpose you need to subtract 1):\n\nest_mean &lt;- sapply(simulations, function(x) mean(x$sim_data - 1))\nest_volatility &lt;- sapply(simulations, function(x) sd(x$sim_data - 1))\n\nYou can now use this matrix to determine the value for every one of these 100 draws after 360 months. Assign this vector to simi$exp_value. Recall that you can use the apply() function to calculate the product of all values in a column of a matrix and that you need to simplify the result of apply(). Use the lapply() function to do generate these vectors across the various simulations.\n\n\nCode\nsimulations &lt;- lapply(simulations, function(x) c(x, \"exp_value\" = list(apply(x$sim_data, 2, FUN = prod, simplify = TRUE))))\n\n\nYou now have for every euro invested today the value for every euro invested 30 years from now for 5 scenario’s in terms of the expected return and volatility and for 100 simulations across these return-volatility combinations. Use these values to calculate summary statistics: quantiles (with probabilities 10%, 25%, 50%, 75% and 90%), mean and standard deviation. Store these in simi$quantiles, simimean and simi$st_dev. You will need three lines of code using lapply():\n\n\nCode\nsimulations &lt;- lapply(simulations, function(x) c(x, \"quantiles\" = list(quantile(x$exp_value, probs = c(0.10, 0.25, 0.50, 0.75, 0.90), names = TRUE))))\nsimulations &lt;- lapply(simulations, function(x) c(x, \"mean\" = mean(x$exp_value, na.rm = TRUE)))\nsimulations &lt;- lapply(simulations, function(x) c(x, \"st_dev\" = sd(x$exp_value, na.rm = TRUE)))\n\n\nNow you can generate a plot. Here is the code to generate the plot for sim1. Try to read it to see what the code is doing. Use ?hist or ?plot to see what these lines are doing:\n\nplot_sim &lt;- hist(simulations$sim1$exp_value, probability = TRUE)\n\n\n\n\n\n\n\nplot(plot_sim, col = \"lightyellow\", border = \"lightgrey\", \n     xlab = \"Expected value\", \n     main = glue::glue(\"Simulation with expected return {simulations$sim1$exp_return} and volatility {simulations$sim1$volatility}\"))\n\n\n\n\n\n\n\n\nNow, generate this plot and store this plot in $plot_sim in every simualations. You can use lapply() to do so. You can leave the plot() code out and only use the part in hist() from the previous code.\n\n\nCode\nsimulations &lt;- lapply(simulations, function(x) c(x, \"plot_sim\" = list(hist(x$exp_value, probability = TRUE))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou now have all your data for your simulations. Now, lets take a closer look at some of the results and answer a couple of questions. Store each answer in a matrix or list as indicated in the question.\n\nWhat is the mean value in each of the 5 simulations? Store this result in a matrix, mean_sim:\n\n\n\nCode\nmean_sim &lt;- sapply(simulations, function(x) x$mean)\nmean_sim\n\n\n     sim1      sim2      sim3      sim4      sim5 \n 5.984450  8.213977 11.126300 12.941953 12.151095 \n\n\n\nWhat is the 10th and 90th percentile in each of these simulations? Store this result in a matrix, low_value and high_value:\n\n\n\nCode\nlow_value &lt;- sapply(simulations, function(x) x$quantiles[1])\nhigh_value &lt;- sapply(simulations, function(x) x$quantiles[5])\nlow_value\n\n\n sim1.10%  sim2.10%  sim3.10%  sim4.10%  sim5.10% \n1.9962861 1.6127327 1.5882490 1.2373738 0.6525462 \n\n\nCode\nhigh_value\n\n\nsim1.90% sim2.90% sim3.90% sim4.90% sim5.90% \n11.08629 18.71673 24.83965 33.50628 30.25813 \n\n\n\nCalculate the standard deviation for each simulation’s expected value. Store this result in a matrix vol_sim:\n\n\n\nCode\nvol_sim &lt;- sapply(simulations, function(x) x$st_dev)\nvol_sim\n\n\n     sim1      sim2      sim3      sim4      sim5 \n 4.757061  9.713904 14.209238 18.484862 19.995729 \n\n\n\nWhich simulation run, in each of the simulations, gave the highest value? Store this result in a list max_run. Do the same for the lowest value in store in min_run:\n\n\n\nCode\nmax_run &lt;- lapply(simulations, function(x) which.max(x$exp_value))\nmin_run &lt;- lapply(simulations, function(x) which.min(x$exp_value))\n\n\n\nSelect the sim5$sim_data column associated with the highest expected value for every simulation. Store in a vector test:\n\n\n\nCode\ntest &lt;- simulations$sim5$sim_data[, max_run$sim5]\n\n\n\nTest if the value of the product of test is equal to the maximum of the expected values for sim5:\n\n\n\nCode\nprod(test) - simulations$sim5$exp_value[[max_run$sim5]] &lt; 10^(-12)\n\n\n[1] TRUE\n\n\n\nGiven the mean, you can calculate the expected value if there wouldn’t by any volatility as\n\n\\[\n(1 + r)^{360}\n\\]\n\nCalculate for every simulation how many runs are below this level. Store the result in a list below_ave:\n\n\n\nCode\nbelow_ave &lt;- lapply(simulations, function(x) sum(x$exp_value &lt; (1 + x$exp_return)^(nmonths)))\nbelow_ave\n\n\n$sim1\n[1] 64\n\n$sim2\n[1] 66\n\n$sim3\n[1] 64\n\n$sim4\n[1] 74\n\n$sim5\n[1] 79\n\n\n\nIs the mean expected value less than the expected value without volatility? Store this in a list with logical values diff_mean_bool:\n\n\n\nCode\ndiff_mean_bool &lt;- lapply(simulations, function(x) (x$mean - (1 + x$exp_return)^(nmonths)) &lt; 0)\ndiff_mean_bool\n\n\n$sim1\n[1] FALSE\n\n$sim2\n[1] FALSE\n\n$sim3\n[1] FALSE\n\n$sim4\n[1] TRUE\n\n$sim5\n[1] TRUE\n\n\n\nHow large is that difference between the actual mans and the mean without volatility? Store this is a list diff_mean:\n\n\n\nCode\ndiff_mean &lt;- lapply(simulations, function(x) (x$mean - (1 + x$exp_return)^(nmonths)))\ndiff_mean\n\n\n$sim1\n[1] 0.2408568\n\n$sim2\n[1] 0.6018453\n\n$sim3\n[1] 1.063751\n\n$sim4\n[1] -0.3256147\n\n$sim5\n[1] -5.298055\n\n\n\nUse the layout for the last histogram to plot the histogram for every simulation\n\n\n\nCode\nlapply(simulations, function(x) \n  plot(x$plot_sim, col = \"lightyellow\", border = \"lightgrey\", \n     xlab = \"Expected value\", \n     main = glue::glue(\"Simulation with expected return {x$exp_return} and volatility {x$volatility}\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$sim1\nNULL\n\n$sim2\nNULL\n\n$sim3\nNULL\n\n$sim4\nNULL\n\n$sim5\nNULL\n\n\nYou can verify your plots in the plots tab in the environment pane. The arrow to the left should allow you to see the 5 plots including a different title.\n\n\n\n\n #| echo: false\n #| error: false\n #| message: false\n #| output: false\n #| warning: false\n\nrm(course, cond, ects, hours, stud1, stud2, stud3, stud4, allstud, mat1, list1, plot_norm)",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#data-frames-and-tibbles",
    "href": "04_Data_structures.html#data-frames-and-tibbles",
    "title": "4  Data structures",
    "section": "4.5 Data frames and tibbles",
    "text": "4.5 Data frames and tibbles\nYou can think about data frames as lists where each column has the same length (as in a matrix) but each column can store a different type of data (as in a list). As in a matrix, a data frame usually has a fixed set of rows and columns but as in a list, these columns can store different types of variables. We will also use a special type of data frame: a tibble. Tibbles are essentially data frames, but with some additional characteristics.\n\n4.5.1 Creating a data frame\n\n4.5.1.1 The basics\nTo create a data frame, you can use the date.frame() function. The first argument are the data for the data frame. In addition, you can add row.names = NULL. By default, R doesn’t add row names other than 1, 2, 3, …. Adding a vector (integer or character) with the row names of specifying which column R needs to use for row names changes that default. Two other arguments check the data: check.rows = FALSE checks if the rows are consistent in terms of their length and in terms of their names; check.names = TRUE checks the names of the variables to see if these are valid variables names and not duplicates. The last two arguments, fix.empty.names = TRUE and stringAsFactors = FALSE add an automatically generated name in case the variable names are empty and changes character variables in factors. Let’s create a data frame, df whose values include numbers, logical values, characters and dates:\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf\n\n  numbers bools characters      dates\n1       1  TRUE          a 2025-03-25\n2       2 FALSE          b 2025-03-26\n3       3 FALSE          c 2025-03-27\n4       4  TRUE          d 2025-03-28\n5       5  TRUE          e 2025-03-29\n\n\nYou can verify that this is a data frame using e.g.\n\nis.data.frame(df)\n\n[1] TRUE\n\n\nor from the class\n\nclass(df)\n\n[1] \"data.frame\"\n\n\nNote that a data frame is also a list:\n\nis.list(df)\n\n[1] TRUE\n\n\nChecking the structure of df\n\nstr(df)\n\n'data.frame':   5 obs. of  4 variables:\n $ numbers   : num  1 2 3 4 5\n $ bools     : logi  TRUE FALSE FALSE TRUE TRUE\n $ characters: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ dates     : Date, format: \"2025-03-25\" \"2025-03-26\" ...\n\n\nyou can see that this structure shows similarities with a named list. From the structure, you can also see that this data frame includes 5 observations for 4 variables. The structure also shows the type of each variable. The length() or ncol() show the number of variables, while nrow() shows the number of observations:\n\nlength(df)\n\n[1] 4\n\nncol(df)\n\n[1] 4\n\nnrow(df)\n\n[1] 5\n\n\nRecall that ncol() and nrow() allowed you to determine the dimensions of a matrix. To see access the names of the variables, you can use\n\nnames(df)\n\n[1] \"numbers\"    \"bools\"      \"characters\" \"dates\"     \n\n\nR returns a character vector with the names of the variables. If the data includes row names, you can ask see them using\n\nrow.names(df)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nA data frame’s columns must have the same length (nrows) and R will sometimes force this to happen. To see this, let’s change a couple of arguments in df &lt;- data.frame():\n\nnumbers is a numeric value, not a vector of 5 values:\n\n\ndf1 &lt;- data.frame(numbers = 10,  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf1\n\n  numbers bools characters      dates\n1      10  TRUE          a 2025-03-25\n2      10 FALSE          b 2025-03-26\n3      10 FALSE          c 2025-03-27\n4      10  TRUE          d 2025-03-28\n5      10  TRUE          e 2025-03-29\n\n\nR copies the value “10” and fills the column “numbers” until the number of values equals the number of rows in the data frame. This is called recycling. R recycles single numeric values to fill a column.\n\nthe boolean vector includes only 3 values, not 5:\n\n\ndf2 &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\n\nError in data.frame(numbers = c(1, 2, 3, 4, 5), bools = c(T, F, F), characters = letters[1:5], : arguments imply differing number of rows: 5, 3\n\ndf2\n\nError: object 'df2' not found\n\n\nHere, R produces an error. It can not fill the bools column to make sure that its number of values matches the number of rows in the data frame. As R doesn’t know what to do, it will not fill this data frame.\n\nthe character vector includes more than 5 values:\n\n\ndf3 &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:8], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\n\nError in data.frame(numbers = c(1, 2, 3, 4, 5), bools = c(T, F, F, T, : arguments imply differing number of rows: 5, 8\n\ndf3\n\nError: object 'df3' not found\n\n\nHere too, R will not execute this command. In this case, R doesn’t know which values to drop from the character vector.\nHowever, with one value, R recycles the character:\n\ndf4 &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf4\n\n  numbers bools characters      dates\n1       1  TRUE          a 2025-03-25\n2       2 FALSE          a 2025-03-26\n3       3 FALSE          a 2025-03-27\n4       4  TRUE          a 2025-03-28\n5       5  TRUE          a 2025-03-29\n\n\nTo see what the other arguments in the date.frame() function, let’s add them and see how they change the output.\n\nspecifying row.names = 3L uses the third column of the data as row names:\n\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"), \n                 row.names = 3L)\ndf\n\n  numbers bools      dates\na       1  TRUE 2025-03-25\nb       2 FALSE 2025-03-26\nc       3 FALSE 2025-03-27\nd       4  TRUE 2025-03-28\ne       5  TRUE 2025-03-29\n\n\n\nAs an alternative, you can add vector with names: c(\"Obs.A\", \"Obs.B\", \"Obs.C\", \"Obs.D\", \"Obs.E\"):\n\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"), \n                 row.names = c(\"Obs.A\", \"Obs.B\", \"Obs.C\", \"Obs.D\", \"Obs.E\"))\ndf\n\n      numbers bools characters      dates\nObs.A       1  TRUE          a 2025-03-25\nObs.B       2 FALSE          b 2025-03-26\nObs.C       3 FALSE          c 2025-03-27\nObs.D       4  TRUE          d 2025-03-28\nObs.E       5  TRUE          e 2025-03-29\n\n\n\nlet’s remove the name bools and see what the function returns:\n\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5), c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf\n\n  numbers c.T..F..F..T..T. characters      dates\n1       1             TRUE          a 2025-03-25\n2       2            FALSE          b 2025-03-26\n3       3            FALSE          c 2025-03-27\n4       4             TRUE          d 2025-03-28\n5       5             TRUE          e 2025-03-29\n\nstr(df)\n\n'data.frame':   5 obs. of  4 variables:\n $ numbers         : num  1 2 3 4 5\n $ c.T..F..F..T..T.: logi  TRUE FALSE FALSE TRUE TRUE\n $ characters      : chr  \"a\" \"b\" \"c\" \"d\" ...\n $ dates           : Date, format: \"2025-03-25\" \"2025-03-26\" ...\n\n\nHere, R creates the name of the logical variable from the vector c(T, F, F, T, R). Is does so by removing the brackets and replacing comma’s and spaces with dots. If you include the ceck.names = FALSE argument, R will use c(T, F, F, T, R) as a name. If you want to avoid this, you need to use fix.empty.names = FALSE.\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5), c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"), \n                 fix.empty.names = FALSE)\ndf\n\n  numbers       characters      dates\n1       1  TRUE          a 2025-03-25\n2       2 FALSE          b 2025-03-26\n3       3 FALSE          c 2025-03-27\n4       4  TRUE          d 2025-03-28\n5       5  TRUE          e 2025-03-29\n\nstr(df)\n\n'data.frame':   5 obs. of  4 variables:\n $ numbers   : num  1 2 3 4 5\n $           : logi  TRUE FALSE FALSE TRUE TRUE\n $ characters: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ dates     : Date, format: \"2025-03-25\" \"2025-03-26\" ...\n\n\nHere, R leaves the name of the variable empty. You can now set your own name. The last argument stringAsFactors = FALSE keeps characters as characters. Changing this into TRUE converts these characters into factors.\n\n\n4.5.1.2 Tibbles and data frames\nTibbles are essentially data frames but come with a couple of special features. First, to use tibbles, you need to load the tibble package included in the tidyverse suite of packages. Second, there are a couple of differences in how a tibble and a data frame handle, e.g. printing or subsetting. First, if you print a tibble, it will highlight some special features and will only show the 10 first observations. Data frames show all observations. For long datasets, you need to add a command telling R to show only e.g. 10 lines. Second, tibbles are more strict in terms of subsetting compared to data frames. As we’ll see, a tibble always returns a tibble, while a data frame can return a vector. Last, tibbles allow for non syntatic column names, e.g. var 1.\nWith respect to the creating of a tibble, the basics are very similar to those for data frames.\nTo illustrate, let’s create a tibble:\n\ndf_tib &lt;- tibble::tibble(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf_tib\n\n# A tibble: 5 × 4\n  numbers bools characters dates     \n    &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;      &lt;date&gt;    \n1       1 TRUE  a          2025-03-25\n2       2 FALSE b          2025-03-26\n3       3 FALSE c          2025-03-27\n4       4 TRUE  d          2025-03-28\n5       5 TRUE  e          2025-03-29\n\n\nand compare the result with the date frame:\n\ndf &lt;- data.frame(numbers = c(1, 2, 3, 4, 5),  bools = c(T, F, F, T, T), characters = letters[1:5], dates = seq.Date(as.Date(\"2025-03-25\"), length.out = 5, by = \"day\"))\ndf\n\n  numbers bools characters      dates\n1       1  TRUE          a 2025-03-25\n2       2 FALSE          b 2025-03-26\n3       3 FALSE          c 2025-03-27\n4       4  TRUE          d 2025-03-28\n5       5  TRUE          e 2025-03-29\n\n\nThe first thing to note is that result shows the number of rows and columns for a tibble, but not for a data frame. In addition, the tibble also shows the type of the data stored in each column, while the data frame doesn’t show this output. Row names in the tibble are shown in grey, indicating that they were automatically generated. You can verify the class of a tibble:\n\nclass(df_tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nHere, you can see that a tibble is also a data frame. The tibble() function includes 2 arguments in addition to data part: .rows = and .name_repair = c(\"check_unique\", \"unique\", \"universal\", \"minimal\"). The former allows you to add the number of rows. You could add this as a check to see if the number of observations in your dataset matches your expectations or to create an empty tibble using .rows = 0. The latter function allows you to tell R how to treat problematic column names. The default value here is check_unique and verifies if a column has a unique name but doesn’t try to repair the name; universal makes names unique and brings them in line with the R syntax; unique makes sure that there are names that that they are unique while `minimal does not repair or any checks other than verifying is a name exits.\n\n\n4.5.1.3 Showing parts of a data frame or tibble\nUsing head(df, n = ) or tail(df, n = ) you can print the first (head) or last (tail) n lines of a data frame or tibble. Suppose you want to see the first 2 lines of df you would use:\n\nhead(df, n = 2)\n\n  numbers bools characters      dates\n1       1  TRUE          a 2025-03-25\n2       2 FALSE          b 2025-03-26\n\n\nTo see that last 3 lines of df_tib:\n\ntail(df_tib, n = 3)\n\n# A tibble: 3 × 4\n  numbers bools characters dates     \n    &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;      &lt;date&gt;    \n1       3 FALSE c          2025-03-27\n2       4 TRUE  d          2025-03-28\n3       5 TRUE  e          2025-03-29\n\n\n\n\n4.5.1.4 Coercing objects to data frame\nUsing as.data.frame() you can change another object in a data frame. Here, the arguments are largely the same as those for data.frame, with the exception that now you need to include an object you want to change into a data frame. For instance, let’s create a 2x3 matrix and add names:\n\nmat &lt;- matrix(round(runif(15), 2), 3, 5)\ncolnames(mat) &lt;- paste(\"var\", 1:5, sep = \"_\")\nrownames(mat) &lt;- paste(\"obs\", 1:3, sep = \"_\")\nmat\n\n      var_1 var_2 var_3 var_4 var_5\nobs_1  0.52  0.71  0.46  0.93  0.25\nobs_2  0.34  0.32  0.76  0.12  0.37\nobs_3  0.94  0.46  0.39  0.81  0.84\n\n\nChanging this matrix in a data frame, using as.data.frame():\n\nmat_df &lt;- as.data.frame(mat)\nmat_df\n\n      var_1 var_2 var_3 var_4 var_5\nobs_1  0.52  0.71  0.46  0.93  0.25\nobs_2  0.34  0.32  0.76  0.12  0.37\nobs_3  0.94  0.46  0.39  0.81  0.84\n\nstr(mat_df)\n\n'data.frame':   3 obs. of  5 variables:\n $ var_1: num  0.52 0.34 0.94\n $ var_2: num  0.71 0.32 0.46\n $ var_3: num  0.46 0.76 0.39\n $ var_4: num  0.93 0.12 0.81\n $ var_5: num  0.25 0.37 0.84\n\n\nNote that R used the row and column names of the matrix to add row and column names to the data frame. You can use your own row names if you add them via row.names = c() to the as.data.frame() function. Note that you can change a date frame (with only numeric variables) into a matrix. This allows you to use matrix operators (matrix algebra). Often this is much faster than writing code to perform the same calculations on a data frame. Using as.data.frame() you can then change the type of your matrix back into a data frame.\nYou can also change other objects in a data frame. For instance, here is a list\n\nlist1 &lt;- list(\n  company = c(\"Firm A\", \"Firm B\", \"Firm C\", \"Firm D\", \"Firm E\"),\n  sales = runif(5, min = 100000, max = 1000000),\n  margin = runif(5, min = 0.20, max = 0.36),\n  region = as.factor(c(1, 1, 2, 2, 2)))\n\nUsing as.data.frame():\n\nlist1_df &lt;- as.data.frame(list1)\nlist1_df\n\n  company    sales    margin region\n1  Firm A 733606.0 0.2465040      1\n2  Firm B 554201.6 0.3128367      1\n3  Firm C 860078.6 0.2024773      2\n4  Firm D 789595.5 0.2356981      2\n5  Firm E 608390.8 0.2843190      2\n\n\nChanges the list into a data frame. What happens with nested lists? To see this, let’s generate a second list:\n\nlist2 &lt;- list(\n  company = c(\"Firm F\", \"Firm G\", \"Firm H\", \"Firm I\", \"Firm J\"),\n  sales = runif(5, min = 1000, max = 10000),\n  margin = runif(5, min = 0.10, max = 0.16),\n  region = as.factor(c(1, 1, 3, 3, 3)))\n\nand create a nested list lest_nest using list1 and list2\n\nlist_nest &lt;- list(list1, list2)\n\nChanging list_nest into a data frame:\n\nlist_nest_df &lt;- as.data.frame(list_nest)\nlist_nest_df\n\n  company    sales    margin region company.1  sales.1  margin.1 region.1\n1  Firm A 733606.0 0.2465040      1    Firm F 9041.202 0.1170825        1\n2  Firm B 554201.6 0.3128367      1    Firm G 3581.459 0.1576101        1\n3  Firm C 860078.6 0.2024773      2    Firm H 9732.313 0.1462119        3\n4  Firm D 789595.5 0.2356981      2    Firm I 3881.885 0.1210791        3\n5  Firm E 608390.8 0.2843190      2    Firm J 2148.158 0.1326172        3\n\n\ncreates a data frame of 8 variables and 5 observations, not a data frame with 4 variables and 10 observations. In other words, here, you’ll need to change the lists on the second level into data frames first e.g. using\n\nlist_nest &lt;- lapply(list_nest, function(x) as.data.frame(x))\n\nand then use list_nest to extract the data frames. If all data frames in the nested list include the same variables, you can use rbind() to add them into one data frame. We will discuss rbind() for data frames more in depth in the next section. However, recall that you have used this function to add rows for matrices.\nUsing as_tibble() you need to specify the object will be changed in a tibble. In addition, you can add the .name_repair = c(\"check_unique\", \"unique\", \"universal\", \"minimal\") argument to repair names. Note that as.tibble (with a dot) also exists. This function has been replaced by as_tibble(). For instance, to coerce a matric into a tibble:\n\nmat_tib &lt;- tibble::as_tibble(mat)\nmat_tib\n\n# A tibble: 3 × 5\n  var_1 var_2 var_3 var_4 var_5\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.52  0.71  0.46  0.93  0.25\n2  0.34  0.32  0.76  0.12  0.37\n3  0.94  0.46  0.39  0.81  0.84\n\n\nNote that as_tibble() doesn’t include the row names. To do so, you need to add a variable where R can store the row names in a tibble. To so do, you add the argument rownames = \"name\" in the as_tibble() function. Doing so, the function will add the rownames from mat as a separate variable to the tibble. The name of this variable is name. For instance, adding the row names of mat to a variable rows in the mat_tib\n\nmat_tib &lt;- tibble::as_tibble(mat, rownames = \"rows\")\nmat_tib\n\n# A tibble: 3 × 6\n  rows  var_1 var_2 var_3 var_4 var_5\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 obs_1  0.52  0.71  0.46  0.93  0.25\n2 obs_2  0.34  0.32  0.76  0.12  0.37\n3 obs_3  0.94  0.46  0.39  0.81  0.84\n\n\nYou can also change a data frame into a tibble:\n\ndf_tib &lt;- tibble::as_tibble(mat_df)\ndf_tib\n\n# A tibble: 3 × 5\n  var_1 var_2 var_3 var_4 var_5\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.52  0.71  0.46  0.93  0.25\n2  0.34  0.32  0.76  0.12  0.37\n3  0.94  0.46  0.39  0.81  0.84\n\n\nIf your data frame has row names and you would like to keep them, you need to add rownames = \"name in the as_tibble() function:\n\ndf_tib &lt;- tibble::as_tibble(mat_df, rownames = \"abcdef\")\ndf_tib\n\n# A tibble: 3 × 6\n  abcdef var_1 var_2 var_3 var_4 var_5\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 obs_1   0.52  0.71  0.46  0.93  0.25\n2 obs_2   0.34  0.32  0.76  0.12  0.37\n3 obs_3   0.94  0.46  0.39  0.81  0.84\n\n\n\n\n4.5.1.5 Functions returning a data frame\nMany functions used to import data in R return a data frame, for instance: read.csv will import tabular data and return a data frame. This same also holds for many other packages that allow you to import data. We will cover examples in Chapter 6.\n\n\n\n4.5.2 Subsetting\n\n4.5.2.1 Subsetting a date frame or tibble: variables (columns)\nRecall that a data frame borrows characteristics from both a list and a matrix. In other words, you can use both the column number as well as its name to subset a column. Recall that there are 3 substting operators: [], [[]] and $. Let’s use each of them with a data frame.\n\n[] with column indices or column names:\n\n\ndf[1]\n\n  numbers\n1       1\n2       2\n3       3\n4       4\n5       5\n\ndf[\"numbers\"]\n\n  numbers\n1       1\n2       2\n3       3\n4       4\n5       5\n\nclass(df[\"numbers\"])\n\n[1] \"data.frame\"\n\n\nAs you can see, here too, as with lists, the [] operator is a preserving operator: it preserves that characteristics of the data frame. Note also see that the syntax resembles the list-syntax: you only include the column you want to extract and you don’t use e.g. [, 2] or [, \"numbers\"]. In other words, you don’t include an index for the rows. With data frames, R assumes you need all rows of the column. If you apply the operator to a tibble, the tibble structure will be preserved as well. Note the difference with subsetting a column with a matrix. There, we used mat[, i] to extract the ith column. Here, there is no reference to a row. You could use df[, 1] to subset the first column. However, in that case, R will return an unnamed vector if the subsetting is applied to a data frame. In other words, R simplifies the result as much as possible: treating a df as a matrix, causes R to simplify the output if possible. Doing the same with a tibble, will not cause a simplified result. Applied to a tibble, `[, 1] will return a tibble.\n\ndf[, 1]\n\n[1] 1 2 3 4 5\n\nis.vector(df[, 1])\n\n[1] TRUE\n\n\n\n[[]] with column indices or column names\n\n\ndf[[1]]\n\n[1] 1 2 3 4 5\n\ndf[[\"numbers\"]]\n\n[1] 1 2 3 4 5\n\nclass(df[[\"numbers\"]])\n\n[1] \"numeric\"\n\nis.vector(df[[\"numbers\"]])\n\n[1] TRUE\n\n\nThis operator returns a simplified result. Here, the first column is no longer a data frame but a vector. In other words [[]] act, as was the case with lists, as the simplifying operator.\n\n$ with column names\n\n\ndf$numbers\n\n[1] 1 2 3 4 5\n\nclass(df$numbers)\n\n[1] \"numeric\"\n\nis.vector(df$numbers)\n\n[1] TRUE\n\n\nAs was the case with lists, the $ operator with a data frame returns a simplified result. In other words, df$numbers is equivalent to df[[\"numbers]]. The $ operator is the most widely used to subset columns in a data frame or tibbles. However, there is one difference between data frames and tibbles. Data frames allows for partial matching while tibbles don’t. For instance, with a data frame:\n\ndf$numb\n\n[1] 1 2 3 4 5\n\n\nwill work even if there is not variable numb. Doing so with a tibble wouldn’t work:\n\ndf_tbl &lt;- tibble::as_tibble(df)\ndf_tbl$numb\n\nWarning: Unknown or uninitialised column: `numb`.\n\n\nNULL\n\n\nas you can see, R didn’t extract the values and gave a warning message.\nWith respect the multiple columns of negative index positions, data frames and tibbles are comparable to lists, vectors or matrices: a negative index position extracts all but the column with the negative index\n\ndf[-4]\n\n  numbers bools characters\n1       1  TRUE          a\n2       2 FALSE          b\n3       3 FALSE          c\n4       4  TRUE          d\n5       5  TRUE          e\n\n\nand selecting two ore more columns is similar to lists or matrices, e.g.:\n\ndf[c(1, 4)]\n\n  numbers      dates\n1       1 2025-03-25\n2       2 2025-03-26\n3       3 2025-03-27\n4       4 2025-03-28\n5       5 2025-03-29\n\n\nYou can extract a column using the pipe operator. Using base R’s pipe:\n\ndf |&gt; _$numbers\n\n[1] 1 2 3 4 5\n\n\nreturns df$numbers. This holds also for tibbles. Note that in case you would use magrittr pipe, you would need to change the _ in a dot ..\n\n\n4.5.2.2 Subsetting individual elements of a data frame or tibble\nThere are three ways to subset an individual value. They all return the same output:\n\ndf[2, 3]\n\n[1] \"b\"\n\ndf[[2, 3]]\n\n[1] \"b\"\n\ndf$characters[2]\n\n[1] \"b\"\n\n\nNegative indices extract all but that value, e.g.\n\ndf[-2, 3]\n\n[1] \"a\" \"c\" \"d\" \"e\"\n\n\nextracts all but the second row of the third column of df.\nHere, there is no difference between a tibble and a data frame.\n\n\n4.5.2.3 Subsetting using a logical index\nData frames show a lot of similarities with other data structures in terms of how you can use logical vectors to subset columns of rows. For instance extracting the dates on the condition that the value in the column numbers is larger than 2:\n\ncond &lt;- df$numbers &gt; 2\ndf$dates[cond]\n\n[1] \"2025-03-27\" \"2025-03-28\" \"2025-03-29\"\n\n\nor selecting multiple columns conditional upon numbers being larger than 2:\n\ndf[cond, 1:3]\n\n  numbers bools characters\n3       3 FALSE          c\n4       4  TRUE          d\n5       5  TRUE          e\n\n\nAs you could with the other data structures you can also extract columns using e.g. grepl(). Extracting variables whose name includes “numbers” or “dates” for instance, can be done using:\n\ndf[grepl(pattern = \"numbers|dates\", colnames(df))]\n\n  numbers      dates\n1       1 2025-03-25\n2       2 2025-03-26\n3       3 2025-03-27\n4       4 2025-03-28\n5       5 2025-03-29\n\n\nAs an alternative, the subset(x, subset, select, drop = FALSE, ...) function allows you to select the variables in a data frame df in select using a condition in subset. For instance, selecting columns “numbers”, “bools” and “character” for the rows where “numbers” is larger than 2:\n\nsubset(df, df$numbers &gt; 2, c(\"numbers\", \"bools\", \"characters\"))\n\n  numbers bools characters\n3       3 FALSE          c\n4       4  TRUE          d\n5       5  TRUE          e\n\n\nRecall that you extracted these values also using df[df$numbers &gt; 2, 1:3].\nIn subsequent chapters, we’ll use {dplyr}’s filter() and select() function to selects observations (filter()) and variables (select).\n\n\n\n4.5.3 Changing a data frame/tibble\n\n4.5.3.1 Changing individual elements\nChanging individual elements of a data frame is straightforward: you reassign their value as you did for vectors or matrices.\n\n\n4.5.3.2 Adding rows or columns\nWith respect to data frames, you can use cbind() and rbind() to add columns and rows to a data frame. These columns can be stored in vector, matrices or data frames. Recall that we used these function also for matrices. Suppose that you have a data frame df1 and vectors D and E. As you can see, df1 has 4 rows and 3 variables. As you may recall from the section on matrices, this means the columns you want to add need at least 4 rows and the rows you want to add need at least 3 columns.\n\ndf1 &lt;- data.frame(A = c(11, 21, 31, 41), B = c(12, 22, 32, 42), C = c(13, 23, 33, 43))\nD &lt;- c(14, 24, 34, 44)\nE &lt;- c(51, 52, 53, 54)\n\nLet’s now use cbind() to add the vector D to df1:\n\ncbind(df1, D)\n\n   A  B  C  D\n1 11 12 13 14\n2 21 22 23 24\n3 31 32 33 34\n4 41 42 43 44\n\n\nHere, R used the name of the vector as a variable name in the data frame df1. What if the vector is not named. To see what happens, let’s use\n\ncbind(df1, c(10, 11, 12, 13))\n\n   A  B  C c(10, 11, 12, 13)\n1 11 12 13                10\n2 21 22 23                11\n3 31 32 33                12\n4 41 42 43                13\n\n\nAs you can see, R selects a name from the values of the vector that was added. In other words, if the vector or matrix isn’t named, you need to add names before using cbind() or set names afterwards. Recall that you can create a component in a list using list$component &lt;- .... As data frames are lists, you can use the same approach to add a new variable to a dataset. For instance, to add the vector D to df1 you can also use\n\ndf1$D &lt;- c(14, 24, 34, 44)\ndf1\n\n   A  B  C  D\n1 11 12 13 14\n2 21 22 23 24\n3 31 32 33 34\n4 41 42 43 44\n\n\nAdding rows uses rbind(). Adding rows to a data frame is only relevant when the row you add include observations for the same variables. Suppose that the vector E included observations for variables A, B and C. Using rbind() you can add them to the data frame:\n\nrbind(df1, E)\n\n   A  B  C  D\n1 11 12 13 14\n2 21 22 23 24\n3 31 32 33 34\n4 41 42 43 44\n5 51 52 53 54\n\n\nTo add a data frame df2\n\ndf2 &lt;- data.frame(G = c(18, 28, 38, 48),  H = c(19, 29, 39, 49))\n\nto df1, you can use the same functions. For instance, adding the columns of df2 to those of df1:\n\ncbind(df1, df2)\n\n   A  B  C  D  G  H\n1 11 12 13 14 18 19\n2 21 22 23 24 28 29\n3 31 32 33 34 38 39\n4 41 42 43 44 48 49\n\n\nand adding the rows of df3\n\ndf3 &lt;- data.frame(A = c(51, 61), B = c(52, 62), C = c(53, 63), D = c(54, 64))\n\nto those of df1 using rbind():\n\nrbind(df1, df3)\n\n   A  B  C  D\n1 11 12 13 14\n2 21 22 23 24\n3 31 32 33 34\n4 41 42 43 44\n5 51 52 53 54\n6 61 62 63 64\n\n\n\n\n4.5.3.3 New columns using other columns\nOften you want to create a new variable where you use other values in your dataset. There are a couple of ways to do so. First you can create a new variable and add the calculation on the right hand side of the assignment operator. As an example, suppose that you want to add the log of A to df1. To do so, you can use\n\ndf1$logA &lt;- log(df1$A)\ndf1\n\n   A  B  C  D     logA\n1 11 12 13 14 2.397895\n2 21 22 23 24 3.044522\n3 31 32 33 34 3.433987\n4 41 42 43 44 3.713572\n\n\nUsing the with(data, expression, ...) you can avoid the references to the data frame in the calculation. The first argument in the function is the data frame where R will look for the variables used in expression. In other words, with(df1 ...) allows you to eliminate df1$ in your calculation. If you use A in that expression, R knows that this A is a variable included in df1. To add a column to df1 calculated as the ratio of df1$A/df1$B you would use:\n\ndf1$ratioAB &lt;- with(df1, A/B)\ndf1\n\n   A  B  C  D     logA   ratioAB\n1 11 12 13 14 2.397895 0.9166667\n2 21 22 23 24 3.044522 0.9545455\n3 31 32 33 34 3.433987 0.9687500\n4 41 42 43 44 3.713572 0.9761905\n\n\nWithout this function, you would have to write\n\ndf1$ratioABalt &lt;- df1$A/df1$B\ndf1\n\n   A  B  C  D     logA   ratioAB ratioABalt\n1 11 12 13 14 2.397895 0.9166667  0.9166667\n2 21 22 23 24 3.044522 0.9545455  0.9545455\n3 31 32 33 34 3.433987 0.9687500  0.9687500\n4 41 42 43 44 3.713572 0.9761905  0.9761905\n\n\nUsing with() you have to assign the result of a calculation to the data frame using df$newvar. Using the within() function, you can avoid this. This function has the same arguments as the with() function, but you add the name of the new variable in the expression part. The within() function returns a new data frame which is a copy of the old data frame plus the columns you added in the expression. In other words, the within() function preserves the “old” data frame and you have to assign the result of within() to a new data frame if you want to access these new values. If you are sure you won’t need the old data frame, you can assign the result of within() to that old data frame. Using within() also allows you to add multiple expressions. As an example, suppose you want to add the sum of A and B as well as the difference between D and C to the data frame (note the {} and the fact that every new variable has a new line without a comma at the end of the line):\n\ndfnew1 &lt;- within(df1, {\n  sumAB &lt;- A + B\n  diffDC &lt;- D - C\n  })\ndfnew1\n\n   A  B  C  D     logA   ratioAB ratioABalt diffDC sumAB\n1 11 12 13 14 2.397895 0.9166667  0.9166667      1    23\n2 21 22 23 24 3.044522 0.9545455  0.9545455      1    43\n3 31 32 33 34 3.433987 0.9687500  0.9687500      1    63\n4 41 42 43 44 3.713572 0.9761905  0.9761905      1    83\n\n\nIf you assign the results to an existing variable, within() overwrites this variable:\n\ndfnew2 &lt;- within(df1, {\n  A &lt;- A / 10\n  B &lt;- B * 10\n  C &lt;- C / D\n  })\ndfnew2\n\n    A   B         C  D     logA   ratioAB ratioABalt\n1 1.1 120 0.9285714 14 2.397895 0.9166667  0.9166667\n2 2.1 220 0.9583333 24 3.044522 0.9545455  0.9545455\n3 3.1 320 0.9705882 34 3.433987 0.9687500  0.9687500\n4 4.1 420 0.9772727 44 3.713572 0.9761905  0.9761905\n\n\nNote that you need to be careful when you design the sequance of expressions. For instance, if you first change A, and then use the value of A in your expression for B, R will use the new values for A as it doesn’t recall what the values of A where before you changed them.\n\n\n4.5.3.4 Deleting rows of columns\nTo delete rows and columns, you can use the familiar way. For instance, you can use\n\nnegative subsetting to remove column “C”\n\n\ndf4 &lt;- df1[-3]\ndf4\n\n   A  B  D     logA   ratioAB ratioABalt\n1 11 12 14 2.397895 0.9166667  0.9166667\n2 21 22 24 3.044522 0.9545455  0.9545455\n3 31 32 34 3.433987 0.9687500  0.9687500\n4 41 42 44 3.713572 0.9761905  0.9761905\n\n\n\nassigning NULL to remove column “B”\n\n\ndf1$B &lt;- NULL\ndf1\n\n   A  C  D     logA   ratioAB ratioABalt\n1 11 13 14 2.397895 0.9166667  0.9166667\n2 21 23 24 3.044522 0.9545455  0.9545455\n3 31 33 34 3.433987 0.9687500  0.9687500\n4 41 43 44 3.713572 0.9761905  0.9761905\n\n\n\nlogical condition to remove all observations that return FALSE (e.g. all observations for variable A that are not equal to 31):\n\n\ndf1[df1$A == 31, ]\n\n   A  C  D     logA ratioAB ratioABalt\n3 31 33 34 3.433987 0.96875    0.96875\n\n\nUsing the within() function, you can use the &lt;- NULL to delete multiple columns from your data frame:\n\ndfnew1 &lt;- within(dfnew1, {\n  A &lt;- NULL\n  ratioAB &lt;- NULL\n  ratioABalt &lt;- NULL\n  sumAB &lt;- NULL\n  })\ndfnew1\n\n   B  C  D     logA diffDC\n1 12 13 14 2.397895      1\n2 22 23 24 3.044522      1\n3 32 33 34 3.433987      1\n4 42 43 44 3.713572      1\n\n\n\n\n\n4.5.4 Data frames and functions\nThere is little difference between the approach you use to functions on a data frame and those for vectors, matrices or lists. This shouldn’t come as a surprise as a data frame is a list which characteristics of a matrix and R functions are vectorized. A couple of examples to illustrate some functions:\n\na summary of a date frame:\n\n\nsummary((df1))\n\n       A              C              D             logA          ratioAB      \n Min.   :11.0   Min.   :13.0   Min.   :14.0   Min.   :2.398   Min.   :0.9167  \n 1st Qu.:18.5   1st Qu.:20.5   1st Qu.:21.5   1st Qu.:2.883   1st Qu.:0.9451  \n Median :26.0   Median :28.0   Median :29.0   Median :3.239   Median :0.9616  \n Mean   :26.0   Mean   :28.0   Mean   :29.0   Mean   :3.147   Mean   :0.9540  \n 3rd Qu.:33.5   3rd Qu.:35.5   3rd Qu.:36.5   3rd Qu.:3.504   3rd Qu.:0.9706  \n Max.   :41.0   Max.   :43.0   Max.   :44.0   Max.   :3.714   Max.   :0.9762  \n   ratioABalt    \n Min.   :0.9167  \n 1st Qu.:0.9451  \n Median :0.9616  \n Mean   :0.9540  \n 3rd Qu.:0.9706  \n Max.   :0.9762  \n\n\n\nmeans per column\n\n\ncolMeans(df1)\n\n         A          C          D       logA    ratioAB ratioABalt \n26.0000000 28.0000000 29.0000000  3.1474942  0.9540381  0.9540381 \n\n\n\nmeans per row:\n\n\nrowMeans(df1)\n\n[1]  7.038538 12.158936 17.228581 22.277659\n\n\n\ntotal sum per column:\n\n\ncolSums(df1)\n\n         A          C          D       logA    ratioAB ratioABalt \n104.000000 112.000000 116.000000  12.589977   3.816153   3.816153 \n\n\n\ntotal sum per row:\n\n\nrowSums(df1)\n\n[1]  42.23123  72.95361 103.37149 133.66595\n\n\n\napply() function: mean per column:\n\n\napply(df1, 2, mean)\n\n         A          C          D       logA    ratioAB ratioABalt \n26.0000000 28.0000000 29.0000000  3.1474942  0.9540381  0.9540381 \n\n\n\nlapply() function: standard deviation per column:\n\n\nlapply(df1, \\(x) sd(x))\n\n$A\n[1] 12.90994\n\n$C\n[1] 12.90994\n\n$D\n[1] 12.90994\n\n$logA\n[1] 0.5700948\n\n$ratioAB\n[1] 0.02648301\n\n$ratioABalt\n[1] 0.02648301\n\n\n\nsapply() function: maximum per column:\n\n\nsapply(df1, \\(x) max(x))\n\n         A          C          D       logA    ratioAB ratioABalt \n41.0000000 43.0000000 44.0000000  3.7135721  0.9761905  0.9761905 \n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nCreate a 20x3 matrix mat with rownames obs_1 … and variable names var_1 … whose values are drawn from a uniform distribution with minimum 50 and maximum 100:\n\n\nCode\nrn &lt;- paste(\"obs\", 1:20, sep = \"_\")\ncn &lt;- paste(\"var\", 1:3, sep = \"_\")\nmat &lt;- matrix(runif(60, 50, 100), 20, 3, dimnames = list(rn, cn))\n\n\nCreate a data frame mat_df and a tibble mat_tb. Note that for the tibble, you need to include tibble::\n\nmat_df &lt;- as.data.frame(mat)\nmat_tb &lt;- tibble::as_tibble(mat)\n\nExtract the column var_1 from both and store in col_df and col_tb using the $ operator:\n\n\nCode\ncol_df &lt;- mat_df$var_1\ncol_tb &lt;- mat_tb$var_1\n\n\nCheck the class of both these columns you extracted:\n\n\nCode\ntypeof(col_df)\n\n\n[1] \"double\"\n\n\nCode\ntypeof(col_tb)\n\n\n[1] \"double\"\n\n\nLet’s now use a real dataset, mtcars, which is part of your R installation. Assign this dataset to a data frame df:\n\n\nCode\ndf &lt;- mtcars\n\n\nUse df to create a tibble tb of mtcars:\n\n\nCode\ntb &lt;- tibble::as_tibble(df)\n\n\nPrint both datasets by running only their name\n\ndata frame\n\n\n\nCode\ndf\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\ntibble:\n\n\n\nCode\ntb\n\n\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\nWhat is the difference in result between a data frame and a tibble?\nTell R to keep the row names from df when it creates the tibble tb and store the results in models:\n\n\nCode\ntb &lt;- tibble::as_tibble(df, rownames = \"models\")\ntb\n\n\n# A tibble: 32 × 12\n   models        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\nExtract the column hp from the df data frame and assign this column to a variable hp\n\n\nCode\nhp &lt;- df$hp\n\n\nExtract the column disp from the tibble tb using the [] operator. Assign this column to a variable disp:\n\n\nCode\ndisp &lt;- tb[\"disp\"]\n\n\nIf you ask R to print this variable (do this in the console) what do you expect will happen: R prints all lines or R prints the first 10 lines?\nExtract from df the observations for cars that include a digit at the end of their name (e.g. Duster 360, Mazda RX4):\n\n\nCode\npat &lt;- \"\\\\d+$\"\ndf[grepl(pattern = pat, x = row.names(df)), ]\n\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nDatsun 710    22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nDuster 360    14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230      22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280      19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nFiat 128      32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nCamaro Z28    13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nFiat X1-9     27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\nDo the same, but now, use the tibble tb\n\n\nCode\npat &lt;- \"\\\\d+$\"\ntb[grepl(pattern = pat, x = tb$models), ]\n\n\n# A tibble: 9 × 12\n  models         mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4     21       6 160     110  3.9   2.62  16.5     0     1     4     4\n2 Datsun 710    22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n3 Duster 360    14.3     8 360     245  3.21  3.57  15.8     0     0     3     4\n4 Merc 230      22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n5 Merc 280      19.2     6 168.    123  3.92  3.44  18.3     1     0     4     4\n6 Fiat 128      32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n7 Camaro Z28    13.3     8 350     245  3.73  3.84  15.4     0     0     3     4\n8 Fiat X1-9     27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n9 Porsche 914…  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n\n\nExtract all observations from df whose am == 1:\n\n\nCode\ndf[df$am == 1, ]\n\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nAdd a new variable to the tibble, tb$mpg_cyl, calculated as the ratio of the variable mpg and cyl:\n\ntb$mpg_cyl &lt;- with(tb, mpg/cyl)\ntb\n\n# A tibble: 32 × 13\n   models        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n# ℹ 1 more variable: mpg_cyl &lt;dbl&gt;\n\n\nUse the within() function to add 3 columns to df: mgp/cyl, mgp/hp and mpg/disp. Store these in mpg_cyl, mpg_hp and mpg_disp. Overwrite df and show the first 5 lines of this new data frame using `head(x, n = 5):\n\n\nCode\ndf &lt;- within(df, {\n  mpg_cyl &lt;- mpg/cyl\n  mpg_hp &lt;- mpg/hp\n  mpg_disp &lt;- mpg/disp\n})\nhead(df, n = 5)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb   mpg_disp\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 0.13125000\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 0.13125000\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 0.21111111\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 0.08294574\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 0.05194444\n                     mpg_hp  mpg_cyl\nMazda RX4         0.1909091 3.500000\nMazda RX4 Wag     0.1909091 3.500000\nDatsun 710        0.2451613 5.700000\nHornet 4 Drive    0.1945455 3.566667\nHornet Sportabout 0.1068571 2.337500\n\n\nUse apply() to calculate the mean per variable in df:\n\n\nCode\napply(df, 2, mean, na.rm = TRUE)\n\n\n        mpg         cyl        disp          hp        drat          wt \n 20.0906250   6.1875000 230.7218750 146.6875000   3.5965625   3.2172500 \n       qsec          vs          am        gear        carb    mpg_disp \n 17.8487500   0.4375000   0.4062500   3.6875000   2.8125000   0.1398688 \n     mpg_hp     mpg_cyl \n  0.1905456   3.8369792 \n\n\nDo the same, but now for the tibble tb:\n\n\nCode\ncond &lt;- sapply(tb, \\(x) is.numeric(x))\napply(tb[cond], 2, mean)\n\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb    mpg_cyl \n  0.437500   0.406250   3.687500   2.812500   3.836979 \n\n\nAsk for a summary table of df\n\n\nCode\nsummary(df)\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb          mpg_disp      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02203  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04956  \n Median :0.0000   Median :4.000   Median :2.000   Median :0.09458  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.13987  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.17740  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.47679  \n     mpg_hp           mpg_cyl     \n Min.   :0.04478   Min.   :1.300  \n 1st Qu.:0.08944   1st Qu.:1.928  \n Median :0.15041   Median :3.108  \n Mean   :0.19055   Mean   :3.837  \n 3rd Qu.:0.24129   3rd Qu.:5.700  \n Max.   :0.58462   Max.   :8.475  \n\n\nPredict the outcome if you would run summary(tb). Create the same table as the result of df but now for tb:\n\n\nCode\nsummary(tb[cond])\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb          mpg_cyl     \n Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :1.300  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:1.928  \n Median :0.0000   Median :4.000   Median :2.000   Median :3.108  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :3.837  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.700  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :8.475",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#other-objects",
    "href": "04_Data_structures.html#other-objects",
    "title": "4  Data structures",
    "section": "4.6 Other objects",
    "text": "4.6 Other objects\n\nmodels\nformulas\n\n\n\n\n\n\n\nBengtsson, Henrik. 2025. matrixStats: Functions That Apply to Rows and Columns of Matrices (and to Vectors). https://github.com/henrikbengtsson/matrixstats.",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#time-series",
    "href": "04_Data_structures.html#time-series",
    "title": "4  Data structures",
    "section": "4.6 Time series",
    "text": "4.6 Time series\n\n4.6.1 Introduction\nTime series are special as their observations are observed, measured or recorded at the specific moment in time (a date or a data/time). In economics and management, a lost of data come in the form of a time series: sales are measured per month quarter or year, accounting data refers to a specific year, semester of quarter, stock prices are recorded by day, hour or minute, inflation or unemployment are usually reported on a monthly basis. This property has a couple of consequences. First, these observations are ordered. The data or time allows to say which observation comes first, which second and which observation comes last. Extracting observations from a time needs to preserve this property. Second, most time frames can be aggregated. For instance, a week is a aggregation of ways, a year an aggregation of quarters, months, weeks or days and an hour is an aggregation of minutes. In order words, you can start from a monthly time series and generate a yearly series. How you do so depends on the series. For instance, you can add 4 quarters of sales to calculate yearly sales. However, this is not the case for, e.g. stock market prices where the sum of prices across time doesn’t make sense. Here, you would need another measure e.g. the price at the end of the last hour of trading as your price for the day or the last price at the end of the month for a monthly series with stock market prices. Third, time can be regular or irregular. If time is regular, then you measure something at evenly spaced moments in time: every month, every year of every minute. If time series are irregular, this is not the case. For instance, if you measure the noise generated by departing airplanes in areas close to the airport, you’ll have measure each time an airplane takes off. Here, you time will show irregular intervals.\nIn addition to pure time series, a lot of datasets include both cross sections (e.g. firms) as well as time series (e.g. sales per year). This is called a panel dataset: for every firm, country, household, … in your dataset, you observe variables at multiple times e.g. on observation for every year for the last 10 years. If you have a dataset that includes sales data for 50 products, you panel dataset includes 500 observations: for every product, you have 10 observations: one per year for each of the 10 years in your dataset.\nTo handle time series, R includes the ts() class. This class is uses regular time intervals. In addition, there are many packages that extend the ability of R to use time series e.g. {zoo} or {xts}. These packages also allow irregular time intervals. The time series equivalent of a tibble is called a tsibble and is used in the {tsibble} package (Wang, Cook, and Hyndman (2020)). This package allows you to change, mutate or time series data. Using these formats, packages such as {quantmod}, {tidyfinance}, {forecast} or {econometrics} all use these formats to e.g. develop quantitative trading strategies ({quantmod}), analyse financial data ({tidyfinance}), develop forecasts ({forecast}) or estimate regressions including methods for time series ({econometrics}).\n\n\n4.6.2 The basics\nIn this section we will use base R’s ts() as wel as the {xts} (eXtendible time series) package. The latter automatically installs {zoo}. To install {xts} you run\n\nif (!require(\"xts\")) install.packages(\"xts\")\n\nLoading required package: xts\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n4.6.2.1 Creating a time series: ts()\nTo create a time series, you need to include both the data as well as the date/time values. With respect to the first, let’s create a vector with 25 values drawn as a sequence starting at 10 in steps of 10:\n\ndata &lt;- seq(10, by = 10, length.out = 25)\n\nNote that data could also include a matrix or a data frame. We now want to create a time series. To do so, we need to add the “data/time” dimension. Using base R’s ts() function, you can add a start, end and a frequency. The start is included as a value or a vector. For instance start = 2001 is start the series in 2001, start = c(2001, 1) will start the series in 2001-01. The frequency shows the sampling frequency of the time series, 1 would refer to year, 4 refers to a quarterly data and 12 to monthly. Specifying the start and frequency allows R to determine the end date from the length of the series. Let’s create a yearly time series for the values in data starting in 2000. To do so, we use:\n\nts_data_year &lt;- ts(data, start = 2000, frequency = 1)\n\nIf you print the series,\n\nts_data_year\n\nTime Series:\nStart = 2000 \nEnd = 2024 \nFrequency = 1 \n [1]  10  20  30  40  50  60  70  80  90 100 110 120 130 140 150 160 170 180 190\n[20] 200 210 220 230 240 250\n\n\nyou see that R created a time series with start in 2000, end in 2024 with frequency equal to 1, i.e. yearly.\nTo create quarterly data, you can use\n\nts_data_quar &lt;- ts(data, start = c(2015, 1), frequency = 4)\nts_data_quar\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n2015   10   20   30   40\n2016   50   60   70   80\n2017   90  100  110  120\n2018  130  140  150  160\n2019  170  180  190  200\n2020  210  220  230  240\n2021  250               \n\n\nR adds the reference to quarters and determines the final quarter from the length of the data. To create a monthly series starting in june, you change the frequency to 12 and change the start month:\n\nts_data_mont &lt;- ts(data, start = c(2023, 6), frequency = 12)\nts_data_mont\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2023                      10  20  30  40  50  60  70\n2024  80  90 100 110 120 130 140 150 160 170 180 190\n2025 200 210 220 230 240 250                        \n\n\nYou can verify that these series are time series using class(). For instance, to check of ts_data_mont is a time series, you use\n\nclass(ts_data_mont)\n\n[1] \"ts\"\n\n\nYou can extend this example to e.g. matrices. In that case, data will be a matrix.\n\n\n4.6.2.2 The {xts} package\nFist let’s load the package\n\nlibrary(xts)\n\nAs you can see, this package loads another package {zoo}. This is because the {xts} relies on some of the functions in the {zoo} package.\nLet’s now create a date/time variable using using seq.POSIXt() with length 25 (consistent with the the length of data) and in intervals of months:\n\ndatetime &lt;- seq.POSIXt(from = as.POSIXct(\"2022-03-25\"), length.out = 25, by = \"months\")\n\nCreating an {xts} object now uses as.xts(). The first argument if this function is the dataset, in this case data. The second argument is the date/time variable:\n\ndata_xts &lt;- as.xts(data, datetime)\n\nInspecting this time series, shows that the dates/times are added to data as row names and in the usual ISO format: %Y-%m-%d.\n\ndata_xts\n\n           [,1]\n2022-03-25   10\n2022-04-25   20\n2022-05-25   30\n2022-06-25   40\n2022-07-25   50\n2022-08-25   60\n2022-09-25   70\n2022-10-25   80\n2022-11-25   90\n2022-12-25  100\n2023-01-25  110\n2023-02-25  120\n2023-03-25  130\n2023-04-25  140\n2023-05-25  150\n2023-06-25  160\n2023-07-25  170\n2023-08-25  180\n2023-09-25  190\n2023-10-25  200\n2023-11-25  210\n2023-12-25  220\n2024-01-25  230\n2024-02-25  240\n2024-03-25  250\n\n\nThe class of the time series data_xts is “xts”, “zoo”. The latter is included because the former builds on the latter.\n\nclass(data_xts)\n\n[1] \"xts\" \"zoo\"\n\n\nNote that the data in {xts} are essentially matrices. In other words, and {xts} object can not store more than one variable type. For most applications, this is usually not too much of an issue. However, if you data includes a mix of types, you’ll need to store the numeric variables in a separate data set.\n\n\n4.6.2.3 Coercing other structures into a time series\nYou can coerce other data structures into a time series object. To illustrate this, let’s first create two other objects: a 50x4 matrix and a data frame. Let’s first create a matrix with values and add a matrix with 50 monthly dates.\n\nmat1 &lt;- matrix(runif(200, min = 50, max = 100), 50, 4)\n\ncolnames(mat1) &lt;- paste(\"var\", 1:4, sep = \"_\")\nrownames(mat1) &lt;- paste(\"obs\", 1:50, sep = \"_\")\n\nmat_dates &lt;- seq.POSIXt(from = as.POSIXct(\"2020-03-25\"), \n                        length.out = 50, \n                        by = \"months\")\n\nmat &lt;- cbind(mat_dates, mat1)\n\nRecall that a matrix is a homogeneous structure. In other words, the dates will be converted into numeric format.\nUsing this matrix, we can create a data frame. Here, we can add various types of data.\n\nmat1_df &lt;- as.data.frame(mat1, row.names = rownames(mat1))\nmat_df &lt;- cbind(mat_dates, mat1_df)\n\nNote that in this case, the column with dates is shown as a date/time variable. Let’s now use the {xts} package to coerce both in a time series format. The as.xts() function has multiple arguments: as.xts(x, order.by, dateFormat = \"POSIXct\", ...). The first, x is the matrix or data frame. The second, order.by = should include a variable that allows R to order the values in x. The dataFormat argument allows you to change the format from the default POSIXct to e.g. Date. Let’s use this function to change the matrix into a time series:\n\nmat_ts &lt;- as.xts(mat, order.by = as.POSIXct(mat[, 1], format = \"%Y-%m-%d\"))\nhead(mat_ts, 5)\n\n            mat_dates    var_1    var_2    var_3    var_4\n2020-03-25 1585090800 61.81532 71.34831 84.25773 87.37033\n2020-04-25 1587765600 83.67822 91.79865 79.53507 90.04232\n2020-05-25 1590357600 73.80289 58.45403 62.74126 71.43401\n2020-06-25 1593036000 62.37750 72.38072 71.67449 92.25952\n2020-07-25 1595628000 64.98585 65.08229 91.09376 85.55991\n\n\nThe function returns a time series, where it used the dates in mat_dates in the first column of mat to add date/time values to the matrix. In doing to, it kept mat_dates as a separate numeric variable in the data set.\nThe data frame includes the data/time variable as a POSIXct type. In other words, the time series includes the date as a separate variable. As a result, you don’t need to coerce that variable into a date in the as.xts() functions. It if sufficient to include it in the order.by = argument:\n\nmat_dfts &lt;- as.xts(mat_df, order.by = mat_df$mat_dates)\nhead(mat_dfts, 5)\n\n            mat_dates    var_1    var_2    var_3    var_4\n2020-03-25 2020-03-25 61.81532 71.34831 84.25773 87.37033\n2020-04-25 2020-04-25 83.67822 91.79865 79.53507 90.04232\n2020-05-25 2020-05-25 73.80289 58.45403 62.74126 71.43401\n2020-06-25 2020-06-25 62.37750 72.38072 71.67449 92.25952\n2020-07-25 2020-07-25 64.98585 65.08229 91.09376 85.55991\n\n\nNote that here too, R kept the mat_dates variable in the time series dataset. However, here you are including various data types in an xts object. Recall that these objects are essentially matrices. R will change the type of these variables. To avoid that, you need to exclude this mat_dates variable from the coercion:\n\nmat_dfts &lt;- as.xts(mat_df[, 2:5], order.by = mat_df$mat_dates)\nhead(mat_dfts, 5)\n\n              var_1    var_2    var_3    var_4\n2020-03-25 61.81532 71.34831 84.25773 87.37033\n2020-04-25 83.67822 91.79865 79.53507 90.04232\n2020-05-25 73.80289 58.45403 62.74126 71.43401\n2020-06-25 62.37750 72.38072 71.67449 92.25952\n2020-07-25 64.98585 65.08229 91.09376 85.55991\n\n\n\n\n\n4.6.3 Subsetting\nLet’s now use mat_dfts to extract specific variables. Most subsetting approaches that we covered for other data structures can be used for xts time series as well. Note that here, if you use the preserving subsetting operator [], the result will always show the relevant data/time as R preserves the structure of the dataset. For example:\n\nextracting columns 2 to 3:\n\n\nhead(mat_dfts[, 2:3], n = 5)\n\n              var_2    var_3\n2020-03-25 71.34831 84.25773\n2020-04-25 91.79865 79.53507\n2020-05-25 58.45403 62.74126\n2020-06-25 72.38072 71.67449\n2020-07-25 65.08229 91.09376\n\n\n\nextracting all columns but the first:\n\n\nhead(mat_dfts[, -1], n = 5)\n\n              var_2    var_3    var_4\n2020-03-25 71.34831 84.25773 87.37033\n2020-04-25 91.79865 79.53507 90.04232\n2020-05-25 58.45403 62.74126 71.43401\n2020-06-25 72.38072 71.67449 92.25952\n2020-07-25 65.08229 91.09376 85.55991\n\n\n\nextracting the values for the 4th row:\n\n\nmat_dfts[4, ]\n\n             var_1    var_2    var_3    var_4\n2020-06-25 62.3775 72.38072 71.67449 92.25952\n\n\nUsing the $ operator, you can extract variables, e.g.\n\nhead(mat_dfts$var_1, n = 10)\n\n              var_1\n2020-03-25 61.81532\n2020-04-25 83.67822\n2020-05-25 73.80289\n2020-06-25 62.37750\n2020-07-25 64.98585\n2020-08-25 52.63511\n2020-09-25 66.57814\n2020-10-25 66.31016\n2020-11-25 53.81508\n2020-12-25 50.25629\n\n\nIn addition, and specifically for time series, you can use the date/times to extract specific components. For instance:\n\na specific date:\n\n\nmat_dfts[\"2020-07-25\"]\n\n              var_1    var_2    var_3    var_4\n2020-07-25 64.98585 65.08229 91.09376 85.55991\n\n\n\na range of dates using [“start/end”]:\n\n\nmat_dfts[\"2020-03-25/2020-07-25\"]\n\n              var_1    var_2    var_3    var_4\n2020-03-25 61.81532 71.34831 84.25773 87.37033\n2020-04-25 83.67822 91.79865 79.53507 90.04232\n2020-05-25 73.80289 58.45403 62.74126 71.43401\n2020-06-25 62.37750 72.38072 71.67449 92.25952\n2020-07-25 64.98585 65.08229 91.09376 85.55991\n\n\n\nfrom the beginning of the series to date [“/end”]:\n\n\nmat_dfts[\"/2020-07-25\"]\n\n              var_1    var_2    var_3    var_4\n2020-03-25 61.81532 71.34831 84.25773 87.37033\n2020-04-25 83.67822 91.79865 79.53507 90.04232\n2020-05-25 73.80289 58.45403 62.74126 71.43401\n2020-06-25 62.37750 72.38072 71.67449 92.25952\n2020-07-25 64.98585 65.08229 91.09376 85.55991\n\n\n\nfrom start to the last date [“start/”]:\n\n\nmat_dfts[\"2023-12-25/\"]\n\n              var_1    var_2    var_3    var_4\n2023-12-25 76.81282 78.46668 88.45526 82.58745\n2024-01-25 91.99313 69.49864 60.66829 86.80344\n2024-02-25 80.60366 63.13936 74.42565 97.02449\n2024-03-25 81.18256 90.97987 52.96503 61.63764\n2024-04-25 76.47097 99.46359 91.42821 96.14635\n\n\n\nan entire year [“2022”]:\n\n\nmat_dfts[\"2022\"]\n\n              var_1    var_2    var_3    var_4\n2022-01-25 83.15106 77.58532 88.85743 91.75516\n2022-02-25 50.57743 93.83828 59.57863 56.19557\n2022-03-25 92.09622 54.00683 66.53835 66.70615\n2022-04-25 62.74961 58.89334 56.34485 68.11273\n2022-05-25 88.70798 87.75809 65.69200 57.85879\n2022-06-25 72.01018 79.88379 61.53902 95.35869\n2022-07-25 94.04675 66.27377 75.93474 86.22792\n2022-08-25 52.57497 91.33814 85.01536 73.57971\n2022-09-25 51.21142 71.47336 53.46573 77.88881\n2022-10-25 70.93977 65.64055 70.35639 55.68449\n2022-11-25 80.27070 94.45147 75.94056 55.40697\n2022-12-25 58.05967 74.95672 81.39050 50.77193\n\n\nIf you have daily data for instance, you can plot a single month adding [“2022-03”]. Here you will extract all values for the month March in 2022.\nUsing first() and last() you can extract the first x weeks of the dataset by including x weeks in the function first() and the last y months by including y months in the function last(). Note that you can refer to weeks even if the periodicity of the dataset is monthly. R will extract the all months within this x week period. Valid periods are seconds, minutes, hours, days, weeks, months, quarters and years. For instance:\n\nextract the data for the first 2 quarters in the dataset (here the first quarter includes only 1 month):\n\n\nfirst(mat_dfts, \"2 quarters\")\n\n              var_1    var_2    var_3    var_4\n2020-03-25 61.81532 71.34831 84.25773 87.37033\n2020-04-25 83.67822 91.79865 79.53507 90.04232\n2020-05-25 73.80289 58.45403 62.74126 71.43401\n2020-06-25 62.37750 72.38072 71.67449 92.25952\n\n\n\nextract the last 2 quarters (note that here the last quarter includes only one month):\n\n\nlast(mat_dfts, \"2 quarters\")\n\n              var_1    var_2    var_3    var_4\n2024-01-25 91.99313 69.49864 60.66829 86.80344\n2024-02-25 80.60366 63.13936 74.42565 97.02449\n2024-03-25 81.18256 90.97987 52.96503 61.63764\n2024-04-25 76.47097 99.46359 91.42821 96.14635\n\n\nCombining first() and last():\n\nextract the first 3 months of the last 4 quarters:\n\n\nfirst(last(mat_dfts, \"4 quarters\"), \"3 months\")\n\n              var_1    var_2    var_3    var_4\n2023-07-25 60.92104 76.31153 93.02249 97.34942\n2023-08-25 86.19620 78.71048 65.48694 94.27129\n2023-09-25 80.78298 81.12048 95.89146 92.30111\n\n\nRecall that mat_dfts includes a monthly time series. You can determine the endpoints for another time interval, e.g. quarter or year. Doing so, R selects the last observations per quarter or per year. In addition to year and quarter, you can also determine the endpoints for months, hours and minutes. Using these endpoints, you can extract the data for these moments.\nLet’s first determine the endpoints per year (i.e. the last observations for a year):\n\nend_year &lt;- endpoints(mat_dfts, on = \"year\")\nend_year\n\n[1]  0 10 22 34 46 50\n\n\nThese observations are included on the 10th row, the 22th row, … . Using this vector to subset the time series now allows to extract the values for all variables in mat_dfts:\n\nmat_dfts[end_year]\n\n              var_1    var_2    var_3    var_4\n2020-12-25 50.25629 71.80983 54.86632 51.58079\n2021-12-25 73.69827 60.67609 59.01273 80.17038\n2022-12-25 58.05967 74.95672 81.39050 50.77193\n2023-12-25 76.81282 78.46668 88.45526 82.58745\n2024-04-25 76.47097 99.46359 91.42821 96.14635\n\n\nThere are two special functions that allow you to extract the core data and the index. The first refers to all variables, other than the date/time index. To extract these variable, you use the coredata() function:\n\ncore &lt;- coredata(mat_dfts)\nhead(core, n = 5)\n\n        var_1    var_2    var_3    var_4\n[1,] 61.81532 71.34831 84.25773 87.37033\n[2,] 83.67822 91.79865 79.53507 90.04232\n[3,] 73.80289 58.45403 62.74126 71.43401\n[4,] 62.37750 72.38072 71.67449 92.25952\n[5,] 64.98585 65.08229 91.09376 85.55991\n\n\nThe index refers to the date/time index. Using the index() function allows you to extract these values:\n\ndatetime &lt;- index(mat_dfts)\nhead(datetime, n = 5)\n\n[1] \"2020-03-25 CET\"  \"2020-04-25 CEST\" \"2020-05-25 CEST\" \"2020-06-25 CEST\"\n[5] \"2020-07-25 CEST\"\n\n\n\n\n4.6.4 Time series functions\n\n4.6.4.1 Data on the time series\nCounting the number of months, quarters or years in a time series dataset can be done using nmonths(), nquarters() or nyears(). For instance, mat_dfts includes:\n\nnmonths(mat_dfts)\n\n[1] 50\n\n\n50 months,\n\nnquarters(mat_dfts)\n\n[1] 18\n\n\n18 quarters and\n\nnyears(mat_dfts)\n\n[1] 5\n\n\n5 years of data.\nNote that the here, the first and last of these five years doesn’t include data for all 12 months in that year.\nYou can determine the periodicity (e.g. monthly, yearly, hourly) using periodicity(). The function estimates the frequency of the time series observations:\n\nperiodicity(mat_dfts)\n\nMonthly periodicity from 2020-03-25 to 2024-04-25 \n\n\n\n\n4.6.4.2 Lags and leads\nIn addition to the function we have introduced for other data structures, there are a couple of function specific to time series. The first function is lag(x, k). This function computes the lagged version of a time series. For instance, with k = 1 the lag of a monthly series shifts the series one month back in time. In doing so, the observation for the lag of march 2025 is february 2025. This allows you to compute the difference between to observations across time. The default value for k = 1. Changing this to e.g. 12 for a monthly series computes the value for the same variable 12 months ago. Because the first k observations are missing, R changes these values into NA. For instance, to determine the monthly change in all variables included in mat_dfts\n\nmat_lag1 &lt;- mat_dfts - lag(mat_dfts, k = 1)\nhead(mat_lag1, 5)\n\n                var_1      var_2      var_3      var_4\n2020-03-25         NA         NA         NA         NA\n2020-04-25  21.862903  20.450336  -4.722668   2.671986\n2020-05-25  -9.875329 -33.344624 -16.793810 -18.608307\n2020-06-25 -11.425395  13.926690   8.933233  20.825511\n2020-07-25   2.608355  -7.298423  19.419271  -6.699614\n\n\nIf you change k = 1 into k = 12 calculated the change relative to the same month in the previous year. This is often referred to as Year of Year (YoY) changes:\n\nmat_lag12 &lt;- mat_dfts - lag(mat_dfts, k = 12)\nlast(mat_lag12, 5)\n\n                var_1      var_2       var_3      var_4\n2023-12-25  18.753147   3.509959   7.0647632  31.815517\n2024-01-25   7.487500 -28.240115  -4.3133763  36.497257\n2024-02-25 -18.817803 -31.976606   0.2734934  16.111089\n2024-03-25   7.618603   9.233648 -21.6129541 -34.376052\n2024-04-25   6.163255   9.653780  29.0678414  -1.707843\n\n\nUsing diff(x, lag = 1, differences = 1) allows you to calculate similar differences. The lag = 1 arguments specifies the lag and is simular to the k = 1 argument in the lag() function. The differences = 1 argument allows you to specify the order of the differencing. The first order (by default) calculate the difference between the levels. The second order calculate the difference in the differences (i.e. second derivative). To illustrate:\n\nmat_dif1 &lt;- diff(mat_dfts, lag = 1, differences = 1)\nhead(mat_dif1, 5)\n\n                var_1      var_2      var_3      var_4\n2020-03-25         NA         NA         NA         NA\n2020-04-25  21.862903  20.450336  -4.722668   2.671986\n2020-05-25  -9.875329 -33.344624 -16.793810 -18.608307\n2020-06-25 -11.425395  13.926690   8.933233  20.825511\n2020-07-25   2.608355  -7.298423  19.419271  -6.699614\n\n\ncalculates the same change as x - lag(x, k = 1). However,\n\nmat_dif2 &lt;- diff(mat_dfts, lag = 1, differences = 2)\nhead(mat_dif2, n = 5)\n\n                var_1     var_2     var_3     var_4\n2020-03-25         NA        NA        NA        NA\n2020-04-25         NA        NA        NA        NA\n2020-05-25 -31.738232 -53.79496 -12.07114 -21.28029\n2020-06-25  -1.550066  47.27131  25.72704  39.43382\n2020-07-25  14.033750 -21.22511  10.48604 -27.52512\n\n\ncalculates the change in the difference: the difference in the difference of the second order difference.\n\n\n4.6.4.3 period.apply\nRecall the apply function for matrices. The period.apply() function has a similar use for time series. The function requires an xts object, an index and a function. The index needs to define non-overlapping intervals. The endpoints() function is an example that allows you to specify these intervals. You can also specify your own vector. As long as it starts and ends with the number of rows in the xts object and includes non overlapping intervals. The period.apply() function will then apply a function to all observations within an interval. For instance, recall that endpoints returns a vector with index breakpoints:\n\nend_year &lt;- endpoints(mat_dfts, on = \"years\")\nend_year\n\n[1]  0 10 22 34 46 50\n\n\nThe first inverval runs from 0 to the 10th observations. The second yearly interval from the 11th to the 22th observation, … . You can now use period.apply() to calculate e.g. the mean for every year:\n\nperiod.apply(mat_dfts, INDEX = end_year, FUN = colMeans)\n\n              var_1    var_2    var_3    var_4\n2020-12-25 63.62546 71.06288 77.21998 81.80398\n2021-12-25 75.99454 74.77712 72.00702 77.25454\n2022-12-25 71.36631 76.34164 70.05446 69.62891\n2023-12-25 78.67123 82.97227 78.22946 75.37588\n2024-04-25 82.56258 80.77036 69.87180 85.40298\n\n\nAs you can see, this code returns the mean value per year for all 4 variables. If it would make more sense to calculate the sum, e.g.\n\nperiod.apply(mat_dfts, end_year, colSums)\n\n              var_1    var_2    var_3    var_4\n2020-12-25 636.2546 710.6288 772.1998 818.0398\n2021-12-25 911.9344 897.3254 864.0843 927.0545\n2022-12-25 856.3958 916.0997 840.6536 835.5469\n2023-12-25 944.0548 995.6672 938.7536 904.5106\n2024-04-25 330.2503 323.0815 279.4872 341.6119\n\n\nMore in general, for every non-overlapping periode in the INDEX, the function period.apply() will apply the function in FUN. The index is a vector with positions what show the end points of every interval. For instance c(0, 3, 6, 9) would introduce intervals covering the first 3 observations, observations 4, 5 and 6, observations 7, 8 and 9, … . For each of these three observations, R would then apply the function in FUN. If this function is colMeans, it would apply, for every variable in the dataset, this function to every time interval and colSums calculates, for every variable in the dataset, the sum of the three components in each of the time intervals.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nMake sure that the {xts} package is loaded.\nCreate a 104x2 matrix data with column names high and low and values runif(104, 100, 200) and runif(104, 10, 20):\n\n\nCode\ndata &lt;- matrix(c(runif(104, 100, 200), runif(104, 10, 20)), 104, 2)\ncolnames(data) &lt;- c(\"high\", \"low\")\n\n\nAdd a weekly time sequence starting 2023-01-01 with 104 weeks and assign the value weeks and add this variable to the data matrix:\n\n\nCode\nweeks &lt;- seq.POSIXt(from = as.POSIXct(\"2023-01-01\", format = \"%Y-%m-%d\", tz = \"UTC\"), length.out = 104, by = \"weeks\")\ndata &lt;- cbind(weeks, data)\n\n\nAdd both in an xts object datats and remove the weeks column:\n\n\nCode\ndatats &lt;- as.xts(data, order.by = as.POSIXct(data[, 1]))\ndatats &lt;- datats[, -1]\n\n\nDetermine the periodicity of datats as well as the number of months and years:\n\n\nCode\nperiodicity(datats)\n\n\nWeekly periodicity from 2023-01-01 01:00:00 to 2024-12-22 01:00:00 \n\n\nCode\nnmonths(datats)\n\n\n[1] 24\n\n\nCode\nnyears(datats)\n\n\n[1] 2\n\n\nDetermine the quarterly end ponts\n\n\nCode\nend_quar &lt;- endpoints(datats, on = \"quarter\")\n\n\nUse the period.apply() function to calculate the sum per quarter of the variables in datats. Store the results in datatsq\n\n\nCode\ndatatsq &lt;- period.apply(datats, end_quar, colSums)\ndatatsq\n\n\n                        high      low\n2023-03-26 01:00:00 1873.203 185.6338\n2023-06-25 02:00:00 2058.894 211.4065\n2023-09-24 02:00:00 1871.825 197.9169\n2023-12-31 01:00:00 2219.496 180.3415\n2024-03-31 01:00:00 1924.175 191.3454\n2024-06-30 02:00:00 2034.071 178.1488\n2024-09-29 02:00:00 1901.548 199.3277\n2024-12-22 01:00:00 1931.220 163.2491\n\n\nCalculate the monthly difference for the variables in datats and store the results in diff_datats:\n\n\nCode\ndiff_datats &lt;- diff(datats, lag = 1, difference = 1)\n\n\nUse lag() to calculate the percentage change in high and store as pct_high:\n\n\nCode\npct_high &lt;- (datats$high - lag(datats$high))/lag(datats$high)",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "04_Data_structures.html#data-tables",
    "href": "04_Data_structures.html#data-tables",
    "title": "4  Data structures",
    "section": "4.7 data tables",
    "text": "4.7 data tables\nA data.table is an enhanced data.frame. This data structure allows you to e.g. search for data inside the table using SQL-type formatting. To uses this data structure, you need to install and load the {data.table} package. To do so, you first install the package (if you haven’t done so yet)\n\ninstall.packages(\"data.table\")\n\nand load the package\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.4.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:xts':\n\n    first, last\n\n\nThe following objects are masked from 'package:zoo':\n\n    yearmon, yearqtr\n\n\nHere, we will not cover data.tables in depth, but give a couple of examples on how it differs from the traditional data.frame. These examples will show why a data.table is usually faster than a data.frame, especially on large datasets. If you need to work with very large datasets, you can use e.g. Barrett et al. (2025) as a starting point for introduction to this data structure.\nLet’s first create a data.table. You’ll see that the basic syntax is comparable to the usual data.frame() syntax:\n\ndt &lt;- data.table(\n  firm = LETTERS[1:25],\n  sales = runif(25, 100, 200), \n  margin = rnorm(25, 10, 2), \n  sector = sample(c(\"services\", \"services\", \"industry\", \"construction\", \"transport\"), 25, replace = TRUE))\nhead(dt, 10)\n\n      firm    sales    margin    sector\n    &lt;char&gt;    &lt;num&gt;     &lt;num&gt;    &lt;char&gt;\n 1:      A 196.4297 11.993541 transport\n 2:      B 136.2853  9.378965 transport\n 3:      C 111.2189 13.642239 transport\n 4:      D 135.9506  9.769425  services\n 5:      E 177.5073 11.911084  services\n 6:      F 106.0136  8.349874  services\n 7:      G 121.1611 13.540675  services\n 8:      H 195.2492 11.021530  services\n 9:      I 196.5682 10.317356  services\n10:      J 191.2377  9.823625  services\n\n\nThis function returns a data.table. As a data.table is an enhanced version of a data.frame, it is also a data.frame.\n\nclass(dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\nIn other words, you can use all data.frame functions or subsetting rules to data.tables:\n\nhead(dt[, 1], 5)\n\n     firm\n   &lt;char&gt;\n1:      A\n2:      B\n3:      C\n4:      D\n5:      E\n\ndt$sales\n\n [1] 196.4297 136.2853 111.2189 135.9506 177.5073 106.0136 121.1611 195.2492\n [9] 196.5682 191.2377 190.2272 156.6190 145.9440 113.6654 174.5056 198.7093\n[17] 188.1009 167.9208 156.7068 173.0615 151.7091 144.0244 106.2058 113.5574\n[25] 174.3352\n\ndt[[\"margin\"]]\n\n [1] 11.993541  9.378965 13.642239  9.769425 11.911084  8.349874 13.540675\n [8] 11.021530 10.317356  9.823625  8.171818 10.430808 12.485047 13.928489\n[15]  9.368508  6.896699  9.374375  8.561754 11.633696 11.025556 11.621973\n[22]  9.256063  8.135490 10.406770  7.344467\n\nhead(dt[sales &gt; 150, 1:3], 5)\n\n     firm    sales    margin\n   &lt;char&gt;    &lt;num&gt;     &lt;num&gt;\n1:      A 196.4297 11.993541\n2:      E 177.5073 11.911084\n3:      H 195.2492 11.021530\n4:      I 196.5682 10.317356\n5:      J 191.2377  9.823625\n\n\nThe result of these 4 subsetting operations return a data.table (preserving operator ([])) or vectors (simplifying operator([[]] or $)). Note that you can use\nThe data.table includes names (columns) and row names (rows are numbered in this case and numbers are stored as $row.names). The attributed further include the class as well as the location in your memory where R stored the data.table.\n\nattributes(dt)\n\n$names\n[1] \"firm\"   \"sales\"  \"margin\" \"sector\"\n\n$row.names\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n\n$class\n[1] \"data.table\" \"data.frame\"\n\n$.internal.selfref\n&lt;pointer: 0x0000018f5a429cd0&gt;\n\n\nHowever, in addition to the subsetting rules for data.frames, data.tables allow you to subset observations using dt[i, j, by] where i are the rows to subset or reorder, j refers to a calculation and by refers to a group. Let’s start with i and extract only firms that are in “industry” or in “transport”:\n\ndt_ind &lt;- dt[sector == \"industry\" | sector == \"transport\"]\ndt_ind\n\n     firm    sales    margin    sector\n   &lt;char&gt;    &lt;num&gt;     &lt;num&gt;    &lt;char&gt;\n1:      A 196.4297 11.993541 transport\n2:      B 136.2853  9.378965 transport\n3:      C 111.2189 13.642239 transport\n4:      K 190.2272  8.171818 transport\n5:      M 145.9440 12.485047 transport\n6:      Q 188.1009  9.374375  industry\n7:      T 173.0615 11.025556  industry\n8:      W 106.2058  8.135490  industry\n9:      Y 174.3352  7.344467 transport\n\n\nAs you would expect, R subsets the data.table and extract only those observation where the boolean operation: sector == \"industry\" | sector == \"transport\" is TRUE and skips all other observations as the boolean operation returns FALSE. Here, you use subsetting rules that you know from the data.frame section.\nLet’s now add a calculation within the subsetting and ask R to calculate the sum, the mean, minimum and maximum values for these two industries. To do so, we use the j position in dt[] where the i position is used to select the industries and the j position is now used to include a calculation. As we have more than one calculation (sum, mean, min and max) we include them within () and add a dot:\n\ndt_ind_sum &lt;- dt[sector == \"industry\" | sector == \"transport\", .(sum(sales), mean(sales), min(sales), max(sales))]\ndt_ind_sum\n\n         V1       V2       V3       V4\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 1421.809 157.9787 106.2058 196.4297\n\n\nWe now have a data.table with the sum, mean, the minimum and maximum values for sales for these two industries. Note that in this case, subsetting for data.tables and data.frames is different. Within a data.frame, you can not add calculations within the subsetting operators.\nTo calculate these for each industry, we can now use the by position. If we include by = \"sector\", R will calculate the sum, mean, min and max for each sector.\n\ndt_ind_sum &lt;- dt[sector == \"industry\" | sector == \"transport\", .(sum(sales), mean(sales), min(sales), max(sales)), by = \"sector\"]\ndt_ind_sum\n\n      sector       V1       V2       V3       V4\n      &lt;char&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: transport 954.4403 159.0734 111.2189 196.4297\n2:  industry 467.3682 155.7894 106.2058 188.1009\n\n\nHere, R reads the dt[] subsetting as: using only industry or sector, calculate the sum, mean, min and max for different value in sector. Here, the only difference values in sector are “industry” or “transport”\n\ndt_ind_sum &lt;- dt[margin &gt; 7.499, .(sum(sales), mean(sales), min(sales), max(sales)), by = \"sector\"]\ndt_ind_sum\n\n         sector        V1       V2       V3       V4\n         &lt;char&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:    transport  780.1051 156.0210 111.2189 196.4297\n2:     services 2032.0241 156.3095 106.0136 196.5682\n3: construction  270.3722 135.1861 113.6654 156.7068\n4:     industry  467.3682 155.7894 106.2058 188.1009\n\n\nA data.table further allows you to create new variable for all observations in a data.table, you can use the j position and crate a new variable as a function of the other variables. To do so, you use the name of the new variable followed by a := and the function R needs to apply. For instance, generating a variable gross_profit as the product of sales and margin (where you divide margin by 100 to obtain a percentage):\n\ndt[, gross_profit := sales * (margin/100), ]\nhead(dt, n = 5)\n\n     firm    sales    margin    sector gross_profit\n   &lt;char&gt;    &lt;num&gt;     &lt;num&gt;    &lt;char&gt;        &lt;num&gt;\n1:      A 196.4297 11.993541 transport     23.55887\n2:      B 136.2853  9.378965 transport     12.78215\n3:      C 111.2189 13.642239 transport     15.17274\n4:      D 135.9506  9.769425  services     13.28159\n5:      E 177.5073 11.911084  services     21.14305\n\n\nBecause you run the operations as creating a new variable, calculating sum, mean or min and max within the subsetting, a data.table is faster, especially on large datasets relative to a data.frame. Using the latter, most of the results shown here would requires R to call functions in other packages. Doing so, slows down the process.\n\n\n\n\n\n\nBarrett, Tyson, Matt Dowle, Arun Srinivasan, Jan Gorecki, Michael Chirico, Toby Hocking, Benjamin Schwendinger, and Ivan Krylov. 2025. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBengtsson, Henrik. 2025. matrixStats: Functions That Apply to Rows and Columns of Matrices (and to Vectors). https://github.com/henrikbengtsson/matrixstats.\n\n\nWang, Earo, Dianne Cook, and Rob J Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics 29 (3): 466–78. https://doi.org/10.1080/10618600.2019.1695624.",
    "crumbs": [
      "Data types and structures",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structures</span>"
    ]
  },
  {
    "objectID": "14_Functions.html",
    "href": "14_Functions.html",
    "title": "14  Functions",
    "section": "",
    "text": "14.1 The function syntax\nTo create a function, you use the following basic template:\ndo_something &lt;- function(arguments) {\n  \n  body: what the function does with the arguments\n}\nIn this template, using function() directive, you create the function to_something. The formal arguments of this function are included function(arguments). The body of the function includes the code that R needs to execute. If you want to use this function, you can call it using do_something(arguments).\nAs far as the name is concerned (do_something), there are little limitations on the choice of the name. However, usually, the name is a verb that tells you what the function does. Recall for instance the {dplyr} “verbs”: mutate, filter, summarize or arrange. Similarly, other packages often used verbs, e.g. {lubridate}‘s make_datetime(), {stringr}’s str_remove(), str_extract or str_glue or {forcats}’ fct_reorder or fct_recode. If you use a noun, if that noun is very descriptive and unambiguous, e.g. mean, sd, max, … .\nThe formal arguments, or the formals of the function are the variables in the function definition that are passed on to the body of the function. A function can have as many arguments as is necessary. Two ore more formal arguments are separated by a comma. They are included between the parenthesis in the function definition. Formal arguments are named, e.g. x, y, mean, option, … . If you call the function, you assign values to these arguments. You can do so in two ways: by matching by name or by matching by position. In the former case, you refer to the name and add a value. For instance, using rnorm() you specify the name of the arguments n, mean and sd: rnorm(n = 1, mean = 10, sd = 25). The some holds for function that you create. If your function includes, e.g. do_something &lt;- function(x, y) then you will include the values of these arguments in the function call: do_something(x = 2, y = 5). The second way to assign values to the arguments is by position. Using do_something(2, 5), R assigns the value of 2 to the first argument and 5 to the second argument. For instance, using rnorm(1, 10, 5), R assigns the value of 1 to n, the value of 10 to the second argument mean and 5 to the third argument sd. In case you refer to the names to assign values to the arguments, the order in which you do so doesn’t matter. For positional matching, this is not the case: here the order matters as R assign the first value to the first argument, the second to the second argument, … .\nThe body of the function includes the code that you want to execute. This code is includes in curly braces and is intended two spaces. R executes the code line by line. The body includes the code and also defines “what” the function returns. With respect to the last part, a function always returns one R object, a vector, a matrix or array, a list or data frame or even a function. In general, without explicit return statement, a function returns the output from the last expression in the body of the function.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html",
    "href": "06_Importing_and_exporting.html",
    "title": "6  Importing and exporting data",
    "section": "",
    "text": "6.1 Reading file names and folders\nBefore we import data, let’s first see how you can determine which files are stored in a directory. Base R includes the function list.files(). This function allows you to see which files are included in a given directory or folder.\nlist.files(path = \".\", \n           pattern = NULL, \n           all.files = FALSE, \n           full.names = FALSE, \n           recursive = FALSE, \n           ignore.case = FALSE, \n           include.dirs = FALSE, \n           no.. = FALSE)\nThe path = \".\" refers to the folder. Recall that you can use the {here} package to set the folder relative to the root folders of the project. This also allows you to define a folder using here::here():\nfolder &lt;- here::here(\"data\", \"raw\")\nIn this case, the object folder will include the name of the folder relative to the project’s root. In other words, it will include c:&gt; ... &gt; root &gt; data &gt; raw. You can verify this using the values part in the environment pane.\nUsing only this argument, list.files() returns all files in the ... &gt; root &gt; data &gt; raw folder. The return structure is a vector. In this case, there are three folders (ameco_csv, ameco_txt and ameco_xlsx) and three files: life_df.Rdata, mtcars.csv and nike_sales_2024.csv. We used these three in Chapter 2. To store these in a separate object, you assign the result of the list.files() function to an object, e.g.\nfiles_data_raw &lt;- list.files(path = folder)\nfiles_data_raw\n\n[1] \"ameco_csv\"           \"ameco_txt\"           \"ameco_xlsx\"         \n[4] \"AMECO1.CSV\"          \"life_df.Rdata\"       \"mtcars.csv\"         \n[7] \"nike_sales_2024.csv\" \"tidy_data.xlsx\"      \"tidy_stores.xlsx\"\nAdding a pattern = as an argument in list.files() allows you to narrow the result of the function to files where a pattern occurs. Here, you can use e.g. regular expressions. For instance suppose that you only want a list of .csv files whose name includes a one or more digits. Do select only those files you can use the regular expression \"\\\\d+.csv$\". This pattern will match all file names that include one or more digits before the . and end with csv. Adding this pattern to list.files() and assigning the result to a vector files_data_csv:\nfiles_data_csv &lt;- list.files(path = folder, \n                             pattern = \"\\\\d+.csv$\")\nfiles_data_csv\n\n[1] \"nike_sales_2024.csv\"\nHere, there is only one file that meets this condition: nike_sales_2024.csv. The other files, life_df.Rdata is not a .csv file. The file mtcars.csv is a .csv file, but doesn’t include digits before the dot. Recall from Chapter 1 that file names should be both human and machine readable. If that is the case, regular expressions will allow you to match almost all patterns in the name of a file.\nThe other arguments in list.files() refer to the files that are shown. With all.names = FALSE, only visible files are shown, invisible files are not. Changing this from FALSE into TRUE would change this. To include the directory path in the file name, you can change the full.names = FALSE to TRUE. You will want to change this option if you want to import multiple files in your session. If you add this option, R stores all file names including the directory or folder where the file is located. Here, R starts from the root folder c:\\ and not from the project’s root. If the data folder includes subfolders, adding recursive = TRUE will return all files in all subfolders. To show the subfolders in the folder, you can use include.dirs = TRUE. The ignore.case = FALSE argument by default uses case sensitive patterns.\nR includes two special cases of list.files(): dir() and list.dirs(). The former is an alias for list.files(). In other words, you can also use dir() and read in the files you folder. For instance,\ndir(path = folder, pattern = \"\\\\d+.csv$\")\n\n[1] \"nike_sales_2024.csv\"\nshow the same result as list.files(). The second function, list.dirs() returns the same output as list.files(all.files = TRUE) and if recursive = TRUE is also returns the full path name.\nUsing the a vector such as files_data_raw or a character such as files_data_csv, you can now import these files.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "05_Tidy_data.html",
    "href": "05_Tidy_data.html",
    "title": "5  Tidy data",
    "section": "",
    "text": "5.1 Introduction\nA dataset is a collection of values, usually numeric, but often also characters, boolean or data/time values (see e.g. Wickham (2014)). A variable measures the same underlying attributes across the observational units. The underlying attribute could be e.g. price, quantity, weight, length, frequency, time, GDP, unemployment, consumer satisfaction, age, religion, size, speed, depth, … . Examples of observational units are individuals, households, firms, countries, regions, firms, departments, products, forests, oceans, transactions, … . A variable measures for all households, firms, products, … in the dataset the same attribute (e.g. income, price, quantity, ….). A dataset usually includes more than one variable. One observation includes all variables that are measured for the same observational unit. For instance, with people, an observations includes the unit as well as variables such as income, age, religion, employment status, experience, educational attainment, … . Values belong to both variables as well as observations. For instance, in a dataset with macro-economic data on countries, the value $600b for GDP belongs to both a variable (GDP) as well as an observation: the country whose GDP is $600b.\nDatasets are often presented in different ways. For instance, consider Table 5.1 and Table 5.2 taken from Wickham (2014). These tables show the effect of two treatments (A and B) on 3 people (John Smith, Jane Doe and Mary Jones). In these tables, the variables are the treatment, the observational units the individuals and the values the results.\nThe first way to present the data includes the variables in the columns and the observational units in the rows and is shown in Table 5.1. One observation then equals all results for both treatment A and B for each individual.\nHere, you see that John Smith didn’t undergo treatment A or that the data that individual and for treatment A are missing. For the two other persons, we have a value for both treatment A and treatment B. Presenting the data in this way, invited people to compare both treatments: reading from left to right; Jane Doe’s results for A where better (of worse, depending on the exact definition of the value) that those of treatment B. The same holds for Mary Johnson.\nHowever, there is also another way to show these results. This is shown in Table 5.2. Here, the treatments are shown in the rows and the individuals in the columns.\nHere, the effect of the treatments on the individuals is read from top to bottom. If you would read from left to right you compare the way in which a treatment affects various individuals/ the effect on Jane Doe of both treatment A and B was larger than on Mary Jones.\nThese two examples show that there are many ways to store data. Here we already had two and there are many more. To analyse data however, you need a standard way to storing the data. Suppose for instance that you want to calculate the difference in results for both treatments. Using the first representation of the data, you would have to subtracts the column “treatment A” from the column “treatment B”. If you are shown the second representation, you would need to subtract the row “treatment A” from the row “treatment B”. However, in the second table, you could also subtract “Jane Doe” from “Mary Jones” to see the difference in treatments between both. To avoid this confusion, in data management and analysis, you need a standard way to store the data. This standard needs to be clear on where you find the variables and observations (rows of columns). A tidy dataset meets such a standard.\nTidy data are not meant to “present” data. First, often datasets include much more data than the one in Table 5.1. Here, there are two treatments and three persons. The small scale of this dataset allows to present it in full in a table that fits a single page. Usually, you don’t have 3 persons, but often there are hundreds of even thousands. In that case, you wouldn’t be able to fit the data in a table such as Table 5.1. To do that, you need a summary. To create the summary table, you need to know where the variables are and where the observations are.\nA tidy dataset makes it easy to use the data in an analysis as its adds structure to the dataset. Doing so, it avoids confusion with respect that what is measured or who or what the observational units are. Second, because the dataset is nicely structured, you can use packages, including {gttable} or {gglot2} in R or similar packages in other programming languages to present the data via visualization or tables. For instance, as these packages can “assume” that the variables are always stored in columns, they can look in columns for specific values to use in tables, models of plots. If that were not the case, you would have to “describe” the dataset to the package, and you would have to do that every time you run an analysis on a different dataset.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "05_Tidy_data.html#introduction",
    "href": "05_Tidy_data.html#introduction",
    "title": "5  Tidy data",
    "section": "",
    "text": "Table 5.1: A typical presentations of a dataset\n\n\n\n\n\n\nTreatment A\nTreatment B\n\n\n\n\nJohn Smith\n-\n2\n\n\nJane Doe\n16\n11\n\n\nMary Johnson\n3\n1\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: Another typical presentation of a dataset\n\n\n\n\n\n\nJohn Smith\nJane Doe\nMary Jones\n\n\n\n\nTreatment A\n-\n16\n3\n\n\nTreatment B\n2\n11\n1",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "05_Tidy_data.html#tidy-data",
    "href": "05_Tidy_data.html#tidy-data",
    "title": "5  Tidy data",
    "section": "5.2 Tidy data",
    "text": "5.2 Tidy data\n\n5.2.1 Introduction\nA dataset is tidy if three conditions are simultaneously met:\n\neach variable is a column\neach observation is a row\neach cell include a single value (except the ones in the first row)\n\nA fourth condition is sometimes added: each type of observational unit form a table (or a dataset).\nWith these 3 or 4 principles, it is easy to see the meaning of a dataset: you find the observational units in the rows, the attributes that are measured for each of these units in the columns and the values - or measurements - in the cells. One row includes all measurements for all attributes (variables) for one observational unit and is called an observation.\nLet’s look at Table 5.3 to see what this means. Here we have 3 variables: year, students and passed and three observational units: 3 courses. Here, the observational unit is a course. The cells show single values: the year in “year”, the number of students (e.g. enrolled) in “students” and the number of students who passed each course in “passed”. Here, one observation shows, for a given year, the number of students and the number of students who passed (the variables) for a specific course (observational unit).\n\n\n\nTable 5.3: Illustration: A tidy dataset. Data refers to an imaginary distribution of students enrolled for various subjects and the number of students passed for two academic years.\n\n\n\n\n\ncourse\nyear\nstudents\npassed\n\n\n\n\nIntroductory economics\n2023\n102\n78\n\n\nIntroductory economics\n2024\n98\n68\n\n\nSociology\n2023\n150\n120\n\n\nSociology\n2024\n130\n98\n\n\nLaw\n2023\n200\n156\n\n\nLaw\n2024\n210\n166\n\n\n\n\n\n\nDo the tables in Table 5.1 or Table 5.2 meet these three conditions? No they don’t. Let’s start with the last table, Table 5.2. Here, the observations are in the columns and the variables in the rows. In other words, this table doesn’t meet conditions 1 or 2. It does meet condition 3: the cells show one value and only one value. What about Table 5.1? Here, the observations are in the rows and you can see that each cell records a single value. The first condition, each variable is a column, is not met. To see this, think about another way to present the data. Here, columns “treatment A” and “treatment B” both include “treatment”. In other words, you would also design the table in another way:\n\n\n\nTable 5.4: Illustration: Alternative for @tbl-treat1\n\n\n\n\n\n\nThis table shows that treatment A and B are actually values and the variable is “treatment”. In other words, the column headers include values (A and B) as well as a variable name. In a tidy dataset, the values, A and B are in a cell. To change Table 5.1 in a tidy table, you need to add these values in a column:\n\n\n\nTable 5.5: Illustration: A tidy version of Table 5.1.\n\n\n\n\n\n\nTreatment\nResult\n\n\n\n\nJohn Smith\nA\n-\n\n\nJane Doe\nA\n16\n\n\nMary Johnson\nA\n3\n\n\nJohn Smith\nB\n2\n\n\nJane Doe\nB\n11\n\n\nMary Johnson\nB\n1\n\n\n\n\n\n\nThere are various ways in which a dataset can be “untidy”. As a matter of fact, there is only one way to store data in a tidy way but there are, theoretically, an unlimited number of ways to do so in an untidy way. In general, Wickham (2014) , lists 5 ways that datasets are untidy\n\ncolumn headers are values, not variable names\nmultiple variables are stored in one column\nvariables are stored in both rows and columns\nmultiple types of observational units are stored in the same table\na single observational unit is stored in multiple tables\n\nIn addition to these 5, sometime the cells contain two values.\nNote the we already met the first: the column headings included values, A and B. A dataset can be untidy because it combines more than one of these ways to be “untidy”. In that case, moving from a untidy dataset to a tidy dataset will include multiple steps.\n\n\n5.2.2 Column headers are values, not variable names\nOften data is stored where column headers or column names are values. Consider Table 5.6 which includes an imaginary distribution of study efficiency across programs. The observational units here are the programs: Business administration, Law, Theology, … . The column headers show study efficiency categories: less than 25%, 25% or more but less than 50%, … . The cells show the number of students in each of the study-efficiency categories. This format is easy to read. However, it is not a tidy format. The column headers are values of a variable. In this case, this variable is “study_efficiency”. This variable is not included among the column names. In other words, you would have to add a title or add a note below the table to make sure that this is a table that others can interpret.\n\n\n\nTable 5.6: Illustration: Column headers are values. Data refers to an imaginary distribution of study efficiency across programs\n\n\n\n\n\n\n\n\n\n\n\n\n\ndegree\n&lt;25%\n[25-50%[\n[25-75%[\n[75-99%[\n100%\n\n\n\n\nBusiness administrations\n25\n75\n102\n178\n75\n\n\nLaw\n60\n84\n189\n175\n56\n\n\nTheology\n5\n26\n53\n57\n54\n\n\nEngineering\n75\n79\n68\n185\n152\n\n\nHistory\n22\n23\n18\n89\n12\n\n\nLanguages\n17\n14\n18\n69\n1\n\n\n\n\n\n\nThe data in Table 5.6 are in so called wide format: variables are included in the columns. In this case, tidying the data includes the step of turning the wide format into a long format. In a long format, the values in the column header (here study-efficiency) are moved from the column header and used as values. The process includes the creation of a new column: “study_efficiency”. This variable records various categories in the column headers of Table 5.6. The result of this process a dataset like Table 5.7 (this table shows only the first lines of a full long version):\n\n\n\nTable 5.7: Illustration: Tidy data for “Column headers are values”. The table shows the first 11 rows of the long format of this dataset\n\n\n\n\n\ndegree\nstudy_efficiency\nnumber_of_students\n\n\n\n\nBusiness administration\n&lt;25%\n25\n\n\nBusiness administration\n[25-50%[\n75\n\n\nBusiness administration\n[50-75%[\n102\n\n\nBusiness administration\n[75-99%[\n178\n\n\nBusiness administration\n100%\n75\n\n\nLaw\n&lt;25%\n60\n\n\nLaw\n[25-50%[\n84\n\n\nLaw\n[50-75%[\n189\n\n\nLaw\n[75-99%[\n175\n\n\nLaw\n100%\n56\n\n\nTheology\n&lt;25%\n5\n\n\n\n\n\n\nNote that the the table now includes a column which shows what the values &lt;25%, … represent: study-efficiency. Changing the format from wide to long involves lengthening the dataset. In Table 5.7 there are more rows compared to Table 5.6, while the number of columns fell from 6 into 3 as 5 columns became 2.\nWide format is often used to store data. In Table 5.6, the values in the column names referred to study-efficiency categories. Other values that often show up in column headers are e.g. years as in Table 5.8. In this table, the data for 3 firms (e.g. market capitalization) is shown per year.\n\n\n\nTable 5.8: Illustration: Wide format with years. Data refers to an imaginary table with market capitalization.\n\n\n\n\n\nfirm\n2020\n2021\n2022\n2023\n2024\n\n\n\n\nA\n1236\n1366\n1455\n1554\n1689\n\n\nB\n201\n204\n215\n325\n410\n\n\nC\n12598\n12698\n12359\n11985\n11453\n\n\n\n\n\n\nNote that here too, there is not an easy way to figure out what the values represent unless the title of the table, or a note below the table adds this value. Why are wide formats so popular? A wide format such as the one in Table 5.8, is often used as it is easy to compare values for a given year across firms or, for a given firm, across years, months or quarters. However, as the table increases, in number of years or in number of firms or both, a quick visual scan of the data become increasingly hard. With 500 firms (e.g. SP500), or with monthly data covering various years, a quick visual scan is impossible. In addition to the easy to visually scan for small tables, a wide format is often easy to input data. If your dataset includes a value per quarter, updating a table is as easy as adding a column. Likewise, if your dataset include the dates and times of the various stages of a delivery (ordered, order picked, in warehouse, picked up by shipping agent, delivered), a wide format is convenient: you add the times in the relevant column in a sheet.\nHowever, say you want to visualize Table 5.8 with the year on the horizontal axis and show the values per firm in a linegraph with the values on the vertical axis. In that case, you would need to select the column names as the values for the horizontal axis. Here, you can see that your column headers are values and not variables: on the horizontal (or vertical) axis of a plot, you add the values of the variable, not the variable names (or column names) as such. In a tidy dataset, as all columns are variables, you add the values, not the column names or variables to the axis of a plot. As another example, suppose you want to add the difference in market capitalization from one year to the other. In Table 5.8, you would need to add 4 columns, one for every year starting 2021 and probably use column names such as “2021-2020”, “2022-2021”. In other words, you are adding the “difference” in values for the year as a header. In a tidy dataset, that change in value would take one column: “difference” and you would calculate that difference as the difference in values, e.g. the difference between two rows.\nPivoting this table from wide into long format requires to take the values in the column headers (2020, 2021, …, 2024) and collect them in a variable “year”. The long format of this table has 15 rows excluding header row and 2 columns: firm and year. In Table 5.9 you can see a selection of the rows in such a long format tidy table.\n\n\n\nTable 5.9: Illustration: Long format for @tbl-wideyears\n\n\n\n\n\nfirm\nyear\nmarket_cap\n\n\n\n\nA\n2020\n1236\n\n\nA\n2021\n1366\n\n\n…\n…\n…\n\n\nA\n2024\n1689\n\n\nB\n2020\n201\n\n\nB\n2021\n204\n\n\n…\n…\n…\n\n\nE\n2024\n11453\n\n\n\n\n\n\nHow does the long format solve e.g. the issues with the axis in a plot. First not that you now have two variables, “year” for the horizontal axis and “market_cap” for the vertical axis. Here, we also have three values for the observational units. These values can now be used to draw 3 lines and add a visual way to differentiate between the three firms (e.g. a different color). In other words, the code now reads as: for every value for year in “year”, show the market capitalization in “market_cap” and do so for every firm in “firm” and selecting a different color for every firm in “firm”. Calculating a the change in the market capitalization from one year to the other adds one column to the data. The calculation is straightforward: for every firm, calculate the difference between the value an its lagged values.\n\n\n5.2.3 Multiple variables stored in one column\nA column can store multiple variables. Let’s take a look at the example in Table 5.10. Here, you have an imaginary dataset with the number of students across nationality groups (EEA, non-EEA) and age group (18-19 and 20-21) in various programs. Here, the column names refer to two variables: nationality and age. To see this, it wouldn’t be hard to split this table in two parts.\n\n\n\nTable 5.10: Illustration: Multiple variables stored in one column. Data refers to an imaginary distribution of students across nationalities (EEA - non-EEA) and ages (18-19 years and 20-21 years). Note that this table could e.g. include another row with merged cells above age for nationality.\n\n\n\n\n\n\n\n\n\n\n\n\nprogram\neea_18-19\neea_20-21\nnoneea_18-19\nnoneea_20-21\n\n\n\n\nBusiness administration\n189\n71\n258\n236\n\n\nLaw\n256\n114\n78\n102\n\n\n\n\n\n\nThe first table would include the age groups in a wide format:\n\n\n\nTable 5.11: Illustration: Split of Table 5.10 according to age.\n\n\n\n\n\nprogram\n18-19\n20-21\n\n\n\n\nBusiness administration\n447\n307\n\n\nLaw\n334\n216\n\n\n\n\n\n\nWhile the second table shows the nationalities, also in a wide format:\n\n\n\nTable 5.12: Illustration: Split of Table 5.10 according to nationality.\n\n\n\n\n\nprogram\neea\nnoneea\n\n\n\n\nBusiness administration\n260\n494\n\n\nLaw\n370\n180\n\n\n\n\n\n\nBoth tables Table 5.11 and Table 5.12 show that the columns in Table 5.10 include two variables. Splitting the latter in two separate tables, shows two tables in wide format. Note that every table here implies a loss of data. In Table 5.11, you loose the nationality; in Table 5.12, you loose the age.\nReturning to Table 5.10, why is this table a problem in terms of data analysis? Suppose you want to calculate the share to non-EEA students in the total. That is not something which is easily done in that table. First you need to add the values in all four columns. This gives you the total number of students in these programs. Second, you need to add the fourth and fifth column. Given these two results, you can now calculate the share. The same holds for the share per age group. This calculation too would require multiple steps: calculate the total, add columns 2 and 4 or 3 and 5 and finally, calculate the share.\nTo create a tidy format, two things must happen. First, the table layout must change from wide into long format. Second, names such as “eea_18-19” have to be split in two parts: “eea” and “18-19”. The first part, “eea” can then be used as a value in a variable “nationality”; the second as a value in a variable “age”. The final table will look like Table 5.13:\n\n\n\nTable 5.13: Illustration: Tidy data for “Multiple variables stored in one column”.\n\n\n\n\n\nProgram\nnationality\nage\nnumber_of_students\n\n\n\n\nBusiness administration\neea\n18-19\n189\n\n\nBusiness administration\neea\n20-21\n71\n\n\nBusiness administration\nnon-eea\n18-19\n258\n\n\nBusiness administration\nnon-eea\n20-21\n236\n\n\nLaw\neea\n18-19\n256\n\n\nLaw\neea\n20-21\n114\n\n\nLaw\nnon-eea\n18-19\n78\n\n\nLaw\nnon-eea\n20-11\n102\n\n\n\n\n\n\nHere, the categories are straightforward and splitting “eea_18-19” or “noneea_20-21” isn’t all that hard if you know how to look for patterns in a character variable: the first part is a series of characters, the second includes 2 digits, a minus sign and 2 digits and both parts are separated by an underscore. However, it is not uncommon that these variables stored in a column include more than two and are less easy to separate. Doing so often requires using regular expressions to split the characters into their values.\n\n\n5.2.4 Variables stored in both rows and columns\nTo illustrate this case, consider Table 5.14. Here, we have a table with imaginary data on train delays across two routes: Brussels - Antwerp and Brussels - Leuven. The first column shows these routes. The second column refers to the year and the third to the week of the year. The fourth column has two values: min and max. The column ’time” shows the minimum (if in the row where delay has minimum) and maximum (if in the row where delay has maximum) delay in minutes for that week.\n\n\n\nTable 5.14: Illustration: Variables are stored in columns and rows. Data refers to an imaginary dataset recording minimum and maximum delay in minutes for trains on a given given route per week.\n\n\n\n\n\nroute\nyear\nweek\ndelay\ntime\n\n\n\n\nbxl-ant\n2025\n2\nmin\n10\n\n\nbxl-ant\n2025\n2\nmax\n89\n\n\nbxl-leu\n2025\n2\nmin\n1\n\n\nbxl-leu\n2025\n2\nmax\n193\n\n\n\n\n\n\nOn the Brussels - Antwerp route, the minimum delay in week 2 of 2025 was 10 minutes while the maximum delay was 89 minutes. Here the problem is in the column “delay” in Table 5.14 because if actually includes variables: min and max. In other words, the column stores variables in its rows. Other variables are in the columns. For instance, time or week are variables (they measure time and a delay). In other words, variables are both in rows (min/max) as well as in columns (e.g. time).\nTo tidy the dataset, “min” and “max” must become columns. In other words, the column “delay” should be removed and two columns “min” and “max” should be added. The column “min” should include all the values that are now included in the rows where “delay” includes min; the column “max” should include all the values that are now included in the rows where “delay” includes max. As the values for the variable “time” are copies into the “min” and “max” columns, this column can be removed from the tidy dataset. Technically, you can split the table in two parts: one that includes only the row that show the minimum delay and one that includes only the rows that show maximum delay. Changing the name of the column from “delay” into “min” for the first and “max” for the second and merging both into a single table creates the tidy version of this dataset. This tidy dataset will look like\n\nIllustration: The tidy version of Table 5.14.\n\n\nroute\nyear\nweek\nmin\nmax\n\n\n\n\nbxl-ant\n2025\n2\n10\n89\n\n\nbxl-leu\n2025\n2\n1\n193\n\n\n\nThe problem with variables stored in both rows and columns is often associated with another issue: some column headers are values, not variable names. To illustrate this problem, consider Table 5.15. This table replaces the column time with 7 columns, one for each day of the week. Doing so, the values in column “mon” to “sun” show the minimum delay on that day (in the row where the column delay has “min”) and the maximum delay that day (in the rol where the column delay has “max’). In other words, on the Brussels - Antwerp route, all trains were delayed in week 2, except for those riding on Thursday and Saturday (where the minimum delay was 0). On monday, they are all delayed a minimum of 10 minutes while the maximum delay that day was 89 minutes.\n\n\n\nTable 5.15: Illustration: Variables are stored in columns and rows. Data refers to an imaginary dataset recording minimum and maximum delay in minutes for trains on a given given route per day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nroute\nyear\nweek\ndelay\nmon\ntue\nwed\nthu\nfri\nsat\nsun\n\n\n\n\nbxl-ant\n2025\n2\nmin\n10\n8\n5\n0\n1\n0\n1\n\n\nbxl-ant\n2025\n2\nmax\n89\n10\n18\n24\n36\n5\n9\n\n\nbxl-leu\n2025\n2\nmin\n1\n9\n5\n4\n2\n8\n1\n\n\nbxl-leu\n2025\n2\nmax\n18\n29\n16\n15\n89\n150\n193\n\n\n\n\n\n\nIn Table 5.15, we have the same issues as in Table 5.14: the column “delay” includes variables: minimum and maximum delay. In addition, the column headers “mon”, “tue”, “wed”, “thu”, “fri”, “sat” and “sun” are values, not variables. Recall that there are many way that you can handle date/time variables in R. Here, using the number of the week (2), the year (2025) and the day (mon, tue, …) allows to determine the exact date: the first day of week 2 in 2025 was 2025-01-06 (using monday as the first day of the week and using ISOweeks).\nIn this this case, the “column headers are values, not variables names” is the first issue we need to solve. Doing so, reduces the problem to the same problem as we had in Table 5.14. The column “delay” includes variable names. Solving that issue returns the tidy dataset in Table 5.16.\n\n\n\nTable 5.16: Illustration: Tidy data for “Variables are stored in columns and rows’.\n\n\n\n\n\nroute\nday\nmin\nmax\n\n\n\n\nblx-ant\n2025-01-06\n10\n89\n\n\nblx-ant\n2025-01-07\n8\n10\n\n\nblx-ant\n2025-01-08\n5\n18\n\n\nblx-ant\n2025-01-09\n0\n24\n\n\nblx-ant\n2025-01-10\n1\n36\n\n\nblx-ant\n2025-01-11\n0\n5\n\n\nblx-ant\n2025-01-12\n1\n9\n\n\nblx-leu\n2025-01-06\n1\n18\n\n\nblx-leu\n2025-01-07\n9\n29\n\n\nblx-leu\n2025-01-08\n5\n16\n\n\nblx-leu\n2025-01-09\n4\n15\n\n\nblx-leu\n2025-01-10\n2\n89\n\n\nblx-leu\n2025-01-11\n8\n150\n\n\nblx-leu\n2025-01-12\n1\n193\n\n\n\n\n\n\n\n\n5.2.5 Multiple types of observational units in the same table\nWe can use Table 5.15 to illustrate another problem: multiple types of observational units are in the same table. To see what this means, let’s look at Table 5.17. This table copies Table 5.15, but adds three variables: “type” for the type of the train, “dur” for the normal time the route takes for the specific type of train and “cap” the capacity of the train. Suppose the same type of train is always used on the same route. In other words, an M7 with capacity 400 is always used on the Brussels - Antwerp route and, using that train, the normal time for that route is 45 minutes. If that is the case, the data on type, dur, and cap will appear multiple times. In Table 5.17, you can see that the values “M7”, “45” and “400” always appear in the observations for bxl-ant. The same holds for bxl-leu, where you always see “M8”, “30” and “250”.\n\n\n\nTable 5.17: Illustration: Multiple types in one table. Data refers to an imaginary dataset recording minimum and maximum delays in minutes for trains on a given given route per day. Data includes the type of the train, the normal time and the capacity of the train.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nroute\nyear\nweek\ntype\ndur\ncap\ndelay\nmon\ntue\nwed\nthu\nfri\n\n\n\n\nbxl-ant\n2025\n2\nM7\n45\n400\nmin\n10\n8\n5\n0\n1\n\n\nbxl-ant\n2025\n2\nM7\n45\n400\nmax\n89\n10\n18\n24\n36\n\n\nbxl-leu\n2025\n2\nM8\n30\n250\nmin\n1\n9\n5\n4\n2\n\n\nbxl-leu\n2025\n2\nM8\n30\n250\nmax\n18\n29\n16\n15\n89\n\n\n\n\n\n\nThe problem here is that Table 5.17 mixes to observational units: the route (bxl-ant) and the train (M7, M8). Here, the data for the unit “train” refer to the type, the duration and the capacity. Technically, you can remove these from Table 5.17 and store them in a different table, e.g. Table 5.18.\n\n\n\nTable 5.18: Illustration: Tidy data for “Multiple types in one table”. Table refers to the data per train. The other table would include all data as in the Illustration: Tidy data for “Variables stored in columns and rows”.\n\n\n\n\n\nroute\ntype\ndur\ncap\n\n\n\n\nbxl-ant\nM7\n45\n400\n\n\nbxl-ant\nM7\n45\n400\n\n\nbxl-leu\nM8\n30\n250\n\n\nbxl-leu\nM8\n30\n250\n\n\n\n\n\n\nIn that case, you would have two tables: Table 5.10 and Table 5.18. Using the fact that the “route” is in both these tables, you can always add the train type to Table 5.10. Doing so will save a lot of memory as the values in this table will be copied on every row. Here, with two routes and 7 days a week, a yearly dataset such as Table 5.16 includes 730 rows (365 for every route). In other words, the values in Table 5.18 would be copies 365 times each for the bxl-ant M7 and 365 times for the bxl-leu M8.\nThis problem might occur in datasets which include customer data. Suppose that you have a dataset with transactions: “who” bought “what”, “when”, “how” and at what where the pricing conditions “price”, “discount”, … .. For the “who” part, you have a loyalty card which includes the name of the “who”, age, city where that person lives, … . Adding these personal items to the dataset with transactions creates a problem where multiple observational units are in the same dataset. In this case, the transactions and the customer.\n\n\n5.2.6 A single observational unit is stored in multiple tables\nThe fifth issues arises often when datasets are regularly updated. Consider for instance, the data on the monthly consumer price index. This dataset includes, for every month and for a large set of items the percentage change in their price. Table 5.19 and Table 5.20 show two imaginary datasets with a very limited number of items in the CPI baskets.\n\n\n\nTable 5.19: Illustrations: One type in multiple tables. Data refers to an imaginary dataset recording inflation data for specific items for one specific month (e.g. April).\n\n\n\n\n\nItem\ninflation\n\n\n\n\nfood and drinks\n2.36%\n\n\nrestaurants and hotels\n9.48%\n\n\nvegetables, fresh\n-1.18%\n\n\nvegetables, frozen\n0.18%\n\n\n\n\n\n\n\n\n\nTable 5.20: Illustrations: One type in multiple tables. Data refers to an imaginary dataset recording inflation data for specific items for one specific month (e.g. may)\n\n\n\n\n\nItem\ninflation\n\n\n\n\nfood and drinks\n4.89%\n\n\nrestaurants and hotels\n1.48%\n\n\nvegetables, fresh\n2.18%\n\n\nvegetables, frozen\n-1.22%\n\n\n\n\n\n\nHere, the problem arises because the first dataset, which refers to April, is published one month before the second, which is published in May. In other words, you have two datasets and need to merge these into one. In this case, the solution if usually straightforward: if the datasets include the same variables, all it takes is add them via rbind() after adding the month as a separate variable. The tidy dataset looks like Table 5.21.\n\n\n\nTable 5.21: Illustration: Tidy data for “One type in multiple tables”.\n\n\n\n\n\nItem\nInflation\nmonth\nyear\n\n\n\n\nfood and drinks\n2.36%\nApril\n2024\n\n\nrestaurants and hotels\n9.48%\nApril\n2024\n\n\nvegetables, fresh\n-1.18%\nApril\n2024\n\n\nvegetables, frozen\n0.18%\nApril\n2024\n\n\nfood and drinks\n4.89%\nmay\n2024\n\n\nrestaurants and hotels\n1.48%\nmay\n2024\n\n\nvegetables, fresh\n2.18%\nmay\n2024\n\n\nvegetables, frozen\n-1.22%\nmay\n2024\n\n\n\n\n\n\nIn other cases, the data is stored in various datasets, for instance, a collection of datasets for firms stores data per firm in a separate file. Here too, the solution is straightforward. However, there is one extra complexity. If the individual tables don’t include the name of the firm, country or city, you might find them in the name of the file that stores the data. If that is the case, adding these datasets requires a couple of steps. First, read the file names and store them. Import each file as a data frame and add a column with the name of the firms, country or city. Merge the files using rbind().\nAs long as the datasets don’t change (much), this is straightforward. However, if the datasets change over time, dealing with this issue can be very complex and by very challenging to create a nice tidy dataset.\n\n\n5.2.7 Multiple values in one cell\nSuppose you have a dataset that looks like the one in table Table 5.22. The variable GDP includes values that refer to the size of the economy in the local currency. Here, these values include two values: the size of the economy and the currency.\n\n\n\nTable 5.22: Illustration: One cell holds multiple values. Date refer to an very rough approximation of GDP in national currencies.\n\n\n\n\n\ncountry\nyear\ngdp\n\n\n\n\nUnited States\n2022\nusd 30338000\n\n\nGermany\n2022\neur 5100000\n\n\nJapan\n2022\nyen 658500000\n\n\n\n\n\n\nA tidy version of Table 5.22 splits the column GDP in two parts: the size of the economy in a variable “gdp” and the currency in a variable “currency”. Doing so results in the tidy dataset in Table 5.23.\n\n\n\nTable 5.23: Illustration: Tidy version of @tbl-mutvalueincell\n\n\n\n\n\ncountry\nyear\ngdp\ncurrency\n\n\n\n\nUnited States\n2022\n30338000\nusd\n\n\nGermany\n2022\n5100000\neur\n\n\nJapan\n2022\n658500000\nyen\n\n\n\n\n\n\nHere, this last column can be used to add the value of the currency for the year in “year”. Doing so requires that you merge this dataset with a dataset on exchange rates, e.g. for the euro/dollar and euro/yen. Doing so would allow you to calculate GDP in a common currency.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "13_Essential_programming_structures.html",
    "href": "13_Essential_programming_structures.html",
    "title": "13  Programming: control structures",
    "section": "",
    "text": "13.1 Conditional execution\nif ... else statements or if statements for short, are probably one of the most used program structures. An if structure executes an expression or a code block with multiple expressions if a condition is met (i.e. if a condition is TRUE). If this is not the case, you can optionally add an expression or code block to execute if the condition is FALSE. Closely related to these statement is the switch statement. Here, you can execute a statement based on the value of a variable. ifelse is a vectorized version of the if statements. {dplyr} includes the both if_else and case_when() function. The former is equivalent to base R’s ifelse but allows you to handle missing values. dplyr::case_when can be used to vectorize multiple ifelse or dplyr::if_else statements. The vector variant of switch is dplyr::case_when.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Programming: control structures</span>"
    ]
  },
  {
    "objectID": "08_Transforming_data.html",
    "href": "08_Transforming_data.html",
    "title": "8  Data transformation",
    "section": "",
    "text": "8.1 Introduction\nIn this chapter, we’ll mainly use {dplyr} (Wickham et al. (2023) with cheat sheet). This package includes functions that allow you to select() variables from a dataset, arrange() or sort a dataset ascending of descending, filter() observations, rename() variables, create new variables using mutate() or calculating summary statistics using summarize(). You can do most of these operations for the full dataset of by_group(). In addition left_join(), right_join(), inner_join() or full_join() allow you to add variables from one dataset to another. In addition to {dplyr} we will also use other tidyverse packages, e.g. {stringr} to work with character variables or {lubridate} if we need to do operations on a date/time variable.\nTo illustrate these functions, we will use {nycflights23} (see Ismay, Couch, and Wickham (2024), a package that includes a number of dataset with respect to all flights departing from one of New York’s airports to another US airport in 2023. Using {nycflights13} (Wickham (2022)) you can try all functions out on another flights dataset including all flights departing from New York airports in 2013. The package {anyflights} (Couch (2025)) {[anyflights] (https://simonpcouch.github.io/anyflights/index.html)} allows you to create datasets for any other airport in the United States including multiple years of data.\nYou should have installed {nycflights23} and {nycflichts13} in Chapter 1. If you didn’t, you can run\ninstall.packages(c(\"nycflights23\", \"nycflights13\"))\nTo use {dplyr} and the other {tidyverse} packages, we need to load them first:\nlibrary(tidyverse)\nlibrary(nycflights23)\nWe will start with operations on one dataset. Here, I will start with the operations on rows (i.e. on observations). We then move to the operations on columns (i.e. variables). Summarizing and creating new variables will be covered next. In doing so, we will also cover how to do these with groups of variables. We then move to operations involving multiple datasets. Note that, after completing these operations, you have a single dataset. In other words, you can apply the functions for single datasets on the result of the joined dataset.\nUsing {dplyr} to answer questions with your data usually involves many steps. For instance, you first group by a variable to then calculate a new variable for observations in each group; you first filter to then summarize across these observations, … . Each {dplyr} function returns a tibble or data frame if the function was applied to a tibble or a data frame. Doing so, you can use the pipe |&gt; operator to connect two more more functions. Doing so allows to write the code in a form that is consistent with “first do, then do: filter() |&gt; mean()” rather than a series of nested functions (mean(filter(...)). In subsequent chapters, e.g. Chapter 9, we’ll use this to “pipe” results into ggplot().",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "17_Office.html",
    "href": "17_Office.html",
    "title": "17  Office",
    "section": "",
    "text": "see https://ardata-fr.github.io/officeverse/index.html",
    "crumbs": [
      "Output",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Office</span>"
    ]
  },
  {
    "objectID": "16_Quarto.html",
    "href": "16_Quarto.html",
    "title": "16  Quarto",
    "section": "",
    "text": "https://r4ds.hadley.nz/quarto.html\nhttps://r4sites-book.netlify.app/",
    "crumbs": [
      "Output",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "05_Tidy_data.html#the-tidyverse",
    "href": "05_Tidy_data.html#the-tidyverse",
    "title": "5  Tidy data",
    "section": "5.3 The tidyverse",
    "text": "5.3 The tidyverse\nThe tidyverse is a collection of packages designed to work with tidy data. In Chapter 1 you installed most of these packages. The {tidyr} package (@man_tidyr) includes function that we will use to tidy data, for instance, by changing the format from wide into long, splitting names of variables in pieces to add them as values to different variables, … . Before you can start to tidy data, you need to import the data first. That is what we will do in the section section, Chapter 6. After you have imported the data, you can use {tidyr} to tidy the data e.g. pivoting a dataset from wide into long, splitting columns in two ore more variables, … . Doing so, you will change your imported data into a tidy dataset. The other packages in the tidyverse all use tidy data: {dplyr} is used to clean and transform data, including e.g. merging various datasets into one larger dataset. These datasets are then used in {ggplot2} or {gttable} to create plots, tables and show visualizations using bar chars, line charts, … . All these packages assume are created to work with tidy data and to do so in a consistent way. For instance, most functions are verbs (pivot, mutate, select, filter, read) and always include the (tidy) dataset as the first argument in the function. In other words, these packages are ideal to use with base R’s pipe operator |&gt; for magritrr’s %&gt;%. You can recognize most of these function as they often use an underscore _ in their function names, e.g. read_csv(). Doing so often differentiates them from base R functions with the same functionality, but often with a dot and other arguments, e.g. read.csv(). Here, we will mostly cover tidyverse functions. To do so, you’ll often have to load a package. Although most of the operations can be done using base R, these tidyverse packages are widely used and perform reasonably fast, even with medium sized datasets. In addition, if you are familiar with these tidyverse packages, it will be straightforward to use base R’s functions.\n\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#reading-file-names-directory",
    "href": "06_Importing_and_exporting.html#reading-file-names-directory",
    "title": "6  Importing and exporting data",
    "section": "6.2 Reading file names, directory",
    "text": "6.2 Reading file names, directory\n\nlist.files(path = \".\", \n           pattern = NULL, \n           all.files = FALSE, \n           full.names = FALSE, \n           recursive = FALSE, \n           ignore.case = FALSE, \n           include.dirs = FALSE, \n           no.. = FALSE)",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#reading-rectangular-tables",
    "href": "06_Importing_and_exporting.html#reading-rectangular-tables",
    "title": "6  Importing and exporting data",
    "section": "6.2 Reading rectangular tables",
    "text": "6.2 Reading rectangular tables\nRectangular tables are datasets where each column has the same number of rows. Columns are separated by a delimiter, for instance a comma, a tab or a white space. The {readr} package includes a number of functions that allows you to import these types of files. Excel or googlesheet files store data in a different way. To import data from an excel spreadsheet, {readxl} includes three functions to do so. For googlesheets, the package is {googlesheets4}. {haven} allows you to read STATA, SAS and SPSS files. Usually, these packages also allow you to write files. The {rio} package, also referred to as the Swiss army knife to import data, calls the functions in these packages to import the data.\n\n6.2.1 Delimited files\nRectangular datasets are stored in rows and columns. Between the columns, there has to be a delimiter to indicate where one column ends and another column starts. Recall that the Nike data in the first example in Chapter 2 were stored in a comma delimited file. A part of that file in excel looks like the one in Figure 6.1\n\n\n\n\n\nFigure 6.1: A comma delimited file\n\n\n\n\n\n\n\n\nHere, you can see that the first row includes the column headers or variable names. These are separated by comma’s: “Month,Region,Main_Category,…”. In the second row, you have the values. These too are separated by comma’s: “November,India,Equipment, …”. In other words, this is a comma delimited file. If you import the data in R or in any other package, that package will interpret these comma’s as indicators that a new column starts. Doing so, packages will create a column “Month” and “Region”, … and include in that column the values “November” and “India”. Here, this file used a comma to indicate a new column, other separators include a space, a tab, a semi-colon, … .\n{readr} includes various functions to read these delimited files. The most general function is read.delim(). If a file is comma delimited, {readr} includes read_csv() and read_csv2(). For tab delimited files you can use {readr}’s read_tsv(). In H. Wickham, Hester, and Bryan (2024), you can find all information with respect to these functions. You can also download the cheatsheet where the first page includes the {readr} functions. Here, we’ll start with the general case and then discuss the functions to import a comma delimited and a tab delimited file. To import these files using “base R” function, includes read.delim(), read.csv() or read.tsv(). Note the dot in the function name as opposed to the underscore for {readr} functions. Here, we will always use {readr} functions. In other words, you always need the _.\nIf you ran library(tidyverse) at the start of this chapter, you don’t have to reload the package. If you didn’t run the library function, you can load {readr} using\n\nlibrary(readr)\n\nAll these function return a tibble.\n\n6.2.1.1 General case: read_delim() and write_delim()\n{readr}’s read_delim() is a function that allows you to read and import most delimited files, whether they are comma, tab, semi-colon or use any other symbol to delimit columns. Because the function is widely applicable, it includes many optional arguments. Here is the full function, including the default values:\n\nread_delim(\n  file,\n  delim = NULL,\n  quote = \"\\\"\",\n  escape_backslash = FALSE,\n  escape_double = TRUE,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  comment = \"\",\n  trim_ws = FALSE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\n\nThe first argument, file includes the path to the file. Zipped files (ending in .zip or .gz for instance) are automatically uncompressed. If you add multiple files and these files all include the same columns, R will read them in at once and stack them. If you read them as individual files, you would have to do this last operation yourself using e.g. rbind(). Using cbind(), you can add two data sets by column. We’ll cover those options more in depth. The file argument can also include a web address. If that is the case and R detects https://, http://, ftps:// or ftp:// R will download the file.\nThe delim argument shows the character used to separate the various columns. Although any symbol can be used, most often used delimiters are a comma , a semi-colon ; a space \" \", a tab \\\\t or a vertical bar |. Note that R uses the regular expression for a tab: \\\\t.\nSome files include character variables. With respect to those, the question is how they are shown. In other words, how does R know that “2” is a string? Or how does R now that “Sales, november” refers to one value and shouldn’t be split up in two in a comma delimited file? The quote argument allows you the specify which character is used to quote strings. By default, R uses \". The escape_double = TRUE argument assumes that a file escapes a quote by doubling it. Recall that an escape symbol “escapes” the normal meaning of another symbol. For instance, the “\\\\d” refers to any digit and the “\\\\” is used to escape both the normal meaning of the “\\” and the “d”. In read_delim() the default is escape_double = TRUE. In other words, he quote symbol (e.g. ““) is doubled to escape it. If that is the case \"\"\"\" will escape the usual interpretation for a quote symbol. In \"\"\"\", the first \"\" are telling R not to interpret the last \"\" as a quote symbol. If the quote symbol is escaped, R will read” as part of the string. For instance in the character value “The color was”blue” “, without an escape on the quote symbol, R wouldn’t know how to interpret”blue”. To avoid this: ” The color was ““blue”” ” shows that the quotes around “blue” are real quotes and not quote symbols. More in general, the escape_backslash = FALSE argument allows you to specify if the backslash is used as an escape symbol. If that is the case “\\\\d”” will be read not as a “d” but as a “digit”. With respect to white spaces, by default R will not remove trailing or leading whitespaces from each field. These white spaces can also refer to tabs. Changing the default trim_ws from FALSE to TRUE, will change that behavior.\nThe three col_ arguments allow you to specify if the first row includes column names. By default, this is what R assumes. If FALSE, R assumes no column names in the first row. This options also allows you to specify the names of the columns in a vector using c(\"col_name_1\", ...). If you want to rename columns, you can add the new names in this c() function. Do do so, you include the new name of the column whose name you want to change as well as the current name. For instance, suppose you want to change the variable name from sales2024 to sales_2024, you can include that in using c(sales_2024 = sales 2024). Doing so, R will change the column name.\nUsing name_repair(), you can check the names in the dataset. By default, R will check if the names are unique. Other options include “minimal” where R checks only if names exist and “universal” which checks the uniqueness of the names and also adjusts the way they are written, e.g. removing white spaces and replacing them with a dot, … .\nYou can specify the type of every column in col_types. The default option, NULL, implies that R will try to guess the column types. You can add the column types using the function cols() or using a list to specify the column types. To fix the column types, you can use a full column type or use an abbreviation. Column types are:\n\ncol_logical() or “l”\ncol_integer() or “i”\ncol_double() or “d”\ncol_character() or “c”\ncol_factor(levels, ordered) or “f”\ncol_date(format = ) or “D” for the format in local date specification\ncol_time(format = ) or “t” for the format in local time specification\ncol_datatime(format = ) or “T” for ISO8601\ncol_number or “n” for numbers containing a grouping mark\ncol_skip() or “_” or “-” for don’t import this column\ncol_guess() or “?” for guess best type.\n\nFor instance if the dataset includes three columns, one integer, one double and one date, you would use col_types = list(col_integer(), col_double(), col_date(format = \"%Y-%m-%d)). As an alternative, you can use cols(col1 = col_integer(), col2 = col_double, ...). Using the short cuts: this is equivalent to cols(col1 = \"i\", col2 = \"d\", ...). Note that for columns that are not included in the cols() function, R will guess the type. If you add skip, R will not import that column. In other words, col_types = list(col_skip(), col_double(), col_data(format : %Y-%m-%d)) in the previous example, wouldn’t import the first integer column. There are a couple of other ways to shorten this code. For instance, suppose that you have a large dataset where almost all columns are numeric, except one or a couple of columns that are not. For instance, you have a column with the name col_char which is a character variable while all others are numeric. Using cols(.default = col_double, col_char = col_character(), ...) you can avoid that you have to include all columns in the cols() function. Here, you tell R that all columns are numeric, except col_char which is included as a character column. In other words, you only include the columns by name whose type deviates from the default numeric and add .default = col_double to indicate that all other columns are numeric (or any other type). Using cols() you can further specify if you only want to read columns with a specific type. Note that you can change the type of the data after importing using e.g. as.numeric(), as.factor() or as.character().\nWith the show_col_types you can ask R to shows the column types after importing the file. By default, this is what R will do. If don’t want this, you can include FALSE. Doing so, R will not show column types after reading.\nThe argument locale() allows you determine the default way to store numeric variables or dates and times for a specific location. For instance, what symbol is used for a decimal sign, how are days, times and months shown, … . The default value is default_locale(). You can check that default using\n\nreadr::default_locale()\n\n&lt;locale&gt;\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n&lt;date_names&gt;\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM\n\n\nThe output shows that the default here used a dot as a decimal sign and a comma as a grouping sign in numbers. The date and time formats will be shown using the familiar year-month-date and time as hour-minute-second. If you want to change any of these values, you can set the locale using\n\nlocale(\n  date_names = \"en\",\n  date_format = \"%AD\",\n  time_format = \"%AT\",\n  decimal_mark = \".\",\n  grouping_mark = \",\",\n  tz = \"UTC\",\n  encoding = \"UTF-8\",\n  asciify = FALSE\n)\n\nFor instance, to use a comma for a decimal mark, you can change the decimal_mark = \".\" in decimal_mark = \",\" and e.g. the grouping mark in grouping = \".\". Doing so, R will write numbers as 123.456,789 and not 132,456.789.\nWith col_select you can select the columns to import. You do so either referring to those columns by name (in the first row) or by column number in a vector using c(). In addition, you can use helper functions such as starts_with() to import only those columns that start with a prefix, ends_with() to import only those columns that end with a suffix, contains() to import columns whose name includes a string, matches() to import columns based on a regular expression or matches a numerical range using num_range(). Using where() you can include a logical condition to select the columns, for instance where(is.numeric). As an example, to import all columns that start with sales and end with 2024 you would use col_select = c(starts_with(\"sales\") & ends_with(\"2024\")). Note that you can use boolean operators such as !, |, … For example !starts_with(\"sales\") would import all columns except those whose name starts with “sales”. Note that you can also remove columns after importing. If you import the full dataset, you can remove columns as part of your data wrangling processes.\nIn addition to selecting the column to read, you can also tell R to skip lines. The first argument to do so is skip = n. If n is equal to one or any other integer, R will ignore the first n lines in the file. This might be necessary if the file include a line with e.g. additional information on the data in the file. If the file includes comments, you can include the comment symbol in comment = \"\". Doing so, R will also ignore these lines if it imports a file. By default, R will skip empty rows. You can change that by changing the default FALSE in TRUE in the skip_empty_rows argument.\nIf you import multiple files, changing the default id = NULL into e.g. id = name will create an additional column name including file name (with full location details). For instance, importing data from two files, sales_2024.csv and sales_2025.csv will add column with the value sales_2024.csv (including full reference to the folder) for all observations from the file with the same name and the value sales_2025.csv for the observation from the file sales_2025. This is very helpful if the name of the files include information on the data that is stored in those files. Suppose for instance that none of these two files include a reference to the year or to sales, using the name of the file will allow you to identify these values. Doing this after importing the files would require additional coding that is sensitive to mistakes. For instance, if the first 20 observations are from file 1 and the second 20 from file 2, you can add a column and include the value file1 for the first 20 observations and file2 for the last 20. However, here you need to determine the number of observations for each file. Doing so manually is usually very sensitive to mistakes. In other words, adding the lines while importing is often the saver solution.\nThe na = c(\"\", \"NA\") changes an empty value or an “NA” character in a missing value. If in addition to those two, your dataset also includes, e.g. “N\\A” as a missing value or “-999999”, you can add these in the c() function. R will treat these values as missing values: c(\"\", \"NA\", \"N/A\", \"-999999\").\nThe argument guess_max = min(1000, n_max) shows that R will use the first 1000 observations to guess the type of a variable. If there aren’t 1000 observations in your dataset, R will use all (n_max). Usually, this is sufficient. However, there are instances where the original files were saved with all NA values for one column at the top. Suppose that you have a dataset with 5000 observations and that there are 1200 missing observations for one of the dataset’s variables. All other values are numeric. Suppose also that the person who saved the file, first ordered it so that all missing values for the variable appear at the top. In this case R will guess the type from first 1000 observations. If you import this file and you scan both the upper as well as bottom observations, you’ll notice that these at the bottom have the wrong type. Re-importing after increasing the 1000 in guess_max = min(1000, n_max) to e.g. 1250, will solve that issue.\nTo see how this function words, let’s try to import a couple of files. To illustrate this and other function, I’ll use the European Commission’s AMECO dataset. You can find a description of all variables in this dataset on the [AMECO pages] (https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Feconomy-finance.ec.europa.eu%2Fdocument%2Fdownload%2F5cb507d5-10ac-4470-ad93-66c91a89bd72_en%3Ffilename%3DAmeco%2520Online%2520list%2520of%2520variables%252020250121.xlsx&wdOrigin=BROWSELINK).\nYou can access this dataset in multiple ways. First, the EU Commission allows you to download all datafiles in one zip file. The format of these files is txt, csv or xlsx. Second, you can access this data set online. In the data &gt; raw folder of your project, you should add ameco_csv, ameco_txt and ameco_xlsx including their files. These files include all files in csv, txt and xlsx format. These files are always named AMECOxx with xx a number referring to the chapter fo the datasets. In ameco_txt there are two files with another name, AMECO1_colheader and AMECO1_skiplines. The ameco_csv folder includes ameco1_part1_csv and ameco1_part2.csv. I created these files.\nWe will start with the txt files. If you open the file, e.g. AMECO1.TXT, a quick inspection shows that the files are delimited with a semicolon ;, that the decimal sign is a dot and that the files include column names. Using these values in read_delim() and assigning this dataset to df_pop (the first chapter of AMECO’s database includes population statistics):\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1.TXT\"), \n  delim = \";\",\n  col_names = TRUE\n)\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 867 Columns: 73\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (6): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT, 2026\ndbl (66): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nlgl  (1): ...73\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe results show that the dataset includes 867 rows and 73 columns. The columns CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT and 2026 were read in as character variables, while all other columns, 1960, 1961, … 2025 are numeric. The last column, 73, is read as a logical value. With the exception of the character type for “2026” and the logical value, this is as expected. Verifying the values in “2026” using View(df_pop) of clicking on the data frame in the environment pane, shows that this variable includes both numbers as well as a lot of dots. One reasons why these might occur are white spaces after the last value in that column. The values for column 73 are all “NA”. This last result is due to the fact that there is a semicolon after the last value 2026 in the first line. For R, this means that it has to create an additional column. That column doesn’t have a name and so R adds “73”. Lets try to address these issues. First, we can remove the 73th column from the column R imports. Using col_select = c(1:72) will reduce the number of columns to import from 73 to 72. That the last column was read as a character might be due to white spaces after the last value. Using trim_ws = TRUE, readr_delim() will remove all white spaces before and after the values. Here is the code:\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1.TXT\"), \n  delim = \";\",\n  col_names = TRUE,\n  trim_ws = TRUE, \n  col_select = c(1:72)\n  )\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 867 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see, we have 72 columns (not 73) and all columns are, as could be expected from their values, either character of numeric. This data set is now ready to start the process of tidying. For instance, we have the issue that column headers are values and not variable names (see Chapter 5). In addition, some variables are also included in the rows: the column “title” includes the variable names: “employees, persons: total economy”, “Total population (national accounts)”, … . In addition, we probably won’t need all the columns (although it could be the case that we need to to create correct variable names). For instance, the column “unit” include the unit of measurement (rate, 1000 persons). Here as all variables are measured in a standard way (the unemployment rate is a rate and all populations values are in 1000 persons), you could probably eliminate them from the columns you import as well.\nLet’s add another issue. The file AMECO1_colheader.TXT does not include a column header. Here, you have a couple of alternatives. First you can read the file without the column names and add them later. Second, you can add the column names as an argument in the read_delim() function. Let’s show the first case. Here we add col_names = FALSE as an argument:\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1_colheader.TXT\"), \n  delim = \";\",\n  col_names = FALSE,\n  trim_ws = TRUE,\n  col_select = c(1:72)\n )\n\nRows: 867 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): X1, X2, X3, X4, X5\ndbl (67): X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nR didn’t read the column names and replaced them by the “names” X1, X2, … X72. In other words, if the files don’t include a column header, R will add its own names using X1, … You can add names to this unnamed data frame using e.g.\n\nnames(df_pop) &lt;- c(\"code\", \"country\", \"sub_chapter\", \"title\", \"unit\", 1960:2026)\n\nThe data set is now named.\nAs an alternative you include these names as an argument in read_delim():\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1_colheader.TXT\"), \n  delim = \";\",\n  col_names = c(\"code\", \"country\", \"sub_chapter\", \"title\", \"unit\", 1960:2026),\n  trim_ws = TRUE,\n  col_select = c(1:72)\n )\n\nRows: 867 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): code, country, sub_chapter, title, unit\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nR now replaces the column headers by the names you include in col_names = c(...).\nThe file AMECO1_skiplines.TXT includes two lines at the top “# AMECO Population data” and “# AMECO EU Commission”. On the third line, the file includes column names. To skip these lines, you can add skip = 2. With col_names = TRUE, R will not import the first two lines and use the third line for read the names of the columns:\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1_skiplines.TXT\"), \n  delim = \";\",\n  col_names = TRUE,\n  trim_ws = TRUE,\n  skip = 2,\n  col_select = c(1:72)\n )\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 867 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLines 1 and 2 both started with a “#”. As an alternative to skip, you can also include comment = \"#\". R doesn’t import lines starting with the symbol after the comment = \"\" argument. Using this, to skip the first two lines (or any other line that starts with a hash tag):\n\ndf_pop &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO1_skiplines.TXT\"), \n  delim = \";\",\n  col_names = TRUE,\n  trim_ws = TRUE,\n  comment = \"#\",\n  col_select = c(1:72)\n )\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 867 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUsing read.delim() import the file AMECO6.TXT from the directory data &gt; raw &gt; ameco_txt. Store this dataset in a tibble df_gdp\n\n\nCode\ndf_gdp &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO6.TXT\"), \n  delim = \";\",\n  col_names = TRUE,\n  trim_ws = TRUE, \n  col_select = c(1:72)\n  )\n\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 3461 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUsing the output: how many lines did you import?\nSuppose that you would like to add your own names to the file AMECO13.TXT:\n\nc(\"code\",\"country\", \"sub_chapter\", \"variable\", \"unit\", paste(\"year\", 1960:2026, sep = \"_\"))\n\n [1] \"code\"        \"country\"     \"sub_chapter\" \"variable\"    \"unit\"       \n [6] \"year_1960\"   \"year_1961\"   \"year_1962\"   \"year_1963\"   \"year_1964\"  \n[11] \"year_1965\"   \"year_1966\"   \"year_1967\"   \"year_1968\"   \"year_1969\"  \n[16] \"year_1970\"   \"year_1971\"   \"year_1972\"   \"year_1973\"   \"year_1974\"  \n[21] \"year_1975\"   \"year_1976\"   \"year_1977\"   \"year_1978\"   \"year_1979\"  \n[26] \"year_1980\"   \"year_1981\"   \"year_1982\"   \"year_1983\"   \"year_1984\"  \n[31] \"year_1985\"   \"year_1986\"   \"year_1987\"   \"year_1988\"   \"year_1989\"  \n[36] \"year_1990\"   \"year_1991\"   \"year_1992\"   \"year_1993\"   \"year_1994\"  \n[41] \"year_1995\"   \"year_1996\"   \"year_1997\"   \"year_1998\"   \"year_1999\"  \n[46] \"year_2000\"   \"year_2001\"   \"year_2002\"   \"year_2003\"   \"year_2004\"  \n[51] \"year_2005\"   \"year_2006\"   \"year_2007\"   \"year_2008\"   \"year_2009\"  \n[56] \"year_2010\"   \"year_2011\"   \"year_2012\"   \"year_2013\"   \"year_2014\"  \n[61] \"year_2015\"   \"year_2016\"   \"year_2017\"   \"year_2018\"   \"year_2019\"  \n[66] \"year_2020\"   \"year_2021\"   \"year_2022\"   \"year_2023\"   \"year_2024\"  \n[71] \"year_2025\"   \"year_2026\"  \n\n\nAdd these names as part of your read_delim() function call and assign this data set to the tibble df_exchange:\n\n\nCode\ndf_exchange &lt;- read_delim(\n  file = here::here(\"data\", \"raw\", \"ameco_txt\", \"AMECO13.TXT\"), \n  delim = \";\",\n  col_names = c(\"code\",\"country\", \"sub_chapter\", \"variable\", \"unit\", paste(\"year\", 1960:2026, sep = \"_\")),\n  trim_ws = TRUE, \n  col_select = c(1:72)\n  )\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 521 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): code, country, sub_chapter, variable, unit\ndbl (67): year_1960, year_1961, year_1962, year_1963, year_1964, year_1965, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe write_delim() function allows you to write delimited files. The function’s arguments include the data frame or tibble to save, the file to write to and the delimiter. These arguments are straightforward. The na = \"NA\" arguments include the symbol that you use for missing values. If you want to add the file to an existing file, change the default append = FALSE to TRUE. The default case overwrites an existing file. If the column names have to be included in the file, you can set the value in col_names to TRUE. The default !append takes the opposite of append: if append is TRUE and the file is added to an existing file, column names won’t be used; if the write function creates a new file and append is FALSE, columns names will be included. If values contain characters that need to be quoted, the option quote allows you to specify if this should be done only if needed (i.e. if a character value include a delimiter and should be quote to avoid that on the next import, this character variable will be split up in two columns), if all values should be quote or if none should be quote. The escape argument has the same interpretation as in the read_delim() function. eol stands for end of line and allows you to specify the end of line character. Usually, this the \\n (as is the case in regular expressions). The full function:\n\nwrite_delim(\n  x,\n  file,\n  delim = \" \",\n  na = \"NA\",\n  append = FALSE,\n  col_names = !append,\n  quote = c(\"needed\", \"all\", \"none\"),\n  escape = c(\"double\", \"backslash\", \"none\"),\n  eol = \"\\n\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  path = deprecated(),\n  quote_escape = deprecated()\n)\n\nWe will write the tibble df_pop to a delimited file data_thesis.txt with a delimiter “|” in the data &gt; tidy folder and accept all other default values. In this case, missing values are included as NA and we can leave the default value. As we don’t have any data yet, so we can not append the file to another file and append = FALSE is the appropriate.\n\nwrite_delim(\n  df_pop,\n  here::here(\"data\", \"tidy\", \"data_thesis.txt\"),\n  delim = \"|\"\n)\n\nYou can verify in your data &gt; tidy folder that you have the data_thesis.txt file in your folder. Opening the file will show that the columns are delimited with “|”. In the “your turn” you created two additional data files: df_gdp and df_exchange. Let’s add the first to the data_thesis.txt file. To do so, we use append = TRUE and delimit the columns using “|”:\n\nwrite_delim(\n  df_gdp,\n  here::here(\"data\", \"tidy\", \"data_thesis.txt\"),\n  delim = \"|\",\n  append = TRUE\n)\n\nIf you open the file data_thesis.txt file, you’ll see that it now includes both population data as well as data on GDP. The population data include the first 868 rows, GDP rows 869 to 3141.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nIn the previous exercise, you also created a tibble df_exchange. Recall that this file doesn’t include the same column names as the names in the other two tibbles df_pop and df_gdp. Add this tibble to the file data_thesis.txt. Do you have to change anything the code we used to append df_gdp to df_pop?\n\n\nCode\nwrite_delim(\n  df_exchange,\n  here::here(\"data\", \"tidy\", \"data_thesis.txt\"),\n  delim = \"|\",\n  append = TRUE\n)\n\n\nDo you know why this code works, even if you had different column headers?\n\n\nCode\n# The function `write_delim()` had two arguments: \n# append = TRUE\n# col_names = !append\n\n# As append is TRUE, col_names is FALSE. \n# As a result, R doesn't include the column names in \n# the data file. \n\n\nYou have three tibbles. Can you add df_pop and df_gdp in one tibble and write that tibble df_combined to a file data_thesis_other.txt where the delimiter is a tab?\n\n\nCode\ndf_combined &lt;- rbind(df_pop, df_gdp)\n\nwrite_delim(\n  df_combined,\n  here::here(\"data\", \"tidy\", \"data_thesis_combined.txt\"),\n  delim = \"\\t\"\n)\n\n\nWrite write the same two data files to a file data_thesis_yetanother.txt and use a white space as a delimiter.\n\n\nCode\ndf_combined &lt;- rbind(df_pop, df_gdp)\n\nwrite_delim(\n  df_combined,\n  here::here(\"data\", \"tidy\", \"data_thesis_yetanother.txt\"),\n  delim = \" \"\n)\n\n\nIf you now compare the last txt file with the other two, you’ll notice that R includes “” around the character variables. As we added a white space to delimit the columns and there are white spaces in the character variables, R needs to include the latter in “” to make sure that a next read_delim() will not separate the character variables among several columns. In the tab delimited file, data_thesis_combined.txt you can also see that tabs were used and not spaces.\n\n\n\n\n\n6.2.1.2 Comma delimited: read_csv() and write_csv\nIf a file is comma delimited a , is used to delimit the columns in a dataset and you can use read_csv() as an alternative for read_delim(). These files are usually saved with the csv extension where csv stands for “comma- seperated values”. Here, the arguments are largely those from latter function, with the exception of the delim argument. For csv files, the delimiter is given: a comma. In other words, read_csv() is a special case of read_delim(). The latter would be equivalent to the former with the delimiter argument equal to a comma: read_delim(... delim = \",\" ..).\n\nread_csv(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  quote = \"\\\"\",\n  comment = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\n\nThe function read_csv2() has the same arguments as read_csv(). In differs from the latter as it uses ; for the field separator (and not , as in read_csv()) and the decimal separator is , (and not a . as in read_csv()). This format, 123.456,78 as opposed to 123,456.78 is common in some European countries. In other words, if your dataset is semicolon delimited with a comma to indicate decimals, you can use read_csv2().\nLet’s try this function with the AMECO files in data &gt; raw &gt; ameco_csv. Here, we will import AMECO10.CSV and assign the dataset to df_bop as the data include the balance of payments data. If you open this file, you’ll see that it is a comma delimited file. If you inspect the column headers, you’ll see that the csv version of the AMECO dataset includes more columns. The first columns include SERIES, CNTRY, TRN, AGG, UNIT, REF, CODE, COUNTRY, SUB-CHAPTER, TITLE and UNIT. In the TXT version, the first columns included CODE, COUNTRY, SUB-CHAPTER, TITLE and UNIT. Note also that the column header “unit” appears two times among csv header names. The number of years is equal: 1960 - 2026. The csv file includes the same 67 columns with data, but has 11 columns with information on the series, units, … . In addition, at the end of the lines, you’ll see 7 comma’s. This suggests that the data will include more then 11 (series) + 67 (values) = 78 columns, but will add 85. Let’s import the dataset while accepting all default values in the function call:\n\ndf_bop &lt;- read_csv(\n  file = here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO10.CSV\")\n)\n\nNew names:\nRows: 1021 Columns: 85\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(7): SERIES, CNTRY, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT...11 dbl (71): TRN,\nAGG, UNIT...5, REF, 1960, 1961, 1962, 1963, 1964, 1965, 1966,... lgl (7):\n...79, ...80, ...81, ...82, ...83, ...84, ...85\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n\n\nLet’s see what read_csv() returns. As expected, it changes the two names including “unit”. This result is due to the default value for name_repair = \"unique\". The default creates unique names for columns adding the number of the column to the name in the csv file. You can further see that R imported 1021 rows and 85 columns. Using the data on the column specification, we see that R identified 7 columns as character and 71 as double. In addition to the columns with years as header, these numeric columns also include TRN, AGG, UNIT…5 and REF. The dataset also includes 7 logical values in columns 79 to 85. The reason for the numeric values requires some knowledge of the AMECO data series: the series code for every row equals the concatenated values of the columns CNTRY, TRN, AGG, UNIT, REF and CODE. In other words, you don’t need to import these, except for the last one, CODE, because this values identifies a series. We can import that file again, eliminating series we don’t want (CNTRY, TRN, …. ) or need (columns 79-85). To do so, we select the first column (SERIES) and the all columns from 7 (CODE) to 78:\n\ndf_bop &lt;- read_csv(\n  file = here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO10.CSV\"),\n  col_select = c(1, 7:78)\n)\n\nNew names:\nRows: 1021 Columns: 73\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): SERIES, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT...11 dbl (67): 1960, 1961,\n1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n\n\nThe dataset includes the series reference, the code, country, sub-chapter, title and unit. Here, R added the column. The other 67 columns include numeric values. All logical values are now removed from the dataset. We could further clean the names of the variables. As in read_delim() to do so, you would have to add a vector with new names in the col_names = argument. If the file includes comments at the top or at the and, there are other lines that R needs to skip, you can add the skip or comment. Note that here too we don’t have a tidy dataset.\nTo write a csv file, you can use the function write_csv(). The arguments here are the same as in the write_delim() function, again with the exception of the delimiter argument:\n\nwrite_csv(\n  x,\n  file,\n  na = \"NA\",\n  append = FALSE,\n  col_names = !append,\n  quote = c(\"needed\", \"all\", \"none\"),\n  escape = c(\"double\", \"backslash\", \"none\"),\n  eol = \"\\n\",\n  num_threads = readr_threads(),\n  progress = show_progress(),\n  path = deprecated(),\n  quote_escape = deprecated()\n)\n\nLet’s write our df_bop to the tidy folder. Here, there are no options to add the delimiter as R will automatically use a comma. Note that this function too is a special case of write_delim(): if you add delim = \",\" in the latter, the output will be the same as using the former. To add the tibble df_bop as data_bop.csv to the tidy folder while accepting all default values for the other arguments:\n\nwrite_csv(\n  df_bop,\n  file = here::here(\"data\", \"tidy\", \"data_bop.csv\")\n)\n\nIf you inspect the file, you’ll see that R adds ” ” to the character variables. This is due to the default “needed” in the argument quote = c(\"needed\", \"all\", \"none\"). As with a white space delimited file, R needs to add those quotes because some of the character variables include a comma. In other words, without a quoting, importing this file wouldn’t work as R would split the characters which include comma’s in various columns.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nImport the files AMECO18.CSV from the data &gt; raw &gt; ameco_csv folder and adding this dataset to df_deficit. Don’t import the columns 2 to 6 and 79 to 85. However, don’t use c(1, 7:78) but use an alternative subsetting operator.\n\n\nCode\ndf_deficit &lt;- read_csv(\n  file = here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO18.CSV\"),\n  col_select = c(1, 7:78)\n  )\n\n\nNew names:\nRows: 622 Columns: 73\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): SERIES, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT...11 dbl (67): 1960, 1961,\n1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n\n\nAppend this file to the data_bop.csv file in your tidy data folder.\n\n\nCode\nwrite_csv(\n  df_deficit,\n  file = here::here(\"data\", \"tidy\", \"data_bop.csv\"), \n  append = TRUE\n)\n\n\n\n\n\n\n\n6.2.1.3 Tab delimited: read_tsv()\nAs is the case with read_csv, read_tsv is a special case of read_delim for datasets that are tab delimited. The arguments of this function are equal to those of read_csv:\n\nread_tsv(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  locale = default_locale(),\n  na = c(\"\", \"NA\"),\n  quoted_na = TRUE,\n  quote = \"\\\"\",\n  comment = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = show_progress(),\n  name_repair = \"unique\",\n  num_threads = readr_threads(),\n  show_col_types = should_show_types(),\n  skip_empty_rows = TRUE,\n  lazy = should_read_lazy()\n)\n\nTo illustrate, let’s read the tab delimited file data_thesis_combined.txt you created in one of the excercises and assign it to df_tab:\n\ndf_tab &lt;- read_tsv(\n  here::here(\"data\", \"tidy\", \"data_thesis_combined.txt\")\n)\n\nRows: 4328 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere, you see that the file is read without any issue. Recall that you created this file. To write tab delimited files, you can use write_tsv(). The arguments of this function are the same of those of the other “write” functions.\n\n\n6.2.1.4 Fixed width files: read_fwf()\nFixed with files are files with a fixed number of positions per column. For instance, the first column includes 5 positions, the second 8. A values in the first column can include a maximum of 5 symbols. It could include less. The second columns starts on position 6, even if the number of values in the first column is e.g. 3. The following 8 positions (6 to 13) are for the second column, even if some of these values hold less than 8 positions.\n{readr} includes a function to import fixed with data: read_fwf(). Most of the arguments in the read_fwf() function are similar to those for the other functions, except the argument col_positions =. Here, you can allow R to guess the positions using fwf_emty(), include the width of each column using fwf_widths() or use fwf_positions() to include the start and end positions. As part of this function you can add names for every column using col_names. Here we’ll illustrate this function using the fwf-sample.txt file which is included in the {readr} sample datasets. To use these datasets, you use readr_example(\"name dataset\") and assign this value to an object:\n\nsample_fwf &lt;- readr_example(\"fwf-sample.txt\")\n\nThe dataset is fixed width and includes 4 columns: first name, last name, state and a social security number. To start, we include fwf_empty() and add column names. This function includes the file as the first argument and col_names as the second:\n\ndf_fwf1 &lt;- read_fwf(\n  sample_fwf, \n  col_positions = fwf_empty(sample_fwf, col_names = c(\"first\", \"last\", \"state\", \"soc_sec\"))\n)\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (4): first, last, state, soc_sec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_fwf1\n\n# A tibble: 3 × 4\n  first last     state soc_sec     \n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       \n1 John  Smith    WA    418-Y11-4111\n2 Mary  Hartford CA    319-Z19-4341\n3 Evan  Nolan    IL    219-532-c301\n\n\nThe function returns a tibble, df_fwf1 which includes 4 columns and 3 variables. The columns are all character columns.\nNow let’s change the col_positions argument and use fwf_widths. The first position is 5 wide, the second 15, the third 10 and the last 12. We”ll also add names:\n\ndf_fwf2 &lt;- read_fwf(\n  sample_fwf, \n  fwf_widths(c(5, 15, 10, 12), col_names = c(\"first\", \"last\", \"state\", \"soc_sec\"))\n)\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (4): first, last, state, soc_sec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_fwf2\n\n# A tibble: 3 × 4\n  first last     state soc_sec     \n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       \n1 John  Smith    WA    418-Y11-4111\n2 Mary  Hartford CA    319-Z19-4341\n3 Evan  Nolan    IL    219-532-c301\n\n\nNote that you can use fwf_widths to merge two “columns” by including both in the width and using one name less in the names argument. Using 20 positions for the first column (5 for the first and 15 for the second), R will merge the first and second columns as it considers the first 20 positions as one variable:\n\ndf_fwf3 &lt;- read_fwf(\n  sample_fwf, \n  fwf_widths(c(20, 10, 12), col_names = c(\"name\", \"state\", \"soc_sec\"))\n)\n\nRows: 3 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\n\nchr (3): name, state, soc_sec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_fwf3\n\n# A tibble: 3 × 3\n  name          state soc_sec     \n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;       \n1 John Smith    WA    418-Y11-4111\n2 Mary Hartford CA    319-Z19-4341\n3 Evan Nolan    IL    219-532-c301\n\n\nUsing the fwf_positions, you can identify fixed with columns via the start and end positions. You include first the start positions in a vector and add a second vector with the end positions. Here, the start position of the first column is 1, the second starts at 6, … . The last position of the first column is 5, the last position of the second column is 19, … . The function call:\n\ndf_fwf4 &lt;- read_fwf(\n  sample_fwf, \n  fwf_positions(c(1, 6, 20, 30), c(5, 19, 29, 42), c(\"first\", \"last\", \"state\", \"soc_sec\"))\n)\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (4): first, last, state, soc_sec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_fwf4\n\n# A tibble: 3 × 4\n  first last     state soc_sec     \n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       \n1 John  Smith    WA    418-Y11-4111\n2 Mary  Hartford CA    319-Z19-4341\n3 Evan  Nolan    IL    219-532-c301\n\n\n\n\n6.2.1.5 Reading multiple files\nIn the previous examples, we imported one file. However, you can also download many files. To illustrate, we’ll use read_csv() but the you can use the same steps with all other functions as well.\n\n6.2.1.5.1 Selecting the files to read\nRecall the list.file() function. This function shows the files in a folder. We can use this function to select the files we want to import. Suppose you want to import AMECO1.CSV and AMECO2.CSV from the data &gt; raw &gt; ameco_csv folder. Using list.files() we can define a vector that includes the names of the files we like to import. Adding full.names = TRUE will store the full file path, including the folder and name of the file. Here I’ll use a regular expression to identify AMECO1.CSV and AMECO2.CSV. This regular expression if straightforward: repetition of upper case ([A-Z]+), a digit 1 or 2 ([1-2]), a dot (.) and CSV, in uppercase as the last components of the name (CSV$):\n\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_csv\")\nfiles_csv &lt;- list.files(path = folder,\n                        pattern = \"[A-Z]+[1-2].CSV$\",\n                        full.names = TRUE)\n\nThe files_csv vector includes two elements: the first refers to the folder and AMECO1.CSV file, the second, to the folders and the AMECO2.CSV file.\n\n\n6.2.1.5.2 Adding multiple files one after another\nIf all files include the same columns, you can use read_csv() to import both these files in one function call. The function will return a tibble were the observations in the second file are added to the observations in the first. Here, we have two files, AMECO1.CSV and AMECO2.CSV. They both included the same columns. Using read_csv() with files_csv for the file argument will return one tibble where the observations of the second file AMECO2.CSV are added to the observations for the first. With respect to the arguments for this function, we know that we have to remove the last columns (else they are read of logical values) and that we can (but don’t have to) remove columns 2 to 6. Here, we will include the id = \"file\" argument. Although both files include various series, it will help to illustrate what this option does:\n\ndf_ameco &lt;- read_csv(\n  files_csv, \n  id = \"file\",\n  col_select = c(1, 7:78)\n)\n\nNew names:\nRows: 3140 Columns: 74\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): SERIES, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT...11 dbl (67): 1960, 1961,\n1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n\n\nIf you inspect df_ameco you can see that the first column now includes a reference to the individual file where the values were found. This information is added to a column “file” as you asked R to do in id = \"file\". This column now allows you to extract information from the file names to add to your dataset. Suppose that you imported various datasets each containing the daily visitors to an exhibition for a specific month and that none of these files include the month in a separate columns but refer to the month only through the name of the file (visitors_may, visitors_june, …). If you add the file name as you import all files, you can extract the month from the name and add it to your dataset. Here,\n\nstringr::str_extract(df_ameco$file, pattern = \"[A-Z]+[1-2]\")\n\ncould extract AMECO1 and AMECO2 from the file column allowing you to assign these values to a new variable in the df_ameco tibble. In Chapter 4 we saw how you can do that. We will discuss additional possibilities using {dplyr}’s mutate() in Chapter 8.\nAs an alternative, to this approach, you could also import the files individually and use rbind() to add both. Recall from Chapter 4 that rbind() adds two more more data frames and returns a data frame. With two data frames, the first with 20 observations and the second with 30, rbind() returns a data frame with 50 observations. On the first 20 rows, it includes the observations from the first data frame and on the next 30 rows, 21 - 50, it includes the observations from the second. To illustrate, let’s use rbind() to combine AMECO17.CSV and AMECO17.CSV. First, we import the files using read_csv(). Here I add the arguments show_col_types = FALSE and name_repair = \"unique_quiet to avoid the output from read_csv().\n\ndf_ameco17 &lt;- df_ameco &lt;- read_csv(\n  here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO17.CSV\"), \n  col_select = c(1, 7:78),\n  show_col_types = FALSE,\n  name_repair = \"unique_quiet\"\n)\n\ndf_ameco18 &lt;- df_ameco &lt;- read_csv(\n  here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO18.CSV\"), \n  col_select = c(1, 7:78),\n  show_col_types = FALSE, \n  name_repair = \"unique_quiet\"\n)\n\nUsing rbind() we can now add both to the tibble df_ameco1718:\n\ndf_ameco1718 &lt;- rbind(df_ameco17, df_ameco18)\n\nIf you inspect df_ameco1718 you can see that R added the observations in df_ameco18 to those in df_ameco17. In the next section, we’ll introduce another way to import multiple files using lapply(). You can use this approach here as well.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUse a regular expression to create a extract the files AMECO7.TXT and AMECO8.TXT from the folder ameco_txt in your data &gt; raw folder. Store these files and their folder structure in an object files_txt\n\n\nCode\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_txt\")\nfiles_txt &lt;- list.files(path = folder,\n                        pattern = \"[A-Z]+[7-8].TXT$\",\n                        full.names = TRUE)\n\n\nImport these files and assign assign these imported files to a tibble df_labcap (you are downloading labor and capital stock data):\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\n\n\nCode\n# Note that txt files are semicolon delimited. You need `read_delim` and add\n# a semicolon as a delimiter. You read columns 1:72 and must add trim_ws = TRUE\n\ndf_labcap &lt;- read_delim(\n  file = files_txt, \n  delim = \";\",\n  col_names = TRUE,\n  trim_ws = TRUE, \n  col_select = c(1:72)\n  )\n\n\nNew names:\n• `` -&gt; `...73`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2141 Columns: 72\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT\ndbl (67): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n6.2.1.5.3 Adding multiple files per column\nIn the previous section, all files had the same columns, but includes different observations. What if you files include the same observations, but different columns? Here, you can use cbind() to add both (see also Chapter 4). Doing so, R will add the columns of the second, third, … data frames in the cbind() function to those in the first. Here, you assume that all observations are equal in all data files. Before you can use cbind() you need to import all files. If the number of files is limited, you can do so by running one of the read_xxx() functions for each dataset. If the number of files is large, you can either use a for loop or you can use the lapply() function. Recall that lapply(x, function(x)) applies a function to every component of x and returns a list (Chapter 4). If x is not a list, both function coerce x to a list using as.list(x). We previously used this function to apply a function to the components in a list. However, you can also apply this function to a vector or matrix. Doing so, lapply() will apply the function to all components of the object matrix of vector x after coercing x to a list and return a list. Here, we will use lapply() to apply the function read.csv() to a vector with the names of the files we need to import. We can generate this last vector using e.g. list.files() or using e.g. paste() to generate the a vector with file names.\nThe data &gt; raw &gt; ameco_csv folder includes two files, ameco1_part1.csv and ameco_part2.cvs. The first includes all observations for AMECO1.CSV but includes only the years from 1960 to 1984, the latter includes all years from 1985 to 2026. Both files don’t include columns we need to deselelect. First we create a vector with these file names. We’ll use list.files() to do so:\n\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_csv\")\npart_csv &lt;- list.files(path = folder,\n                        pattern = \"[a-z]+[1]_[a-z]+[1-2].csv$\",\n                        full.names = TRUE)\npart_csv\n\n[1] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_csv/ameco1_part1.csv\"\n[2] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_csv/ameco1_part2.csv\"\n\n\nlapply will use as.list(part_csv) and apply the function to every component of that list:\n\nas.list(part_csv)\n\n[[1]]\n[1] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_csv/ameco1_part1.csv\"\n\n[[2]]\n[1] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_csv/ameco1_part2.csv\"\n\n\nIn other words, it applies the function to every file name, including the folder.\nWe can now import these files and assign them to a list df_list. To do so, we use lapply() and use read_csv() as the function:\n\ndf_list &lt;- lapply(part_csv, function(x) read_csv(x))\n\nNew names:\nRows: 867 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): SERIES, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT dbl (25): 1960, 1961,\n1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 867 Columns: 42\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(42): 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT...11` -&gt; `UNIT`\n\n\nThe function applies read_csv to every components x in the part_csv vector (coerced to a list). read_csv returns a tibble. In other words, the object df_list includes two tibbles. The first tibble includes the dataset in ameco1_part1.csv, the second the dataset in ameco1_part2.csv.\nRecall that [[ ]] is the simplifying operator. In other words, df_list[[1]] will return the first tibble, df_list[[2]] the second. You can now use cbind()to add the columns of the second to the first:\n\ndf_bothparts &lt;- cbind(df_list[[1]], df_list[[2]])\n\nYou can verify that this returns a tibble df_bothparts with 867 observations for 73 variables.\nIf you have many data frames in your list, a shorter way add them using cbind is through the do.call(what, args) function. This function applies the function in what to the arguments in args. In this case what is cbind and args is the list with data frames.\n\ndf_bothparts &lt;- do.call(cbind, df_list)\n\nWe will discuss do.call more in depth in Chapter 13.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nUsing a regular expression, import AMECO11.CSV and AMECO12.CSV and store their names and path in an object files_import\n\n\nCode\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_csv\")\nfiles_import &lt;- list.files(path = folder,\n                        pattern = \"[A-Z]+1[1-2].CSV$\",\n                        full.names = TRUE)\n\n\nImport these files and don’t show any output from the read_csv function.\n\n\nCode\ndf_list &lt;- lapply(files_import, \\(x) read_csv(x,\n                                             col_select = c(1, 7:78),\n                                             show_col_types = FALSE, \n                                             name_repair = \"unique_quiet\")\n                  )\n\n\nWould cbind() work to add these two in one tibble `df_1112?\n\n\nCode\n# Adding both using cbind causes an error as the number of rows and differs\ndf_1112 &lt;- cbind(df_list[[1]], df_list[[2]])\n\n\nError in data.frame(..., check.names = FALSE): arguments imply differing number of rows: 387, 3829\n\n\nWhat If you\n\n\n\n\n\n\n6.2.1.6 Reading compressed files\nLarge files or a collection of files are often stored in an compressed archive. You can recognize these archives from their name: “.zip”, “.gz”, … . {readr}’s import function allow you to import these compressed files. To illustrate, we’ll use ameco11_zip which is the data &gt; raw &gt; ameco_csv folder. This zip file includes the file AMECO1.CSV. Recall that we have to select the columns 1 and 7:78. Using read_csv() we can import all files in the zip archive. The file reference now refers to the zipped file and not a csv file:\n\ndf_zip &lt;- read_csv(\n  file = here::here(\"data\", \"raw\", \"ameco_csv\", \"ameco1.zip\"),\n  col_select = c(1, 7:78)\n)\n\nNew names:\nRows: 867 Columns: 73\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): SERIES, CODE, COUNTRY, SUB-CHAPTER, TITLE, UNIT...11 dbl (67): 1960, 1961,\n1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n\n\nYou can now verify that R imported the zip file, decompressed it and read in the file AMECO1.CSV. To read multiple files in a zip file, you can first unzip or decompress them using base R’s unzip() function. In this function you add the zip file in the argument zipfile. The files argument allows you to select the files to extract. The default value extracts all files from the zipped archive. Using list = TRUE lists the files in the zip but doesn’t extract them. The files will be extracted to exdir and by default, R will overwrite files in that directory. If that directory doesn’t exist, R will create it.\n\nunzip(zipfile, \n      files = NULL, \n      list = FALSE, \n      overwrite = TRUE,\n      junkpaths = FALSE, \n      exdir = \".\", \n      unzip = \"internal\",\n      setTimes = FALSE)\n\nTo illustrate, let’s extract the file ameco1.zip to the data &gt; raw folder.\n\nunzip(\n  here::here(\"data\", \"raw\", \"ameco_csv\", \"ameco1.zip\"), \n  list = FALSE,\n  exdir = here::here(\"data\", \"raw\")\n)\n\nYou can now use one of {readr}’s import functions. To import multiple files stored in an archive, you first need to extract them to a directory and then import them in the same way as we covered when showing how to import more then one file from a folder.\nUsing base R’s zip() function, you can create an archive. R also includes untarand tar to extract or create a Tar archive.\n\n\n6.2.1.7 Reading from remote locations\nAll previous functions imported files stored on your computer. Suppose that you have a file that is available on the internet. As an example, the file mtcars.csv can be found on github via https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv. Here, we have a csv file. To import this file, you can use read_csv and use the location of the file including its name to add to the file argument:\n\ndf_mtcars &lt;- read_csv(\n  file = \"https://github.com/tidyverse/readr/raw/main/inst/extdata/mtcars.csv\"\n)\n\nRows: 32 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere, we didn’t add any other options, but you can include all other options that are available in the read.csv function. In addition, you can use this approach to download other types of delimited files. All you need to do is change the function in line with the delimiter.\n\n\n6.2.1.8 Fast read files fread()\nThe {data.table} package (Barrett et al. (2025)) includes a “fast” read and “fast” write function fread() and fwrite(). Recall that data.tables (Chapter 4) are useful to work with large datasets. fread() and fwrite() are functions to read or write these large datasets. To use these function, you have to load the {data.table} package. If you didn’t install it, you have to run the following code once:\n\ninstall.packages(\"data.table\")\n\nLoading the package in memory uses:\n\nlibrary(data.table)\n\nThe fread() function allows you to import data using a large range of option. The arguments of this function include:\n\nfread(input, \n      file, \n      text, \n      cmd, \n      sep = \"auto\", \n      sep2 = \"auto\", \n      dec = \"auto\", \n      quote = \"\\\"\",\n      nrows = Inf, \n      header = \"auto\",\n      na.strings = getOption(\"datatable.na.strings\",\"NA\"),\n      stringsAsFactors = FALSE, \n      verbose = getOption(\"datatable.verbose\", FALSE),\n      skip = \"__auto__\", \n      select = NULL, \n      drop = NULL, \n      colClasses = NULL,\n      integer64 = getOption(\"datatable.integer64\", \"integer64\"),\n      col.names,\n      check.names = FALSE, \n      encoding = \"unknown\",\n      strip.white = TRUE, \n      fill = FALSE, \n      blank.lines.skip = FALSE,\n      key = NULL, \n      index = NULL,\n      showProgress = getOption(\"datatable.showProgress\", interactive()),\n      data.table = getOption(\"datatable.fread.datatable\", TRUE),\n      nThread = getDTthreads(verbose),\n      logical01 = getOption(\"datatable.logical01\", FALSE),\n      logicalYN = getOption(\"datatable.logicalYN\", FALSE),\n      keepLeadingZeros = getOption(\"datatable.keepLeadingZeros\", FALSE),\n      yaml = FALSE, \n      tmpdir = tempdir(), \n      tz = \"UTC\"\n)\n\nfread() reads the input argument and, after inspection, defers it either to file, text or cmd. The first is used to import datasets, the second to read text based data and the third, cmd is a command to pre-process a file. The difference between the first two is based on the presence of an end of line symbol (\\n in regular expressions). If an end of line is not present, it is a dataset, if and end of line is present, it is seen as a text. Here, most files are datasets. The sep arguments by default detects the separator and scans the dataset to see if the separator is a comma, a tab, a vertical bar, a semicolon or a colon. The second separator sep2 allows fread() to detect separators withing columns (i.e. columns including a list). For instance if a column’s values are “green, white, red” and columns are tab delimited, fread() returns a list column where each cell is a vector. The function’s other main arguments detect if the file includes a header header = auto, allow you to specify missing values through a character vector including the values that should be read as missing via na.strings, to add column names using col.names, a vector with the column names to use, to skip the first lines skip, to select the columns to read using a vector with column names or column numbers, to drop columns from the import process using a vector with columns numbers of names to drop, to specify the type of the columns using a vector of types in colClasses or to check names, strip white spaces using strip.white.\nThe function returns a data.table unless the option data.table = getOption(\"datatable.fread.datatable\", TRUE) is set to FALSE. In that case, the function returns a data frame.\nTo illustrate the use of fread, let’s import AMECO1.CSV and accept all other default values, other than data.table:\n\ndf_ameco1 &lt;- fread(here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO1.CSV\"),\n                   data.table = getOption(\"datatable.fread.datatable\", FALSE))\n\nYou can verity that fread() returns a data frame.\n\nclass(df_ameco1)\n\n[1] \"data.frame\"\n\n\nFrom the environment pane, you can see that this data frame includes 85 columns. Recall that columns 79-85 shouldn’t be read and that columns 2-6 include information that is included in the SERIES column. To remove these columns, we have to options. We can drop them from the file:\n\ndf_ameco1 &lt;- fread(here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO1.CSV\"),\n                   drop = c(2:6, 79:85),\n                   data.table = getOption(\"datatable.fread.datatable\", FALSE))\n\nor we can include the columns we want to select\n\ndf_ameco1 &lt;- fread(here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO1.CSV\"),\n                   select = c(1, 7:78),\n                   data.table = getOption(\"datatable.fread.datatable\", FALSE))\n\ndf_ameco1 now includes the 73 columns for 867 observations.\nTo import multiple files using fread(), you first read them in a list using lapply(). {data.table} includes a function rbindlist() to bind multiple data tables in a list. Here, this function returns a data table. However, as a data table is also a data frame and you can change the type of a data table to a data frame only using as.data.frame. Let’s first create a vector with files to read:\n\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_csv\")\nfiles_csv &lt;- list.files(path = folder,\n                        pattern = \"[A-Z]+[1-2].CSV$\",\n                        full.names = TRUE)\n\nUsing lapply we can now import the datasets in a list df_ameco_list:\n\ndf_ameco_list &lt;- lapply(files_csv, \\(x) fread(x, drop = c(2:6, 79:85)))\n\nUsing this list and rbindlist, we can add these datasets in one data table\n\ndf_ameco_dt &lt;- rbindlist(df_ameco_list)\n\nUsing tibble::as.tible() we can change the type of this data table in a tibble.\nfwrite() offers a fast way to write a file. As with write_csv(), can create a new file or append an existing file. I refer to the Barrett et al. (2025) manual for full details of this function.\n\n\n\n6.2.2 Spreadsheets\nData is often stored in spreadsheets such as Microsoft’s excel or Google sheets. Here we will use these two to illustrate how you can import data from such as sheet. Often, you’ll see that the arguments in the function you need to read and write these files are very similar to those we already covered.\n\n6.2.2.1 Microsoft Excel\n\n6.2.2.1.1 Excel: read_excel(), read_xls() and read_xlsx\nTo import Excel files, you need the package {readxl}. In H. Wickham and Bryan (2023), you can find the full description of this package and its functions. On the second page of the cheatsheet, you can find the {readxl} functions. You installed this package as part of the {tidyverse} collection in (seq-intro?). If you ran library(tidyverse) at the start of this chapter, you don’t have to reload the package. If you didn’t run the library function, you can load {readxl} using\n\nlibrary(readxl)\n\nExcel files are either stored as “.xls” or “.xlsx” files. To read these files, {readxl} includes three functions: read_excel(), read_xls() and read_xlsx. The first guesses the extension, the second is used to import known “.xls” files while the latter can be used if it is known that the file is an “.xlsx” file. The arguments of these three function are identical.\n\nread_excel()\n\n\nread_excel(\n  path,\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = readxl_progress(),\n  .name_repair = \"unique\"\n)\n\n\nread_xls()\n\n\nread_xls(\n  path,\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = readxl_progress(),\n  .name_repair = \"unique\"\n)\n\n\nread_xlsx()\n\n\nread_xlsx(\n  path,\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  progress = readxl_progress(),\n  .name_repair = \"unique\"\n)\n\n\n\n6.2.2.1.2 Excel: the basics\nExcel files are also referred to as workbooks. In a workbook, data can be stored in various sheets. By default, sheets are labelled “Sheet1” … . A new workbook has one sheet. You can add sheets or rename them. A sheet includes rows and columns. Row are referred to with numbers while column are referred to with upper case A, B, C, … . With respect to the number of columns and rows in a sheet, there is a huge difference between the modern “xlsx” format and the older “xls” format. The modern “xlsx” format allows for 16384 columns from A, B, C, … XFD and 1048576 rows numbered from 1 to 1048576. The older “xls” format includes a maximum of 65536 rows and a maximum of 256 columns A, B, … IV. That this file format can have big consequences was illustrated during the Covid pandemic. In the UK, the authorities stored the number of confirmed covid cases in an xls file. As the number of confirmed cases was higher than 65536, nearly 16000 cases were not reported to the track and trace authorities. The xls file simply dropped them as the number the cases exceeded the number of rows. You can find that story in an article on The Verge.\nA cell is located in one row and one column. You can refer to a cell using two styles. First, you can include the column and the row. For instance, A1 refers to the cell in the column A and row 1 and C4 refers to the cell in column C and row 4. The other way uses “RxCy”. For instance, R1C1 refers to the cell in the first row and the first column or R10C26 refers to the cell in the 10th row and the 26th column. R1C1 is equal to A1 and R10C26 is equal to Z10. There is a third way to refer to a cell. From the menu bar “Formulas”, select “Define names” in the ribbon and you can include the name of an individual cell. If you do so, you can refer to that cell using its name.\nWithin a sheet, you there refer to a cell using e.g. B4. This is a relative reference. Suppose that you refer to cell B4 in another cell, say D4. If you copy this reference from D4 one position to the right to E4, the cell will now refer to C4 (one column to the right of B4). If you copy this reference from D4 one position down to D5, the reference will change to B5 (one row down from B4). This is also called a relative reference. An absolute reference includes a $ sign before the column, the row or both. Suppose that you refer to cell $B4 in cell D4. If you now copy this reference from D4 one position to the right to E4, the cell will continue to $B4. If you copy the reference to $B4 one position down from D4 to D5, the reference will change to $B5. Here, adding the $ sign to the column keeps the reference to the same column. Likewise, suppose to you refer to cell B$4 in cell D4 and copy from D4 one position to the right to E4, the cell will refer to C$4. Copying one position down from D4 to D5 will leave the cell reference to B$4. Here, adding a $ sign to the row, keeps the position of the row fixed at 4. Using $B$4 keeps both positions fixed. In the latter case, the references are absolute. If you only fix a column or a row (i.e. one $ sign), there is mixed reference.\nUsing the “R1C1” referencing style, you would refer to a cell two columns to the right as RC[+2] and two rows above as R[-2]C. Referring to a cell three columns to the left and four rows below: R[-3]C[+4]. A reference to R5C3 (C5) is always absolute.\nIf there are multiple sheets in your workbook, you can refer to a cell in another sheet using “sheetname!A4”. If you didn’t include a name of a sheet, a reference to the value in the second row and the third column C in Sheet2 would be “Sheet2!C2”. If you add $ signs, you have an absolute reference to the column, row or both in the other sheet “Sheet2”.\nIn addition to a reference to a cell, you can also refer to a range (or numerous cells). A range could include cells in the same column, multiple cells in th same row or a multiple cells among extending across different rows and columns. You can refer to multiple (adjacent) cells in the same column using A1:A10. Here, you refer to all cells in the first 10 rows of column A. Using A1:D1 refers to all cells on the first row but spread across columns A, B, C and D. To select all values in a column, you can use E:E (all values in column E). To reference the entire 8th row, you can use 8:8. If you want to refer to various columns (all rows), you can use E:G (all columns E, F and G). Likewise, using 1:3 refers to all columns in rows 1, 2 and 3. To reference adjacent cells in crossing various rows and columns, you can use A1:D5. This range includes all cells in rows 1 to 5 and in column A to D (20 cells in total). Referring to the same range across many sheet is known as 3D referencing. For instance Sheet1:Sheet4!A1:D4 will refer to the range A1:D4 (first 4 rows and first 4 columns) in the sheets Sheet1, Sheet2, Sheet3 and Sheet4.\n\n\n6.2.2.1.3 Extracting information on the Excel workbook\n{readxl} includes functions that allow you to extract information on a sheet. To illustrate, we”ll use the AMECO_multiplesheets.xlsx workbook in data &gt; raw &gt; ameco_xlsx folder.\nLet’s first determine the file format. Although the xlsx in the name is clear, we can use excel_format(path, guess = TRUE) to verify that this is the case. To “guess” the format, the function uses the extension or the signature number of a file. All extensions “xlsx”, “xlsm”, “xltx” and “xltm” return “xlsx” while “xls” returns “xls”. The signature number refers to the numbers associated with each file type.\n\nexcel_format(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"),\n  guess = TRUE)\n\n[1] \"xlsx\"\n\n\nThe function is also used in read_excel() to guess the file format.\nThe function excel_sheets() shows the sheets in a workbook. The AMECO_multiplesheets.xlsx file includes three sheets. You can extract the names of these sheets using:\n\nexcel_sheets(path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"))\n\n[1] \"population\"  \"consumption\" \"investment\" \n\n\nThe three sheets are called “population”, “consumption” and “investment”. If you assign this result to a vector, you can use the names of these sheets in your code.\n\n\n6.2.2.1.4 Reading an Excel file\nBefore we import a file that includes multiple sheets, let’s import one sheet, AMECO1.XLSX from the data &gt; raw &gt; ameco_xlsx folder. To do so, we’ll use read_excel() even if we know that this sheets is an xlsx sheet. The {readxl} functions treat an empty cell as a missing value. In AMECO1.XLSX missing values are shown using “NA”. Here, we add this to the na = argument to show that both empty calls and “NA’s are missing values. We will accept all other default values and show only the will change at the latter stage:\n\ndf_ameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nThe function reads the type of the columns from the excel file. You can verify that R imported 78 variables for 867 observations. From the previous cases, you might recall that not all columns are equally relevant. Importing the first would allow to recreate columns 2, 3, 4, 5 and 6. In {readxl} functions, you can include the coll_cols() in the range argument and refer to the column of a range of columns you want to import. However, these columns needs to be adjacent. In other words, referring to “A” and G:BZ”, the columns you would like to extract, doesn’t work and you need to delete them from the data frame. To show this:\n\ndf_ameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = cell_cols(c(\"A\", \"G:BZ\")),\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nThe tibble includes 78 variables, not 73. If you inspect this tibble, you will see that all columns are included. To tells R it shouldn’t import columns, there is an alternative. Using col_types you can specify the type of the column (test, numeric, logical, date, time, list of skip). The latter options, skip, tells R no to import this column. To use this option, we first define a vector with values to keep and values to skip. R should keep the first column. So we can use “guess” to allow R to guess its value (as an alternative, you can add “text”. Then we have 5 columns that R can skip. Using rep(\"skip\", 5) we can tell R to skip these columns when its reads the data. Columns 7 - 78 can be kept. Here we use “guess” to shorten the code, but you could include rep(\"text\", 5), rep(\"numeric\", 67) and include the exact type of the column. To create the vector with columns to skip/keep, we can use:\n\ncol &lt;- c(\"guess\", rep(\"skip\", 5), rep(\"guess\", 72))\n\nAdding this vector in col_types =col, R will import only columns 1 and 7 to 78:\n\ndf_ameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE, \n  col_types = col,\n  na = c(\" \", \"NA\")\n \n)\n\nYou can use the range argument to import parts of a sheet. To do so, you can use ranges as you would define them in excel. To illustrate, let’s extract a range starting in row 10 of colum L and ending in row 20 of column O. If you extract a range that doesn’t include column headers, you have to specify this by changing col_names = FALSE.\n\ndf_ameco1range &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = \"L10:O20\",\n  col_names = FALSE, \n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n\n\nR now add column headers ...1, ...2, … . As an alternative, you can add your own names:\n\ndf_ameco1range &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = \"L10:O20\",\n  col_names = paste0(\"var\",1:4),\n  na = c(\" \", \"NA\")\n)\n\nHere, we included the range using the column and row indicators. As an alternative, you can define the range relative to an initial reference cell. To do so you use range = anchored(\"ref.cell\", c(rows, cols)). The reference cell the cell in one corner of the range. The rows in the dim argument count the number of rows (including the one for the reference cell) to include in the range while the cols add the number of columns, including the one for the reference cell, to include in the range. Extracting “L10:O20” and starting from L10 included 4 columns (L, M, N and O) and 11 rows (10, 11, …20)\n\ndf_ameco1range2 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = anchored(\"L10\", dim = c(11, 4)),\n  col_names = paste0(\"var\",1:4),\n  na = c(\" \", \"NA\")\n)\n\nYou can verify that both are equal using\n\nlength(base::setdiff(df_ameco1range, df_ameco1range2))\n\n[1] 0\n\n\nUsing cell_rows() in the range = arguments allows you to select specific rows to import. As with the cell_colls() argument, rows must be adjacent.\n\ndf_ameco1range3 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = cell_rows(4:8),\n  col_names = paste0(\"var\", 1:78),\n  na = c(\" \", \"NA\")\n)\n\nNote that you can skip one row or a range of rows using the skip = argument. Using this argument, you can tell R not to import one or multiple rows. The used this argument in the previous part on reading delimited files.\nThe sheets agument allows you to specify to sheet the {readxl} functions must import. Recall that AMECO_multiplesheets.xlxs includes 3 sheets: “population”, “consumption” and “investment”. The {readxl} function include two ways to include the sheet: by name or by number. In the former case, you include the sheet’s name. The second numbers the sheets as they appear in the workbook. In other words, 1 refers to “population”, 2 to consumption, … To extract the population sheet, you have two alternatives. First you use the name:\n\ndf_popname &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"), \n  sheet = \"population\",\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nor you include the number\n\ndf_popnumb &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"), \n  sheet = 1,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nYou can verify that both sheets are the same and refer to the population data in AMECO1.xlsx. In case there are multiple sheets in a workbook, R will import the first if no alternative is given.\nYou can not import more than one sheet. To do so, you must use e.g. lapply(). Here, we have a vector with sheet names and we know the function that R needs to apply. lapply() shows the result in a list:\n\nsheets &lt;- excel_sheets(path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"))\n\nlist_sheets &lt;- lapply(sheets, \n  \\(x) read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO_multiplesheets.xlsx\"), \n  sheet = x,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n))\n\nNew names:\nNew names:\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nlist_sheets includes three tibbles, one per sheet. You can extract these tibbles using the simplifying subsetting operator for lists:\n\ndf_pop &lt;- list_sheets[[1]]\ndf_cons &lt;- list_sheets[[2]]\ndf_inv &lt;- list_sheets[[3]]\n\n\n\n\n6.2.2.2 Googlesheets\nGoogle sheets stores data on Google drive. R includes two packages that allow you to work with Google drive and Google sheets. The first is {googledrive}, see D’Agostino McGowan and Bryan (2023). The package that allows you to import data from Google drive is {googlesheets4}, see Bryan (2023). The cheatsheet includes {googlesheet4} functions on its second page. To use both, you need to install them\n\ninstall.packages(\"googlesheets4\")\ninstall.packages(\"googledrive\")\n\nand load them in the memory\n\nlibrary(googlesheets4)\nlibrary(googledrive)\n\nGoogle sheets are spreadsheets and are stored in the cloud. Technically, {googlesheets4} using Google’s Sheets API v4 (hence the 4 in the the name, googlesheets4). In other words, you connect to these spreadsheets using an API. These sheets are stored on Google drive. The {googledrive} package allows you to work with files on Google drive. Note that some sheets may be private, other shared or public. If sheets are private, you’ll need to make sure that you have access to these sheets.\nTo find files on Google drive you can use drive_find(). Using this function, you can search for files whose name includes a pattern or search for a specific type of file. The pattern can include regular expressions. For instance, to find files includes “sales” you can use drive_find(pattern = \"sales\") or to find all spreadsheets use drive_find(type = \"spreadsheets\").\nTo read Google sheet spreadsheets, {googlesheet4} includes read_sheet() and read_range(). Both are identical. The read_sheet() however, evokes the other packages as it starts with read and ends with what to read. The full read_sheet() function includes the following arguments:\n\nread_sheet(\n  ss,\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  .name_repair = \"unique\"\n)\n\nThe function returns a tibble.\nIn read_sheet(), ss refers to a spreadsheet. Using Google sheets, you can identify a spreadsheet via an URL, a spreadsheet identifier or you can use {googledrive}’s drive_get(\"name\") function and use the result as the first argument in read_sheet(). Most other arguments should be familiar from previous functions.\n\n\n\n6.2.3 Other packages and formats\n\n6.2.3.1 R files: read_rds() and write_rds()\nThe functions allow you to read R’s rds files. While the previous functions focused on reading and writing non-R data, using write_rds() use can save a data set as and rds file. Doing so, you can can read it using read_rds(). The write_rds() function includes the following arguments:\n\nwrite_rds(\n  x,\n  file,\n  compress = c(\"none\", \"gz\", \"bz2\", \"xz\"),\n  version = 2,\n  refhook = NULL,\n  text = FALSE,\n  ...\n)\n\nHere, x refers to the R object that you want to write to the rds format. You write that file using the name and the path included in file. You can specify if the file needs compression (“gz”, “bz2” or “xz”). By default, this is not the case. You can leave all other options to their default values. Let’s read a csv file and write it as an rds file:\n\ndf_bop &lt;- read_csv(\n  file = here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO10.CSV\"),\n  col_select = c(1, 7:78),\n  name_repair = \"unique_quiet\",\n  show_col_types = FALSE\n)\n\nUsing write_rds() you can now save this file in the data &gt; tidy folder as bop.rds:\n\nwrite_rds(\n  df_bop,\n  here::here(\"data\", \"tidy\", \"bop.rds\")\n)\n\nReading this object back into the session uses\n\ndf_boprds &lt;- read_rds(here::here(\"data\", \"tidy\", \"bop.rds\"))\n\nSaving a file as an rds file is useful if you and your colleagues work with R. If others in the team use STATA, SAS or SPSS, saving a datafile as a csv file is often preferable. Although these packages all read multiple types of data files, using a widely used format such as csv is most of the time the safest choice. However, if all team members use R, saving a dataset as an rds is often very efficient as reading it doesn’t require reading one type of file and transforming it in to another.\n\n\n6.2.3.2 Haven (STATA, SAS, SPSS)\nSTATA, SAS and SPSS are widely used in many fields to do data analysis. Using {haven} H. Wickham, Miller, and Smith (2023) allows you to read files saved in STATA’s dta format, in SAS sas7bdat and sas7bcat files and SPSS ’s sav files. {Haven} also allows you to write dta, sav and xpt (SAS transport format) files. As an example, you can read STATA files using read_dtaor read_stata. Both function return a tibble and are identical. The function arguments include\n\nread_dta(\n  file,\n  encoding = NULL,\n  col_select = NULL,\n  skip = 0,\n  n_max = Inf,\n  .name_repair = \"unique\"\n)\n\nread_stata(\n  file,\n  encoding = NULL,\n  col_select = NULL,\n  skip = 0,\n  n_max = Inf,\n  .name_repair = \"unique\"\n)\n\nAs you can see, these function are very closed to the functions we covered: the function needs a file to read and you can select columns, skip rows and repair names. The encoding argument is sometimes relevant for Mac of Linus users as {haven} assumes default windows encoding. If there is an error use \"latin1\". I refer to the {haven} pages for more detail on these functions. Note that some of these extra arguments are specific for the type of file (STATA, SAS, SPSS) and might require some knowledge on how these packages store their datafiles.\n\n\n6.2.3.3 Other: Json, XML\nJson and XML are two data interchange formats. JSON refers to JavaScript Object Notation. XML refers to eXtensible Markup Language. Both are popular ways to exchange data. Here, it is not the objective to discuss JSON or XML file data structures in detail. I’ll show an example and add references. Doing so, you’ll be able discover how to use these data structures quite fast.\nIn R, you can use {jsonlite} (Ooms (2014)) and {xml2} (H. Wickham, Hester, and Ooms (2025)) to import data stored in that format. To install these packages, you can use\n\ninstall.packages(\"jsonlite\", \"xml2\")\n\nTo illustrate JSON, here is an example taken from Jeroen Ooms ’vignette on the {jsonlite} package. Let’s first load this package\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nSuppose that you have a dataet that includes the name, age and occupation of 3 persons. In addition, you have one missing observation.\n\n[\n{\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"},\n{\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n{},\n{\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]\n\nThe first observations is stored between {} and includes “variables” and “values” or “Key-Value” pairs: {“Name” : “Mario”, “Age” : 32, “Occupation” : “Plumber”}. Other people in the data set are Peach and Bowser. For the latter, the age is missing.\n{jsonlite} includes two function: fromJSON and toJSON. These are comparable with the read and write functions in the previous sections. The first imports the data, the second writes and R object to a JSON file. To import, fromJSON has the following arguments:\n\nfromJSON(\n  txt,\n  simplifyVector = TRUE,\n  simplifyDataFrame = simplifyVector,\n  simplifyMatrix = simplifyVector,\n  flatten = FALSE,\n  ...\n)\n\nHere, txt refers to the JSON file or URL. The simplify arguments are used to simplify the JSON structure into an R structure. In the example, the 3 persons are in an array of objects. fromJSON will simplify this result to a data frame. A JSON array of primitives is comparable to an R vector and the funtion will simplify to an R vector. A JSON array of arrays simplifies to a matrix.\nLet’s use the example to extract the information in the JSON data structure and store it as an R data frame. We first create the JSON structure and than use fromJSON to\n\ndatajson &lt;- '[{\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}]'\n  \ndf_json &lt;- fromJSON(datajson)\ndf_json\n\n    Name Age Occupation\n1  Mario  32    Plumber\n2  Peach  21   Princess\n3   &lt;NA&gt;  NA       &lt;NA&gt;\n4 Bowser  NA      Koopa\n\n\nYou can write this file to a JSON format using toJSON(). For instance\n\ndatatojson &lt;- toJSON(df_json)\n\ncreates a JSON structure. In the environment pane, you can see that the datatojson object is a jsonobject. You’ll also recognize the structure from this object. Here it includes the escape backslash.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#apis",
    "href": "06_Importing_and_exporting.html#apis",
    "title": "6  Importing and exporting data",
    "section": "6.4 API’s",
    "text": "6.4 API’s\nAPIs (Application Programming Interfaces) are sets of rules and protocols that enable software applications to communicate with each other. For out purpose here, the communication involves an exchange of data. There are two parties involved in the exchange: the client and the server. The latter stores the data. The client sends a request (e.g. for data) through an API to the server. The API sends your request to the database on the server. The server checks the database with the data to see if the data is available and, if so, sends to output back to the API. The API then sends the output to the client.\nHere the client is R. Using R, you send a request for data to an API to e.g. the European Commission (AMECO), the ECB, FED, Yahoofinance, … . The API of these institutions then check if the data is available and if so, return the result, through the API back to R. The data are now available to use.\nAPI’s are especially useful for datasets with a regular update. For instance, exchange rates, stock market prices, employment or inflation statistics, data from quarterly surveys on labour market outcome, poverty, environmental stress, … These are usually updated every minute, day, month or quarter. Writing the code for the API once allows you to connect to that data and update as needed. Doing so, you’ll have to invest a little effort to write the code, but you’ll save time as updating, tidying and e.g. using the data in your analysis will require much less time.\nAPI’s can be private or public. In the former case, access to the data is restricted to those within a company or firm or to those with an access key. To receive an access key, it is often sufficient to create an account. In other cases, access is conditional on a subscription. Pure public API’s don’t need a key and you can access them without any account of payment, although some do require to install a key. To get the key, you have to go the website of the organisation offering API access and ask for an acces key. Usually, this can be done fast and online.\nIt is never a good idea to write the key in your code. As a general rule, you should store your key in another file. To do so, you can use the .Renviron file. This file will save your user keys. To edit create and edit this file you can use the edit_r_environ() function from the {usethis} package (H. Wickham et al. (2024)). You need to install this package first\n\ninstall.packages(\"usethis\")\n\nand load it in the memory\n\nlibrary(usethis)\n\nWarning: package 'usethis' was built under R version 4.4.3\n\n\nIf you enter edit_r_environ() in the console, R opens a new window. In that window you can install your API key e.g. using\n\nABC_API_KEY = {api_key}\n\nHere ABC stands for the organisation that issued the key. In your code, you can then use\n\nSys.getenv(ABC_API_KEY)\n\nIn that way, your API keys stay out of your code.\nBecause API’s call use a web connection, we also install {httr} (H. Wickham (2023)) and {httr2} (H. Wickham (2025)) which both include functions to use with URL’s and webrequests.\n\ninstall.packages(\"httr\", \"httr2\")\n\nYou can load them in memory\n\nlibrary(httr)\nlibrary(httr2)\n\n\n6.4.1 Introduction\nThere are many package that are designed to use API’s to connect to datasets. Doing so, they have to tell these packages that data you want. These packages then set up a connection, send a request to the server and return a tibble, data frame or data table. Here, I illustrate how you can use API’s in R using APU that allow you to retreive financial data, central bank data and data from a statistical office. However, there are many R packages that allow you to retrieve data. Because of this, it is impossible to give an exhaustive overview of all those that you can use. Here, the idea is to illustrate the basic setup. After you finish this section, you should be able to learn how to use other API quite fast.\nTo test is you can: if you have a Spotify account, you can use {spotifyr} to analyse your playlists. Here is an example of such an analysis for 4 countries.\n\n\n6.4.2 Examples of R API packages\n\n6.4.2.1 Financial data: yahoofinancer and tidyfinance\n{yahoofinancer} allows you to retrieve stock market data from the Yahoo finance database. You need to install the package using\n\ninstall.packages(\"yahoofinancer\")\n\nand load it in the session\n\nlibrary(yahoofinancer)\n\nAll financial products trade under a ticker symbol. For instance, Amazon.com trades under the ticker AMZN, Microsoft under the ticker MSFT, … . You can find these ticker symbols, using e.g. Yahoo finance. Ticker symbols for US stocks do not include a reference to the country, tickers symbols for other countries usually include a reference to the country using “ticker.country”, e.g. “SAP.DE”. In addition to stocks, you have indices, exchange rates, … . Say you would like to analyse the stock market price for Coca Cola (ticker symbol KO). To do so using {yahoofinancer} you first initiate the ticker using Ticker$new() function. This function creates a Ticker class object that allows you to retrieve information with respect to Coca Cola. In your environment pane you’ll see an Object containing active binding.\n\nKO &lt;- Ticker$new(\"KO\")\n\nUsing KO you can now access most of the data available in Yahoo’s finance database. If you type KO in the console, you’ll see what type of information you can retrieve. The most recent market price. Using KO$regular_market_price\n\nKO$regular_market_price\n\n[1] 70.37\n\n\nshows the most recent market price. To see the history, you can use KO$et_history(). Here, you can set the period (“1d”, “5d”, “1mo”, “3mo”, “6mo”, “1y”, “2y”, “5y”, “10y”, “ytd” and “max). Here, year to data (since January 1st) is the default. In addition to the period, you can also select the interval: from”1m”, “2m”, “5m”, “15m”, “30m”, “60m”, “90m”, 1h”, “1d”, “5d”, “1wk”, “1mo” and “3mo”. You can also specify a start and end data using the %Y-%m-%d format. To illustrate, you can download the data for Coca Cola using a monthly interval from the start of 2024 to the end of 2024 using:\n\nKO$get_history(start = \"2024-01-01\", \n               end = \"2024-12-31\", \n               interval = \"1mo\")\n\n                  date    volume  high   low  open close adj_close\n1  2024-01-01 05:00:00 293691500 60.46 58.66 58.80 59.49  57.30405\n2  2024-02-01 05:00:00 284353000 61.62 58.79 59.57 60.02  57.81458\n3  2024-03-01 05:00:00 287613500 61.43 58.97 59.90 61.18  58.93195\n4  2024-04-01 04:00:00 300592200 62.83 57.93 61.18 61.77   59.9762\n5  2024-05-01 04:00:00 238746700 63.76 61.07 61.80 62.93  61.10251\n6  2024-06-01 04:00:00 222367100 64.36 61.95 62.71 63.65  61.80161\n7  2024-07-01 04:00:00 257584900 67.83 62.29 64.03 66.74  65.30469\n8  2024-08-01 04:00:00 311241600 72.57 66.70 67.00 72.47  70.91146\n9  2024-09-01 04:00:00 306271700 73.53 70.38 72.44 71.86  70.31458\n10 2024-10-01 04:00:00 295543300 72.75 65.26 72.10 65.31  64.34355\n11 2024-11-01 04:00:00 332061100 65.66 61.39 65.47 64.08  63.13176\n12 2024-12-01 05:00:00 391133900 64.19 61.53 64.05 62.26  61.80391\n\n\nThe history, returned as a data frame, includes the data, the volume (number of stocks traded), the highest and lowest price for that month as well as the open and closing price. The adjusted close takes intro account e.g. stock splits, dividends paid out in stock, … . If you assign is series to an object, you can now use these data in your work. If you only need the e.g. data and adjusted close, you can use the usual subsetting operators:\n\nKO$get_history(start = \"2024-01-01\", \n               end = \"2024-12-31\", \n               interval = \"1mo\")[, c(1, 7)]\n\n                  date adj_close\n1  2024-01-01 05:00:00  57.30405\n2  2024-02-01 05:00:00  57.81458\n3  2024-03-01 05:00:00  58.93195\n4  2024-04-01 04:00:00   59.9762\n5  2024-05-01 04:00:00  61.10251\n6  2024-06-01 04:00:00  61.80161\n7  2024-07-01 04:00:00  65.30469\n8  2024-08-01 04:00:00  70.91146\n9  2024-09-01 04:00:00  70.31458\n10 2024-10-01 04:00:00  64.34355\n11 2024-11-01 04:00:00  63.13176\n12 2024-12-01 05:00:00  61.80391\n\n\nIn addition to the stock market prices, Yahoo Finance offers other data, e.g. valuations (using valuation_measures or technical trading data (using technical_insights):\n\nKO_val &lt;- KO$valuation_measures\n\n\nKO_tech &lt;- KO$technical_insights\n\nIf you download data on international stocks, adding the currency to convert to a common currency, is possible using\n\nKO_cur &lt;- KO$currency\n\nIn addition to the Ticker class, {yahoofinancer} also includes the Index class. This class allows you to download data on market indices. These market indices usually start with a ^, e.g. ^STOXX50E for the Euro Stoxx 50, or ^HSI for the Hang Seng Index. Creating a new index class follows the same procedure as the one for an individual stock:\n\neuro50 &lt;- Index$new(\"^STOXX50E\")\n\nYou can new get the history using get_history.\n{yahoofinancer} includes all currency symbols. Using\n\ncur &lt;- get_currencies()\n\nyou can download these currencies including their symbol, their long name and their local name. To convert a value in one currency into the other, you can use\n\ntoday &lt;- Sys.Date()\ncurrency_converter(from = \"EUR\", to = \"USD\", start = \"2025-03-01\", end = today, interval = \"1d\" )\n\n         date     high      low     open    close volume adj_close\n1  2025-03-03 1.050023 1.039047 1.041385 1.041385      0  1.041385\n2  2025-03-04 1.055955 1.047175 1.048504 1.048504      0  1.048504\n3  2025-03-05 1.078586 1.060288 1.062699 1.062699      0  1.062699\n4  2025-03-06 1.085105 1.078249 1.079494 1.079494      0  1.079494\n5  2025-03-07 1.088566 1.078214 1.078795 1.078795      0  1.078795\n6  2025-03-10 1.087347 1.080625 1.086059 1.086059      0  1.086059\n7  2025-03-11 1.093028 1.083682 1.083940 1.083940      0  1.083940\n8  2025-03-12 1.093458 1.087725 1.091417 1.091417      0  1.091417\n9  2025-03-13 1.089918 1.082298 1.088625 1.088625      0  1.088625\n10 2025-03-14 1.091012 1.083130 1.085564 1.085564      0  1.085564\n11 2025-03-17 1.092896 1.086921 1.088127 1.088127      0  1.088127\n12 2025-03-18 1.095446 1.089704 1.091918 1.091918      0  1.091918\n13 2025-03-19 1.094571 1.087595 1.093948 1.093948      0  1.093948\n14 2025-03-20 1.091751 1.081830 1.091191 1.091191      0  1.091191\n15 2025-03-21 1.086130 1.080567 1.085670 1.085670      0  1.085670\n16 2025-03-24 1.085694 1.079075 1.083576 1.083576      0  1.083576\n17 2025-03-25 1.083072 1.077865 1.080357 1.080357      0  1.080357\n18 2025-03-26 1.080229 1.076878 1.078749 1.078749      0  1.078749\n19 2025-03-27 1.081876 1.074206 1.074206 1.074206      0  1.074206\n20 2025-03-28 1.084528 1.076589 1.080100 1.080100      0  1.080100\n\n\nThis function returns the exchange rate from currency in from (e.g. the EURO) to the currency in to (e.g. the Dollar) for the period starting in start and ending in end in intervals of interval. Note that the most recent observation is you use today &lt;- Sys.Date and use today for end, shows the intraday value.\n{tidyfinance} is a package that was created to support portfolio management. As part of this package, you can use a number of function to download data using various API’s e.g. Financial Modelling Prep (FMP) allowing you to access (free of charge) 250 call per day, with 5 years of historical fundamental data allowing you to do financial statement analysis of discounted cash flow analysis; Yahoo’s finance API to download stock market data, currencies, indices, … ; the WRDS database, CRSP or (parts of compustat). We used this package to download stock market data in Chapter 2.\nLet’s first get this package in memory:\n\nlibrary(\"tidyfinance\")\n\nWe will illustrate the download_data() function for 5 European companies: SAP, ASML, BNP, LVMH and the Belgian Lotus Bakeries:\n\ntickers &lt;- c(\"SAP.DE\", \"ASML.AS\", \"BNP.PA\", \"MC.PA\", \"LOTB.BR\")\n\nTo download the data for this year to today and save the results in a data frame port, you can run\n\nstart &lt;- \"2010-01-01\"\nend &lt;- Sys.Date()\nport &lt;- download_data(\n  type = \"stock_prices\",\n  symbols = tickers,\n  start_date = start,\n  end_date = end\n)\n\nThe function returns a tidy dataset with data on the volume, high and low and open and close as well as the adjusted price. To download data on index constituents, you can use download_data(type = \"constituents\", index = \"index\"). The supporred indices are available via\n\nindices &lt;- list_supported_indexes()\n\nFor instance, for the Euro Stoxx 50, the constituents are\n\neuro50_con &lt;- download_data(type = \"constituents\", index = \"EURO STOXX 50\")\n\nTo download macro-economic data, you dan call the relevant source in download_data() and add the series via series = c(), e.g. c(\"GDP\", \"CPIAUNC\") for data on GDP and inflation from the Federal Reserve of Saint Louis FRED database.\n\n\n6.4.2.2 Central banking: ECB, FED, … .\nCentral banks often publish data with daily, weekly or monthly updates. The Federal reserve’s online data portal is known as the Federal Reserve Economic Data or FRED. You can access the ECB’s data warehouse using the ECB’s Data portal. Most other central banks offer online access to their data. In addition, the Bank of International Settlements publishes regular updates on many financial series. Here I will use the {ecb} package (Persson (2023)) to illustrate how you can use API’s to access data. There is a nice illustration of how you can use this package on Dima Diachkov’s blog.\nTo use the ECB package, you need to install it first:\n\ninstall.packages(\"ecb\")\n\nand load it in memory.\n\nlibrary(ecb)\n\nTo retrieve data from the ECB’s data warehouse, you use the get_data() function. This function as two arguments, key and filter. The first refers to a character string that identifies this series. Every series in the date warehouse had a unique identifier. For instance the key for the daily the EUR/USD exchange rate is “EXR.D.USD.EUR.SP00.A”. You can find these keys on the ECB’s data portal site (see Figure 6.2).\n\n\n\n\n\nFigure 6.2: ECB Exchange rate keys\n\n\n\n\n\n\n\n\nHere you can see the overall structure of the key: the data refer to an exchange rate (EXR), are daily (D), concerns the USD.EUR and is a spot rate (SP00). To show another result, the average point forecast for the Euro area (changing composition) inflation rate by professional forecasters is known as SPF.W.U2.HICP.POINT.LT.Q.AVG ( Figure 6.3)\n\n\n\n\n\nFigure 6.3: Long terms inflationi forecasts\n\n\n\n\n\n\n\n\nIf data refers to a country, the reference is included on the third position of the name. In addition, if the data are unadjusted for seasons, the key shows N while adjustedd data show S. For instance, the monthly unemployment rate for 15-74 aged in Austria in Figure 6.4 show a couple of series:\n\n\n\n\n\nFigure 6.4: Unemployment rate Austria, monthly\n\n\n\n\n\n\n\n\nThe filter arguments allows you to include a startPeriodand endPeriod where periods are included as YYYY for annual data, YYYY-S[1-2] for semi-annual data (YYYY-S1), YYYY-Q[1-4] for quarterly data, YYYY-MM for monthly data, YYYY-W[01-53] for weekly data and YYYY-MM-DD for daily data. To retrieve data updated since a certain point in time, you can use filter = list(updataAfter = YYYY-MM-DDTHH:MM:SS+01:00). With filter = list(firstNObservations = x) or filter = list(lastNObservations = x) you reduce the data to the first x or last x observations for each series.\nLet’s retrieve the data for the USD/EUR exchange rate since the start of 2025. To do so, we first set the key and the filters in a separate object. We then use get_data(key, filter) to import the data:\n\nkey &lt;- \"EXR.D.USD.EUR.SP00.A\"\nfilters &lt;- list(startPeriod = \"2025-01-01\")\n\nusdeur &lt;- ecb::get_data(key, filters)\n\nThe API returns a tibble which includes data on the series (e.g. frequency, currency, …) as well as the “obstime” and “obsvalue”. The first is the date/time of the observation; the second the exchange rate. With respect to the data/time, the tibble shows it as a character:\n\nclass(usdeur$obstime)\n\n[1] \"character\"\n\n\nTo change this to data/time, you can use the one the the date or date/time functions we covered during the lectures on dates and times. For instance\n\nusdeur$obstime &lt;- as.Date(usdeur$obstime)\n\nIn addition, you probably don’t need the columns on frequency, the currency in the denominator (which is the Euro) nor the exchange rate type of suffix. If you need reminders, you can include these in the name of the tibble. For isntance,\n\nusdeur_spot_d &lt;- usdeur[c(\"currency\", \"obstime\", \"obsvalue\")]\n\nreduces the data to 3 columns: the currency (USD), the time and the value.\nWhat if you need more series. There are a couple of ways to add multiple series. The first adds them in the key using +. Suppose for instance that, in addition to the USD exchange rate, you also need the GBP. Here, the series is EXR.D.GDP.EUR.SP00.A. Using this observation, you can add the include GBP+USD in your series key:\n\nkey &lt;- \"EXR.D.GBP+USD.EUR.SP00.A\"\n\nYou can now use the get_data() function to import the data (we will use the same period):\n\nkey &lt;- \"EXR.D.GBP+USD.EUR.SP00.A\"\nfilters &lt;- list(startPeriod = \"2025-01-01\")\n\ngbpusdeur &lt;- ecb::get_data(key, filters)\n\nWhat is you need two series for multiple countries? Suppose that you would like to import the percentage change in the Harmonized Consumeer Price Index excluding food and energy (i.e. core HICP inflation) for all EU countries. Here, the key is “ICP.M.XX.N.XEF000.4.ANR”. In this series, “XX” is the code for the country. For instance, DE for Germany, “EE” for Estonia or “ES” for Spain. Here, we can use the same approach as we used for the two currencies: we add “DE+EE+ES+…” un the country code. The country symbols are\n\ncountrycode &lt;- c(\"AT\", \"BE\", \"BG\", \"HR\", \"CY\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\", \"GR\", \"HU\", \"IE\", \"IT\", \"LV\", \"LT\", \"LU\", \"MT\", \"NL\", \"PL\", \"PT\", \"RO\", \"SK\", \"SL\", \"ES\", \"SE\")\n\nWe can now use these codes to create the hicpkey. First we use paste with collaps  = 0 to generate the “AT+BE+ … +SE” character variable\n\ncountrycode &lt;- paste(countrycode, collapse = \"+\")\ncountrycode\n\n[1] \"AT+BE+BG+HR+CY+CZ+DK+EE+FI+FR+GR+HU+IE+IT+LV+LT+LU+MT+NL+PL+PT+RO+SK+SL+ES+SE\"\n\n\nWe can now use paste0 to generate the hicpkey:\n\nhicpkey &lt;- paste0(\"ICP.M.\", paste(countrycode, collapse = \"+\"), \".N.XEF000.4.ANR\")\nhicpkey\n\n[1] \"ICP.M.AT+BE+BG+HR+CY+CZ+DK+EE+FI+FR+GR+HU+IE+IT+LV+LT+LU+MT+NL+PL+PT+RO+SK+SL+ES+SE.N.XEF000.4.ANR\"\n\n\nUsing this key, we can use import all data for the last 12 months\n\nfilter &lt;- list(lastNObservations = 12)\ncore_hicp &lt;- get_data(hicpkey, filter)\n\nThe dataset core_hicp includes 300 observations for 8 variables. You can now change change the time of obstime and remove variables that you don’t need.\n\n\n6.4.2.3 Global economic institutions: World bank, IMF, …\nRecall from Chapter 2 that we already use the {wbstats} to download data on life expectancy at birth. The package interacts with the World bank’s data service. Let’s load {wbstats} in memory\n\nlibrary(wbstats)\n\nIn Chapter 2, the keys were given\n\nmy_indicators &lt;- c(\n  life_exp = \"SP.DYN.LE00.IN\", \n  gdp_capita = \"NY.GDP.PCAP.CD\", \n  pop = \"SP.POP.TOTL\"\n)\n\nand you could use them in the {wbstats} call:\n\nlife_df &lt;- wb_data(my_indicators, start_date = 1960, end_date = 2022)\n\nHow do you find these key’s? Here, there are two options. First, as you did with the ECB’s data, you can do to the data pages of the World Bank and search for the indicators. For instance, if you search for life expectancy at birth, you’ll find the page in Figure 6.5. You can find the code in the URL of the page as well as in the details on the data: “SP.DYN.LE00.IN”.\n\n\n\n\n\nFigure 6.5: Life expectancy at birth, World Bank\n\n\n\n\n\n\n\n\nThe second way to search the data is to use the {wbstats} wb_search() function. The package includes the series that are available in the wb_cachelist\n\nstr(wb_cachelist, max.level = 1)\n\nList of 8\n $ countries    : tibble [304 × 18] (S3: tbl_df/tbl/data.frame)\n $ indicators   : tibble [16,649 × 8] (S3: tbl_df/tbl/data.frame)\n $ sources      : tibble [63 × 9] (S3: tbl_df/tbl/data.frame)\n $ topics       : tibble [21 × 3] (S3: tbl_df/tbl/data.frame)\n $ regions      : tibble [48 × 4] (S3: tbl_df/tbl/data.frame)\n $ income_levels: tibble [7 × 3] (S3: tbl_df/tbl/data.frame)\n $ lending_types: tibble [4 × 3] (S3: tbl_df/tbl/data.frame)\n $ languages    : tibble [23 × 3] (S3: tbl_df/tbl/data.frame)\n\n\nIt is good idea to update the data in list. Sometimes, the World Bank changes the data it collects. To do so you can create a new cachelist using\n\nnew_cashlist &lt;- wb_cache()\n\nYou can now use wb_search(pattern, fields, extra, cache, ignore.case = TRUE). With a new cache, you include the new_cache in that argument. In that way, you are sure that you have the most recent data.\nLet’s search for “life expectancy”:\n\nlife_search &lt;- wb_search(pattern = \"life expectancy\", cache = new_cashlist)\n\nwb_search returns a tibble with 7 observations. In other words, there are 7 indicators that match this pattern. Inspecting these observations, you see the indicator we need is “life expectancy at birth, total”:\n\nlife_search &lt;- wb_search(pattern = \"life expectancy at birth, total\", cache = new_cashlist)\n\nWe can now use this results for find the indicator:\n\nlife_search$indicator_id\n\n[1] \"SP.DYN.LE00.IN\"\n\n\nWe can find the other variables in a similar way:\n\ngdp_search &lt;- wb_search(pattern = \"gdp per capita \\\\(constant 2015\", cache = new_cashlist)\n\nThe indicator for per capita GDP is\n\ngdp_search$indicator_id\n\n[1] \"NY.GDP.PCAP.KD\"\n\n\n\npop_search &lt;- wb_search(pattern = \"^population, total\", \n                        fields = c(\"indicator\"), cache = new_cashlist)\n\nWith the third variable key, we can now build the import function:\n\nmy_indicators &lt;- c(\n  life_exp = life_search$indicator_id, \n  gdp_capita = gdp_search$indicator_id, \n  pop = pop_search$indicator_id\n)\n\nand use the wb_data function to import the data:\n\ndataset &lt;- wb_data(my_indicators, \n                   country = \"countries_only\", \n                   start_date = 2000, \n                   end_date = 2020)\n\nAs you can see, this function also includes the option to limit the search to one for more countries. To do so, you list the countries using their ISO2 or ISO3 code. As an alternative you can select country groups by e.g. income_level of region. To see country codes or groups, you can use {wbstats}\n\nstr(wb_countries())\n\ntibble [296 × 18] (S3: tbl_df/tbl/data.frame)\n $ iso3c             : chr [1:296] \"ABW\" \"AFE\" \"AFG\" \"AFR\" ...\n $ iso2c             : chr [1:296] \"AW\" \"ZH\" \"AF\" \"A9\" ...\n $ country           : chr [1:296] \"Aruba\" \"Africa Eastern and Southern\" \"Afghanistan\" \"Africa\" ...\n $ capital_city      : chr [1:296] \"Oranjestad\" NA \"Kabul\" NA ...\n $ longitude         : num [1:296] -70 NA 69.2 NA NA ...\n $ latitude          : num [1:296] 12.5 NA 34.5 NA NA ...\n $ region_iso3c      : chr [1:296] \"LCN\" NA \"SAS\" NA ...\n $ region_iso2c      : chr [1:296] \"ZJ\" NA \"8S\" NA ...\n $ region            : chr [1:296] \"Latin America & Caribbean\" \"Aggregates\" \"South Asia\" \"Aggregates\" ...\n $ admin_region_iso3c: chr [1:296] NA NA \"SAS\" NA ...\n $ admin_region_iso2c: chr [1:296] NA NA \"8S\" NA ...\n $ admin_region      : chr [1:296] NA NA \"South Asia\" NA ...\n $ income_level_iso3c: chr [1:296] \"HIC\" NA \"LIC\" NA ...\n $ income_level_iso2c: chr [1:296] \"XD\" NA \"XM\" NA ...\n $ income_level      : chr [1:296] \"High income\" \"Aggregates\" \"Low income\" \"Aggregates\" ...\n $ lending_type_iso3c: chr [1:296] \"LNX\" NA \"IDX\" NA ...\n $ lending_type_iso2c: chr [1:296] \"XX\" NA \"XI\" NA ...\n $ lending_type      : chr [1:296] \"Not classified\" \"Aggregates\" \"IDA\" \"Aggregates\" ...\n\n\nFor instance, reducing the country dataset to countries in “Low income” categories:\n\nwbcountries &lt;- wb_countries()\ncountries &lt;- wbcountries[wbcountries$income_level == \"Low income\", ]$iso3c\ndataset2 &lt;- wb_data(my_indicators, \n        country = countries, \n        start_date = 2015,\n        end_date = 2018)\n\nFor the International Monetary Fund, {imfr} allows you to retrieve data from one of the IMF’s databases. The function imf_data() in that package allows you to specify the database, the indicator, the country, frequency and both the start and end date.\n\n\n6.4.2.4 DBnomics R client\nThe DBnomics R client allows you to access many data series, including those from the World Bank or the European Central Bank, European Commission’s AMECO database, the Bank for International Settlements (BIS), Organisation for Economic Cooperation and Development (OECD), International Labour Organisation (ILO), Eurostat, World Trade Organisation (WTO), … . Data is available in csv, xlsx, html and json format.\n\ninstall.packages(\"rdbnomics\")\n\n\nlibrary(rdbnomics)\n\nLet’s see how you can use {rdbnomics} to get data from AMECO. Recall that the AMECO dataset includes a series identifyer. For instance, the population data for Germany, included in AMECO1 files is DEU.1.0.0.0.NPDT. Using {rdbnomics} rdb() function, you can import that series using:\n\ndatapop &lt;- rdb(ids = \"AMECO/NPTD/DEU.1.0.0.0.NPTD\")\n\nwhere “AMECO/NPTD” refers to the source, AMECO, and the variable, total population. This function returns a data table and includes all data as all as the name of the variable, the frequency, the period, the provider (AMECO), … .\n\nstr(datapop)\n\nClasses 'data.table' and 'data.frame':  67 obs. of  17 variables:\n $ @frequency     : chr  \"annual\" \"annual\" \"annual\" \"annual\" ...\n $ Country        : chr  \"Germany\" \"Germany\" \"Germany\" \"Germany\" ...\n $ dataset_code   : chr  \"NPTD\" \"NPTD\" \"NPTD\" \"NPTD\" ...\n $ dataset_name   : chr  \"Total population (National accounts)\" \"Total population (National accounts)\" \"Total population (National accounts)\" \"Total population (National accounts)\" ...\n $ freq           : chr  \"a\" \"a\" \"a\" \"a\" ...\n $ Frequency      : chr  \"Annually\" \"Annually\" \"Annually\" \"Annually\" ...\n $ geo            : chr  \"deu\" \"deu\" \"deu\" \"deu\" ...\n $ indexed_at     : POSIXct, format: \"2024-11-16 01:11:23\" \"2024-11-16 01:11:23\" ...\n $ original_period: chr  \"1960\" \"1961\" \"1962\" \"1963\" ...\n $ original_value : chr  \"72777.68\" \"73340.13\" \"73987.95\" \"74676.17\" ...\n $ period         : Date, format: \"1960-01-01\" \"1961-01-01\" ...\n $ provider_code  : chr  \"AMECO\" \"AMECO\" \"AMECO\" \"AMECO\" ...\n $ series_code    : chr  \"DEU.1.0.0.0.NPTD\" \"DEU.1.0.0.0.NPTD\" \"DEU.1.0.0.0.NPTD\" \"DEU.1.0.0.0.NPTD\" ...\n $ series_name    : chr  \"Annually – 1000 persons – Germany\" \"Annually – 1000 persons – Germany\" \"Annually – 1000 persons – Germany\" \"Annually – 1000 persons – Germany\" ...\n $ unit           : chr  \"1000-persons\" \"1000-persons\" \"1000-persons\" \"1000-persons\" ...\n $ Unit           : chr  \"1000 persons\" \"1000 persons\" \"1000 persons\" \"1000 persons\" ...\n $ value          : num  72778 73340 73988 74676 75280 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nIt is now sufficient to select the columns with the data you need.\nIf you want more series from the same or another provider, you can include them after the ids = argument in a vector, e.g. c(“AMECO/NPDT/DEU.1.0.0.0.NPDT”, “AMECO/NPDT/DNK.1.0.0.0.NPDT”). In these the second series if from another provider, you add the reference to that series from that provider.\nAs an alternative, rdb also allows to build a request using a easier approach. In this function call, you first specify the source (AMECO), the variable (unemployment or ZUTN) and then a list with countries as dimensions:\n\ndataunemp &lt;- rdb(\"AMECO\", \"ZUTN\", dimensions = list(geo = c(\"deu\", \"dnk\")))\n\nOther examples are available online.\n\n\n6.4.2.5 Statistical offices: EUROSTAT\nEurostat is statistical agency of the European Union. As most statistical agencies in the world do, Eurostats makes its huge collection of data available on the web. Here, the data is not limited to economic or financial data, but includes data on e.g. poverty, environmental pollution, migration, … . In “Using Eurostat with R” you will find a very good overview of how you can use R to retrieve Eurostat data. For access and import Eurostat data, you can use the package {eurostat} (Lahti et al. (2023)). Here, I will only include the two main functions of this package: search_eurostat and get_eurostat.\nTo install, you can run\n\ninstall.packages(\"eurostat\")\n\nand load in memory\n\nlibrary(\"eurostat\")\n\nThe statistics that are available on Eurostat can be found on its data portal. The function search_eurostat() allows you to search the database. For instance, to see which datasets Eurostat collects with respect to “high-speed internet coverage”, you can use its data portal. Here, you will also find the keys you need to retrieve the data via the {eurostat} package. For instance, the key for the data on “population by educational attainment, level, sex and age” is edat_lfse_9903. As an alternative, you can use the search_eurostat() function included in the {eurostat} package.\n\nsearch &lt;- search_eurostat(pattern = \"Population by educational attainment level, sex and age\")\n\nThe function returns 6 values. We can now store these values in a key. The first two observations show the series we want. The first show the population in 1000’s and the second in percent. If you need the percentage of the population, you can save this code in an object e.g. key_eurostat:\n\nkey_eurostat &lt;- search$code[2]\n\nTo import the data, you can use get_eurostat()\n\ndata_eurostat &lt;- get_eurostat(id = key_eurostat)\n\n\nindexed 0B in  0s, 0B/s\nindexed 2.15GB in  0s, 2.15GB/s\n                                                                              \n\n\nTable edat_lfs_9903 cached at C:\\Users\\u0063587\\AppData\\Local\\Temp\\RtmpIh7Dfy/eurostat/a955112b2cbfdd2fa6bcb3bf69689a05.rds\n\n\nIn this dataset, includes all observations for all EU countries and is also cached in an rds file. Here you retrieved all data available in for that series. To import only data for since 2021 and only for Denmark and Germany, you can add this constraint to the filter argument using a list of filter\n\ndata_eurostat &lt;- get_eurostat(id = key_eurostat,\n                              filter = list(geo = c(\"DE\", \"DE\"), \n                                            sinceTimePeriod = 2021)\n                              )\n\nTable edat_lfs_9903 cached at C:\\Users\\u0063587\\AppData\\Local\\Temp\\RtmpIh7Dfy/eurostat/d929515323b645cf9980a1d61040b2b5.rds",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#webscraping",
    "href": "06_Importing_and_exporting.html#webscraping",
    "title": "6  Importing and exporting data",
    "section": "6.5 Webscraping",
    "text": "6.5 Webscraping",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#text-files",
    "href": "06_Importing_and_exporting.html#text-files",
    "title": "6  Importing and exporting data",
    "section": "6.6 Text files",
    "text": "6.6 Text files",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#other-duckdb",
    "href": "06_Importing_and_exporting.html#other-duckdb",
    "title": "6  Importing and exporting data",
    "section": "6.8 Other: duckDB",
    "text": "6.8 Other: duckDB\nhttps://duckdb.org/\n\n\n\n\n\n\nBarrett, Tyson, Matt Dowle, Arun Srinivasan, Jan Gorecki, Michael Chirico, Toby Hocking, Benjamin Schwendinger, and Ivan Krylov. 2025. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBryan, Jennifer. 2023. Googlesheets4: Access Google Sheets Using the Sheets API V4.\n\n\nD’Agostino McGowan, Lucy, and Jennifer Bryan. 2023. Googledrive: An Interface to Google Drive.\n\n\nOoms, Jeroen. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between JSON Data and r Objects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\nPersson, Eric. 2023. Ecb: Programmatic Access to the European Central Bank’s Statistical Data Warehouse. https://CRAN.R-project.org/package=ecb.\n\n\nWickham, Hadley. 2023. Httr: Tools for Working with URLs and HTTP.\n\n\n———. 2025. Httr2: Perform HTTP Requests and Process the Responses. https://httr2.r-lib.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel Files.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher. 2024. Usethis: Automate Package and Project Setup. https://usethis.r-lib.org.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2025. Xml2: Parse XML. https://xml2.r-lib.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://haven.tidyverse.org.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#reading-file-names-and-folders",
    "href": "06_Importing_and_exporting.html#reading-file-names-and-folders",
    "title": "6  Importing and exporting data",
    "section": "6.2 Reading file names and folders",
    "text": "6.2 Reading file names and folders\nBefore we import data, let’s first see how you can determine which files are stored in a directory. Base R includes the function list.files(). This function allows you to see which files are included in a given directory or folder.\n\nlist.files(path = \".\", \n           pattern = NULL, \n           all.files = FALSE, \n           full.names = FALSE, \n           recursive = FALSE, \n           ignore.case = FALSE, \n           include.dirs = FALSE, \n           no.. = FALSE)\n\nThe path = \".\" refers to the folder. Recall that you can use the {here} package to set the folder relative to the root folders of the project. This also allows you to define a folder using here::here():\n\nfolder &lt;- here::here(\"data\", \"raw\")\n\nIn this case, the object folder will include the name of the folder relative to the project’s root. In other words, it will include c:&gt; ... &gt; root &gt; data &gt; raw. You can verify this using the values part in the environment pane.\nUsing only this argument, list.files() returns all files in the ... &gt; root &gt; data &gt; raw folder. The return structure is a vector. In this case, there are three folders (ameco_csv, ameco_txt and ameco_xlsx) and three files: life_df.Rdata, mtcars.csv and nike_sales_2024.csv. We used these three in Chapter 2. To store these in a separate object, you assign the result of the list.files() function to an object, e.g.\n\nfiles_data_raw &lt;- list.files(path = folder)\nfiles_data_raw\n\n[1] \"ameco_csv\"           \"ameco_txt\"           \"ameco_xlsx\"         \n[4] \"AMECO1.CSV\"          \"life_df.Rdata\"       \"mtcars.csv\"         \n[7] \"nike_sales_2024.csv\"\n\n\nAdding a pattern = as an argument in list.files() allows you to narrow the result of the function to files where a pattern occurs. Here, you can use e.g. regular expressions. For instance suppose that you only want a list of .csv files whose name includes a one or more digits. Do select only those files you can use the regular expression \"\\\\d+.csv$\". This pattern will match all file names that include one or more digits before the . and end with csv. Adding this pattern to list.files() and assigning the result to a vector files_data_csv:\n\nfiles_data_csv &lt;- list.files(path = folder, \n                             pattern = \"\\\\d+.csv$\")\nfiles_data_csv\n\n[1] \"nike_sales_2024.csv\"\n\n\nHere, there is only one file that meets this condition: nike_sales_2024.csv. The other files, life_df.Rdata is not a .csv file. The file mtcars.csv is a .csv file, but doesn’t include digits before the dot. Recall from Chapter 1 that file names should be both human and machine readable. If that is the case, regular expressions will allow you to match almost all patterns in the name of a file.\nThe other arguments in list.files() refer to the files that are shown. With all.names = FALSE, only visible files are shown, invisible files are not. Changing this from FALSE into TRUE would change this. To include the directory path in the file name, you can change the full.names = FALSE to TRUE. You will want to change this option if you want to import multiple files in your session. If you add this option, R stores all file names including the directory or folder where the file is located. Here, R starts from the root folder c:\\ and not from the project’s root. If the data folder includes subfolders, adding recursive = TRUE will return all files in all subfolders. To show the subfolders in the folder, you can use include.dirs = TRUE. The ignore.case = FALSE argument by default uses case sensitive patterns.\nR includes two special cases of list.files(): dir() and list.dirs(). The former is an alias for list.files(). In other words, you can also use dir() and read in the files you folder. For instance,\n\ndir(path = folder, pattern = \"\\\\d+.csv$\")\n\n[1] \"nike_sales_2024.csv\"\n\n\nshow the same result as list.files(). The second function, list.dirs() returns the same output as list.files(all.files = TRUE) and if recursive = TRUE is also returns the full path name.\nUsing the a vector such as files_data_raw or a character such as files_data_csv, you can now import these files.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#rio-the-swiss-army-knife",
    "href": "06_Importing_and_exporting.html#rio-the-swiss-army-knife",
    "title": "6  Importing and exporting data",
    "section": "6.4 Rio: the Swiss army knife",
    "text": "6.4 Rio: the Swiss army knife",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#data-on-the-web",
    "href": "06_Importing_and_exporting.html#data-on-the-web",
    "title": "6  Importing and exporting data",
    "section": "6.6 Data on the web",
    "text": "6.6 Data on the web\n\n6.6.0.1 Data files on the web\n\n\n6.6.0.2 Webscraping",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#reading-non-rectangular-data",
    "href": "06_Importing_and_exporting.html#reading-non-rectangular-data",
    "title": "6  Importing and exporting data",
    "section": "6.7 Reading non-rectangular data",
    "text": "6.7 Reading non-rectangular data\nmeltr()\nreadr::read_lines()",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#rio-the-swiss-army-knife-of-date-input-and-output",
    "href": "06_Importing_and_exporting.html#rio-the-swiss-army-knife-of-date-input-and-output",
    "title": "6  Importing and exporting data",
    "section": "6.3 Rio: the Swiss army knife of date input and output",
    "text": "6.3 Rio: the Swiss army knife of date input and output\n{Rio} is a package that includes two functions to import import() and export export() datasets. To install this package, use\n\ninstall.packages(\"rio\")\n\nTo load it in the memory, use\n\nlibrary(rio)\n\nWarning: package 'rio' was built under R version 4.4.3\n\n\n{Rio}’s import function\n\nimport(\n  file,\n  format,\n  setclass = getOption(\"rio.import.class\", \"data.frame\"),\n  which,\n  ...\n)\n\nimports the file and infers the format from the file. As an alternative, you can add the format via format. There is a short cut for comma delimited filesn “,” , semicolon delimited files “;” and for |. For other formats you can use the “ext” with “ext” the format (e.g. those realdy discussed such as csv, tsv, fwf, xls, xlsx, dta, sav, sat7bat, red, json, xml) but also matlab, Eviews, Apache Arrow Parquet, … . To do to, {rio} calls the relevant functions such as fread, read_xlsx, read_dta, read_sav, … . You can find all the supported file formats in the {rio} documentation. The setclass arguments allows you to specify the output. By default import() returns a data frame, but you can add tbl_df, tblor tibble if the {tibble} package is installed. which allows you to identify the sheet in an Excel workbook, or a file in a compressed archive. For instance, addding sheet = 1 as argument imports the first sheets in the workbook. The ... allow you to include more option that are part of the function {rio} calls to import the data.\nTo import multiple files, {rio} includes the function import_list():\n\nimport_list(\n  file,\n  setclass = getOption(\"rio.import.class\", \"data.frame\"),\n  which,\n  rbind = FALSE,\n  rbind_label = \"_file\",\n  rbind_fill = TRUE,\n  ...\n)\n\nThis function returns a list of data frames, unless the rbind argument is TRUE. In that case, the function returns a data frame with observations stacked on after the other. The rbind_label allows you to include a column with the source file. If there are any missing columns, rbind_fill = TRUE fills these columns with “NA”.\nTo write files, {rio} includes the export() and export_list() functions. The former includes three arguments, x, the data frame to export, file, the name of the file to export to and format the format of the file. If there is not format, the format is inferred from the name of the file. Because {rio} calls the relevant write functions to perform this task, you can also include options included in those. If you want to export multiple data frame, you add them in a list and use export_list(). The arguments are similar to those for the export() function, but x now refers to a list and not a data frame. In addition, you can add an archive to export to e.g. zip or tar formats.\n{rio} can also convert files from one format into the other. To do so, the convert() function first imports the dataset and then exports the data frame to the new format. The function requires the in_file to convert, the out_file, the name of the file to convert to. In addition, you can add options for the relevant import()and export() functions.\nThere are two helper utility functions: get_info() and `g\nBecause it includes so many import and export functions, {rio} is referred to as the Swiss army knife of data import and export. The advantage is that it can import a wide variety of formats out of the box and that you can install more formats as you need. However, as it calls on other functions to run the import and write processes, it slows the import and export operations down.\nLet’s first use import() to import AMECO1.CSV.\n\ndf_rio1 &lt;- import(\n  here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO1.CSV\")\n)\n\nYou can verify, using e.g. str(df_rio1) that the import function imported 85 variables. The last 7 are all logical variables whose values include NA. Recall from read_csv() that we should skip the last columns 79-85 and that we can skip columns 2-6. {rio} calls fread to import csv files. Recall that fread() includes drop = to indicate the lines to drop. If we add this to the import() function, this function adds this option to the fread() function. In other words,\n\ndf_rio1 &lt;- import(\n  here::here(\"data\", \"raw\", \"ameco_csv\", \"AMECO1.CSV\"),\n  drop = c(2:6, 79:85)\n)\n\ndrops columns 2:6 and 79:85. We use this argument in the fread() section to show how you can drop colums using that function. If you inspect df_rio1, you’ll see that these columns are now dropped.\nAs a second example, let’s use import_list() to import AMECO1.XLSX and AMECO2.XLSX. First define the vector with these names and folders:\n\nfolder &lt;- here::here(\"data\", \"raw\", \"ameco_xlsx\")\npart_xlsx &lt;- list.files(path = folder,\n                        pattern = \"[A-Z]+[1-2].XLSX$\",\n                        full.names = TRUE)\npart_xlsx\n\n[1] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_xlsx/AMECO1.XLSX\"\n[2] \"C:/Users/u0063587/OneDrive - KU Leuven/Onderwijs/KULeuven Campus Brussel/Opleidingsonderdelen/Data and programming skills/Cursus/Syllabus/Data-and-programming-skills/data/raw/ameco_xlsx/AMECO2.XLSX\"\n\n\nUsing import_list() we can now import these files and immediately create one data frame. Recall that read_xlsx can not drop non-adjacent columns. To do so, we added a vector col_type in the read_xlsx function. Here, we add this argument to the import_list() function:\n\ndf_xlsx &lt;- import_list(\n  part_xlsx,\n  setclass = getOption(\"rio.import.class\", \"data.frame\"),\n  which,\n  rbind = TRUE,\n  rbind_label = \"_file\",\n  rbind_fill = TRUE,\n  col_types = c(\"guess\", rep(\"skip\", 5), rep(\"guess\", 72))\n)\n\nYou can verify that df_xlsx includes both Excel files. The data frame also include a column which shows the file names for each observations.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#data-on-the-web-webscraping",
    "href": "06_Importing_and_exporting.html#data-on-the-web-webscraping",
    "title": "6  Importing and exporting data",
    "section": "6.6 Data on the web: webscraping",
    "text": "6.6 Data on the web: webscraping\n\n6.6.0.1 Webscraping",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "06_Importing_and_exporting.html#expand-you-skills",
    "href": "06_Importing_and_exporting.html#expand-you-skills",
    "title": "6  Importing and exporting data",
    "section": "6.5 Expand you skills",
    "text": "6.5 Expand you skills\n\n6.5.1 Data on the web: webscraping\n{rvest} H. Wickham (2024) is a package that allows you to collect data by scraping websites. API are made available by those who own a website and, in doing so agree to share the data. Webscraping is different: you retrieve data without the explicit consent of the owner of the web page. To scrape data from the web, you need some knowledge of html of css selectors. If you need to use {rvest} to collect data, I refer to chapter 24 in Wickham Hadley, Cetinkaya-Rundel, and Grolemund (2023) and this post on the statsandr blog.\n\n\n6.5.2 Reading non-rectangular data\nIn the previous sections, we covered rectangular data: csv, spreadsheets, API all return a data frame with the same number of rows as there are columns. Non-rectangular data does not fit this description. As an example, let’s consider the following dataset\n\nExample: non rectangular dataset\n\n\ntwo\nplus\n4\nnot\n10\n\n\n24\nhours\n7\nopen\n\n\n\n7\n11\n\n\nFirm = TRUE\n\n\n\nThis dataset is not rectangular. The first column includes 3 rows and so does the second. The third and fourth column include 2 rows, as does the last. However with respect to the last, these rows are 1 and 3 while in columns 2 and 3, rows 1 and 2 have data, 3 doesn’t. Let’s create a csv file for the dataset. To do so, I’ll use writeLines() add the data between quotation marks with comma’s to separate the “columns”. I’ll write the file to the reports folder in the project weird.csv\n\nwriteLines(\n\"two, plus, 4, not, 10\n24, hours, 7, open \n7, 11, , , Firm = TRUE\",\nhere::here(\"reports\", \"weird.csv\")\n)\n\nLet’s use {readr}’s read_csv() function to read this file\n\ntest1 &lt;- readr::read_csv(here::here(\"reports\", \"weird.csv\"), col_names = FALSE )\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 3 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): X1, X2, X4, X5\ndbl (1): X3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you verity test you’ll see that R coerces all columns to character, except the third, which is read as double. The empty cell’s are NA. Using the package {metlr} (H. Wickham, Garmonsway, and Hester (2024)), which you can install\n\ninstall.packages(meltr)\n\nand low in memory\n\nlibrary(meltr)\n\nWarning: package 'meltr' was built under R version 4.4.3\n\n\nRegistered S3 methods overwritten by 'meltr':\n  method           from \n  print.date_names readr\n  print.locale     readr\n\n\n\nAttaching package: 'meltr'\n\n\nThe following objects are masked from 'package:readr':\n\n    AccumulateCallback, ChunkCallback, clipboard, DataFrameCallback,\n    datasource, date_names, date_names_lang, date_names_langs,\n    default_locale, fwf_cols, fwf_empty, fwf_positions, fwf_widths,\n    ListCallback, locale, melt_csv, melt_csv_chunked, melt_csv2,\n    melt_csv2_chunked, melt_delim, melt_delim_chunked, melt_fwf,\n    melt_table, melt_table2, melt_tsv, melt_tsv_chunked, problems,\n    show_progress, SideEffectChunkCallback, stop_for_problems,\n    tokenizer_csv, tokenizer_delim, tokenizer_fwf, tokenizer_line,\n    tokenizer_log, tokenizer_tsv, tokenizer_ws\n\n\nincludes a couple of function that you can use to read this in another way: melt_delim(), melt_csv(), melt_csv2(), melt_tsv() or melt_fwf(). You probably spot the similarities with {readr}’s functions designed to import rectangular data. They largely share the same arguments, but they return a different result. Let’s use melt_csv() to read weird.csv:\n\ntest2 &lt;- melt_csv(here::here(\"reports\", \"weird.csv\"))\n\ntest2 returns a tibble were all values are adding in a different row:\n\ntest2\n\n# A tibble: 14 × 4\n     row   col data_type value      \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n 1     1     1 character two        \n 2     1     2 character plus       \n 3     1     3 integer   4          \n 4     1     4 character not        \n 5     1     5 integer   10         \n 6     2     1 integer   24         \n 7     2     2 character hours      \n 8     2     3 integer   7          \n 9     2     4 character open       \n10     3     1 integer   7          \n11     3     2 integer   11         \n12     3     3 missing   &lt;NA&gt;       \n13     3     4 missing   &lt;NA&gt;       \n14     3     5 character Firm = TRUE\n\n\nThe columns of this tibble include: the row number, the column number, the data type and the individual value for each cell. For instance on the first 1 and third column, you’ll find the integer 5, the second row of the fifth column is NA, … . This tibble now allows you to identify values that you need. For instance, suppose you only need numeric values in this weird dataset:\n\ntest2[test2$data_type == \"double\" | test2$data_type == \"integer\", ]\n\n# A tibble: 6 × 4\n    row   col data_type value\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n1     1     3 integer   4    \n2     1     5 integer   10   \n3     2     1 integer   24   \n4     2     3 integer   7    \n5     3     1 integer   7    \n6     3     2 integer   11   \n\n\nNote that this would not have been possible with read_csv() output as the numeric values are spread across columns. You can now use this “weird” dataset to collect all other variables you need.\n\n\n6.5.3 Databases\nHere we didn’t cover databases. A database stores data in tables. You can think of tables as a data frame: the table stores variables for observations. Usually, these data bases are stored on a server. If they are large, downloading them in memory - in other words, creating a dataframe after importing them - is usually not possible. You connect to a database and use SQL (structured query language) to select variables, and observations.\nIn R, the {DBI} package allows you to connect to a database and retrieve data. In Chapter 8, well use {dplyr}, the package {dbplyr} allows you to use the familiar {dplyr} syntax to select and filter data in a database as it “translates’ {dplyr} verbs in SQL. To read more on R and databases, I refer you to chapter 21 of Wickham Hadley, Cetinkaya-Rundel, and Grolemund (2023).\n\n\n\n\n\n\nBarrett, Tyson, Matt Dowle, Arun Srinivasan, Jan Gorecki, Michael Chirico, Toby Hocking, Benjamin Schwendinger, and Ivan Krylov. 2025. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBryan, Jennifer. 2023. Googlesheets4: Access Google Sheets Using the Sheets API V4.\n\n\nD’Agostino McGowan, Lucy, and Jennifer Bryan. 2023. Googledrive: An Interface to Google Drive.\n\n\nLahti, Leo, Janne Huovari, Markus Kainu, Przemyslaw Biecek, Diego Hernangomez, Daniel Antal, and Pyry Kantanen. 2023. “Eurostat: Tools for Eurostat Open Data.” Computer software. https://github.com/rOpenGov/eurostat.\n\n\nOoms, Jeroen. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between JSON Data and r Objects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\nPersson, Eric. 2023. Ecb: Programmatic Access to the European Central Bank’s Statistical Data Warehouse. https://CRAN.R-project.org/package=ecb.\n\n\nWickham, Hadley. 2023. Httr: Tools for Working with URLs and HTTP.\n\n\n———. 2024. Rvest: Easily Harvest (Scrape) Web Pages. https://rvest.tidyverse.org/.\n\n\n———. 2025. Httr2: Perform HTTP Requests and Process the Responses. https://httr2.r-lib.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel Files.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher. 2024. Usethis: Automate Package and Project Setup. https://usethis.r-lib.org.\n\n\nWickham, Hadley, Cetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (2nd Edition). Sebastopol, CA: O’Reilly Media.\n\n\nWickham, Hadley, Duncan Garmonsway, and Jim Hester. 2024. Meltr: Read Non-Rectangular Text Data. https://r-lib.github.io/meltr/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2025. Xml2: Parse XML. https://xml2.r-lib.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://haven.tidyverse.org.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Importing and exporting data</span>"
    ]
  },
  {
    "objectID": "07_Tidying_data.html",
    "href": "07_Tidying_data.html",
    "title": "7  Tidying data",
    "section": "",
    "text": "7.1 Introduction\nIn Chapter 6, we saw various ways to import data in R. {readr}, {readxl}, {googlesheets4}, {haven}, {fread} or {rio} allows you to import datasets in various formats. However, these imported data frames, data tables or tibbles are usually not in a tidy format. Recall from Chapter 5 that data sets are tidy is they meet three conditions (Wickham (2014)):\nA fourth condition is sometimes added: each type of observational unit form a table (or a dataset).\nGenerating a tidy dataset, that is a dataset that meets all these conditions is the second step in a typical datascience project (see Figure 1).\nChapter 5 lists 6 main violations of one or more of these conditions: column headers are values, not variable names; multiple variables are stored on one column; variables are stored in both rows and columns; multiple observational units are stored in the same table; a single observational unit is stored in multiple tables and a single cell includes more than one variable. In some cases, there are other issues that we need to address. For instance, in an excel sheet, columns sometimes include empty cells. For instance, in Table 7.1, the column “year” includes empty cells below each year.\nIn addition, there might be missing values that you could eliminate because they are not really missing. As an example, consider the following dataset which includes the number of hours each student has schedules lectures per day (Table 7.2).\nIf you read this table in R, the empty cells will show as missing values. However, here, these values are not missing: they refer to days with no scheduled lectures. In other words, the values are not missing in the sense that we know that there should be a value, but there is no record of it (such as a respondent in a survey who refused to disclose his or her income). Here we know that this is not the case: if there are missing values in this dataset, they refer to days where students did not have a class scheduled.\nTo tidy the data we will use {tidyr} (Wickham, Vaughan, and Girlich (2024) with cheatsheet) and {dplyr} (Wickham et al. (2023) with cheatsheet). In addition we’ll read in data using {readxl}. You can load these packages in memory:\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readxl)\nThe dataset, tidy_data.xlsx is available on your data &gt;  raw directory. This workbook includes a number of sheets. The name of the sheets refers to the tables in Chapter 5.\nexcel_sheets(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"))\n\n [1] \"example51\"    \"example51-1\"  \"example54\"    \"example56\"    \"example58\"   \n [6] \"example510\"   \"example514\"   \"example515\"   \"example517\"   \"example519-1\"\n[11] \"example519-2\" \"example522\"   \"other1\"\nThe sheet “example51” includes that data in Table 5.1, “example54” the table in Table 5.4, … . In case a dataset includes a variation of the dataset in e.g. Table 5.1, it is shown with a “-1”. For instance, “example51-1” includes a variation of “example51”. The next sections follow the same order as these on Chapter 5. To see why data is not tidy, I refer to those sections. Here, we will focus on the process of tidying.\nTo show the tables, I’ll use knitr::kable(table). If you don’t have knitr installed, you can install it\ninstall.packages(\"knitr\")\nTo limit the output of some table knitr::kable(table[x:y, ]) is used to show only the rows x to y.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidying data</span>"
    ]
  },
  {
    "objectID": "07_Tidying_data.html#tidy-data",
    "href": "07_Tidying_data.html#tidy-data",
    "title": "7  Tidying data",
    "section": "7.2 Tidy data",
    "text": "7.2 Tidy data\n\n7.2.1 Column headers are values, not variable names\nThe data in Table 5.1 are includes in the sheet “example51” in the “tidy_data.xlsx” workbook. We’ll first read that sheet and assign it the tibble example51:\n\nexample51 &lt;- read_xlsx(path = here::here(\"data\", \"raw\", \"tidy_data.xlsx\"), \n                       sheet = \"example51\"\n                       )\n\nNew names:\n• `` -&gt; `...1`\n\n\nAs the sheet doesn’t include a name for the first column, R added the name ...1. Let’s first change this name to id. To do so, we’ll use {dplyr}’s rename() function. This function takes the following arguments\n\nrename(.data, ...)\n\nwere .data refers to the dataset and ... includes a new name for the columns using new_name = old_name where you can use tidy-select() arguments such as starts_with(), ends_with(), contains(), matches(), num_range(), lastcol(n) with default n = 0 to select the last nth column from the last column (by default, select the last column) or everything() to select every column. We will cover this &lt;tidy-select&gt; syntax more in depth in Chapter 8 where we will apply this syntax to select column in a dataset. In this case, we use the new name = old_name: id = ...1. In addition, we’ll change upper case T in lower case t. We’ll use the |&gt;operator. Recall that this operator uses what is on its left hand side as the first argument of the function on its right hand side. Here dataset |&gt; rename(new_name = old_name is equivalent to rename(dataset, new_name = old_name).\n\nexample51 &lt;- example51 |&gt; rename(id = ...1, treatment_A = Treatment_A, treatment_B = Treatment_B)\n\nIn this dataset, treatment_A and treatment_B include values: A and B of the variable Treatment. To see this, suppose that you would like to show the results in a bar chart where the treatment is shown on the horizontal axis and the values on the vertical axis. In a graph, you show values of a variable. In other words, you show A and B was values for the variable treatement on the horizontal axis and the values 16, 1, 2, 11 and 1 as the effects of that treatment on the vertical axis. The dataset in example51 does not allow you to do so: your two values for treatment are spread across 2 columns. In other words, your column names include values. In addition, the dataset does not “stand on its own”. The variables treatment_A nor treatment_B disclose the meaning of the values in the cells 16, 1, 2, 11 or 1. Without any additional information, there is no way to know if these refer to e.g. the effect of a treatment, its cost, the number of days enrolled in a treatment, … .\nThe data in example51 are in wide format we need to change this from wide into long format. In other words, we need a column treatment whose values are A and B. To do so, we can use {tidyr}’s pivot_longer() function\n\npivot_longer(\n  data,\n  cols,\n  ...,\n  cols_vary = \"fastest\",\n  names_to = \"name\",\n  names_prefix = NULL,\n  names_sep = NULL,\n  names_pattern = NULL,\n  names_ptypes = NULL,\n  names_transform = NULL,\n  names_repair = \"check_unique\",\n  values_to = \"value\",\n  values_drop_na = FALSE,\n  values_ptypes = NULL,\n  values_transform = NULL\n)\n\nLets review some of the main arguments of this function. The dataset you want to pivot longer is the first argument, data. The second, cols refers to the columns to pivot. cols_vary = \"fastest\" keeps the individual rows from the columns in cols close together in the output. If you have one or more columns that don’t need to change to longer, this often orders the new dataset in an intuitive way. Changing this to “slowest” is a good idea if all columns of the dataset are includes in cols. In that case, the function keeps the column in close close together.\nnames_to specifies the names of the new columns R creates. By default, pivot_longer() adds one column, name. In this column, R will include as values the names of the columns in cols. To see what R does with example51, let’s use pivot_longer() with the default value for names_to = names to pivot the columns treatment_A and treatment_B. To refer to these two columns, there are a couple of alternatives, e.g. cols = c(\"treatment_A\", \"treatment_B\"): including a vector with column names to pivot longer; cols = !id: pivot all columns except id longer, starts_with(\"treat\") to pivot all columns whose name starts with “treat” longer, cols = contains(\"ment\") to pivot all columns whose name includes “ment” longer, … . Here, we’ll choose starts_with()\n\nex_511 &lt;- example51 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"))\n\nThe output shows a dataset in long format, where the names of the columns are now values for the variable “name”.\n\n\n\n\nTable 7.3: Output: pivot_longer() using example51 with default value for names_to = ‘name’\n\n\n\n\n\n\nid\nname\nvalue\n\n\n\n\nJohn Smith\ntreatment_A\nNA\n\n\nJohn Smith\ntreatment_B\n2\n\n\nJane Doe\ntreatment_A\n16\n\n\nJane Doe\ntreatment_B\n11\n\n\nMary Johnson\ntreatment_A\n1\n\n\nMary Johnson\ntreatment_B\n1\n\n\n\n\n\n\n\n\nChanging “name” into e.g. “treatment” will create a long format dataset with a new variable “treatment”. In Table 7.3 the values in the cells in example511 are assigned to a variable value. This is the default for values_to = \"value\". You can change this and add your own name if you set e.g. values_to = \"result\". Recall that on its own, the dataset in example51 does not disclose its content. Here, you can add a variable name that offers some information on “what” the values in the cells refer to.\n\nex_511 &lt;- example51 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"), names_to = \"treatment\", values_to = \"result\")\n\n\n\n\n\nTable 7.4: Output: pivot_longer() using example51 with default value for names_to = ‘treatment’ and values_to = ‘result’\n\n\n\n\n\n\nid\ntreatment\nresult\n\n\n\n\nJohn Smith\ntreatment_A\nNA\n\n\nJohn Smith\ntreatment_B\n2\n\n\nJane Doe\ntreatment_A\n16\n\n\nJane Doe\ntreatment_B\n11\n\n\nMary Johnson\ntreatment_A\n1\n\n\nMary Johnson\ntreatment_B\n1\n\n\n\n\n\n\n\n\nIn Table 7.3 and Table 7.4 pivot_longer() keeps the individual rows together: the values for John Smith are shown in rows 1 and 2, Jane Doe’s in row 3 and 4 and Mary Johnson’s in row 5 and 6. Doing so is in line with the cols_vary = \"fastest\" argument. In the next “try this out”, you’ll be asked to change this argument.\n\n\n\n\n\n\nTry this out\n\n\n\n\n\nUse pivot_longer() and change the default cols_vary in slowest. Change the way to identify the columns treatment_A and treatment_B in the cols = argument and use ends_with(). Assign this long dataset to ex_tto1. Before you run the code, try to predict what the dataset will look like.\n\n\nCode\nex_tto1 &lt;- example51 |&gt; \n  pivot_longer(cols = ends_with(c(\"A\", \"B\")),\n               cols_vary = \"slowest\",\n               names_to = \"treatment\",\n               values_to = \"result\")\n\n\nBefore you look at the table: try to predict what the table will look like and how this will be different from Table 7.4\n\n\n\n\nTable 7.5: Output: pivot_longer() with cols_vary = ‘slowest’\n\n\n\n\n\n\nid\ntreatment\nresult\n\n\n\n\nJohn Smith\ntreatment_A\nNA\n\n\nJane Doe\ntreatment_A\n16\n\n\nMary Johnson\ntreatment_A\n1\n\n\nJohn Smith\ntreatment_B\n2\n\n\nJane Doe\ntreatment_B\n11\n\n\nMary Johnson\ntreatment_B\n1\n\n\n\n\n\n\n\n\n\n\n\nThere are two alternatives for name_to: NULL and adding more than one name. Let’s look at the first option. The second option will be covered in the next section.\n\nex_511 &lt;- example51 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"), \n               names_to = NULL, \n               values_to = \"result\")\n\npivot_longer() deletes the values in the column names from the table and pivots only the values in the cells. In other words, R adds the values in the second column for each “id” in a new row but doesn’t add a new column.\n\n\n\n\nTable 7.6: Output: pivot_longer() using example51 with default value for names_to = ‘NULL’\n\n\n\n\n\n\nid\nresult\n\n\n\n\nJohn Smith\nNA\n\n\nJohn Smith\n2\n\n\nJane Doe\n16\n\n\nJane Doe\n11\n\n\nMary Johnson\n1\n\n\nMary Johnson\n1\n\n\n\n\n\n\n\n\nIn Table 7.4, the column “treatment” includes the values but they are included as treatment_A, treatment_B. Using names_prefix = we can remove the string treatment_ from these values. This argument removes an expression which is included at the start (prefix) of the column header. You can identify that expression using a regular expression. As the expression at the start of the variable name always includes “treatment_”, we can include this string or e.g. “[a-z]+_:\n\nex_511 &lt;- example51 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"),\n               names_prefix = \"[a-z]+_\",\n               names_to = \"treatment\", \n               values_to = \"result\")\n\nThe dataset is now tidy and looks like the one in Table 5.5:\n\n\n\n\nTable 7.7: Tidy dataset for example51\n\n\n\n\n\n\nid\ntreatment\nresult\n\n\n\n\nJohn Smith\nA\nNA\n\n\nJohn Smith\nB\n2\n\n\nJane Doe\nA\n16\n\n\nJane Doe\nB\n11\n\n\nMary Johnson\nA\n1\n\n\nMary Johnson\nB\n1\n\n\n\n\n\n\n\n\nHere, Table 7.7 is a tidy dataset. Every column includes one variable: the first the “id” of the person who took the treatment; column “treatement” shows the treatment and the third column “result” shows the result of the treatment. Suppose that you want to show this table in a graph: for the horizontal axis, you would be able to refer to the variable, not the variable including its values. In addition, assuming that the name “result” has a specific meaning for a “treatment”, you can now see that the table shows this result.\nThe argument values_drop_na = FALSE keeps missing values in the dataset. If this is set to TRUE, pivot_longer() will drop all rows that include “NA”. In general, this should be avoided, unless you know that the missing values are not “missing” but include data that are in effect not existing. For instance, in a dataset where each row include a week and each column shows a day and cells include the number of hours of lectures for a class group, empty cells actually refer to a day of the week without class. However, not that even in that case, it might be safer to set these values to 0.\nHow can you deal with a table as in Table 5.4? This table shows a table as you’ll see them often in a spreadsheet. It is a table that is meant to be used in a report and shows e.g. the treatment using a merged cell on top of a row which includes its values. You can find this table in sheet “example54” in the spreadsheet “tidy_data.xlsx”. First, you can remove that row from the data you import. Using read_xlsx() you can do so by adding skip = 1. Doing so, you’ll keep the values A and B as column names. Using pivot_longer(), you can now create a tidy dataset:\n\nexample54 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"), \n                       sheet = \"example54\", \n                       skip = 1)\n\nex_541 &lt;- pivot_longer(example54, \n                       cols = !id,\n                       names_to = \"treatment\",\n                       values_to = \"result\")\n\nYou can verify that this dataset is tidy:\n\n\n\n\nTable 7.8: Tidy dataset for example54\n\n\n\n\n\n\nid\ntreatment\nresult\n\n\n\n\nJohn Smith\nA\nNA\n\n\nJohn Smith\nB\n2\n\n\nJane Doe\nA\n16\n\n\nJane Doe\nB\n11\n\n\nMary Johnson\nA\n1\n\n\nMary Johnson\nB\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nThe workbook “tidy_data.xlsx” includes the tables for Table 5.6 and Table 5.8. The first, “example56” includes (imaginary) data in study efficiency. The second, “example58” includes imaginary data on market capitalization. The sheets for both are in “tidy_data.xlsx”. There is a tidy table: Table 5.7 for the first and Table 5.9. Read these sheets and generate a tidy dataset. Call the first ex_56 and the second ex_58. Here, I will add the {knitr} output to produce the tables.\nLet’s start with Table 5.6:\nImport the sheet “example56” and assign it to ex_56\n\n\nCode\nex_56 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"), \n                   sheet = \"example56\")\n\n\nPivot the dataset. Select the columns to pivot and create a variable study_eff for the values in the columns and num_students for the student numbers.\n\n\nCode\nex_56 &lt;- pivot_longer(ex_56, \n                      cols = !degree, \n                      names_to = \"study_eff\",\n                      values_to = \"num_students\")\n\n\nIs your table tidy? Compare your result with Table 5.7\n\n\nCode\nknitr::kable(ex_56)\n\n\n\n\n\ndegree\nstudy_eff\nnum_students\n\n\n\n\nBusiness administrations\n&lt;25%\n25\n\n\nBusiness administrations\n[25-50%[\n75\n\n\nBusiness administrations\n[25-75%[\n102\n\n\nBusiness administrations\n[75-99%[\n178\n\n\nBusiness administrations\n1\n75\n\n\nLaw\n&lt;25%\n60\n\n\nLaw\n[25-50%[\n84\n\n\nLaw\n[25-75%[\n189\n\n\nLaw\n[75-99%[\n175\n\n\nLaw\n1\n56\n\n\nTheology\n&lt;25%\n5\n\n\nTheology\n[25-50%[\n26\n\n\nTheology\n[25-75%[\n53\n\n\nTheology\n[75-99%[\n57\n\n\nTheology\n1\n54\n\n\nEngineering\n&lt;25%\n75\n\n\nEngineering\n[25-50%[\n79\n\n\nEngineering\n[25-75%[\n68\n\n\nEngineering\n[75-99%[\n185\n\n\nEngineering\n1\n152\n\n\nHistory\n&lt;25%\n22\n\n\nHistory\n[25-50%[\n23\n\n\nHistory\n[25-75%[\n18\n\n\nHistory\n[75-99%[\n89\n\n\nHistory\n1\n12\n\n\nLanguages\n&lt;25%\n17\n\n\nLanguages\n[25-50%[\n14\n\n\nLanguages\n[25-75%[\n18\n\n\nLanguages\n[75-99%[\n69\n\n\nLanguages\n1\n1\n\n\n\n\n\nFor the second, Table 5.8 in “example58”, write the full code in one line using the |&gt; operator: read and pivot. Add the values in the cells to market_cap and the names of the column to years.\n\n\nCode\nex_58 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"), sheet = \"example58\") |&gt;\n  pivot_longer(cols = !firm, names_to = \"years\", values_to = \"market_cap\")\n\n\nHow does you result compare with Table 5.9?\n\n\nCode\nknitr::kable(ex_58)\n\n\n\n\n\nfirm\nyears\nmarket_cap\n\n\n\n\nA\n2020\n1236\n\n\nA\n2021\n1366\n\n\nA\n2022\n1455\n\n\nA\n2023\n1554\n\n\nA\n2024\n1689\n\n\nB\n2020\n201\n\n\nB\n2021\n204\n\n\nB\n2022\n215\n\n\nB\n2023\n325\n\n\nB\n2024\n410\n\n\nC\n2020\n12598\n\n\nC\n2021\n12698\n\n\nC\n2022\n12359\n\n\nC\n2023\n11985\n\n\nC\n2024\n11453\n\n\n\n\n\n\n\n\n\n\n7.2.2 Multiple variables are stored in one column\nFor now, the columns included one value. In Table 5.10, the column names include two variable: the nationality of the student as well as the age. The data for this table are in “example510” in the workbook “tidy_data.xlsx”. With multiple values in one column, we have to extract all these values and add each to their own column. To do so, we need to add more than one name in names_to. In that case, R also needs to know how to separate the names of the columns. To do so, R needs additional information. You can include this extra information in the names_sep = NULL or names_pattern = NULL. The former, names_sep = can either take a numeric vector with positions or a single string.\nLet’s first read the sheets:\n\nexample510 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),\n                         sheet = \"example510\")\n\nWe need to pivot all the columns except the first, Using cols = !program we can now pivot all columns longer. The column names include data on the nationality and age. In the columns, an underscore, _, separates these values. In names_to we now include 2 variables: nationality and age. The names_sep = \"_\" will be used by pivot_longer() to separate the values. In other words, the column name “eea_18-19” will be split in “eea” and “18-19”. The first value will be assigned to nationality and the second to the variable age. The data in the cells are student numbers and we can assign these values to a variable num_stud. The code for the operation is:\n\nex_510 &lt;- example510 |&gt; \n  pivot_longer(cols = !program,\n               names_to = c(\"nationality\", \"age\"), \n               names_sep = \"_\",\n               values_to = \"num_stud\")\n\nThe first 6 rows of the dataset look like:\n\n\n\n\nTable 7.9: Output: pivot_longer() using example51 with names_sep = ’_’\n\n\n\n\n\n\nprogram\nnationality\nage\nnum_stud\n\n\n\n\nBusiness administration\neea\n18-19\n189\n\n\nBusiness administration\neea\n20-21\n71\n\n\nBusiness administration\nnoneea\n18-19\n258\n\n\nBusiness administration\nnoneea\n20-21\n236\n\n\nLaw\neea\n18-19\n256\n\n\nLaw\neea\n20-21\n114\n\n\nLaw\nnoneea\n18-19\n78\n\n\nLaw\nnoneea\n20-21\n102\n\n\n\n\n\n\n\n\nYou can verify that this table is the same as Table 5.13. pivot_longer() kept the values per program close. This is due to the default value for cols_vary = \"fastest\" argument. If you would change that into “slowest”, R would keep the columns together. In this case, R would keep first include the values per nationality, then per age and add the program:\n\nex_5101 &lt;- example510 |&gt; \n  pivot_longer(cols = !program,\n               cols_vary = \"slowest\", \n               names_to = c(\"nationality\", \"age\"), \n               names_sep = \"_\",\n               values_to = \"num_stud\")\n\nknitr::kable(ex_5101)\n\n\n\n\nprogram\nnationality\nage\nnum_stud\n\n\n\n\nBusiness administration\neea\n18-19\n189\n\n\nLaw\neea\n18-19\n256\n\n\nBusiness administration\neea\n20-21\n71\n\n\nLaw\neea\n20-21\n114\n\n\nBusiness administration\nnoneea\n18-19\n258\n\n\nLaw\nnoneea\n18-19\n78\n\n\nBusiness administration\nnoneea\n20-21\n236\n\n\nLaw\nnoneea\n20-21\n102\n\n\n\n\n\nAs an alternative to names_sep you can use names_pattern =. Here, you can add a regular expression. In that expression and using (.) or (.*) you can indicate which parts are values for the variables in names_to. Using (.) you extract one symbol. With (.*) you allow for more than one. Here, we would like to extract only nationality and the age. In other words, we would like to extract the “eea” and “noneea” and the “18-19” and “20-21”. Using a regular expression: “(.*)_(.*)” extracts the value before and after the underscore:\n\nex_5102 &lt;- example510 |&gt; \n  pivot_longer(cols = !program, \n               names_to = c(\"nationality\", \"age\"), \n               names_pattern = \"(.*)_(.*)\",\n               values_to = \"num_stud\")\n\nThe dataset now looks like Table 5.13:\n\n\n\n\nTable 7.10: Output: pivot_longer() using example51 with names_sep = ’_’\n\n\n\n\n\n\nprogram\nnationality\nage\nnum_stud\n\n\n\n\nBusiness administration\neea\n18-19\n189\n\n\nBusiness administration\neea\n20-21\n71\n\n\nBusiness administration\nnoneea\n18-19\n258\n\n\nBusiness administration\nnoneea\n20-21\n236\n\n\nLaw\neea\n18-19\n256\n\n\nLaw\neea\n20-21\n114\n\n\nLaw\nnoneea\n18-19\n78\n\n\nLaw\nnoneea\n20-21\n102\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\n\n“tidy_data.xlsx” includes a variation on Table 5.1. Sheet “example51-1” is similar to “example51” but includes 4 columns:\n\nExample: variation on @tbl-treat1\n\n\n\n\n\n\n\n\n\nid\ntreatment_1_A\ntreatment_1_B\ntreatment_2_A\ntreatment_2_B\n\n\n\n\nJohn Smith\n\n2\n12\n10\n\n\nJane Doe\n16\n11\n15\n18\n\n\nMary Johnson\n1\n1\n19\n12\n\n\n\nLet’s interpret that digit as the stage of the treatment and the letter as the treatment plan. In other words, the column headers include 2 values: the stage as well as the plan. A tidy dataset includes a column for both. Assing the values in the cells to the variable “result”.\nFirst import the sheets and assign the sheets to the tibble example51_1:\n\n\nCode\nexample51_1 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),\n                         sheet = \"example51-1\")\n\n\nYou can now pivot the columns. Identify the columns to pivot using starts_with. Can you use names_sep = \"_\" to pivot to do so?\n\n\nCode\nex_51_1 &lt;- example51_1 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"),\n               names_to = c(\"treatment\", \"stage\", \"plan\"), \n               names_sep = \"_\",\n               values_to = \"result\")\n\n# You can not use names_sep = \"_\" to extract two columns. The names include 2 underscores. In other words, you'll need\n# three columns to pivot to. One column will always include \"treatment\", i.e. the name of a variable. \n\n\nNow you names_pattern = to extract only the “1”, “2” for stage and “A” and “B” for plan.\n\nex_51_1 &lt;- example51_1 |&gt; \n  pivot_longer(cols = starts_with(\"treat\"),\n               names_to = c(\"stage\", \"plan\"), \n               names_pattern = \"[a-z]+_(.)_(.)\",\n               values_to = \"result\")\nknitr::kable(ex_51_1)\n\n\n\n\nid\nstage\nplan\nresult\n\n\n\n\nJohn Smith\n1\nA\nNA\n\n\nJohn Smith\n1\nB\n2\n\n\nJohn Smith\n2\nA\n12\n\n\nJohn Smith\n2\nB\n10\n\n\nJane Doe\n1\nA\n16\n\n\nJane Doe\n1\nB\n11\n\n\nJane Doe\n2\nA\n15\n\n\nJane Doe\n2\nB\n18\n\n\nMary Johnson\n1\nA\n1\n\n\nMary Johnson\n1\nB\n1\n\n\nMary Johnson\n2\nA\n19\n\n\nMary Johnson\n2\nB\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry this out\n\n\n\n\n\n{tidyr} includes a dataset who including new tuberculosis cases across the world. The dataset includes the country name, country ISO2 and ISO0 code and the year and 54 observations for new tuberculosis cases. The cases are shown in variables such as new_sp_m014. The variable name includes “new” to indicate the data concern new cases, “sp” which is on of the four methods to diagnose TBC. The other include “rel”, “sn” and “ep”. The m refers to the gender, m for male and f for female. The last group of digits includes the age: 014, 1524, 2534, … 5564 and 65 (65 and older). All variables are shown in that way, except for those for the method “rel”: for those, there is no underscore between new and rel: newrel_f5565 and not new_rel_f5565. To illustrate new_ep_f1524 refers to new cases, discovered with the EP method, for girls aged 15-24. In other words, the column names includes information on the status “old”/“new, the method, (EP, SP, SN or REL) and the age of the person who was diagnosed. The format is consistent, except for the method rel here the the colums include rewrel and not rew_rel. All cases are”new”. So, here there is no new information.\nTo Import the dataset, you can refer to the {tidyr} package and the dataset.\n\ndata_who &lt;- tidyr::who\n\nBefore you proceed, first visualize the data.\nUsing pivot_longer you will extract data on the method used to detect tuberculosis, the gender and age of all new cases. In other words, you’ll create three columns to store these values: “method”, “gender” and “age”. The values are the number of cases. So you can store these in the column “cases”. First think about the column you need to pivot. Which one do need to change from wide to long and which ones don’t? You can identify those who do (or those who don”t) using a short expression, e.g. using starts_with(), contains() … . Now think about the regular expression you’ll use to extract the information from the column names. In general, they look as “new” then often an underscore but not always (i.e. the underscore after “new” is optional, then the method (which is a value you have to extract, can take 2 of 3 letters), an underscore, the gender (a value you have to extract, either m or f) and age (a value you need to extract with multiple digits).\n\n\nCode\ndata_who &lt;- data_who |&gt; pivot_longer(cols = starts_with(\"new\"),\n                                     names_to = c(\"method\", \"gender\",  \"age\"), \n                                     names_pattern = \"new_?(.+)_(.)(.+)\", \n                                     values_to = \"cases\"\n                                     )\n\n\n\n\n\n\n\n7.2.3 Variables are stored in both rows and columns\n\n7.2.3.1 Widening the dataset\nIn Table 5.14 the variables appear in both rows and columns. Sheet “example514” includes the data for this example. The table includes data on delays - minimum and maximum - for an imaginary train service between Brussels and Antwerpen and Brussels and Leuven. Here, the data in the column delay: min and max are variables, not values. In other words, the rows which include min have to become a column “min” and the rows which include the value max have to become a variable “max”. Here, we are actually making the dataset wider. In the previous examples, we make the dataset longer as data dat was spread across columns was added in rows. Here, we are going to use data in rows to add to columns. To so so, we can use pivot_wider()\n\npivot_wider(\n  data,\n  ...,\n  id_cols = NULL,\n  id_expand = FALSE,\n  names_from = name,\n  names_prefix = \"\",\n  names_sep = \"_\",\n  names_glue = NULL,\n  names_sort = FALSE,\n  names_vary = \"fastest\",\n  names_expand = FALSE,\n  names_repair = \"check_unique\",\n  values_from = value,\n  values_fill = NULL,\n  values_fn = NULL,\n  unused_fn = NULL\n)\n\nHer you can recognize a lot of the arguments form the pivot_longer() function. However, note that you have to interpret them sometimes a bit different. For instance, names_prefix = in pivot_longer() refers to a prefix to remove, here, it refers to a prefix to add to create a new column name. Using pivot_wider() and in addition to the data, you include in names_from the column name or names where the new columns are stored and in values_from the columns where the values are. To see how this works, let’s first import the sheet “example514”\n\nexample514 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),\n                         sheet = \"example514\")\n\nThe data includes 4 columns but the column “delay” includes variables in its rows, not values. The column time shows the associated minimum delay. In other words, the names to use for the new variables are included in the column “delay” and the values in the column “time”. Using these in pivot_wider():\n\nex_514 &lt;- pivot_wider(example514, \n                      names_from = \"delay\",\n                      values_from = \"time\")\n\nThe datset is not in a tidy format:\n\n\n\n\nTable 7.11: Tidy dataset for example54\n\n\n\n\n\n\nroute\nyear\nweek\nmin\nmax\n\n\n\n\nbxl-ant\n2025\n2\n10\n89\n\n\nbxl-leu\n2025\n2\n1\n193\n\n\n\n\n\n\n\n\nTo illustrate the use of pivot_wider() we will try to widen some of the tables we change from wide to long. Let’s start with “example56”, where the data includes the study efficiency of students across programs. Let’s first create the long version\n\nex_56 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example56\") |&gt; \n  pivot_longer(cols = !degree, names_to = \"study_eff\",values_to = \"num_students\")\nknitr::kable(ex_56[1:6, ])\n\n\n\n\ndegree\nstudy_eff\nnum_students\n\n\n\n\nBusiness administrations\n&lt;25%\n25\n\n\nBusiness administrations\n[25-50%[\n75\n\n\nBusiness administrations\n[25-75%[\n102\n\n\nBusiness administrations\n[75-99%[\n178\n\n\nBusiness administrations\n1\n75\n\n\nLaw\n&lt;25%\n60\n\n\n\n\n\nThe values that we need in the column names are in the variable study_eff and the values are in the column num_students. Using this to widen this dataset:\n\nex_56w &lt;- pivot_wider(ex_56, \n                      names_from = \"study_eff\",\n                      values_from = \"num_students\")\n\nknitr::kable(ex_56w[1:6, ])\n\n\n\n\ndegree\n&lt;25%\n[25-50%[\n[25-75%[\n[75-99%[\n1\n\n\n\n\nBusiness administrations\n25\n75\n102\n178\n75\n\n\nLaw\n60\n84\n189\n175\n56\n\n\nTheology\n5\n26\n53\n57\n54\n\n\nEngineering\n75\n79\n68\n185\n152\n\n\nHistory\n22\n23\n18\n89\n12\n\n\nLanguages\n17\n14\n18\n69\n1\n\n\n\n\n\nIn “example510” we had multiple values stored in the column names: nationality and age. Let’s first recreate the long format of this dataset:\n\nex_510 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example510\") |&gt;\n  pivot_longer(cols = !program, names_to = c(\"nationality\", \"age\"), names_sep = \"_\", values_to = \"num_stud\")\nknitr::kable(ex_510[1:6, ])\n\n\n\n\nprogram\nnationality\nage\nnum_stud\n\n\n\n\nBusiness administration\neea\n18-19\n189\n\n\nBusiness administration\neea\n20-21\n71\n\n\nBusiness administration\nnoneea\n18-19\n258\n\n\nBusiness administration\nnoneea\n20-21\n236\n\n\nLaw\neea\n18-19\n256\n\n\nLaw\neea\n20-21\n114\n\n\n\n\n\nTo widen this dataset, we need to add the values in “nationalty” to those in “age” to create column names. The values are in “num_stud”. pivot_wider() offers multiple options to do so. First, you can add both columns in names_from = and allow pivot_wider() to add the values in “nationality” and “age”:\n\nex_510w1 &lt;- pivot_wider(ex_510, \n                        names_from = c(\"nationality\", \"age\"), \n                        values_from = \"num_stud\")\nknitr::kable(ex_510w1)\n\n\n\n\n\n\n\n\n\n\n\nprogram\neea_18-19\neea_20-21\nnoneea_18-19\nnoneea_20-21\n\n\n\n\nBusiness administration\n189\n71\n258\n236\n\n\nLaw\n256\n114\n78\n102\n\n\n\n\n\nHere, R uses the default names_sep = \"_\" to add the values in “nationality” to those in “age”. If you would like to use another separator, you can add it as an argument to pivot_wider(). For instance, to add “in age group” as a separator:\n\nex_510w2 &lt;- pivot_wider(ex_510,\n                        names_from = c(\"nationality\", \"age\"), \n                        names_sep = \" in age group \",\n                        values_from = \"num_stud\")\nknitr::kable(ex_510w2)\n\n\n\n\n\n\n\n\n\n\n\nprogram\neea in age group 18-19\neea in age group 20-21\nnoneea in age group 18-19\nnoneea in age group 20-21\n\n\n\n\nBusiness administration\n189\n71\n258\n236\n\n\nLaw\n256\n114\n78\n102\n\n\n\n\n\nUsing names_prefix = adds a prefix to the names of the columns. For instance, suppose that the column “week” refers to the week of the year and you pivot that column wider, adding names_prefix = \"week_\" would generate column names such as week_1, week_2, … . In additon, you can use names_glue = NULL to create names using the values in the column to pivot. In the next example, pivot_wider() will create the column names using the values in nationality and age.\n\nex_510w3 &lt;- pivot_wider(ex_510,\n                        names_from = c(\"nationality\", \"age\"), \n                        names_glue = \"nationality {nationality} and {age}\",\n                        values_from = \"num_stud\")\nknitr::kable(ex_510w3)\n\n\n\n\n\n\n\n\n\n\n\nprogram\nnationality eea and 18-19\nnationality eea and 20-21\nnationality noneea and 18-19\nnationality noneea and 20-21\n\n\n\n\nBusiness administration\n189\n71\n258\n236\n\n\nLaw\n256\n114\n78\n102\n\n\n\n\n\nNote that you can “widen” one variable. For instance to widen only “age” while you keep “nationality” long, you only include “age” in the names_from argument:\n\nex_510w4 &lt;- pivot_wider(ex_510, \n                        names_from = c(\"age\"), \n                        values_from = \"num_stud\")\nknitr::kable(ex_510w4)\n\n\n\n\nprogram\nnationality\n18-19\n20-21\n\n\n\n\nBusiness administration\neea\n189\n71\n\n\nBusiness administration\nnoneea\n258\n236\n\n\nLaw\neea\n256\n114\n\n\nLaw\nnoneea\n78\n102\n\n\n\n\n\n\n\n7.2.3.2 Longer and wider\nConsider example “example515” which was also shown in Table 5.15: the data include values in the columns and there are variables in the rows. The first issue arises because the days of the week are actually values of a variable “week”. The second because the minimum and maximum delays are variables that are stored in rows. Let’s first read the data:\n\nex_515 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example515\")\n\nHere, we need to first use pivot_longer() and create a column day. The values can be stored in e.g. minutues. This solves the first part of the problem: the values in the column names. To show this intermediate result, we”ll save it ex_515l. The table shows the first 6 lines.\n\nex_515l &lt;- pivot_longer(ex_515, cols = !c(\"route\", \"year\", \"week\", \"delay\"),\n                        names_to = \"day\", \n                        values_to = \"minutes\")\nknitr::kable(ex_515l[1:6, ])\n\n\n\n\nroute\nyear\nweek\ndelay\nday\nminutes\n\n\n\n\nbxl-ant\n2025\n2\nmin\nmon\n10\n\n\nbxl-ant\n2025\n2\nmin\ntue\n8\n\n\nbxl-ant\n2025\n2\nmin\nwed\n5\n\n\nbxl-ant\n2025\n2\nmin\nthu\n0\n\n\nbxl-ant\n2025\n2\nmin\nfri\n1\n\n\nbxl-ant\n2025\n2\nmin\nsat\n0\n\n\n\n\n\nHere, we have a dataset where there is only one issue left: the column “delay” includes variables. Using pivot_wider() we can now address this issue:\n\nex_515w &lt;- pivot_wider(ex_515l, names_from = \"delay\", values_from = \"minutes\")\nknitr::kable(ex_515w)\n\n\n\n\nroute\nyear\nweek\nday\nmin\nmax\n\n\n\n\nbxl-ant\n2025\n2\nmon\n10\n89\n\n\nbxl-ant\n2025\n2\ntue\n8\n10\n\n\nbxl-ant\n2025\n2\nwed\n5\n18\n\n\nbxl-ant\n2025\n2\nthu\n0\n24\n\n\nbxl-ant\n2025\n2\nfri\n1\n36\n\n\nbxl-ant\n2025\n2\nsat\n0\n5\n\n\nbxl-ant\n2025\n2\nsun\n1\n9\n\n\nbxl-leu\n2025\n2\nmon\n1\n18\n\n\nbxl-leu\n2025\n2\ntue\n9\n29\n\n\nbxl-leu\n2025\n2\nwed\n5\n16\n\n\nbxl-leu\n2025\n2\nthu\n4\n15\n\n\nbxl-leu\n2025\n2\nfri\n2\n89\n\n\nbxl-leu\n2025\n2\nsat\n8\n150\n\n\nbxl-leu\n2025\n2\nsun\n1\n193\n\n\n\n\n\nThe dataset is now tidy. In Table 5.16 we also merged the variable “week” and “day” into a data. In Chapter 8, we’ll see how you can use {dplyr}’s mutate() to do that.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nYou using “example58” you created a dataset in long format with market capitalization per firm end per year. The long form of this dataset is here:\n\n\nCode\nex_58 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"), sheet = \"example58\") |&gt;\n  pivot_longer(cols = !firm, names_to = \"years\", values_to = \"market_cap\")\nknitr::kable(ex_58)\n\n\n\n\n\nfirm\nyears\nmarket_cap\n\n\n\n\nA\n2020\n1236\n\n\nA\n2021\n1366\n\n\nA\n2022\n1455\n\n\nA\n2023\n1554\n\n\nA\n2024\n1689\n\n\nB\n2020\n201\n\n\nB\n2021\n204\n\n\nB\n2022\n215\n\n\nB\n2023\n325\n\n\nB\n2024\n410\n\n\nC\n2020\n12598\n\n\nC\n2021\n12698\n\n\nC\n2022\n12359\n\n\nC\n2023\n11985\n\n\nC\n2024\n11453\n\n\n\n\n\nRecreate the wide form of this dataset and store it as ex_58w:\n\n\nCode\nex_58w &lt;- pivot_wider(ex_58, names_from = \"years\", values_from = \"market_cap\")\n\nknitr::kable(ex_58w)\n\n\n\n\n\nfirm\n2020\n2021\n2022\n2023\n2024\n\n\n\n\nA\n1236\n1366\n1455\n1554\n1689\n\n\nB\n201\n204\n215\n325\n410\n\n\nC\n12598\n12698\n12359\n11985\n11453\n\n\n\n\n\nPredict the outcome from these code blocks\n\nex_58w2 &lt;- pivot_wider(ex_58, names_from = \"firm\", values_from = \"market_cap\")\n\nknitr::kable(ex_58w2)\n\n\n\n\nyears\nA\nB\nC\n\n\n\n\n2020\n1236\n201\n12598\n\n\n2021\n1366\n204\n12698\n\n\n2022\n1455\n215\n12359\n\n\n2023\n1554\n325\n11985\n\n\n2024\n1689\n410\n11453\n\n\n\n\n\n\nex_58w3 &lt;- pivot_wider(ex_58, names_from = c(\"firm\", \"years\"), values_from = \"market_cap\", names_glue = \"{firm} in {years}\")\nknitr::kable(ex_58w3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA in 2020\nA in 2021\nA in 2022\nA in 2023\nA in 2024\nB in 2020\nB in 2021\nB in 2022\nB in 2023\nB in 2024\nC in 2020\nC in 2021\nC in 2022\nC in 2023\nC in 2024\n\n\n\n\n1236\n1366\n1455\n1554\n1689\n201\n204\n215\n325\n410\n12598\n12698\n12359\n11985\n11453\n\n\n\n\n\nUsing the output from the last code block: change that back from wide to long\n\nex_58l &lt;- pivot_longer(ex_58w3, cols = everything(), \n                       names_to = c(\"firm\", \"years\"),\n                       names_pattern = \"(.) in (.+)\",\n                       values_to = \"market_cap\")\nknitr::kable(ex_58l)\n\n\n\n\nfirm\nyears\nmarket_cap\n\n\n\n\nA\n2020\n1236\n\n\nA\n2021\n1366\n\n\nA\n2022\n1455\n\n\nA\n2023\n1554\n\n\nA\n2024\n1689\n\n\nB\n2020\n201\n\n\nB\n2021\n204\n\n\nB\n2022\n215\n\n\nB\n2023\n325\n\n\nB\n2024\n410\n\n\nC\n2020\n12598\n\n\nC\n2021\n12698\n\n\nC\n2022\n12359\n\n\nC\n2023\n11985\n\n\nC\n2024\n11453\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry this out\n\n\n\n\n\nUsing who you created a long verion of this dataset. Now let’s try to change that back in the wide version.\nFirst, recreate that long version\n\ndata_who &lt;- tidyr::who\ndata_who &lt;- data_who |&gt; pivot_longer(cols = starts_with(\"new\"),\n                                     names_to = c(\"method\", \"gender\",  \"age\"), \n                                     names_pattern = \"new_?(.+)_(.)(.+)\", \n                                     values_to = \"cases\"\n                                     )\nknitr::kable(data_who[1:6, ])\n\n\n\n\ncountry\niso2\niso3\nyear\nmethod\ngender\nage\ncases\n\n\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n014\nNA\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n1524\nNA\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n2534\nNA\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n3544\nNA\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n4554\nNA\n\n\nAfghanistan\nAF\nAFG\n1980\nsp\nm\n5564\nNA\n\n\n\n\n\nUsing data_who: recreate the wide version and store it in who_wide\n\n\nCode\nwho_wide &lt;- pivot_wider(data_who, names_from = c(\"method\", \"gender\", \"age\"), values_from = \"cases\")\nknitr::kable(who_wide[1:6, 1:10])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\niso2\niso3\nyear\nsp_m_014\nsp_m_1524\nsp_m_2534\nsp_m_3544\nsp_m_4554\nsp_m_5564\n\n\n\n\nAfghanistan\nAF\nAFG\n1980\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAF\nAFG\n1981\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAF\nAFG\n1982\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAF\nAFG\n1983\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAF\nAFG\n1984\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAF\nAFG\n1985\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nNotice that you could add “new” back to the names if you want to: using names_glue()\n\n\n\n\n\n\n7.2.4 Multiple types of observational units are stored in the same table\nTable 5.17 builds on “example515” by adding data on the train that is used for each route. Doing so, this dataset mixes observational units: the includes the route with delays as well as trains with capacity, normal duration, … . It is better to store data on different observational units in different datasets. As long as the data include a common identifyer, you can merge these datasets in case you would need data stored in one to add to the other. We will split this dataset in two parts: trains and delays using {dplyr}’s select() function (see (sec_datatrans?)). Let’s first import sheet “example517”:\n\nex_517 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example517\")\n\nUsing select() we can now select the columns in ex_517 to assign to the trains dataset and the columns to assign to the delays dataset. For the first, we will include the route as common identifier and all data in the columns type, dur and cap. In delays, we will store all other variables and add route to allow to merge both datasets. There are many ways to select the columns using select(): name them in a vector, use one of the starts_with, ends_with … functions … . Here we will subset using the names of columns:\n\ntrains &lt;- ex_517 |&gt; select(route, type:cap)\ndelays &lt;- ex_517 |&gt; select(route:week, delay:fri)\n\nWe can now tidy both datasets. For the first, we will use {dplyr}’s distinct() function to eliminate duplicate rows. For the second, we can use the longer and wider approach. Let’s start with the first. distinct()’s main argument is the dataset. The function returns a tibble where all duplicate rows have been removed:\n\ntrains &lt;- distinct(trains)\n\nYou can verify that the dataset trains now includes two rows.\nFor the delays, we will use the longer and wider approach:\n\ndelays &lt;- pivot_longer(delays, cols = !c(\"route\", \"year\", \"week\", \"delay\"), names_to = \"day\", values_to = \"minutes\") |&gt;\n   pivot_wider(names_from = \"delay\", values_from = \"minutes\")\n\nIf you need trains data in the delay dataset, you can join both. Using {dplyr}’s inner_join(x, y, by) you can merge datasets x and y if they both include a common identifier includes in join_by(). Here, the common identifier is the route and we want to merge delays with trains. If both data frames include one common variable, you don’t have to include the identifier. R will use the common variable to merge.\n\nmerged_data &lt;- inner_join(delays, trains, join_by(route))\nknitr::kable(merged_data[1:6, ])\n\n\n\n\nroute\nyear\nweek\nday\nmin\nmax\ntype\ndur\ncap\n\n\n\n\nbxl-ant\n2025\n2\nmon\n10\n89\nM7\n45\n400\n\n\nbxl-ant\n2025\n2\ntue\n8\n10\nM7\n45\n400\n\n\nbxl-ant\n2025\n2\nwed\n5\n18\nM7\n45\n400\n\n\nbxl-ant\n2025\n2\nthu\n0\n24\nM7\n45\n400\n\n\nbxl-ant\n2025\n2\nfri\n1\n36\nM7\n45\n400\n\n\nbxl-leu\n2025\n2\nmon\n1\n18\nM8\n30\n250\n\n\n\n\n\n\n\n7.2.5 A single observational unit is stored in multiple tables\nIf data on a single observational unit is stored in multiple tables; you can add these tables using, e.g. {dplyr}’s bind_rows() or bind_cols() function. The first adds observations for the same observational unit, the second add variables for the observational unit. Using bind_rows(). If there are variables in one dataset that are not in the other, bind_rows() will add missing values for the observations where that column was not included. As an example, sheets #example519-1” and “example519-2” include data on the change in price for 3 items on the consumer price index for the months of April and May. Let’s read both sheets:\n\nex_5191 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example519-1\")\nex_5192 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example519-2\")\n\nUsing bind_rows(), we now add both datasets. Including .id = \"month\" R will create an additional column to show where the data came from\n\nex_519 &lt;- bind_rows(ex_5191, ex_5192, .id = \"month\")\nknitr::kable(ex_519)\n\n\n\n\nmonth\nItem\ninflation\n\n\n\n\n1\nfood and drinks\n2.36%\n\n\n1\nrestaurants and hotels\n9.48%\n\n\n1\nvegetables, fresh\n-1.18%\n\n\n1\nvegetables, frozen\n0.18%\n\n\n2\nfood and drinks\n4.89%\n\n\n2\nrestaurants and hotels\n1.48%\n\n\n2\nvegetables, fresh\n2.18%\n\n\n2\nvegetables, frozen\n2.19%\n\n\n\n\n\nYou can now change the values in the month column to reflect their real month:\n\nex_519$month &lt;- case_when(ex_519$month == 1 ~ \"April\",\n                          ex_519$month == 2 ~ \"May\")\nknitr::kable(ex_519)\n\n\n\n\nmonth\nItem\ninflation\n\n\n\n\nApril\nfood and drinks\n2.36%\n\n\nApril\nrestaurants and hotels\n9.48%\n\n\nApril\nvegetables, fresh\n-1.18%\n\n\nApril\nvegetables, frozen\n0.18%\n\n\nMay\nfood and drinks\n4.89%\n\n\nMay\nrestaurants and hotels\n1.48%\n\n\nMay\nvegetables, fresh\n2.18%\n\n\nMay\nvegetables, frozen\n2.19%\n\n\n\n\n\n\n\n7.2.6 A single cell includes more than one value\nSometimes a single cell includes more than one value. Consider the example in sheet “example522” where the values refer to GDP for three countries, but the currency is included in the value. Here, the currency is one variable, the value is the other.\n\nex_522 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"example522\")\nknitr::kable(ex_522)\n\n\n\n\ncountry\nyear\ngdp\n\n\n\n\nUnited States\n2022\nusd 30338000\n\n\nGermany\n2022\neur 5100000\n\n\nJapan\n2022\nyen 658500000\n\n\n\n\n\nHere, we can use {tidyr}’s pivot_wider_delim() function to split the column into multiple new columns. The function takes as its main arguments the variable to split, the delimiter and the names of the new columns. Here, the delimiter is a space:\n\nex_522 &lt;- ex_522 %&gt;% separate_wider_delim(gdp, delim = \" \", names = c(\"currency\", \"gdp\"))\n\nAll we have to do now is change the type of the gdp variable from character to numeric:\n\nex_522$gdp &lt;- as.numeric(ex_522$gdp)\n\nYou can verify that this is not a tidy dataset:\n\n\n\n\nTable 7.12: Tidy dataset for example522\n\n\n\n\n\n\ncountry\nyear\ncurrency\ngdp\n\n\n\n\nUnited States\n2022\nusd\n30338000\n\n\nGermany\n2022\neur\n5100000\n\n\nJapan\n2022\nyen\n658500000\n\n\n\n\n\n\n\n\nIn addition to separate_wider_delim() {tidyr} also includes separate_wider_position() and separate_wider_regex(). If the column has to be split at a certain position, the second function allows you to do so. If you can use a regular expression, you can use the third function. {tidyr} also includes separate_longer_delim() and separate_longer_position(). These function split character variable in many rows.\n\n\n7.2.7 Missing values\nDatasets often include missing values. There are a couple of functions that you can use to deal with these missing values. The choice to fill, drop or replace missing values requires good knowledge of the reason why values are missing. The choice also affects e.g. averages or other statistics.\n\n7.2.7.1 fill\nConsider sheet “other1” in the workbook “tidy_data.xlsx”.\n\noth1 &lt;- read_xlsx(here::here(\"data\", \"raw\", \"tidy_data.xlsx\"),sheet = \"other1\")\nknitr::kable(oth1)\n\n\n\n\nyear\nquarter\nvisitors\n\n\n\n\n2020\n1\n20201\n\n\nNA\n2\n20202\n\n\nNA\n3\n20203\n\n\nNA\n4\n20204\n\n\n2021\n1\n20211\n\n\nNA\n2\n20212\n\n\nNA\n3\n20213\n\n\nNA\n4\n20214\n\n\n2022\n1\n20221\n\n\nNA\n2\n20222\n\n\nNA\n3\n20223\n\n\nNA\n4\n20224\n\n\n2023\n1\n20231\n\n\nNA\n2\n20232\n\n\nNA\n3\n20233\n\n\nNA\n4\n20234\n\n\n2024\n1\n20241\n\n\nNA\n2\n20242\n\n\nNA\n3\n20243\n\n\nNA\n4\n20244\n\n\n\n\n\nThe table shows that the dataset includes missing values in the first column. However, in this case, these missing values are probably due to the fact that the excel sheet “other1” doesn’t include all the values for each year, but only shows one value in merged cells. Here, we can ask R to fill the data using the function fill(data, ..., .direction = c(\"down\", \"up\", \"downup\", \"updown\"). The first argument of this function is a data frame. The ... allow you to specify columns. The third argument shows the direction of the fill: down (default), up, downup (i.e. first down and then up) or updown (first up and then down). With de default “down”, R will fill the first missing values in the year column with the value in the first row until it sees a second, non missing value. Using that value, R will fill the missing values immediately below that non-missing value until it meets a new non-missing value. To illustrate for “other1”:\n\noth1f &lt;- oth1 |&gt; fill(year, .direction = \"down\" )\nknitr::kable(oth1f[1:6, ])\n\n\n\n\nyear\nquarter\nvisitors\n\n\n\n\n2020\n1\n20201\n\n\n2020\n2\n20202\n\n\n2020\n3\n20203\n\n\n2020\n4\n20204\n\n\n2021\n1\n20211\n\n\n2021\n2\n20212\n\n\n\n\n\nDirection (note the . before .direction) “up” first starts with missing values. If R meets the non-missing value, that value is used the fill the rows above that non-missing value. With “downup” R first tries to fill the missing values “down”. If there are any left, R fills them up. The opposite happens with “updown”.\nYou can use fill if you know that the missing value is not really missing but was caused by the fact that a value was left out. In the example here, other1 didn’t include the years for every year. However, you know that these years should have been 2020, 2021, 2023 or 2024. In other words, here, if filling these missing values was straightforward.\n\n\n7.2.7.2 drop\nUsing drop_na(data, ...) you can ask R to drop all observations (rows) in the dataset in data with missing values in any of the column in .... If this is empty, all columns are used. In other words, drop_na(data) removes all missing values from the dataset. Let’s see what this would do to other1:\n\noth1d &lt;- oth1 |&gt; drop_na(year)\nknitr::kable(oth1d)\n\n\n\n\nyear\nquarter\nvisitors\n\n\n\n\n2020\n1\n20201\n\n\n2021\n1\n20211\n\n\n2022\n1\n20221\n\n\n2023\n1\n20231\n\n\n2024\n1\n20241\n\n\n\n\n\nHere, you can see that R drops all missing observations from the dataset. In this case, as there are no other variables with missing observations, dropping all missing values this is equivalent to dropping all missing values from the dataset.\nLet’s add a couple of missing values to other variables:\n\nothmissing &lt;- oth1\nothmissing$quarter[1] &lt;- NA\nothmissing$quarter[8] &lt;- NA\nothmissing$quarter[12] &lt;- NA\nothmissing$visitors[5] &lt;- NA\nothmissing$visitors[10] &lt;- NA\nothmissing$visitors[11] &lt;- NA\n\nLet’s now drop observations but only if they are in the “visitors” column:\n\nothmissing1 &lt;- othmissing |&gt; drop_na(visitors)\nknitr::kable(othmissing1)\n\n\n\n\nyear\nquarter\nvisitors\n\n\n\n\n2020\nNA\n20201\n\n\nNA\n2\n20202\n\n\nNA\n3\n20203\n\n\nNA\n4\n20204\n\n\nNA\n2\n20212\n\n\nNA\n3\n20213\n\n\nNA\nNA\n20214\n\n\n2022\n1\n20221\n\n\nNA\nNA\n20224\n\n\n2023\n1\n20231\n\n\nNA\n2\n20232\n\n\nNA\n3\n20233\n\n\nNA\n4\n20234\n\n\n2024\n1\n20241\n\n\nNA\n2\n20242\n\n\nNA\n3\n20243\n\n\nNA\n4\n20244\n\n\n\n\n\nHere you can R that R dropped the values for the first quarter of 2021 and the second and third quarter of 2022. All other rows with missing values are kept.\nWith respect to Table 7.2, dropping the missing values in this dataset would remove all days where there are not lectures. Let’s first use that table to create a tibble in a tidy format where all days with 0 lectures (e.g. monday in week 41 or 43, thusday in week 40 or 43) are included as NA.\n\nschedule &lt;- tibble(\n  week = c(40, 41, 42, 43),\n  mon = c(4, NA , 4, NA ),\n  tue = c(4, 6, 4, 6),\n  wed = c(6, 6, 6, 6),\n  thu = c(NA, 2, 2, NA ),\n  fri = c(6, 6, 6, 6),\n  sat = c(NA, NA, 2, 2),\n  sun = c(NA, NA, NA, NA)\n)\n\nschedule &lt;- schedule |&gt; pivot_longer(cols = !week, names_to = \"day\", values_to = \"hours\")\n\nIf we drop all NA values, the dataset will look like this:\n\nscheduledrop &lt;- schedule |&gt; drop_na(hours)\nknitr::kable(scheduledrop)\n\n\n\n\nweek\nday\nhours\n\n\n\n\n40\nmon\n4\n\n\n40\ntue\n4\n\n\n40\nwed\n6\n\n\n40\nfri\n6\n\n\n41\ntue\n6\n\n\n41\nwed\n6\n\n\n41\nthu\n2\n\n\n41\nfri\n6\n\n\n42\nmon\n4\n\n\n42\ntue\n4\n\n\n42\nwed\n6\n\n\n42\nthu\n2\n\n\n42\nfri\n6\n\n\n42\nsat\n2\n\n\n43\ntue\n6\n\n\n43\nwed\n6\n\n\n43\nfri\n6\n\n\n43\nsat\n2\n\n\n\n\n\nThe tibble scheduledrop shows only days with lectures. The days without are dropped from the dataset. If you calculate the mean number of hours on e.g. a monday,\n\nmean(scheduledrop$hours[scheduledrop$day == \"mon\"])\n\n[1] 4\n\n\nR use week 40 (4) and week 42 (4) to show an average of 4. This is indeed the average for the number of hours of lectures conditional upon the fact that there were lectures. In other words, if there were lectures on a monday, the average number of hours was 4. You would have the same result if you calculate the average number of hours for the tibble schedule with NA values. In that case, you would add na.rm = TRUE. Recall that this argument is required in case R needs to calculate the mean (or other statistics) with missing values. R doesn’t drop them by default. In this case, the code reads:\n\nmean(schedule$hours[schedule$day == \"mon\"], na.rm = TRUE)\n\n[1] 4\n\n\n\n\n7.2.7.3 replace\nWith replace_na(data, replace, ...) you can replace missing values in the dataset data. The replace argument shows with what these missing values must be replaced. Here, you can differentiate between coloumns. You can do so by adding the columns in the data frame in a list with the value that R has to use to replace per column. For instance,\n\nothmissing2 &lt;- othmissing |&gt; replace_na(list(year = 0, quarter = 99, visitors = -9999))\nknitr::kable(othmissing2)\n\n\n\n\nyear\nquarter\nvisitors\n\n\n\n\n2020\n99\n20201\n\n\n0\n2\n20202\n\n\n0\n3\n20203\n\n\n0\n4\n20204\n\n\n2021\n1\n-9999\n\n\n0\n2\n20212\n\n\n0\n3\n20213\n\n\n0\n99\n20214\n\n\n2022\n1\n20221\n\n\n0\n2\n-9999\n\n\n0\n3\n-9999\n\n\n0\n99\n20224\n\n\n2023\n1\n20231\n\n\n0\n2\n20232\n\n\n0\n3\n20233\n\n\n0\n4\n20234\n\n\n2024\n1\n20241\n\n\n0\n2\n20242\n\n\n0\n3\n20243\n\n\n0\n4\n20244\n\n\n\n\n\nLet’s use schedule and replace the missing values in the column hours with 0. Here, this shows the actual number of hours: replacing missing values with “0” is consistent with the fact that there were no lectures and, in other words, the number of hours in class was 0:\n\nschedulereplace &lt;- schedule |&gt; replace_na(list(hours = 0))\nknitr::kable(schedulereplace)\n\n\n\n\nweek\nday\nhours\n\n\n\n\n40\nmon\n4\n\n\n40\ntue\n4\n\n\n40\nwed\n6\n\n\n40\nthu\n0\n\n\n40\nfri\n6\n\n\n40\nsat\n0\n\n\n40\nsun\n0\n\n\n41\nmon\n0\n\n\n41\ntue\n6\n\n\n41\nwed\n6\n\n\n41\nthu\n2\n\n\n41\nfri\n6\n\n\n41\nsat\n0\n\n\n41\nsun\n0\n\n\n42\nmon\n4\n\n\n42\ntue\n4\n\n\n42\nwed\n6\n\n\n42\nthu\n2\n\n\n42\nfri\n6\n\n\n42\nsat\n2\n\n\n42\nsun\n0\n\n\n43\nmon\n0\n\n\n43\ntue\n6\n\n\n43\nwed\n6\n\n\n43\nthu\n0\n\n\n43\nfri\n6\n\n\n43\nsat\n2\n\n\n43\nsun\n0\n\n\n\n\n\nIf you calculate the average for monday, you now see 2:\n\nmean(schedulereplace$hours[schedulereplace$day == \"mon\"])\n\n[1] 2\n\n\nHere, R calculates the minimum for week 40 (4), 41 (0), 42 (4) and 43 (0) and returns 2. In other words, R shows the average for all mondays and includes mondays without hours. Here, R shows how many hours students spend in class on any given monday. As there are 2 mondays with and 2 mondays without lectures, this average is 2. Note here the difference between drop_na() and replace_na(). The first shows the average, conditional upon there being lectures on a monday; the second shows the average across all mondays. To show the conditional mean in the replace_na case, you have to add the condition that there were lectures on a monday (schedulereplace$hours &gt; 0) explicitly in the mean statement:\n\nmean(schedulereplace$hours[schedulereplace$day == \"mon\" & schedulereplace$hours &gt; 0])\n\n[1] 4\n\n\nThe result is now equal to 4 as R uses only observations where there are lectures on a monday. In other words, using replace_na() to replace missing values with 0 because missing values are observations where 0 is an appropriate value, is not without implication for other statistics. The 0 are included in e.g. the mean or standard deviation. However, the advantage is that you have to be explicit on the type of statistic you calculate: the unconditional or conditional mean. The code explicitly shows the conditions in the statement where you subset the variable.\n\n\n\n\n\n\nYour turn\n\n\n\n\n\nThe workbook “tidy_stores.xlsx” includes 3 worksheets\n\nexcel_sheets(here::here(\"data\", \"raw\", \"tidy_stores.xlsx\"))\n\n[1] \"Same store gross profits\" \"Same store labour costs\" \n[3] \"Same store EBITDA\"       \n\n\nIf you open the workbook, you’ll see that these sheets were meant to show, not meant to analyse. Here, you’ll have to tidy the data in these sheets to develop one tibble. First, let’s import the sheets. The column total equals the sum of the values in the columns Q1-Q4. In other words, we don’t need to import that column. The first row is centered and shouldn’t be imported either. You can further see that there are no column names for the years or the city. You can use lapply import these sheets and add column names “year”, “city”, “Q1”, “Q2”, “Q3” and “Q4”.\n\n\nCode\nsheets &lt;- excel_sheets(here::here(\"data\", \"raw\", \"tidy_stores.xlsx\"))\n\nlist_sheets &lt;- lapply(sheets, \n                      \\(x) read_excel(\n                        path = here::here(\"data\", \"raw\", \"tidy_stores.xlsx\"),\n                        sheet = x,\n                        range = \"C5:H28\",\n                        col_names = c(\"year\", \"city\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n                        na = c(\" \", \"NA\"))\n                      )\n\n\nYou can now extract three tibbles from this list: grossmar, labcost and ebitda:\n\n\nCode\ngrossmar &lt;- list_sheets[[1]]\nlabcost &lt;- list_sheets[[2]]\nebitda &lt;- list_sheets[[3]]\n\n\nFirst fill the years.\n\n\nCode\ngrossmar &lt;- grossmar |&gt; fill(year)\nlabcost &lt;- labcost |&gt; fill(year)\nebitda &lt;- ebitda |&gt; fill(year)\n\n\nYou can now pivot the dataset. For each dataset, use a different way to identify the columns to pivot\n\n\nCode\ngrossmar &lt;- grossmar |&gt; pivot_longer(cols = !c(\"year\", \"city\"), names_to = \"quarter\", values_to = \"grossmar\")\nlabcost &lt;- labcost |&gt; pivot_longer(cols = starts_with(\"Q\"), names_to = \"quarter\", values_to = \"labcost\")\nebitda &lt;- ebitda |&gt; pivot_longer(cols = contains(\"Q\"), names_to = \"quarter\", values_to = \"ebitda\")\n\n\nCollect these three datasets in one dataset shops but make sure that you don’t duplicate columns city, year and quarter:\n\n\nCode\nshops &lt;- bind_cols(grossmar, labcost[, 4], ebitda[, 4])\n\n\n\n\n\n\n\n\n\n\n\nTry this out\n\n\n\n\n\nYou imported the AMECO datasets. Here, you’ll tidy them. First, import the AMECO1.XLSX dataset and assign it to ameco1:\n\n\nCode\nameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nThe first columns in the AMECO dataset include e.g. the - SERIES: a code which identifies every series for every country, - CNTRY: the country ISO3 code, - TRN AGG, UNIT and REF, variables that show how a variable is measured, - CODE: the shortcode for the variable, - COUNTRY: the long name of the country, - SUB-CHAPTER: the subchapter of the AMECO dataset in the first AMECO datafile (chapter 1), - TITLE: the full title of the variable and - UNIT: the unit of measurement.\nThe values are shown in columns that refer to years as their name.\nThe CODE identifies a variable. For instance, NPTD is “Total population (national accounts)” and is measures in “1000 persons”, NPCN is “Population: 0 to 14 years” and is measured in “1000 persons”. This CODE doesn’t identify a country. In other words, we need to keep a variable to identify a country. Let’s take the variable COUNTRY. If we keep the CODE and COUNTRY, we have all we need to identify a series. Because the code is not very informative, we’ll also select the TITLE and UNIT of measurement. Then, we will select the variables we need: CODE, COUNTRY and columns with a year as header.\n\n\nCode\nameco2 &lt;- ameco1 |&gt; select(CODE, TITLE, UNIT...11)\nameco1 &lt;- ameco1 |&gt; select(CODE, COUNTRY, matches(\"\\\\d{4}\"))\n\n\nCan you see why this dataset, ameco1 is not tidy? What is the solution?\n\n\nCode\n# The columns 1960:2026 are values stored as column header. Here, the variable is year\n# The series NPTD, ... are variables but they appear in the column CODE\n# The problem here is: values are stored in both rows and columns\n# Solution: pivot longer and wider\n# longer: create a column year with the years\n# wider: create as many columns as there are variables in the rows\n\n\nTidy the dataset using pivot:\n\n\nCode\n# first longer\n\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = matches(\"[0-9]{4}\"), names_to = \"year\", values_to = \"value\")\n\n# then wider\n\nameco1 &lt;- ameco1 |&gt; pivot_wider(names_from = \"CODE\", values_from = \"value\")\n\n\nIs the dataset tidy? Let’s inspect a subset on rows 2000-2010 and in column 1-10\n\nknitr::kable(ameco1[2000:2010, 1:10])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOUNTRY\nyear\nNPTD\nNPTN\nNPCN\nNPAN\nNPON\nNPAN1\nNPON1\nNLTN\n\n\n\n\nPortugal\n2016\n10356.52\n10356.52\n1457.044\n6713.741\n2185.732\n7824.682\n1074.791\n5267.000\n\n\nPortugal\n2017\n10340.12\n10340.12\n1436.322\n6674.545\n2229.257\n7815.323\n1088.479\n5316.210\n\n\nPortugal\n2018\n10334.63\n10334.63\n1416.380\n6644.314\n2273.940\n7812.103\n1106.151\n5332.380\n\n\nPortugal\n2019\n10354.45\n10354.45\n1399.628\n6627.668\n2327.150\n7823.188\n1131.631\n5348.340\n\n\nPortugal\n2020\n10384.85\n10384.85\n1383.187\n6616.292\n2385.367\n7840.360\n1161.300\n5260.570\n\n\nPortugal\n2021\n10407.71\n10407.71\n1365.940\n6604.818\n2436.948\n7854.057\n1187.710\n5315.990\n\n\nPortugal\n2022\n10468.87\n10468.87\n1359.647\n6622.947\n2486.274\n7898.035\n1211.187\n5486.280\n\n\nPortugal\n2023\n10578.17\n10578.17\nNA\n6636.193\nNA\n7953.321\nNA\n5568.920\n\n\nPortugal\n2024\n10671.26\n10671.26\nNA\n6642.830\nNA\n7993.088\nNA\n5621.072\n\n\nPortugal\n2025\n10713.94\n10713.95\nNA\n6649.472\nNA\n8017.067\nNA\n5669.550\n\n\nPortugal\n2026\n10739.66\n10739.66\nNA\n6656.122\nNA\n8033.101\nNA\n5712.844\n\n\n\n\n\nRecall that we also have ameco2. This dataset includes a lot of duplicate rows. Keep only the unique rows:\n\n\nCode\nameco2 &lt;- ameco2 |&gt; distinct()\nknitr::kable(ameco2[1:5, ])\n\n\n\n\n\nCODE\nTITLE\nUNIT…11\n\n\n\n\nNPTD\nTotal population (National accounts)\n1000 persons\n\n\nNPTN\nTotal population\n1000 persons\n\n\nNPCN\nPopulation: 0 to 14 years\n1000 persons\n\n\nNPAN\nPopulation: 15 to 64 years\n1000 persons\n\n\nNPON\nPopulation: 65 years and over\n1000 persons\n\n\n\n\n\nThis dataset shows, for every variable, the long name and the unit of measurement. In other words, if you need the long name or the unit of measurement, you can find it in this dataset.\n\n\n\n\n\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidying data</span>"
    ]
  },
  {
    "objectID": "07_Tidying_data.html#introduction",
    "href": "07_Tidying_data.html#introduction",
    "title": "7  Tidying data",
    "section": "",
    "text": "each variable is a column\neach observation is a row\neach cell include a single value (except the ones in the first row)\n\n\n\n\n\n\n\nTable 7.1: Example: empty cells in columns. The table includes imaginary data on poverty and social exclusion in a country for the capital region, the main cities in the country and the rural areas\n\n\n\n\n\nyear\nregion\npoverty\nsocial_exclusion\n\n\n\n\n2022\ncapital\n30\n36\n\n\n\ncity\n18\n24\n\n\n\nrural\n26\n42\n\n\n2023\ncapital\n32\n36\n\n\n\ncity\n19\n29\n\n\n\nrural\n24\n38\n\n\n2024\ncapital\n29\n34\n\n\n\ncity\n22\n26\n\n\n\nrural\n33\n40\n\n\n\n\n\n\n\n\n\n\nTable 7.2: Example: number of hours scheduled for each day, per week\n\n\n\n\n\nweek\nmon\ntue\nwed\nthu\nfri\nsat\nsun\n\n\n\n\n40\n4\n4\n6\n\n6\n\n\n\n\n41\n\n6\n6\n2\n6\n\n\n\n\n42\n4\n4\n6\n2\n6\n2\n\n\n\n43\n\n6\n6\n\n6\n2",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidying data</span>"
    ]
  },
  {
    "objectID": "08_Transforming_data.html#introduction",
    "href": "08_Transforming_data.html#introduction",
    "title": "8  Data transformation",
    "section": "",
    "text": "NYC Flights 23\n\n\n\n\n\n{nycflights23} (Ismay, Couch, and Wickham (2024)) includes multiple datasets. To use these datasets, use nycflights23::dataset, e.g. to use the flights data, use nycflights23::fligths. The flights dataset includes variables with on the flights departing from one of the three New York airports with a destination in the US. You can import this dataset and verify its structure using:\n\nnycflights &lt;- nycflights23::flights\nstr(nycflights)\n\ntibble [435,352 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:435352] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ month         : int [1:435352] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:435352] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:435352] 1 18 31 33 36 503 520 524 537 547 ...\n $ sched_dep_time: int [1:435352] 2038 2300 2344 2140 2048 500 510 530 520 545 ...\n $ dep_delay     : num [1:435352] 203 78 47 173 228 3 10 -6 17 2 ...\n $ arr_time      : int [1:435352] 328 228 500 238 223 808 948 645 926 845 ...\n $ sched_arr_time: int [1:435352] 3 135 426 2352 2252 815 949 710 818 852 ...\n $ arr_delay     : num [1:435352] 205 53 34 166 211 -7 -1 -25 68 -7 ...\n $ carrier       : chr [1:435352] \"UA\" \"DL\" \"B6\" \"B6\" ...\n $ flight        : int [1:435352] 628 393 371 1053 219 499 996 981 206 225 ...\n $ tailnum       : chr [1:435352] \"N25201\" \"N830DN\" \"N807JB\" \"N265JB\" ...\n $ origin        : chr [1:435352] \"EWR\" \"JFK\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:435352] \"SMF\" \"ATL\" \"BQN\" \"CHS\" ...\n $ air_time      : num [1:435352] 367 108 190 108 80 154 192 119 258 157 ...\n $ distance      : num [1:435352] 2500 760 1576 636 488 ...\n $ hour          : num [1:435352] 20 23 23 21 20 5 5 5 5 5 ...\n $ minute        : num [1:435352] 38 0 44 40 48 0 10 30 20 45 ...\n $ time_hour     : POSIXct[1:435352], format: \"2023-01-01 20:00:00\" \"2023-01-01 23:00:00\" ...\n\n\n\nflights: fights data for all flights that departed from a given airport. The data includes variables\n\nyear, month, data: integers showing the date of departure\ndep_time, arr_time: the actual departure and arrival times shown as HHMM or HMM e.g. 503 is three minutes past 5 AM\nsched_dep_time, sched_arr_time: the scheduled departure and arrival times shown as HHMM or HMM e.g. 852 is 52 minutes past 8 AM\ndep_delay, arr_delay: departure and arrival delays, in minutes with negative times representing flights with early departures or arrivals\ncarrier: a two letter carrier abbreviation. The dataset “airlines” includes more information on these carriers\nflight: flight number\ntailnum: tail number of the airplane\norigin, dest: the airports where the flight departed and where it landed. The dataset “airports” includes more in formation on these airports.\nair_time: the number of minutes in the air (time in the air, between take-off and landing, excluding taxiing to and from gates)\ndistance: the distance between origin and destination in miles\ntime_hour: scheduled date and hour of the flight’s departure. Note that the first 5 observations refer to flights that were scheduled for late december 31, 2022 but were delayed into 2023. This variable is POSIXct.\n\n\nThe airlines dataset includes the names of the airlines:\n\nstr(nycflights23::airlines)\n\ntibble [14 × 2] (S3: tbl_df/tbl/data.frame)\n $ carrier: chr [1:14] \"9E\" \"AA\" \"AS\" \"B6\" ...\n $ name   : chr [1:14] \"Endeavor Air Inc.\" \"American Airlines Inc.\" \"Alaska Airlines Inc.\" \"JetBlue Airways\" ...\n\n\n\nairlines:\n\ncarrier: the carrier abbreviation\nname: airline names in full\n\n\n\nstr(nycflights23::airports)\n\ntibble [1,251 × 8] (S3: tbl_df/tbl/data.frame)\n $ faa  : chr [1:1251] \"AAF\" \"AAP\" \"ABE\" \"ABI\" ...\n $ name : chr [1:1251] \"Apalachicola Regional Airport\" \"Andrau Airpark\" \"Lehigh Valley International Airport\" \"Abilene Regional Airport\" ...\n $ lat  : num [1:1251] 29.7 29.7 40.7 32.4 67.1 ...\n $ lon  : num [1:1251] -85 -95.6 -75.4 -99.7 -157.9 ...\n $ alt  : num [1:1251] 20 79 393 1791 334 ...\n $ tz   : num [1:1251] -5 -6 -5 -6 -9 -7 -6 -5 -5 -6 ...\n $ dst  : chr [1:1251] \"A\" \"A\" \"A\" \"A\" ...\n $ tzone: chr [1:1251] \"America/New_York\" \"America/Chicago\" \"America/New_York\" \"America/Chicago\" ...\n\n\n\nairports: airport metadata including\n\nfaa: federal aviation abbreviation, identifying each airport\nname: full name\nlat´,lon,alt,tz,dst,tzone`: latitude, longitude, altitude, timezone offset relative to UTC, daylight savings time and time zone name\n\n\n\nstr(nycflights23::planes)\n\ntibble [4,840 × 9] (S3: tbl_df/tbl/data.frame)\n $ tailnum     : chr [1:4840] \"N101DQ\" \"N101DU\" \"N101HQ\" \"N101NN\" ...\n $ year        : int [1:4840] 2020 2018 2007 2013 2020 NA 2007 2013 1998 NA ...\n $ type        : chr [1:4840] \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" ...\n $ manufacturer: chr [1:4840] \"AIRBUS\" \"C SERIES AIRCRAFT LTD PTNRSP\" \"EMBRAER-EMPRESA BRASILEIRA DE\" \"AIRBUS INDUSTRIE\" ...\n $ model       : chr [1:4840] \"A321-211\" \"BD-500-1A10\" \"ERJ 170-200 LR\" \"A321-231\" ...\n $ engines     : int [1:4840] 2 2 2 2 2 2 2 2 2 2 ...\n $ seats       : int [1:4840] 199 133 80 379 199 133 80 379 182 133 ...\n $ speed       : int [1:4840] 0 0 0 0 0 0 0 0 0 0 ...\n $ engine      : chr [1:4840] \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" ...\n\n\n\nplanes: planes metadata including\n\ntailnum: tail number\nyear: first year of operation\ntype: manufacturer and model: type of airplane\nengines: number of engines\nseats: capacity\nspeed: undefined, equals 0 in the dataset\nengine: engine type\n\n\n\nstr(nycflights23::weather)\n\ntibble [26,204 × 15] (S3: tbl_df/tbl/data.frame)\n $ origin    : chr [1:26204] \"JFK\" \"JFK\" \"JFK\" \"JFK\" ...\n $ year      : int [1:26204] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ month     : int [1:26204] 1 1 1 1 1 1 1 1 1 1 ...\n $ day       : int [1:26204] 1 1 1 1 1 1 1 1 1 1 ...\n $ hour      : int [1:26204] 0 1 2 3 4 5 6 7 8 9 ...\n $ temp      : num [1:26204] NA NA NA NA NA NA NA NA NA NA ...\n $ dewp      : num [1:26204] NA NA NA NA NA NA NA NA NA NA ...\n $ humid     : num [1:26204] NA NA NA NA NA NA NA NA NA NA ...\n $ wind_dir  : num [1:26204] 0 190 190 250 170 0 250 230 260 250 ...\n $ wind_speed: num [1:26204] 0 4.6 5.75 5.75 8.06 ...\n $ wind_gust : num [1:26204] 0 5.3 6.62 6.62 9.27 ...\n $ precip    : num [1:26204] NA NA NA 0.02 NA NA NA NA NA NA ...\n $ pressure  : num [1:26204] NA NA NA NA NA NA NA NA NA NA ...\n $ visib     : num [1:26204] 0.25 2.5 0.25 4 0.75 0.75 0.24 0.5 8 5 ...\n $ time_hour : POSIXct[1:26204], format: \"2023-01-01 09:00:00\" \"2023-01-01 10:00:00\" ...\n\n\n\nweather: hourly weather data per NY airport including\n\nyear, month, day, hour: integers showing year, month, day and hour\ntem, dewp: temperature and dewpoint in Fahrenheit\nwind_dir and wind_speed, wind_gust: data on wind speed and direction. Wind direction is in degrees, speed and gust in miles per hour.\nhumid: relative humidity\nprecip: data on precipitation, in inches\npressure: sea level pressure in millibars\nvisib: visibility in miles\ntime_hour: POSIXct date/time\n\n\nLet’s look at the first observations for the 10 first columns in for nycflights:\n\nhead(nycflights[1:10])\n\n# A tibble: 6 × 10\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2023     1     1        1           2038       203      328              3\n2  2023     1     1       18           2300        78      228            135\n3  2023     1     1       31           2344        47      500            426\n4  2023     1     1       33           2140       173      238           2352\n5  2023     1     1       36           2048       228      223           2252\n6  2023     1     1      503            500         3      808            815\n# ℹ 2 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;\n\n\nThe first flight to depart in 2023 did so one minute past midnight (dep_time: 1) but was scheduled to depart sched_dep_time on 20:38. The departure delay, dep_delay was 203 minutes. In other words, that flight was suppose to depart on December 31, 20:38. You can see this as the difference between the actual departure and the scheduled departure is 203 (delay) minutes of 3 hours and 33 minutes. The flight was scheduled to arrive sched_arr_time 1 minute past midnight. It actually arrived (arr_time) 3:28. In other words, it arrived 205 minutes delay (arr_delay) or 3 hours and 35 minutes. The carrier (`carrier) was UA.\n\nhead(nycflights[11:19])\n\n# A tibble: 6 × 9\n  flight tailnum origin dest  air_time distance  hour minute time_hour          \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n1    628 N25201  EWR    SMF        367     2500    20     38 2023-01-01 20:00:00\n2    393 N830DN  JFK    ATL        108      760    23      0 2023-01-01 23:00:00\n3    371 N807JB  JFK    BQN        190     1576    23     44 2023-01-01 23:00:00\n4   1053 N265JB  JFK    CHS        108      636    21     40 2023-01-01 21:00:00\n5    219 N17730  EWR    DTW         80      488    20     48 2023-01-01 20:00:00\n6    499 N925AN  EWR    MIA        154     1085     5      0 2023-01-01 05:00:00\n\n\nThe flight number flight was 628 and the tailnumberof the plane was N25201. It departed from origin EWR and as destined for dest SMF. Using airports and a function that we will cover in this chapter, you can verify that these are Newark Liberty International Airport in New York and Sacramento International Airport. Note that the variable flight doesn’t show individual flights, but a flight number with multiple observations per flight number.\n\nnycflights23::airports |&gt; filter(faa ==  \"EWR\" | faa == \"SMF\")\n\n# A tibble: 2 × 8\n  faa   name                                  lat    lon   alt    tz dst   tzone\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 EWR   Newark Liberty International Airpo…  40.7  -74.2    18    -5 A     Amer…\n2 SMF   Sacramento International Airport     38.7 -122.     27    -8 A     Amer…\n\n\nThe flight took (airtime) 367 minutes of 6 hours and 7 minutes. The distance between EWR and SMF is 2500 miles (distance). The scheduled departure hour was 20 (hour) and 38 minutes (minute). In POSIXct, this was time_hour, 2023-01-01 20:00:00. Note that the dataset here includes a mistake. The time_hour for the first flights was in 2022 and not in 2023. This is also the case for the flights with a sched_dep_time on 23:00, 23:44, 21:40 and 20:48: they all left in 2023 but were scheduled to do so in 2022.\nNote that the carrier variable is also included in the airlines dataset, the tailnum variable is part of the planes dataset and that the time_hour is also part of the weather dataset. This will allow us to join these datasets.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "08_Transforming_data.html#transforming-one-dataset",
    "href": "08_Transforming_data.html#transforming-one-dataset",
    "title": "8  Data transformation",
    "section": "8.2 Transforming one dataset",
    "text": "8.2 Transforming one dataset\n\n8.2.1 Ungrouped and grouped data\nRecall from Chapter 4 that factors are categorical variables that allow you to identify a (limited) number of categories or groups. Datasets often include these types of variables. R allows you to perform operations on both ungrouped as well as grouped data. A lot of the function that we will cover here, allow you to do so. For instance, dividing a value by its group mean, filtering observations across groups or summarizing data for each group. To group data you can use the group_by function\n\ngroup_by(.data, ..., .add = FALSE, .drop = group_by_drop_default(.data))\n\nThe first argument in this function is the dataset. You can add the variable, variables or computations to group by in the .... The third, .add = FALSE will remove any prior grouping of the data. If you change this into TRUE, the grouping function will group grouped data. The last argument, .drop by default drops groups for factor levels that don’t appear in the data. This is only FALSE if previous groups were obtained setting .drop = FALSE. Note the . before .drop and .add. This . refers to the fact that the argument is using the dataset .data. For instance, .add adds to the groups that are port of that file .data.\nInternally, R orders the groups in ascending order. This means that the output of functions following the group_by() function show the result in this order. If the grouping variable in ... is an ordered factor, R will use this ordering. The function returns a data frame (or a tibble). This allows you to use its output in subsequent calculations or visualizations e.g. using the |&gt; pipe. In other words, it allows you to group data in a work flow dataset |&gt; group_by() |&gt; summarize() |&gt; ....\nTo illustrate how group_by works, consider the following tibble\n\ndfg &lt;- tibble(\n  id = c(rep(\"A\", 6), rep(\"B\", 6)), \n  var1 = rep(c(1, 2), 6),\n  var2 = rep(c(1, 2, 3), 4),\n  var3 = seq(from = 10, by = 10, length.out = 12))\n\nThis tibble has 4 variables and 12 observations. The first variable, id, includes 3 levels, “A”, “B” and “C”. Within every idgroup, there are two levels “1” and “2” in the first subgroup variable var1. The second variable, var2 includes 3 levels, “1”, “2” and “3”. The last variable, var3 is a sequence with data.\nUsing group_by(id) R creates a grouped tibble. You can see that from the output of this code:\n\ndfg |&gt; group_by(id)\n\n# A tibble: 12 × 4\n# Groups:   id [2]\n   id     var1  var2  var3\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 A         1     1    10\n 2 A         2     2    20\n 3 A         1     3    30\n 4 A         2     1    40\n 5 A         1     2    50\n 6 A         2     3    60\n 7 B         1     1    70\n 8 B         2     2    80\n 9 B         1     3    90\n10 B         2     1   100\n11 B         1     2   110\n12 B         2     3   120\n\n\nThe second line now reads: “Groups: id [2]”. In other words, R used the two values in id to identify groups. If you now assign this result to dfg to another object, R will use these groups in all subsequant operations (e.g. calculations of summary statistics). Note that does not change how R show the tibble. In other words, R doesn’t group the dataset in its output. Internally, however, it’s observations are grouped.\nNote that dfg |&gt; group_by(id) is equivalent to group_by(dfg, id).\nLet’s first calculate the mean of var3 without grouping. To do so, we can use the summarize() function. This function will be covered later in this chapter. In general, it requires an argument structure summarize(dataset, name_of_stat = summary_function(variable)):\n\ndfg |&gt; \n  summarize(ave = mean(var3))\n\n# A tibble: 1 × 1\n    ave\n  &lt;dbl&gt;\n1    65\n\n\nThe result is a tibble with one value: 65 the average of 10, 20, …, 120. Let’s now group using id:\n\ndfg |&gt; \n  group_by(id) |&gt; \n  summarize(ave = mean(var3))\n\n# A tibble: 2 × 2\n  id      ave\n  &lt;chr&gt; &lt;dbl&gt;\n1 A        35\n2 B        95\n\n\nThis code returns a tibble with 2 variables: id and the mean for var3 per id group. In this tibble, 35 is the average of 10, 20, 30, 40, 50 and 60 (the values for var3 where id is A) and 95 is the average of the values for var3 where id is B: 70, 80, 90, 100, 110 and 120. As you can see, R returns the result and shows the group variables in ascending order.\nWithing group_by you can add more than one variable. In that case, R will first group by the first and then by the second. For instance, using both id and var1, R will first create 2 groups: one for A and one for B. In the second step, for the values in A, R will use the values in var1 to add another group.\n\ndfg |&gt; \n  group_by(id, var1) |&gt; \n  summarize(ave = mean(var3))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 3\n# Groups:   id [2]\n  id     var1   ave\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    30\n2 A         2    40\n3 B         1    90\n4 B         2   100\n\n\nWe now have a tibble with 3 variables: id (the first group), var1 the second group and ave. The values in the last column show the average for var3 for all observations who are in both groups. For instance, 30, the first value, is the average of 10, 30 and 50: the values for var3 if id is A and the second grouping variable, var1 is 1; the value of 100 is the average of the values for which id is B and var1 is 2: 80, 100 and 120.\nNote that the output is a tibble. In other words, we can use it in e.g. a plot using {ggplot2}, a table using {gt} or use in a statistical model.\nYou can add a grouping variable using a calculation. In dfg we have two variables, var1 and var2 whose values are 1 and 2, and 1, 2 and 3. Using their sum, we can create a new variable that we can use to group. This variable will take values 2, 3, 4 and 5 where value 2 is for observations where both var1 and var2 equal 1; value 3 of for observations where either var1 is 2 and var2 is 1 or vice versa,…\n\ndfg |&gt; \n  group_by(sum12 = var1 + var2) |&gt; \n  summarize(ave = mean(var3))\n\n# A tibble: 4 × 2\n  sum12   ave\n  &lt;dbl&gt; &lt;dbl&gt;\n1     2    40\n2     3    75\n3     4    55\n4     5    90\n\n\nHere, the function cut(var, n) can be useful. This function cuts the values of a variable in n groups. For instance, using var3 and asking for 3 groups:\n\ndfg |&gt; \n  group_by(var_gr3 = cut(var3, 3)) |&gt; \n  summarize(ave = mean(var3))\n\n# A tibble: 3 × 2\n  var_gr3       ave\n  &lt;fct&gt;       &lt;dbl&gt;\n1 (9.89,46.7]    25\n2 (46.7,83.3]    65\n3 (83.3,120]    105\n\n\nAs you can see in the result, R sets boundaries for the 3 groups based on the values of var3 equal to 46.7, 83.3 and 120. Doing so, R has 4 values within each group.\nTo illustrate the output for ordered factors, let’s add one to d1. The factor is equal to id except that the values “A” and “B” are now ordered in descending order:\n\ndfg$id2ord &lt;- factor(dfg$id, levels = c(\"B\", \"A\"), ordered = TRUE)\n\nCalculating the mean for var3 using this new variable in group_by()\n\ndfg |&gt; \n  group_by(id2ord) |&gt; \n  summarize(ave = mean(var3))\n\n# A tibble: 2 × 2\n  id2ord   ave\n  &lt;ord&gt;  &lt;dbl&gt;\n1 B         95\n2 A         35\n\n\nreturn a tibble that show the values for id2ord from B to A.\ncount() and add_count() are very useful function to see how many values there are per group. The first returns the number, the second add this number as a separate column to the dataset. For instance\n\ndfg |&gt; count(id)\n\n# A tibble: 2 × 2\n  id        n\n  &lt;chr&gt; &lt;int&gt;\n1 A         6\n2 B         6\n\n\nshows that there are 6 observations in each group defined by id;\n\ndfg |&gt; count(id, var1)\n\n# A tibble: 4 × 3\n  id     var1     n\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A         1     3\n2 A         2     3\n3 B         1     3\n4 B         2     3\n\n\nand that there are 3 observations for each group defined by both id and var1. Using add_count() you can add the number of observations for each group to the dataset. If you don’t specify a name, R will add n. If you add sort = TRUE R will show that largest groups on top\n\ndfg &lt;- dfg |&gt; add_count(id, sort = TRUE, name = \"nobs_id\")\ndfg\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 A         1     1    10 A            6\n 2 A         2     2    20 A            6\n 3 A         1     3    30 A            6\n 4 A         2     1    40 A            6\n 5 A         1     2    50 A            6\n 6 A         2     3    60 A            6\n 7 B         1     1    70 B            6\n 8 B         2     2    80 B            6\n 9 B         1     3    90 B            6\n10 B         2     1   100 B            6\n11 B         1     2   110 B            6\n12 B         2     3   120 B            6\n\n\nThe resulting tibble shows the new column nobs_id. Here, the sort is not relevant as each group is equally large. However, if that is not the case, this allows you to see which group includes most observations, e.g. which product category includes most products.\nUsing ungroup() you can ungroup a tibble:\n\nungroup(dfg)\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 A         1     1    10 A            6\n 2 A         2     2    20 A            6\n 3 A         1     3    30 A            6\n 4 A         2     1    40 A            6\n 5 A         1     2    50 A            6\n 6 A         2     3    60 A            6\n 7 B         1     1    70 B            6\n 8 B         2     2    80 B            6\n 9 B         1     3    90 B            6\n10 B         2     1   100 B            6\n11 B         1     2   110 B            6\n12 B         2     3   120 B            6\n\n\nIn general, you should add this after a operations that include a grouped tibble. In that way, you avoid that subsequent calculation continue to use the grouping.\n\n\n8.2.2 Functions on rows\n\n8.2.2.1 arrange()\nIn general, there are two operations that you can perform on rows: you can arrange them and show them in a particular order or you can filter them and use only a subset of observations. With respect to the first, arrange(), allows you to sort datasets. This is the {dplyr} version of sort(). However, unlike sort() arrange() always sorts missing values at the end. By default, arrange() sorts in ascending order. Using desc() with the variable to sort, will do so in descending order. arrange() will not take into account groups within grouped data frames. To arrange by group, you need to add .by_group = TRUE.\nTo illustrate, let’s arrange dfg in ascending order of var3:\n\ndfg |&gt; arrange(var3)\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 A         1     1    10 A            6\n 2 A         2     2    20 A            6\n 3 A         1     3    30 A            6\n 4 A         2     1    40 A            6\n 5 A         1     2    50 A            6\n 6 A         2     3    60 A            6\n 7 B         1     1    70 B            6\n 8 B         2     2    80 B            6\n 9 B         1     3    90 B            6\n10 B         2     1   100 B            6\n11 B         1     2   110 B            6\n12 B         2     3   120 B            6\n\n\nThe result is a tibble, with all columns sorted according to the ascending values in var3. In other words, you can now save or use this tibble in subsequent operations and exploit the that the values are now ordered in a particular way, e.g. to show them in visualizations or use the order in an analysis.\nYou can add more than one variable in arrange(). For instance, sorting in ascending order of var2 and descending order of var3:\n\ndfg |&gt; arrange(var2, desc(var3))\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 B         2     1   100 B            6\n 2 B         1     1    70 B            6\n 3 A         2     1    40 A            6\n 4 A         1     1    10 A            6\n 5 B         1     2   110 B            6\n 6 B         2     2    80 B            6\n 7 A         1     2    50 A            6\n 8 A         2     2    20 A            6\n 9 B         2     3   120 B            6\n10 B         1     3    90 B            6\n11 A         2     3    60 A            6\n12 A         1     3    30 A            6\n\n\nHere you can see that var2 is first sorted. If the values for that variable are equal, then R sorts the variables of the other variable, var2 in descending order. In addition to variables, you can also include calculations. For instance, to sort the dataset based on the ratio of var3 to var2:\n\ndfg |&gt; arrange(var3/var2)\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 A         1     1    10 A            6\n 2 A         2     2    20 A            6\n 3 A         1     3    30 A            6\n 4 A         2     3    60 A            6\n 5 A         1     2    50 A            6\n 6 B         1     3    90 B            6\n 7 A         2     1    40 A            6\n 8 B         2     2    80 B            6\n 9 B         2     3   120 B            6\n10 B         1     2   110 B            6\n11 B         1     1    70 B            6\n12 B         2     1   100 B            6\n\n\nTo use arrange() with grouped data, you need to add the by_group = TRUE in the function call. The is because arrange() doesn’t sort values per group, unless you explicitly tell it to do so. For instance, say we add arrange after group_by(var2) and use this to arrange in descending order of var3. In other word here we first group by var2 before we ask R to arrange descending using var3. As the results show, R doesn’t group the data in the end result.\n\ndfg |&gt; \n  group_by(var2) |&gt; \n  arrange(desc(var3))\n\n# A tibble: 12 × 6\n# Groups:   var2 [3]\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 B         2     3   120 B            6\n 2 B         1     2   110 B            6\n 3 B         2     1   100 B            6\n 4 B         1     3    90 B            6\n 5 B         2     2    80 B            6\n 6 B         1     1    70 B            6\n 7 A         2     3    60 A            6\n 8 A         1     2    50 A            6\n 9 A         2     1    40 A            6\n10 A         1     3    30 A            6\n11 A         2     2    20 A            6\n12 A         1     1    10 A            6\n\n\nTo do so, you need to add by_group = TRUE in the arrange() function:\n\ndfg |&gt; \n  group_by(var2) |&gt; \n  arrange(desc(var3), .by_group = TRUE)\n\n# A tibble: 12 × 6\n# Groups:   var2 [3]\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 B         2     1   100 B            6\n 2 B         1     1    70 B            6\n 3 A         2     1    40 A            6\n 4 A         1     1    10 A            6\n 5 B         1     2   110 B            6\n 6 B         2     2    80 B            6\n 7 A         1     2    50 A            6\n 8 A         2     2    20 A            6\n 9 B         2     3   120 B            6\n10 B         1     3    90 B            6\n11 A         2     3    60 A            6\n12 A         1     3    30 A            6\n\n\nAgain note the .by_group. As was the case with e.g. .add for group_by this refers to the fact that R needs data from the data frame. Here, that data frame is dfg |&gt; group_by(var2) (recall that this function returns a data frame or tibble). Using the grouping information in this data frame, R will add groups in arrange().\n\n\n8.2.2.2 slice()\nThere are a number of slice() functions. They all allow you to show rows by their locations.\n\nslice(.data, ..., .by = NULL, .preserve = FALSE)\nslice_head(.data, ..., n, prop, by = NULL)\nslice_tail(.data, ..., n, prop, by = NULL)\nslice_min(.data, order_by, ..., n, prop, by = NULL, with_ties = TRUE, na_rm = TRUE)\nslice_max(.data, order_by, ..., n, prop, by = NULL, with_ties = TRUE, na_rm = TRUE)\nslice_sample(.data, ..., n, prop, by = NULL, weight_by, replace = FALSE)\n\nLet’s start with the first function, slice(). This function shows the rows that you include in the statement. To do so, R first needs the dataset in .data. Second are needs the rows to show. This is what you can add in .... To identify row indices, you can range a x:y to filter rows x to y, use a vector with indices or use negative subsetting to remove observations. You can add groups only for this operation using .by = (note the .). Here you specify the variable(s) used for grouping. If the data is grouped, .preserve = FALLS recalculates the groups. If TRUE, is keeps the groups as is. These functions return a tibble. To illustrate using dfg. To see observations 5:8, you can use, using a range:\n\ndfg |&gt; slice(5:8)\n\n# A tibble: 4 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     2    50 A            6\n2 A         2     3    60 A            6\n3 B         1     1    70 B            6\n4 B         2     2    80 B            6\n\n\nor, using a vector with index positions:\n\ndfg |&gt; slice(c(5, 6, 7, 8))\n\n# A tibble: 4 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     2    50 A            6\n2 A         2     3    60 A            6\n3 B         1     1    70 B            6\n4 B         2     2    80 B            6\n\n\nor, using negative subsetting\n\ndfg |&gt; slice(c(-1:-4, -9:-12))\n\n# A tibble: 4 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     2    50 A            6\n2 A         2     3    60 A            6\n3 B         1     1    70 B            6\n4 B         2     2    80 B            6\n\n\nUsing .by with ungrouped data, allows to to show the index positions by group. For instance, to show observations 2 and 3 for each group in id:\n\ndfg |&gt; slice(2:3, .by = id)\n\n# A tibble: 4 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         2     2    20 A            6\n2 A         1     3    30 A            6\n3 B         2     2    80 B            6\n4 B         1     3    90 B            6\n\n\nWith grouped data,\n\ndfg |&gt; group_by(id) |&gt; slice(2:3)\n\n# A tibble: 4 × 6\n# Groups:   id [2]\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         2     2    20 A            6\n2 A         1     3    30 A            6\n3 B         2     2    80 B            6\n4 B         1     3    90 B            6\n\n\nR keeps the group structure.\nslice_head() and slice_tail() show the first n and last n observations in the dataset .data. As an alternative, you can include a proportion. In that case, these functions return the proportion of the observations at the top or bottom. Here prop will be rounded down in case prop times the number of observations is not an integer. If you add a group using by (note the absence of a .) you group the data for this operation only. To illustrate, we will use slide_head() and show\n\nthe first 5 observations:\n\ndfg |&gt; slice_head(n = 5)\n\n# A tibble: 5 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 A         2     1    40 A            6\n5 A         1     2    50 A            6\n\n\nshow the first 30% of observations (recall that R rounds down to the integer):\n\ndfg |&gt; slice_head(prop = 0.30)\n\n# A tibble: 3 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n\n\nshow the first 2 observations by group in id\n\ndfg |&gt; slice_head(n = 2, by = id)\n\n# A tibble: 4 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 B         1     1    70 B            6\n4 B         2     2    80 B            6\n\n\n\nslice_min() and slice_max() allow you to show the smallest or largest n or prop of the values if the data is order_by one or more variables. The argument with_ties = TRUE will show more than n observations in case there is a tie. For instance, suppose that the maximum values are 10, 9, 9, 8, 8, 8, 7 and you would want to see the 5 highest, R would show 6 values: 10, 9, 9, 8, 8 and 8. With with_ties = FALSE R show only 5. Using by (note the absence of a .) R shows the smallest or largest values by group. The last argument, na.rm = TRUE will drop missing values. If FALSE, missing values are sorted at the end and will show is e.g. slice_max(n = m) would include them in the largest m observations. To illustrate, you find the largest three values for the dataset dfg ordered by var3 using:\n\ndfg |&gt; slice_max(order_by = var3, n = 3)\n\n# A tibble: 3 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 B         2     3   120 B            6\n2 B         1     2   110 B            6\n3 B         2     1   100 B            6\n\n\nTo find the smallest 2 values per group of id and ordered by var3:\n\ndfg |&gt; slice_min(order_by = var3, n = 3, by = id)\n\n# A tibble: 6 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 B         1     1    70 B            6\n5 B         2     2    80 B            6\n6 B         1     3    90 B            6\n\n\nNote that his is equivalent to\n\ndfg |&gt; group_by(id) |&gt; arrange(var3, .by_group = TRUE) |&gt; slice_head(n = 3)\n\n# A tibble: 6 × 6\n# Groups:   id [2]\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 B         1     1    70 B            6\n5 B         2     2    80 B            6\n6 B         1     3    90 B            6\n\n\nIn this code, you first group by id, then arrange ascending while keeping the group structure and you finish with selecting the first 3 observations per group. Here, those 3 will be the values where var3, which are sorted in ascending order, are the lowest.\nslice_sample() allows you to draw a random sample from the dataset. This is often a usefull function is you did calculations with your data and you want to check them or if you have a very large dataset and don not want to use all observations to calculate e.g. the mean of median value of a variable. In addition to the data, you need to include the sample size (either in absolute terms (n) or in relative terms (prop)) and you can specify is the sample needs to be weighted and if the sample is drawn with replacement. The defaults for both these arguments is FALSE. If you include a variable in by, the sample is drawn per group. For instance, to draw a random sample of 5 from dfg:\n\ndfg |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 B         1     3    90 B            6\n2 B         2     1   100 B            6\n3 A         1     1    10 A            6\n4 B         2     2    80 B            6\n5 A         1     3    30 A            6\n\n\nThe slice_() functions return a tibble. Using arrange() and then slice_() or using slice_min()or slice_max() you can select the largest of smallest values in a dataset and collect them in a tibble. You can then use these largest or smallest values in a tibble. We used these steps in Chapter 2 when we selected the top 10 selling product lines for the Nike dataset.\nIn base R, head() and tail() offer alternatives for slice_head() and slice_tail().\n\n\n8.2.2.3 first(), last() and nth()\nThese functions allow you to extract the first, the last or the nth value from a variable. The first two functions include the vector or data frame. With a vector or variable, these function return the first of last value. If you include a data frame, the first or last row is shown. In addition, you can add order_by to order the variable or data frame. In that case, first or last show the maximum of minimum value. With a data frame, you have to subset the variable, e.g. dfg$var3. Finally, na.rm = FALSE shows that R will include missing values. There is one additional argument for nth: the exact position. A positive integer starts from the top, a negative integer from the bottom of the variable or data frame. To illustrate, the first row in dfg, ordered by var3,\n\ndfg |&gt; first(order_by = dfg$var3)\n\n# A tibble: 1 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n\n\nthe last row in dfg\n\ndfg |&gt; last()\n\n# A tibble: 1 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 B         2     3   120 B            6\n\n\nand the fifth row from the top:\n\ndfg |&gt; nth(n = 5)\n\n# A tibble: 1 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     2    50 A            6\n\n\n\n\n8.2.2.4 distinct()\nUsing distinct(.data, ..., .keep_all = FALSE) you can keep only unique observations in a dataset. We used this function in Chapter 7 to create a dataset with unique train characteristics. In the function, .data refers to the dataset. If you want to determine uniqueness using one variable or the result of a calculation, you can include those in the ... part. In that case, you can also specify is the result show only the unique values in that column (default) of all observations. To show the latter, change .keep_all = FALSE in TRUE. In that case, the first occurrence of all other observations is shown.\nThis function returns a tibble with the unique values. dfg doesn’t have copied rows. Here the result will be the dataset\n\ndfg |&gt; distinct()\n\n# A tibble: 12 × 6\n   id     var1  var2  var3 id2ord nobs_id\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n 1 A         1     1    10 A            6\n 2 A         2     2    20 A            6\n 3 A         1     3    30 A            6\n 4 A         2     1    40 A            6\n 5 A         1     2    50 A            6\n 6 A         2     3    60 A            6\n 7 B         1     1    70 B            6\n 8 B         2     2    80 B            6\n 9 B         1     3    90 B            6\n10 B         2     1   100 B            6\n11 B         1     2   110 B            6\n12 B         2     3   120 B            6\n\n\nHowever, including a variable, returns the unique values for this variable:\n\ndfg |&gt; distinct(var1)\n\n# A tibble: 2 × 1\n   var1\n  &lt;dbl&gt;\n1     1\n2     2\n\n\nunless you include .keep_all = TRUE. In that case, R returns the full rows with the first occurrences of the other variables. In other words,\n\ndfg |&gt; distinct(var1, .keep_all = TRUE)\n\n# A tibble: 2 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n\n\nreturns a tibble with the unique values for var1 and the first occurrences of the other values in the other variables.\n\n\n8.2.2.5 filter()\nThe filter(.data, ..., .by = NULL, .preserve = FALSE) function is used to subset a dataset to extract only those rows that meet one or more conditions. This function is the workhorse of data management and analysis as it allows you to search in specific observations that meet one or more conditions in a dataset. In other words, this is the main tool to use if you want to answer questions such as “what is the destination airport where the arrival delay is highest?” (nycflights), “Which firms have market capitalization &gt; 1 billion and trade more than 1m shares per day? (yahoofinancer)?” or “For which high income countries holds that life expectancy at birth is less than the average for this income group while per capita gdp is higher than average for high income countries and where more than 10m people live?” (World Bank’s life expectancy at birth data).\nThe first argument in the function is the dataset. The second, ... allows you to define the conditions. These conditions return a logical TRUE or FALSE. .by allows you to define groups only for this single filtered operations. With .preserve = FALSE R will change the groups if you filter a grouped dataset and these groups are not all in the result of the filter. Changing this to TRUE, preserve all groups.\nThis function return a tibble that includes only the observations (rows) that meet the conditions. These rows appear in the same order as they appear in the dataset. This tibble includes all columns. Because the function returns a tibble, you can save or continue to pipe its result for separate analysis or to show in tables and graphs.\nRecall from Chapter 3 that the boolean operators include\n\n==: test for equality\n&gt;, &lt;, &gt;=, &lt;=: larger than, smaller than, larger than or equal to, smaller than or equal to\n&: logical “and”: TRUE & TRUE = TRUE, all others are FALSE\n|: logical “or”: returns only false in case FALSE | FALSE, else TRUE\n!: logical “not”\nis.na(): returns TRUE if a value is NA, else FALSE\n!is.na(): returns TRUE is a value is not missing, else (missing value) FALSE\n\nThe logical conditions can include calculations, e.g. var1 &gt; mean(var1, na.rm = TRUE),\nWith grouped data, filter() is performed per group. To illustrate the use of this function, we”ll give a couple of examples using dfg:\n\nfilter observations with id is equal to A:\n\n\ndfg |&gt; filter(id == \"A\")\n\n# A tibble: 6 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 A         2     1    40 A            6\n5 A         1     2    50 A            6\n6 A         2     3    60 A            6\n\n\n\nfilter observations where id is not equal to B and var1 is less than half the value of var2:\n\n\ndfg |&gt; filter(!(id == \"B\") & var1 &lt; var2/2)\n\n# A tibble: 1 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     3    30 A            6\n\n\n\nfilter observations where var3 is less than its mean:\n\n\ndfg |&gt; filter(var3 &lt; mean(var3, na.rm = TRUE))\n\n# A tibble: 6 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 A         2     1    40 A            6\n5 A         1     2    50 A            6\n6 A         2     3    60 A            6\n\n\n\nfilter observations where var3 is less than its mean or var1 is equal to 1\n\n\ndfg |&gt; filter(var3 &lt; mean(var3, na.rm = TRUE) | var1 == 1)\n\n# A tibble: 9 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         1     1    10 A            6\n2 A         2     2    20 A            6\n3 A         1     3    30 A            6\n4 A         2     1    40 A            6\n5 A         1     2    50 A            6\n6 A         2     3    60 A            6\n7 B         1     1    70 B            6\n8 B         1     3    90 B            6\n9 B         1     2   110 B            6\n\n\n\nfilter observations where, for every group in id var3 is larger than its mean\n\n\ndfg |&gt; group_by(id) |&gt; filter(var3 &gt; mean(var3))\n\n# A tibble: 6 × 6\n# Groups:   id [2]\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 A         2     1    40 A            6\n2 A         1     2    50 A            6\n3 A         2     3    60 A            6\n4 B         2     1   100 B            6\n5 B         1     2   110 B            6\n6 B         2     3   120 B            6\n\n\nThis function is often used in conjunction with others. Using the functions that we covered here, you can use filter e.g. to select the largest 5 values in a dataset that meet a number of conditions. For instance, selecting the observations where var1 equals 1 with the highest 5 values for var3 is done using:\n\ndfg |&gt; filter(var1 == 1) |&gt; arrange(desc(var3)) |&gt; slice_head(n = 5)\n\n# A tibble: 5 × 6\n  id     var1  var2  var3 id2ord nobs_id\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;int&gt;\n1 B         1     2   110 B            6\n2 B         1     3    90 B            6\n3 B         1     1    70 B            6\n4 A         1     2    50 A            6\n5 A         1     3    30 A            6\n\n\n\n\n\n\n\n\nYour turn: NYC Flights\n\n\n\n\n\nFirst import the flights dataset from {nycflights23} and assign it to nycflights\n\nnycflights &lt;- nycflights23::flights\n\nThe dataset with all flights departing from airports in New York to destinations in the US is not in your session. You can see that you imported 435352 observations for 19 variables.\nIf necessary, load the {tidyverse} package in your memory:\n\nlibrary(tidyverse)\n\nFirst, let’s focus on the missing values: use rowSums() (see Chapter 4) and filter() to filter the observations with missing values and assign these rows to nycmissing. Determine the number of missing observations:\n\n\nCode\nnycmissing &lt;- nycflights |&gt; filter(rowSums(is.na(nycflights)) &gt; 0)\nnrow(nycmissing)\n\n\n[1] 12534\n\n\nHere, we have missing values. Let’s first try to analyse why they are missing. To do so, we will use nycmissing and try to establish the main reason why there are missing values in these observations. Doing so could potentially enable us to replace these missing values with a value, or to add a column (see after mutate()) that includes some information on these flights with missing data. First let’s ask for a sample of 20 observations for the missing values\n\n\nCode\nnycmissing |&gt; slice_sample(n = 20)\n\n\n# A tibble: 20 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     6    25       NA           1610        NA       NA           1805\n 2  2023     8     7       NA           2125        NA       NA           2245\n 3  2023     4    15       NA           2100        NA       NA           2231\n 4  2023     7    18       NA           1559        NA       NA           1749\n 5  2023     4    21       NA            759        NA       NA           1101\n 6  2023     7    25       NA           2029        NA       NA           2254\n 7  2023     4    15       NA           1705        NA       NA           2016\n 8  2023     7     2       NA           1930        NA       NA           2045\n 9  2023     7    19       NA           1559        NA       NA           1749\n10  2023     7    16       NA            600        NA       NA            859\n11  2023     8     7       NA           1609        NA       NA           1824\n12  2023     6    26     2226           2140        46       NA             58\n13  2023     3    14       NA           1359        NA       NA           1536\n14  2023     9    10       NA           1958        NA       NA           2314\n15  2023     7     9       NA           1517        NA       NA           1634\n16  2023     7    13       NA            805        NA       NA           1057\n17  2023     6    24       NA           2102        NA       NA           2304\n18  2023     9    10       NA            708        NA       NA           1000\n19  2023     6    16       NA           1615        NA       NA           1750\n20  2023     2    27       NA           2159        NA       NA           2301\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFrom this sample, you can see that there are flights where both the departure time (dep_time) and the arrival time arr_time are missing. Because there are no values for these times, there are no values for the departure delay (dep_delay) or (arr_delay). In addition, there are no value for (air_time). It is probably safe to assume that these are all cancelled flights. Determine how many of the flights meet these 5 conditions:\n\n\nCode\nnycmissing |&gt; filter(is.na(dep_time) & is.na(arr_time) & is.na(dep_delay) & is.na(arr_delay) & is.na(air_time))\n\n\n# A tibble: 10,738 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1       NA           2245        NA       NA            146\n 2  2023     1     1       NA           1655        NA       NA           2012\n 3  2023     1     1       NA            920        NA       NA           1202\n 4  2023     1     1       NA            959        NA       NA           1306\n 5  2023     1     1       NA           2030        NA       NA           2334\n 6  2023     1     2       NA           1000        NA       NA           1219\n 7  2023     1     2       NA           1458        NA       NA           1810\n 8  2023     1     2       NA           1125        NA       NA           1407\n 9  2023     1     2       NA           1930        NA       NA           2234\n10  2023     1     2       NA           2259        NA       NA            346\n# ℹ 10,728 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThe results show that 10738 flights were cancelled if we define cancelled as a flight with not values for departure time, arrival time, departure delay, arrival delay and airtime.\nLet’s no focus on the flights that are not cancelled. Filter those flights using the condition that a cancelled flights is a flight with no values on departure time, arrival time, departure delay, arrival delay and air time.\n\n\nCode\nnycmissing |&gt; filter(!(is.na(dep_time) & is.na(arr_time) & is.na(dep_delay) & is.na(arr_delay) & is.na(air_time)))\n\n\n# A tibble: 1,796 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      753            730        23     1211            855\n 2  2023     1     1      949            900        49       NA           1525\n 3  2023     1     1     2000           1959         1     1537           2229\n 4  2023     1     1     2151           2159        -8     1110             12\n 5  2023     1     2      711            715        -4     1313            955\n 6  2023     1     2      738            730         8     1140            919\n 7  2023     1     2      821            800        21     1309           1120\n 8  2023     1     2      831            840        -9     1237           1042\n 9  2023     1     2     1547           1545         2       44           1910\n10  2023     1     2     1700           1645        15     2359           2035\n# ℹ 1,786 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAs could be expected, we have 1796 observations that are not cancelled but have missing observations (total number of missing minus the number of cancelled). Here you can see that there are flights that don’t include a value for arrival time, arrival delay or airtime. First let’s focus on the cases where there are values for departure time and arrival time, but there is no value for arrival delay or airtime. Filter these observations:\n\n\nCode\nnycmissing |&gt; filter((!is.na(dep_time) & !is.na(arr_time) & is.na(arr_delay)) | (!is.na(dep_time) & !is.na(arr_time) & is.na(air_time)))\n\n\n# A tibble: 1,081 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      753            730        23     1211            855\n 2  2023     1     1     2000           1959         1     1537           2229\n 3  2023     1     1     2151           2159        -8     1110             12\n 4  2023     1     2      711            715        -4     1313            955\n 5  2023     1     2      738            730         8     1140            919\n 6  2023     1     2      821            800        21     1309           1120\n 7  2023     1     2      831            840        -9     1237           1042\n 8  2023     1     2     1547           1545         2       44           1910\n 9  2023     1     2     1700           1645        15     2359           2035\n10  2023     1     2     2105           2059         6      122           2325\n# ℹ 1,071 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nThe result shows that there are 1081 flights (the size of the tibble) where there are values for dep_time as well as arr_time. The results also show that there are multiple observations where there is no value for arr_delay or air_time or on both of these variables. Although it is always a good idea to do some additional checks, these results suggest that we can replace some these missing values with their actual arrival delays. All it would take is calculate the difference, in minutes, between the arrival time and the scheduled arrival time. As we have all these values, these calculation would be straightforward.\n\n\nCode\nnycmissing |&gt; filter(!is.na(dep_time) & !is.na(arr_time))\n\n\n# A tibble: 1,081 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      753            730        23     1211            855\n 2  2023     1     1     2000           1959         1     1537           2229\n 3  2023     1     1     2151           2159        -8     1110             12\n 4  2023     1     2      711            715        -4     1313            955\n 5  2023     1     2      738            730         8     1140            919\n 6  2023     1     2      821            800        21     1309           1120\n 7  2023     1     2      831            840        -9     1237           1042\n 8  2023     1     2     1547           1545         2       44           1910\n 9  2023     1     2     1700           1645        15     2359           2035\n10  2023     1     2     2105           2059         6      122           2325\n# ℹ 1,071 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nRecall that the non-cancelled flights also includes observations where there was a departure time but no arrival time. To investigate these case further, filter the observations that meet these two conditions:\n\n\nCode\nnycmissing |&gt; filter(!is.na(dep_time) & is.na(arr_time))\n\n\n# A tibble: 715 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      949            900        49       NA           1525\n 2  2023     1     2     1857           1800        57       NA           2140\n 3  2023     1     2     2315           1829       286       NA           2139\n 4  2023     1     4      950            940        10       NA           1302\n 5  2023     1     4     1141            845       176       NA           1232\n 6  2023     1    11      703            600        63       NA            720\n 7  2023     1    12     2346           2034       192       NA           2359\n 8  2023     1    19     1817           1622       115       NA           1809\n 9  2023     1    19     1918           1929       -11       NA           2109\n10  2023     1    19     2014           2015        -1       NA           2328\n# ℹ 705 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nWe now have 715 observations that meet this condition. Note that in doing so, we account for all non-cancelled flights in the missing data: with 1081 flights were we lack arrival delays or airtimes, here we add another 715 where there is a value for departure time (i.e. the flight left NYC) but here is no value for arrival time (i.e. the flight never arrived according to these records). Before you consider these as “missing” these cases need futher investigation to see if and to what extend you can not “explain” some of these missing values or to investigate if there is not pattern in these observations (e.g. always the same airports, observations clustered in time (suggesting weather conditions), … . ). This dataset could, for instance, include flights that e.g. due to bad weather had to return to return to their departure airport. In other words, they were not cancelled prior return after departing. However, at this stage, it is not possible to do this analysis. Note that these 715 flights account for 0.16% of all observations.\nLets return to nycflights. Answer the following questions:\nFor each of the three NYC airports, use summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)): what is the average departure delay on each of these three airports\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt; \n  summarise(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin mean_dep_delay\n  &lt;chr&gt;           &lt;dbl&gt;\n1 EWR              15.4\n2 JFK              15.9\n3 LGA              10.8\n\n\nUsing the previous result: can you order the tibble with the mean values in ascending order of the mean_dep_delay?\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt; \n  summarise(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(mean_dep_delay) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin mean_dep_delay\n  &lt;chr&gt;           &lt;dbl&gt;\n1 LGA              10.8\n2 EWR              15.4\n3 JFK              15.9\n\n\nCode\n# As R calculates the variable for the summary table, that variable is known to R. In other words\n# you can use it in a subsequent step.\n\n\nFor each of the three NYC airports, what where the top 3 flights with the longest departure delay?\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt; \n  arrange(desc(dep_delay), .by_group = TRUE) |&gt; \n  slice_head(n = 3) |&gt;\n  ungroup()\n\n\n# A tibble: 9 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2023    12    17     1953           1340      1813     2155           1543\n2  2023    10     1     1240            659      1781     1407            835\n3  2023     3    17     2027           1830      1557     2346           2139\n4  2023     4    20      926            619      1627     1135            822\n5  2023     4    30     1818           1617      1561     2001           1820\n6  2023     9    10      949           1100      1369     1316           1406\n7  2023     4    25     1201            659      1742     1315            818\n8  2023     2     7     2045           1700      1665     2352           2025\n9  2023    10    29      856            600      1616     1050            805\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nCode\n# Don't forget: arrange() requires an explicit .by_group = TRUE argument. \n\n\nSome flights depart before their scheduled departure time. For each of the three airports and if a flight departs before schedule: what is the average time flights depart before they are scheduled to (use summarize(mean_dep_neg = mean(dep_delay, na.rm = TRUE))) and show the result in descending order of the mean negative departure delay:\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt; \n  filter(dep_delay &lt; 0) |&gt; \n  summarize(mean_dep_neg = mean(dep_delay, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_dep_neg)) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin mean_dep_neg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 JFK           -5.21\n2 EWR           -5.47\n3 LGA           -6.09\n\n\nFor each of the airlines, what was the longest arrival delay?\n\n\nCode\nnycflights |&gt; \n  group_by(carrier) |&gt; \n  arrange(desc(arr_delay), .by_group = TRUE) |&gt; \n  slice(1) |&gt;\n  ungroup()\n\n\n# A tibble: 14 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     6    25      951           1455      1136     1426           1715\n 2  2023    12    17     1953           1340      1813     2155           1543\n 3  2023     8     9      926           1655       991     1247           1955\n 4  2023     7    19     2304            700       964      348           1058\n 5  2023     9     9     1156           1700      1136     1634           2001\n 6  2023     8    10      617           1056      1161     1009           1328\n 7  2023     7     9      955           1055      1380     1157           1255\n 8  2023     7    14      421           1000      1101      901           1455\n 9  2023    11    26     1611           1330       161     1753           1512\n10  2023     7    28     1005           1859       906     1236           2158\n11  2023     4     4     1243           1400      1363     1517           1548\n12  2023     7    15     1918           1954      1404       42           2353\n13  2023     6    26     2252           1420       512       37           1540\n14  2023     7    14     1232           1730      1142     1448           1926\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nDetermine the average delay (use summarize(ave = mean(dep_delay, na.rm = TRUE))) for flights departing in feburary between 15:00 (including 15:000) and 18:00 (not including 18:00). In other words, how much was the average flight delayed in february if it departed between 15:00:00 and 17:59:59?\n\n\nCode\nnycflights |&gt; \n  filter(month == 2 & hour &gt;= 15 & hour &lt; 18) |&gt; \n  summarise(ave = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n    ave\n  &lt;dbl&gt;\n1  14.6\n\n\nTo illustrate slice_sample() calculate the mean of the arrival delay, arr_delay for the full dataset as well as for 50% of the observations. You can use summarize(ave = mean(arr_delay, na.rm = TRUE)) to calculate the mean.\n\nfor all observations:\n\n\n\nCode\nnycflights |&gt; summarize(ave = mean(arr_delay, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n    ave\n  &lt;dbl&gt;\n1  4.34\n\n\n\nfor 50% of all observations\n\n\n\nCode\nnycflights |&gt; \n  slice_sample(prop = 0.50) |&gt; \n  summarize(ave = mean(arr_delay, na.rm = TRUE))\n\n\n# A tibble: 1 × 1\n    ave\n  &lt;dbl&gt;\n1  4.35\n\n\nAs you can see if you run this code multiple times, the average is around the true average. In some cases, as the dataset becomes very large, using a large enough sample will show an an average wich is very closed to the true value in the dataset.\n\n\n\n\n\n\n8.2.3 Functions on columns\nThe previous functions worked on rows. There are also function what work on columns. Recall that columns in a data frame or tibble are variables and that these can have different types. In other words, one column of variable includes numeric data, another character, boolean or data/time variables or factors. In other words, you can see these columns often as vectors and use functions that were covered in Chapter 4 for each specific type of vector.\nTo illustrate the these column functions, we’ll use the following tibble, which consists of a series of variable names and random values\n\ndfc &lt;- tibble(\n  var1 = sample(1:100, 2),\n  var2 = sample(1:100, 2),\n  var_1 = sample(1:100, 2),\n  var_2 = sample(1:100, 2),\n  new.var_x = round(runif(2), 2),\n  new.var_y = round(runif(2), 2),\n  old.var_x = round(runif(2), 2),\n  old.var_y = round(runif(2), 2),\n  NEW.x = sample(1:100, 2),\n  NEW.y = sample(1:100, 2),\n  OLD.x = sample(1:100, 2),\n  OLD.y = sample(1:100, 2),\n  str_char_1 = c(\"F\", \"F\"),\n  str_char_2 = c(\"Z\", \"W\"),\n  char_var12 = c(\"A\", \"B\"),\n  char_var14 = c(\"A\", \"D\"),\n  X326_TZ_01 = c(F, F),\n  Y236_TZ_02 = c(T, T)\n)\n\nTo show the output of these functions, we willl use str().\n\n8.2.3.1 select()\nThis function is one of the key function of {dplyr}. select() allows you to select or remove columns from your dataset. The arguments of this function include the dataset in .data as well as the columns to select in ....\n\nselect(.data, ...)\n\nThere are multiple ways to identify the columns you want to select. The first is to include them in a c() function using both positions or column names. For instance\n\nselect the first 3 columns of dfc using a range of index positions (consecutive columns):\n\ndfc |&gt; select(1:3) |&gt; str()\n\ntibble [2 × 3] (S3: tbl_df/tbl/data.frame)\n $ var1 : int [1:2] 38 48\n $ var2 : int [1:2] 30 80\n $ var_1: int [1:2] 90 14\n\n\nselect the first three columns using a range of column names (consecutive columns):\n\ndfc |&gt;\n  select(var1:var_1) |&gt; str()\n\ntibble [2 × 3] (S3: tbl_df/tbl/data.frame)\n $ var1 : int [1:2] 38 48\n $ var2 : int [1:2] 30 80\n $ var_1: int [1:2] 90 14\n\n\nselect columns 2, 12 and 14 using index positions (non-consecutive columns):\n\ndfc |&gt;\n  select(c(2, 12, 14)) |&gt; str()\n\ntibble [2 × 3] (S3: tbl_df/tbl/data.frame)\n $ var2      : int [1:2] 30 80\n $ OLD.y     : int [1:2] 36 93\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n\n\nselects columns 2, 12 and 14 using column names (non-consecutive columns)\n\ndfc |&gt; \n  select(c(var2, OLD.y, str_char_2)) |&gt; str()\n\ntibble [2 × 3] (S3: tbl_df/tbl/data.frame)\n $ var2      : int [1:2] 30 80\n $ OLD.y     : int [1:2] 36 93\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n\n\n\nRecall that you can subset data structures using a logical index. Using the &lt;tidy-select&gt; syntax (Henry and Wickham (2024)) there is a straightforward way to select columns. This syntax includes the following functions that allows you to select one or more columns. Each of these functions returns a logical vector equal to TRUE if a condition is met and FALSE if that is not the case. Using that vector, R selects the variables.\n\nstarts_with(match, ignore.case = FALSE, vars = NULL)\nends_with(match, ignore.case = FALSE, vars = NULL)\ncontains(match, ignore.case = FALSE, vars = NULL)\nmatches(match, ignore.case = FALSE, vars = NULL)\nIn all these functions match refers to an expression. If more than one expression is included, they should by included in c(). For the first three functions, the expression is exact (e.g. “var”, “new”, …). Using matchyou can also include regular expressions. The second argument, ignore.case = FALSE is familiar from other character functions. The last argument, vars allows you to include a vector of variable names. By default, the variable names are taken from the dataset.\nnum_range(prefix, range, suffix = \"\", width = NULL, vars = NULL)\nUsing this function you can select variables such as var1x, var2x or x001, x002. To do so, you can include a prefix, e.g. “var”, a suffix (e.g. “x” in var1x) and a numerical range, e.g. 1:2. Adding a width allows you to select columns such as x002 where width in this case is 3: x002 includes three numbers (002).\neveryting(vars = NULL)\nSelect all columns or those in vars.\nlast_col(offset = 0L, vars = NULL)\nSelect the last column by default. If offset is different from zero (e.g. 10), this function selects the 10nd variable from the end. By default, this function selects the last column from the dataset.\ngroup_cols()\nSelect the columns that are used to group a dataset.\nall_of(), any_of(x, vars = NULL)\nHere, x is a vector with variable names. R will select all columns that are included in this vector. The first is strict: if one of the names in the vector doesn’t appear in the data, R will return a error. The second doesn’t. If you add not, !any_of can be used to remove variables from a dataset.\nwhere(fn)\nThe argument fn refers to a function that returns TRUE or FALSE. For instance, to select all numeric columns, where(is.numeric) can be used. Likewise, to select all variables whose minimum is negative: where(\\(x) min(x) &lt; 0) would select these columns.\n\nYou can combine these conditions using boolean &, | or !. Here, we’ll illustrate a couple of examples using dfc. The other functions in this section will further illustrate the use of these &lt;tidy-select&gt; syntax.\n\nselect numeric variables in the dataset:\n\ndfc |&gt;\n  select(where(is.numeric)) |&gt; str()\n\ntibble [2 × 12] (S3: tbl_df/tbl/data.frame)\n $ var1     : int [1:2] 38 48\n $ var2     : int [1:2] 30 80\n $ var_1    : int [1:2] 90 14\n $ var_2    : int [1:2] 75 27\n $ new.var_x: num [1:2] 0.06 0.19\n $ new.var_y: num [1:2] 0.23 0.42\n $ old.var_x: num [1:2] 0.93 0.49\n $ old.var_y: num [1:2] 0.64 0.6\n $ NEW.x    : int [1:2] 36 59\n $ NEW.y    : int [1:2] 29 83\n $ OLD.x    : int [1:2] 25 54\n $ OLD.y    : int [1:2] 36 93\n\n\n\nThis is useful e.g. if you want to caculate summary statistics for all numeric variables. Using this where() statement allows you to select all columns that you can use in e.g. calculate mean or median statistics.\n\nselect numeric variables and add a conditions, e.g. only numeric variabels with a mean &lt; 2\n\ndfc |&gt;\n  select(where(is.numeric)) |&gt; \n  select(where(\\(x) mean(x) &lt; 2)) |&gt; str()\n\ntibble [2 × 4] (S3: tbl_df/tbl/data.frame)\n $ new.var_x: num [1:2] 0.06 0.19\n $ new.var_y: num [1:2] 0.23 0.42\n $ old.var_x: num [1:2] 0.93 0.49\n $ old.var_y: num [1:2] 0.64 0.6\n\n\nNote that here you use \\(x) mean(x). As an alternative you can use ~ mean(.x). We met the former way to referring to functions in Chapter 4. The last approach is also referred to as “purrr” style. The third option would be to write function(x) mean(x).\nselect the columns that end with “_y”\n\ndfc |&gt;\n  select(ends_with(\"y\")) |&gt; str()\n\ntibble [2 × 4] (S3: tbl_df/tbl/data.frame)\n $ new.var_y: num [1:2] 0.23 0.42\n $ old.var_y: num [1:2] 0.64 0.6\n $ NEW.y    : int [1:2] 29 83\n $ OLD.y    : int [1:2] 36 93\n\n\nremove columns (var1, var2, var_1, var_2) using a vector:\n\nrem &lt;- c(\"var1\", \"var2\", \"var_1\", \"var_2\")\ndfc |&gt; \n  select(!any_of(rem)) |&gt; str()\n\ntibble [2 × 14] (S3: tbl_df/tbl/data.frame)\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\n\n\n\n8.2.3.2 rename() and rename_with()\nThe first function, rename() allows you to rename variables (columns) and takes as values\n\nrename(.data, ...)\n\nThe first argument is the dataset. The ... are used to specify the columns to change. The order of the names in this function is new_name = old_name. You can define the new names both within as well as outside of the rename() function. In the latter case, you first define a vector using the new_name = old_name format. This vector can be used inside the rename() function as part of the all_of() or any_of() functions. You use the first if all variables in the vector with new names are also included int he data. If that is not the case and you vector with names could include names that are not part of the dataset, you can use any_of.\nTo illustrate, let’s change the names of columns NEW.x and NEW.y to new.x and new.y:\n\ndfc |&gt; rename(c(new.x = NEW.x, new.y = NEW.y)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ new.x     : int [1:2] 36 59\n $ new.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nThere is another way to do this. Let’s first define a vector change_name and use this vector to set the new_name = old_name. We’ll change the name of OLD.x and OLD.y in very.old.x and very.old.y. To do this with a vector, first create the named vector change_name\n\nchange_name &lt;- c(very.old.x = \"OLD.x\", very.old.y = \"OLD.y\")\n\nNow we will use this vector in rename(). Here we will use all_of(change_name). Doing so, rename() will change the name of all elements in change_name:\n\ndfc |&gt; rename(all_of(change_name)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ very.old.x: int [1:2] 25 54\n $ very.old.y: int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nSuppose that change_name includes variable names that are not in dfc, e.g.\n\nchange_name &lt;- c(very.old.x = \"OLD.x\", very.old.y = \"OLD.y\", very.old.z = \"OLD.z\")\n\nIn that case, rename() with all_of() will not find one of the names (OLD.z) and will show an error:\n\ndfc |&gt; rename(all_of(change_name)) |&gt; str()\n\nError in `rename()`:\nℹ In argument: `all_of(change_name)`.\nCaused by error in `all_of()`:\n! Can't subset elements that don't exist.\n✖ Element `OLD.z` doesn't exist.\n\n\nUsing any_of() solves this issue: rename()will change the name of any of the elements that are in change_name:\n\ndfc |&gt; rename(any_of(change_name)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ very.old.x: int [1:2] 25 54\n $ very.old.y: int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nrename_with() allows you to include a functions to change the name of variables:\n\nrename_with(.data, .fn, .cols = everything(), ...)\n\nIn the function, .fn refers to the function will be used to change the names. The .cols = everything() argument allows you to specify which columns to rename. The default, everyting() applies the function .fn to all columns. You can use the &lt;tidy-select&gt; syntax, e.g. starts_with(), ends_with(), contains(), matches() or num_ranges() or last_col(), group_col(), a range using : or boolean operators. With respect to the function, this function can include any function that operates on a character variable, e.g. toupper, tolower, gsub(), stringr::str_replace() … . A lot of these function were covered in Chapter 3. If the functions include an argument that refers to the dataset, you have to add a ~ before the function and add .x in the function’s argument list.\nA couple of examples:\n\nchange everything to lower:\n\ndfc |&gt;\n  rename_with(tolower, \n              .cols = everything()) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ new.x     : int [1:2] 36 59\n $ new.y     : int [1:2] 29 83\n $ old.x     : int [1:2] 25 54\n $ old.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ x326_tz_01: logi [1:2] FALSE FALSE\n $ y236_tz_02: logi [1:2] TRUE TRUE\n\n\nrename columns whose name starts with char in str_var12 and str_var14:\n\ndfc |&gt;\n  rename_with( ~str_replace(.x, \"char\", \"str\"),\n              .cols = starts_with(\"char\")) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ str_var12 : chr [1:2] \"A\" \"B\"\n $ str_var14 : chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nHere we used the str_replace(string, pattern, replacement) function and introduced the dataset via .x in the function’s string argument. To do so, we used the ~ before the function. As an alternative, we would have used \\(x) str_replace(x, ...). To change only columns that start with char and not those that include it (e.g. str_char_1), you restrict the columns to change using starts_with() to those columns whose names start with “char”.\nreplace all dots in column names with an underscore\n\ndfc |&gt;\n  rename_with( \\(x) gsub(\"\\\\.\", \"_\", x)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new_var_x : num [1:2] 0.06 0.19\n $ new_var_y : num [1:2] 0.23 0.42\n $ old_var_x : num [1:2] 0.93 0.49\n $ old_var_y : num [1:2] 0.64 0.6\n $ NEW_x     : int [1:2] 36 59\n $ NEW_y     : int [1:2] 29 83\n $ OLD_x     : int [1:2] 25 54\n $ OLD_y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nNote the the regular expression for “\\\\.”. Without these escape backslashes, R would interpret the . as any character and will replace any character with an underscore.\nreplace tolower column names who include “TZ” using a regular expression to include the last two column names: X326_TZ_01 and Y236_TZ_02 (and would match any pattern uppercase, 3 digits, underscore, TZ, underscore, two digits):\n\ndfc |&gt;\n  rename_with(tolower, .cols = matches(\"[A-Z][0-9]{3}_TZ_[0-9]{2}\")) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ x326_tz_01: logi [1:2] FALSE FALSE\n $ y236_tz_02: logi [1:2] TRUE TRUE\n\n\nreplace variables var1 and var2 - a numerical range with prefix var - with VAR:\n\ndfc |&gt;\n  rename_with(~str_replace(.x, \"var\", \"VAR\"), \n              .cols = num_range(prefix = \"var\", range = 1:2)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ VAR1      : int [1:2] 38 48\n $ VAR2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\n\n\n\n8.2.3.3 relocate()\nTo change the order of the columns {dplyr} include relocate():\n\nrelocate(.data, ..., .before = NULL, .after = NULL)\n\nFor the columns in .data and identified in ... you can move the column before or after .before or .after. If both these arguments are kept at their default values, columns are moved to the left. For the the identification of the columns and their before .before or .after you can use &lt;tidy-select&gt; syntax: e.g. starts_with(), ends_with(), contains(), matches() or num_ranges() or last_col(), group_col(), a range using : , boolean operators or using the `where()´ function. A couple of examples to illustrate this function:\n\nrelocating the columns starting with oldor OLD before new or NEW:\n\ndfc |&gt;\n  relocate(matches(\"old|OLD\"), .before = matches(\"new|NEW\")) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nrelocate all character columns to the last columns\n\ndfc |&gt;\n  relocate(where(is.character), .after = last_col()) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_x : num [1:2] 0.06 0.19\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_x : num [1:2] 0.93 0.49\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.x     : int [1:2] 36 59\n $ NEW.y     : int [1:2] 29 83\n $ OLD.x     : int [1:2] 25 54\n $ OLD.y     : int [1:2] 36 93\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n\n\nrelocate columns ending with x to the first positions\n\ndfc |&gt; \n  relocate(ends_with(\"x\"), .before = last_col(length(dfc) - 1)) |&gt; str()\n\ntibble [2 × 18] (S3: tbl_df/tbl/data.frame)\n $ new.var_x : num [1:2] 0.06 0.19\n $ old.var_x : num [1:2] 0.93 0.49\n $ NEW.x     : int [1:2] 36 59\n $ OLD.x     : int [1:2] 25 54\n $ var1      : int [1:2] 38 48\n $ var2      : int [1:2] 30 80\n $ var_1     : int [1:2] 90 14\n $ var_2     : int [1:2] 75 27\n $ new.var_y : num [1:2] 0.23 0.42\n $ old.var_y : num [1:2] 0.64 0.6\n $ NEW.y     : int [1:2] 29 83\n $ OLD.y     : int [1:2] 36 93\n $ str_char_1: chr [1:2] \"F\" \"F\"\n $ str_char_2: chr [1:2] \"Z\" \"W\"\n $ char_var12: chr [1:2] \"A\" \"B\"\n $ char_var14: chr [1:2] \"A\" \"D\"\n $ X326_TZ_01: logi [1:2] FALSE FALSE\n $ Y236_TZ_02: logi [1:2] TRUE TRUE\n\n\nTo identify the position before the first column, we use last_col(n) with n the number of columns before the end. Here we define n as the total number of column in the dataset minus 1.\n\n\n\n8.2.3.4 pull()\nThe function pull() is equivalent to subsetting a data frame using the $ subsetting operator. In other words, the function acts as a simplifying subsetting operator. As pull() allows you to use the |&gt; operator, it is often convenient in series of pipes to use this function. The arguments are\n\npull(.data, var = -1, name = NULL, ...)\n\nHere, .data refers to the dataset, var refers to the column to pull from the dataset. You can use both a name as well as an index position. Positive integers count from the left, negative integers from the last column on the right. By default, this function pulls the last column. The function returns an unnamed vector by default (name = NULL). The name argument allows to to create a named vector if you add a column where R can find the names.\nLet’s use this function to pull the first variable from dfc:\n\nusing its name:\n\ndfc |&gt; pull(var = var1)\n\n[1] 38 48\n\n\nusing its position:\n\ndfc |&gt; pull(var = 1)\n\n[1] 38 48\n\n\nadding a name using the values in char_var12:\n\ndfc |&gt; pull(var = var1, name = char_var12)\n\n A  B \n38 48 \n\n\n\n\n\n8.2.3.5 glimpse()\nThe glimpse() function is comparable to str(). It shows all column in a dataset, including their type. Compared to str() glimpse() tries to show as much values as it can. To compare what both return, here we use both with dfg. Note that you can use both functions with the pipe operator.\n\nglimpse(dfg)\n\nRows: 12\nColumns: 6\n$ id      &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\"\n$ var1    &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2\n$ var2    &lt;dbl&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3\n$ var3    &lt;dbl&gt; 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120\n$ id2ord  &lt;ord&gt; A, A, A, A, A, A, B, B, B, B, B, B\n$ nobs_id &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6\n\n\n\nstr(dfg)\n\ntibble [12 × 6] (S3: tbl_df/tbl/data.frame)\n $ id     : chr [1:12] \"A\" \"A\" \"A\" \"A\" ...\n $ var1   : num [1:12] 1 2 1 2 1 2 1 2 1 2 ...\n $ var2   : num [1:12] 1 2 3 1 2 3 1 2 3 1 ...\n $ var3   : num [1:12] 10 20 30 40 50 60 70 80 90 100 ...\n $ id2ord : Ord.factor w/ 2 levels \"B\"&lt;\"A\": 2 2 2 2 2 2 1 1 1 1 ...\n $ nobs_id: int [1:12] 6 6 6 6 6 6 6 6 6 6 ...\n\n\n\n\n\n\n\n\nYour turn: NYC Flights\n\n\n\n\n\nIn case you need to download and install the dataset, you can run. Recall that you need the {tidyverse} package to access {dplyr} functions. If you don’t have it in memory, you can use library(tidyverse). Here, we won’t save results. Doing so, the datasets keeps its original structure for the next excercises.\n\nnycflights &lt;- nycflights23::flights\n\nnyflights includes variables hour, minute and time_hour. Relocate these after day\n\n\nCode\nnycflights |&gt; relocate(c(hour, minute, time_hour), .after = day)\n\n\n# A tibble: 435,352 × 19\n    year month   day  hour minute time_hour           dep_time sched_dep_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;                 &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1    20     38 2023-01-01 20:00:00        1           2038\n 2  2023     1     1    23      0 2023-01-01 23:00:00       18           2300\n 3  2023     1     1    23     44 2023-01-01 23:00:00       31           2344\n 4  2023     1     1    21     40 2023-01-01 21:00:00       33           2140\n 5  2023     1     1    20     48 2023-01-01 20:00:00       36           2048\n 6  2023     1     1     5      0 2023-01-01 05:00:00      503            500\n 7  2023     1     1     5     10 2023-01-01 05:00:00      520            510\n 8  2023     1     1     5     30 2023-01-01 05:00:00      524            530\n 9  2023     1     1     5     20 2023-01-01 05:00:00      537            520\n10  2023     1     1     5     45 2023-01-01 05:00:00      547            545\n# ℹ 435,342 more rows\n# ℹ 11 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;,\n#   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n#   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;\n\n\nRename all columns including “dep” by replacing “dep” by “departure”. Use str() to see if the outcome is as expected.\n\n\nCode\nnycflights |&gt; rename_with(\\(x) str_replace(x, \"dep\", \"departure\"), \n                          .cols = contains(\"dep\")) |&gt; \n  str()\n\n\ntibble [435,352 × 19] (S3: tbl_df/tbl/data.frame)\n $ year                : int [1:435352] 2023 2023 2023 2023 2023 2023 2023 2023 2023 2023 ...\n $ month               : int [1:435352] 1 1 1 1 1 1 1 1 1 1 ...\n $ day                 : int [1:435352] 1 1 1 1 1 1 1 1 1 1 ...\n $ departure_time      : int [1:435352] 1 18 31 33 36 503 520 524 537 547 ...\n $ sched_departure_time: int [1:435352] 2038 2300 2344 2140 2048 500 510 530 520 545 ...\n $ departure_delay     : num [1:435352] 203 78 47 173 228 3 10 -6 17 2 ...\n $ arr_time            : int [1:435352] 328 228 500 238 223 808 948 645 926 845 ...\n $ sched_arr_time      : int [1:435352] 3 135 426 2352 2252 815 949 710 818 852 ...\n $ arr_delay           : num [1:435352] 205 53 34 166 211 -7 -1 -25 68 -7 ...\n $ carrier             : chr [1:435352] \"UA\" \"DL\" \"B6\" \"B6\" ...\n $ flight              : int [1:435352] 628 393 371 1053 219 499 996 981 206 225 ...\n $ tailnum             : chr [1:435352] \"N25201\" \"N830DN\" \"N807JB\" \"N265JB\" ...\n $ origin              : chr [1:435352] \"EWR\" \"JFK\" \"JFK\" \"JFK\" ...\n $ dest                : chr [1:435352] \"SMF\" \"ATL\" \"BQN\" \"CHS\" ...\n $ air_time            : num [1:435352] 367 108 190 108 80 154 192 119 258 157 ...\n $ distance            : num [1:435352] 2500 760 1576 636 488 ...\n $ hour                : num [1:435352] 20 23 23 21 20 5 5 5 5 5 ...\n $ minute              : num [1:435352] 38 0 44 40 48 0 10 30 20 45 ...\n $ time_hour           : POSIXct[1:435352], format: \"2023-01-01 20:00:00\" \"2023-01-01 23:00:00\" ...\n\n\nWhich routes (origin-destination pair) are very busy (i.e. have most flights)? Show the number of routes per origin-destination pair. Show the busiest routs on top.\n\n\nCode\nnycflights |&gt; count(origin, dest, sort = TRUE)\n\n\n# A tibble: 251 × 3\n   origin dest      n\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n 1 JFK    LAX   10045\n 2 LGA    ORD    9923\n 3 LGA    BOS    8217\n 4 LGA    ATL    7883\n 5 JFK    SFO    7440\n 6 EWR    MCO    7262\n 7 JFK    BOS    6432\n 8 LGA    DFW    5972\n 9 JFK    MIA    5930\n10 EWR    ATL    5915\n# ℹ 241 more rows\n\n\nAdd a column to nycflights with the number of observations (flights) for every origin-destination pair. Name this column n_flights_route (number of flights per route). Don’t sort the result.\n\n\nCode\nnycflights &lt;- nycflights |&gt; add_count(origin, dest, name = \"n_flights_route\")\n\n\nLet’s focus on the delays. Select all variables that include delay, group by carrier and filter all observations with a departure delay of more than 300 hours and select the highest arrival delays and show the results for each carriers with only the arrival delay column and return a tibble that shows: first the carrier and then the arrival delay.\n\n\nCode\nnycflights |&gt; \n  select(c(dep_delay, arr_delay, carrier)) |&gt;  # you need to include carriers to use it in group_by\n  group_by(carrier) |&gt;  \n  filter(dep_delay &gt; 300) |&gt; \n  arrange(desc(arr_delay), .by_group = TRUE) |&gt;  # recall, the `by_group = TRUE`\n  slice(1) |&gt; \n  select(arr_delay, carrier) |&gt;\n  relocate(arr_delay, .after = carrier) |&gt; # show the columns as you want them to appear\n  ungroup()\n\n\n# A tibble: 13 × 2\n   carrier arr_delay\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 9E           1271\n 2 AA           1812\n 3 AS           1012\n 4 B6           1010\n 5 DL           1233\n 6 F9           1241\n 7 G4           1382\n 8 HA           1086\n 9 NK            878\n10 OO           1409\n11 UA           1489\n12 WN            537\n13 YX           1162\n\n\n\n\n\n\n\n\n8.2.4 Summarizing data: summarize() or summarise()\nsummarize() and summarise() are synonyms and you can use them interchangeably. This function allows you to calculate summary statistics for one or more variables in a dataset. If the data is grouped, these statistics are calculated by group. The function returns a new data frame. In other words, you can use the output to e.g. visualize the data to subsequent data transformations (using e.g. arrange(), slice(), …)\nThe function’s arguments are\n\nsummarize(.data, ..., .by = NULL, .groups = NULL)\n\nHere, .data refers to the dataset, .by to the selection of columns that will be used to group, but just for this function and .groups refers to the grouping structure of the result. If this argument is drop, the grouping structure is dropped, keep keeps the grouping structure of the data and drop_last drops the last level of grouping. If this is not specified, and there is only one row in the results, this arguments is set to drop_last, else to keep.\nThe ... is where you include the summary statistics you want to calculate. To do so, you include a name for the summary statistic as well as the function. With respect to these function, you will recognize most as they were covered in e.g. Chapter 4 when we discusses statistical functions:\n\ncentral tendency and location\n\nmean(), median(): mean and median\nquantile(): quantiles of the distribution\n\ndispersion\n\nrange(), min(), max(): minimum and maximum values\nIQR(): interquartile range (third minus first quartile)\nsd(), var(): standard deviation and variance\n\ncount data\n\nn(): total number of observations\nn_distinct() : total number of unique observations\n\nother functions\n\nsum(): sum of all values\nprod(): product of all values\nfirst(), last(), nth(): first, last and nth element\n\nstatistical functions:\n\npnorm(q), pt(q), … to calculate the probability that the value of an outcome is smaller than or equal to q\ndnorm(q), dt(q), … to calculate the probability that a value is equal to q\nqnorm(p), qt(p), … to calculate the value for which holds that its probability is smaller than or equal to q\n\n\nFor each function, you can add the arguments for that function (e.g. na.rm = TRUE, trim or the probs = c(0.10, 0.25, 0.50, 0.75, 0.90) for the quantiles.\nTo illustrate how you can use this function to summarize your data, we’ll use dfs:\n\ndfs &lt;- tibble(\n  vgroup = c(rep(\"A\", 600), rep(\"B\", 400)),\n  vsgroup = sample(c(1, 2, 3), 1000, replace = TRUE),\n  vnor52 = rnorm(1000, 5, 2),\n  vunf01 = runif(1000, 0, 100)\n)\n\nwhich includes a grouping variable vgroup with values A and B, a subgrouping variable, vsgroup with values 1, 2 and 3 and two random draws from a distribution: vnor52 a normal distribution with mean 5 and standard deviation 2 and vunf01 a uniform distribution with minimum 0 and maximum 1.\nLet’s first use the full dataset and calculate the mean and standard deviation for vnor52, the variable that was drawn from a normal distribution with mean 5 and standard deviation 2:\n\ndfs |&gt; summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52))\n\n# A tibble: 1 × 2\n  meanv1  sdv1\n   &lt;dbl&gt; &lt;dbl&gt;\n1   4.98  2.00\n\n\nThe code shows that you can include as many summary statistics as you like, both for one as well as for more variables. The key here is that you add a name to each statistics and that you separate them with a comma. For instance, adding the mean and standard deviation for vunf01:\n\ndfs |&gt; summarize(\n  meanv1 = mean(vnor52), \n  meanv2 = mean(vunf01),\n  sdv1 = sd(vnor52),\n  sdv2 = sd(vunf01))\n\n# A tibble: 1 × 4\n  meanv1 meanv2  sdv1  sdv2\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   4.98   50.4  2.00  28.3\n\n\nThe function returns a tibble, where each column shows one of the values for the summary statistics we asked for and the variable names are the names we set in the summarize() call. The columns are ordered in line with the summary statistics in the summarize() function: meanv1, meanv2, sdv1 and sdv2.\nIf we add a group,\n\ndfs |&gt; group_by(vgroup) |&gt;\n  summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52))\n\n# A tibble: 2 × 3\n  vgroup meanv1  sdv1\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 A        4.95  1.98\n2 B        5.03  2.04\n\n\nR adds one line per group and shows the variables that was used to group in the first column. With subgroups,\n\ndfs |&gt; group_by(vgroup, vsgroup) |&gt;\n  summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52))\n\n`summarise()` has grouped output by 'vgroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 4\n# Groups:   vgroup [2]\n  vgroup vsgroup meanv1  sdv1\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            1   5.03  2.04\n2 A            2   4.91  2.00\n3 A            3   4.91  1.89\n4 B            1   5.11  2.03\n5 B            2   5.18  2.01\n6 B            3   4.80  2.09\n\n\nR shows one summary statistics per subgroup. The tibble is grouped for vsgroup and not for vgroup. In other words, and in line with the default for .groups() R dropped the last group, in this case the subgroup. To keep the group structure, you can add .groups = \"keep\":\n\ndfs |&gt; group_by(vgroup, vsgroup) |&gt;\n  summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52), .groups = \"keep\")\n\n# A tibble: 6 × 4\n# Groups:   vgroup, vsgroup [6]\n  vgroup vsgroup meanv1  sdv1\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 A            1   5.03  2.04\n2 A            2   4.91  2.00\n3 A            3   4.91  1.89\n4 B            1   5.11  2.03\n5 B            2   5.18  2.01\n6 B            3   4.80  2.09\n\n\nThe difference between \"drop_last\" and \"keep\" is relevant is you want to add further calculations. For instance, suppose that you are interested in the maximum mean per subgroup. Here, the default for .groups would show\n\ndfs |&gt; group_by(vgroup, vsgroup) |&gt;\n  summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52)) |&gt;\n  summarize(max_mean = max(meanv1))\n\n`summarise()` has grouped output by 'vgroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 2\n  vgroup max_mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 A          5.03\n2 B          5.18\n\n\nIn other words, it shows the maximum value for the mean across each of the 3 subgroups in group A and group B. With \"keep\", the results keep their group structure. In doing so, the following code wouldn’t add any value:\n\ndfs |&gt; group_by(vgroup, vsgroup) |&gt;\n  summarize(\n  meanv1 = mean(vnor52), \n  sdv1 = sd(vnor52), .groups = \"keep\") |&gt;\n  summarize(max_man = max(meanv1))\n\n`summarise()` has grouped output by 'vgroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   vgroup [2]\n  vgroup vsgroup max_man\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 A            1    5.03\n2 A            2    4.91\n3 A            3    4.91\n4 B            1    5.11\n5 B            2    5.18\n6 B            3    4.80\n\n\nAs there is only one mean value per subgroup, the maximum equals that value.\nLet’s calculate the probability that the mean of vnor52 is less than or equal to 5 if drawn from a normal distribution with mean 5 and standard deviation 2. To do so, we add the function mean(vnor52) as one of the arguments in the pnorm() function. Here, R will first calculate the mean for vnor52 and in a second step, determine the probability that you would have a value which is less than or equal to this mean for a normal distribution with mean 5 and standard deviation 2:\n\ndfs |&gt;\n  summarize(\n    probmeanv2 = pnorm(mean(vnor52), mean = 5, sd = 2)\n  )\n\n# A tibble: 1 × 1\n  probmeanv2\n       &lt;dbl&gt;\n1      0.496\n\n\nDoing so by group, would show this probability per group or subgroup.\nThe mean, standard deviation or a probability are all functions that return one output. A statistic such as range() returns 2 values and quantile() functions with probs = c(0.10, 0.25, 0.50, 0.75, 0.90 return 5. Let’s use these in summarize():\n\ndfs |&gt;\n  summarize(\n    rv1 = range(vnor52))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 2 × 1\n    rv1\n  &lt;dbl&gt;\n1 -1.26\n2 11.5 \n\n\nWith more than two arguments, the tibble shows more rows:\n\ndfs |&gt;\n  summarize(\n    qv1 = quantile(vnor52, probs = c(0.10, 0.25, 0.50, 0.75, 0.90)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 5 × 1\n    qv1\n  &lt;dbl&gt;\n1  2.45\n2  3.60\n3  4.95\n4  6.38\n5  7.53\n\n\nAlthough summarize() returns output in both cases where the summary statistics returns more than one value. However, if summary statistics include more than one row, R advises to use reframe() and not summarize(). This function returns the same output but will never return a grouped tibble and can show multiple rows per group. For instance:\n\ndfs |&gt; \n  reframe(\n    qv1 = quantile(vnor52, probs = c(0.10, 0.25, 0.50, 0.75, 0.90)))\n\n# A tibble: 5 × 1\n    qv1\n  &lt;dbl&gt;\n1  2.45\n2  3.60\n3  4.95\n4  6.38\n5  7.53\n\n\nNote that both summarize nor deframe can show output where one statistics has more rows than another.\nTo avoid deframe() you define your test statistics in the summarize() function in a way that they produce one return value. For instance, both min() and max() produce one statistic each, while range produces 2, qvu10 = quantile(vunf01, probs = c(0.10)) and qu90 = quantile(vunf01, probs = c(0.90)) return two individual statistics.\nWe calculated the mean for both vnor52and vunf01. However, in the code, we included all means separately. With a limited number of variables, that is not too demanding. However, as the number of variables in the dataset expands, it is less evident to include of all of these statements in separate lines of code. Using across() you can automate that process:\n\nacross(.cols, .fns, ..., .names = NULL, .unpack = FALSE)\n\nIn this function, .cols() includes the columns to summarize. Here, you can use &lt;tidy-select&gt; syntax (starts_with, ends_with, contains, matches, where, …). The function or functions in .fns can refer to a build in function, a lambda function (e.g. ~ function (.x) or \\(x) function(x)) or a list with functions (e.g. list(ave = mean, stdev = sd, sum(is.na(.x)))). R applies these function all all columns identified in .cols. The .names argument allows you to specify how these columns will be included in the output of the functions. By default, R used {.col}_{.fn} where the first component is the column name and the second is the function name. If there is only one function in .fns, R drops this last component.\nUsing across(), we can now summarize the dataset dfs across all columns in .cols. Let’s start with one function, mean and apply that function to both vnor52 and vunf01. In .cols we can include these variables using c(\"vnor52\", \"vunf01\") or using matches(\"[a-z]+\\\\d{2}). We want to return the results using ave_columnname, e.g. ave_vnor52, … . To do so, we include .names = \"ave_{.col}\":\n\ndfs |&gt;\n  summarize(\n    across(matches(\"[a-z]+\\\\d{2}\"), \n           mean, \n           .names = \"ave_{.col}\"\n    )\n  )\n\n# A tibble: 1 × 2\n  ave_vnor52 ave_vunf01\n       &lt;dbl&gt;      &lt;dbl&gt;\n1       4.98       50.4\n\n\nIncluding both the mean and the standard deviation requires a list with functions: list(mean, sd). We want to show the names of the results as function_of_cols, i.e. as {.fn}_of_{.cols}:\n\ndfs |&gt;\n  summarize(\n    across(matches(\"[a-z]+\\\\d{2}\"), \n           list(ave = mean, stdev = sd), \n           .names = \"{.fn}_of_{.col}\"\n    )\n  )\n\n# A tibble: 1 × 4\n  ave_of_vnor52 stdev_of_vnor52 ave_of_vunf01 stdev_of_vunf01\n          &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1          4.98            2.00          50.4            28.3\n\n\nThe return show that R applies all functions to the first variable, then to the second, … . The results in the tibble show this sequence. The first two columns refers to the output with applied to vnor52, the third and fourth column to vunf01, … .\nUsing a lambda function, you can add further summary statistics. Often you want to now the number of missing values. is.na() is TRUE is a value is missing and FALSE if that is not the case. Applied to a column, sum(is.na) equals the number of values for which is.na() returned TRUE (i.e. was missing). Here, we need to use a lambda function as R needs to now where to put the column name in a nested functions such as sum(is.na()). We can do so using \\(x) sum(is.na(x)) of ~ sum(is.na(.x)):\n\ndfs |&gt;\n  summarize(\n    across(matches(\"[a-z]+\\\\d{2}\"), \n           \\(x) sum(is.na(x)), \n           .names = \"n_na_{.col}\"\n    )\n  )\n\n# A tibble: 1 × 2\n  n_na_vnor52 n_na_vunf01\n        &lt;int&gt;       &lt;int&gt;\n1           0           0\n\n\n\n\n\n\n\n\nYour turn: NYC Flights\n\n\n\n\n\nIn case you need to download and install the dataset, you can run.\n\nnycflights &lt;- nycflights23::flights\n\nRecall that you need the {tidyverse} package to access {dplyr} functions. If you don’t have it in memory, you can use library(tidyverse). Here, we won’t save results. Doing so, the datasets keeps its original structure for the next exercises.\nWe used summarize() in the “Your turn” with e.g. filter(). Here, we also use both to show summary statistics for the flights departing from a New York airport. Recall that the dataset includes missing values. In other words, don’t forget na.rm = TRUE in e.g. mean(), sd(), … or filter the rows first to eliminate all missing values.\nHow many fights departed from each of the three NYC airports? Store you result in total flights\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt;\n  summarize(total_flights = n())\n\n\n# A tibble: 3 × 2\n  origin total_flights\n  &lt;chr&gt;          &lt;int&gt;\n1 EWR           138578\n2 JFK           133048\n3 LGA           163726\n\n\nHow many flights departed per month from JFK? Store this result as tot_m_jfk.\n\n\nCode\nnycflights |&gt; \n  group_by(month) |&gt; \n  filter(origin == \"JFK\") |&gt; \n  summarize(tot_m_jfk = n())\n\n\n# A tibble: 12 × 2\n   month tot_m_jfk\n   &lt;int&gt;     &lt;int&gt;\n 1     1     10918\n 2     2     10567\n 3     3     12158\n 4     4     11638\n 5     5     11822\n 6     6     11014\n 7     7     11188\n 8     8     11130\n 9     9     10760\n10    10     10920\n11    11     10520\n12    12     10413\n\n\nOn average, which 5 months are the busiest (defined as the number of departing flights) on La Guardia Airport (LGA)? Use tot_m_lga to save your summary statistic.\n\n\nCode\nnycflights |&gt;\n  group_by(month) |&gt;\n  filter(origin == \"LGA\") |&gt;\n  summarize(tot_d_lga = n()) |&gt; \n  arrange(desc(tot_d_lga)) |&gt;\n  slice_head(n = 5)\n\n\n# A tibble: 5 × 2\n  month tot_d_lga\n  &lt;int&gt;     &lt;int&gt;\n1     3     14763\n2     5     14517\n3     8     14074\n4    10     13861\n5     4     13816\n\n\nYou have three airports in New York. Is there a difference in terms of the median distance (distance) for flights departing from each of them (i.e. are some airports used for long distance flights, while other are used for regional flights)? Store your statistic in med_dis.\n\n\nCode\nnycflights |&gt;\n  group_by(origin) |&gt;\n  summarize(med_dis = median(distance, na.rm = TRUE))\n\n\n# A tibble: 3 × 2\n  origin med_dis\n  &lt;chr&gt;    &lt;dbl&gt;\n1 EWR        937\n2 JFK        944\n3 LGA        733\n\n\nFor the departure delay, show the minimum, maximum, median and mean value for every airport in NYC. Use max_dep_delay, min_dep_delay, med_dep_delay and ave_dep_delay to store these statistics. Arrange in ascending order of origin.\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt; \n  summarize(\n    max_dep_delay = max(dep_delay, na.rm = TRUE),\n    min_dep_delay = min(dep_delay, na.rm = TRUE),\n    med_dep_delay = median(dep_time, na.rm = TRUE),\n    ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(origin)\n\n\n# A tibble: 3 × 5\n  origin max_dep_delay min_dep_delay med_dep_delay ave_dep_delay\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 EWR             1813           -29          1401          15.4\n2 JFK             1627           -50          1422          15.9\n3 LGA             1742           -33          1346          10.8\n\n\nRewrite the code and avoid the addition of na.rm = TRUE.\n\n\nCode\nnycflights |&gt; \n  filter(rowSums(is.na(nycflights)) == 0) |&gt;  \n  group_by(origin) |&gt; \n  summarize(\n    max_dep_delay = max(dep_delay),\n    min_dep_delay = min(dep_delay),\n    med_dep_delay = median(dep_time),\n    ave_dep_delay = mean(dep_delay)) |&gt; \n  arrange(origin)\n\n\n# A tibble: 3 × 5\n  origin max_dep_delay min_dep_delay med_dep_delay ave_dep_delay\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 EWR             1813           -29          1400          15.2\n2 JFK             1627           -50          1422          15.7\n3 LGA             1742           -33          1346          10.7\n\n\nCode\n# you filter rows excluding those with missing data using rowSums(is.na(nycflights) == 0)\n\n\nUse across() to produce the same table for both dep_delay and arr_delay. Show the result as function_column:\n\n\nCode\nnycflights |&gt; \n  filter(rowSums(is.na(nycflights)) == 0) |&gt; # \n  group_by(origin) |&gt; \n  summarize(\n    across(c(dep_delay, arr_delay), \n           list(max = max, min = min, med = median, ave = mean), \n           .names = \"{.fn}_{.col}\"\n    )) |&gt;\n  arrange(origin)\n\n\n# A tibble: 3 × 9\n  origin max_dep_delay min_dep_delay med_dep_delay ave_dep_delay max_arr_delay\n  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 EWR             1813           -29            -2          15.2          1812\n2 JFK             1627           -50            -2          15.7          1633\n3 LGA             1742           -33            -4          10.7          1737\n# ℹ 3 more variables: min_arr_delay &lt;dbl&gt;, med_arr_delay &lt;dbl&gt;,\n#   ave_arr_delay &lt;dbl&gt;\n\n\nCarriers operate of all three airports. Is there is difference between the number of carriers on each of these three New York airports? Store your result in n_carrier. Make sure that the results are ordered from low to high.\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt;\n  summarize(n_carrier = length(unique(carrier))) |&gt;\n  arrange(n_carrier)\n\n\n# A tibble: 3 × 2\n  origin n_carrier\n  &lt;chr&gt;      &lt;int&gt;\n1 JFK            8\n2 LGA           10\n3 EWR           11\n\n\nCalculate the number of flights for each of the carriers in the first quarter of 2023. Store your statistic in n_q1 (hint: the first quarter includes January, February and March). Sort your result ascending.\n\n\nCode\nnycflights |&gt; \n  group_by(carrier) |&gt;\n  filter(month &lt;= 3) |&gt;\n  summarize(n_q1 = n()) |&gt;\n  arrange(n_q1)\n\n\n# A tibble: 14 × 2\n   carrier  n_q1\n   &lt;chr&gt;   &lt;int&gt;\n 1 MQ         74\n 2 HA         90\n 3 G4        130\n 4 F9        269\n 5 OO       1503\n 6 AS       1763\n 7 WN       3054\n 8 NK       3474\n 9 AA      10599\n10 9E      12661\n11 DL      14576\n12 B6      18193\n13 UA      20311\n14 YX      23598\n\n\nProduce a table which shows the number of different destinations (i.e. routes) per airport. Store this is n_route and arrange the tibble ascending. Are some airports servicing more destinations than others?\n\n\nCode\nnycflights |&gt; \n  group_by(origin) |&gt;\n  summarize(n_route = length(unique(dest))) |&gt;\n  arrange(n_route) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin n_route\n  &lt;chr&gt;    &lt;int&gt;\n1 JFK         71\n2 LGA         88\n3 EWR         92\n\n\n\n\n\n\n\n8.2.5 Creating new variables: mutate()\n\n8.2.5.1 The function\nThere are two ways to add variables to a dataset: one is to create them using existing variables and the second is by adding columns from another dataset. mutate() allows you to do the first. The next section will introduce the second. This function includes the following arguments:\n\nmutate(\n  .data,\n  ...,\n  .by = NULL,\n  .keep = c(\"all\", \"used\", \"unused\", \"none\"),\n  .before = NULL,\n  .after = NULL\n)\n\nThe first argument .data includes the dataset. The ... the formula to compute the new variable. Using .by you can define groups only for this operation. The .keep argument allows you to specify what should happen with the columns in .data: should they all be kept (this is the default “all”), should, in addition to the newly created ones, only the columns used in ... be kept (“used”), should, in addition to the newly created ones, the columns not used in ... be kept(“unused”) or should only the new columns be kept (in addition to grouping variable “none”)? The first is the default. The second is useful for checking you work. In that case, the mutate() functions shows only the columns you used and the column you created in one data frame. Doing so, you can easily verify if the results in the new columns are correct. The third option is useful when you don’t need the “old” column in your work. For instance, suppose that you will only with rescaled values, then you can keep your dataset compact of you only include those in the result and skip the unscaled variables. Here, you will keep all variables that you didn’t scale in your dataset. The last option is useful when you will only work with the new variables and only those. The last two options allow you to specify where R will add the columns. By default, R adds new column to the right hand side. Using .before or .after you can specify an alternative location. This option can be useful is you want to keep similar variables close to one another. For instance, if you use mutate() to calculate the lag of a variable, it is often convenient to include this lagged value right after the non lagged one.\nIf you include multiple calculation in the ... part, the result of a prior calculation is available for a subsequent one.\n\n\n8.2.5.2 Useful functions\nUsing mutate(), you can use most functions that we have covered so far in Chapter 3 and Chapter 4. Recall that all these function are vectorized and work on an element by element basis. As you can see a data frame as a collection of vectors, these functions will operate on the columns of a data frame - the variables - in the same way as they do for individual vectors. In other words, you can use all mathematical operations, log(), abs(), exp(), pnorm(), pt(), cumsum(), cumprod(), cummax(), cummean() if the variable is numeric, {stringr} functions, paste(), paste0(), grep() or grepl() with character variables, {lubridate} functions for data/time data or {forcats} functions for factors . In addition to these base R functions, {dplyr} add a couple of functions. To illustrate how these work, I’ll use a the following vector:\n\nv1 &lt;- c(1, 2, 8, 8, -4, -3, -45, -4, 15, 22)\n\n\ncummean(): calculates the cumulative mean. The first value of the output is the first value of the vector. The second value is the sum of the first and second value in the vector, divided by 2; the third value if the sum of the first three values in the vector, divided by three, … . In other words, cummean() equals cumsum() divided by the index position:\n\n\ncummean(v1)\n\n [1]  1.000000  1.500000  3.666667  4.750000  3.000000  2.000000 -4.714286\n [8] -4.625000 -2.444444  0.000000\n\n\n\ncumany(x), where x is a logical condition: the function returns FALSE until the first element where it is TRUE, after that, it continues to return TRUE. Using this function with the condition v1 &lt; 0 shows FALSE for the first 4 positions. On the fifth position the condition is met and the function returns TRUE after position 4. Note that !x returns TRUE and FALSE in the opposite case.\n\n\ncumany(v1 &lt; 0)\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nThis function is useful to help filter observations. Suppose for instance that you have a data frame with the valuation of companies. You are interested in the return for companies since they reached a total valuation of usd 1 billion. Using cumany(value &gt;= 1 billion) you create a vector which is FALSE as long as the valuation of a of company was less than usd 1 billion and TRUE for all observations since the first time the value reached your threshold, even if the value fall back below that level. Using filter(cumany(value &gt;= 1 billion)) filters the dataset and returns the dataset that you need to answer you question: what is the average return for a company since it reached the usd 1 billion values for the first time.\n\ncumall(x), where x is a logical condition: the function returns TRUE as long as a condition is met. Upon the first FALSE, the function continues to return FALSE after that. With the condition v1 &gt; 5 cumall() will be TRUE for the first two cases as 1 and 2 are smaller than 5. On the third position, this condition is violated once and the function returns FALSE from that position onward. In other words, it shows how long a specific condition is met.\n\n\ncumall(v1 &lt; 5)\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSuppose that a course is offered fully online. Video’s are posted every week on Monday. Initially, a lot of students watch the video that day. However every week, the number of students who do so later in the week or who don’t do so at all rises. The university wants to know if students who didn’t watch the Monday’s video for the entire week will, from that week onward, catch up, stay behind schedule or decide to skip more weeks. Using cumall() the university would be able to select all weeks after a student decided not to watch that Monday’s video. The condition “did the student watch that week” would be TRUE as long as the student did, but return to FALSE from the first week that the student didn’t. Using that condition, the university could filter students who failed to watch and analyse if and to what extent they, for instance, catch up, continue to run one video behind schedule or would skipp more video’.\nlead(x, n = k) and lag(x, n = k): by default k = 1 the function returns by default, the next (lead()) or previous (lag()) position. With k &gt; 1 the functions returns the value k positions forward or backward. With the default value, the last position with lead() and the first position with lag() will be missing.\n\nlead(v1)\n\n [1]   2   8   8  -4  -3 -45  -4  15  22  NA\n\nlag(v1)\n\n [1]  NA   1   2   8   8  -4  -3 -45  -4  15\n\n\nNote that these functions are usually only relevant in case your data are ordered by time. Without time, the lead or lag values are usually not informative. In a dataset with marks for a course where students are ordered by R number, the lead would be the mark of the student whose R number follows and the lag would be the mark of the student whose R number lags one or more positions. Unless the R number is informative, this wouldn’t add much value. However, is the dataset is ordered using the time a student spends preparing and attending class, a lead or lag would.\n\nrow_number(), min_rank(), dense_rank() give a rank number to each row ordering data from small to high. The three function differ in how they handle ties: the first will rank ties different where the difference follows the order in which a value appears in the data. For instance, with v1, 8 on the third and fourth position and -4 on the fifth and eight position have different rank, where e.g. the rank of -4 on the fifth position 2 (second smallest) which rank of -4 on the eight position if 3. The second and third set ties equal. The second will not reduce the number of ranks (e.g. will show 1, 2, 2, 4, … 10), while the third will leave no gap (1, 2, 2, 3, … 8).\n\n\nrow_number(v1)\n\n [1]  5  6  7  8  2  4  1  3  9 10\n\nmin_rank(v1)\n\n [1]  5  6  7  7  2  4  1  2  9 10\n\ndense_rank(v1)\n\n [1] 4 5 6 6 2 3 1 2 7 8\n\n\nIn addition, there are a couple of vectorized if then else functions. These functions check is a condition after if is met, is that is the case, they proceed with the code after then. If the condition is not met, they move to the code after else. A vectorized if then else functions avoids a for loop. Using such a loop, you would write code where R starts at the first observation in a variable, checks the condition, and returns a value, then moves to the second observation, checks the condition and returns a value, … . In other words, it would require something as `for (observations in dataset) {if condition the else}.\n\nif_else()\n\n\nif_else(condition, true, false, missing = NULL, ...)\n\nThe function tests if the condition in condition is met. If that is the case, and the condition returns TRUE, the part in true is completed. The the condition is FALSE, the part in false is completed. If the condition includes missing values, missing = NULL allows you to add values for that case. For instance:\n\nif_else(v1 &gt; 0, \"pos\", \"neg\")\n\n [1] \"pos\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"pos\"\n\n\nUsing this function, you can avoid a loop for (i in 1:length(v1))....\n\ncase_when()\n\n\ncase_when(..., .default = NULL, .ptype = NULL, .size = NULL)\n\nThis function allows you to include multiple if_else() statements in the ... part. This part includes the condition and the result if the condition is met using condition ~ result. If none of the conditions is met, .default = NULL shows what the function should return. By default it returns noting (i.e. a missing value). This function avoids statements that include multiple if then else statement: if … then … else(if .. then .. else [if . then . else]) where if then else statements are nested in the else condition. Here, you can add these conditions in .... For instance, for v1 if a value is negative, then it should be “neg” else, if it is smaller than 10 it should be “ten” else if it is smaller than 20 if should be “twenty” and else “thirty”:\n\nv1 &lt;- c(1, 2, 8, 8, -4, -3, -45, -4, 15, 22)\ncase_when(\n  v1 &lt; 0 ~ \"neg\",\n  v1 &lt; 10 ~ \"ten\", \n  v1 &lt; 20 ~ \"twenty\",\n  v1 &gt;= 20 ~ \"thirty\",\n  .default = NA\n)\n\n [1] \"ten\"    \"ten\"    \"ten\"    \"ten\"    \"neg\"    \"neg\"    \"neg\"    \"neg\"   \n [9] \"twenty\" \"thirty\"\n\n\nWith if_else() the similar result would require:\n\nv1 &lt;- c(1, 2, 8, 8, -4, -3, -45, -4, 15, 22)\nif_else(v1 &lt; 0, \"neg\", \n        if_else(v1 &lt; 10, \"ten\", \n                if_else(v1 &lt; 20, \"twenty\", \"thirty\")))\n\n [1] \"ten\"    \"ten\"    \"ten\"    \"ten\"    \"neg\"    \"neg\"    \"neg\"    \"neg\"   \n [9] \"twenty\" \"thirty\"\n\n\n\ncase_match()\n\ncase_when() uses a logical condition, this function uses a value. For all the values in.x, if finds (if any) the matching value and its alternative in .... If a match is found, the function changes the value in .x. If there is no match, .default shows the alternative. This alternative is missing is no alternative is offered.\n\ncase_match(.x, ..., .default = NULL, .ptype = NULL)\n\nTo illustrate:\n\nv2 &lt;- c(\"a\", \"b\", \"a\", \"b\", \"c\", \"e\")\n\ncase_match(v2,\n  \"a\" ~ 10, \n  \"b\" ~ 20,\n  \"c\" ~ 30,\n  .default = 100\n)\n\n[1]  10  20  10  20  30 100\n\n\nAs you can see, R sets all values “a” equal to 10, “b” equal to 20 and “c” equal to 30. As there is no value for “e”, R uses the default. In this case, the default is 100. With .default = NULL this would be NA. This function is useful to e.g. recode factor levels.\n\n\n8.2.5.3 Examples\nTo illustrate the use of this function, we’ll use a tibble dfm which includes a numeric and character variables, both with integer and double as well as factors:\n\ndfm &lt;- tibble(\n  var_1s = sample(1:100, 1000, replace = TRUE), \n  var_2s = sample(letters, 1000, replace = TRUE),\n  var1d = runif(1000, 5, 10),\n  var2d = rnorm(1000, 5, 5),\n  fac_15 = as.factor(sample(1:5, 1000, replace = TRUE)),\n  char_XX = sample(c(LETTERS, letters), 1000, replace = TRUE)\n)\n\nFirst, we’ll use the fill dataset dfm and calculate and calculate the absolute value of var1d and var2d and the ratio of both var1d/var2d. We’ll keep the default .keep and add these values after var2d:\n\ndfm |&gt;\n  mutate(\n    absvar1d = abs(var1d), \n    absvar2d = abs(var2d),\n    rabs12d = absvar1d/absvar2d, \n    .after = var2d\n  )\n\n# A tibble: 1,000 × 9\n   var_1s var_2s var1d var2d absvar1d absvar2d rabs12d fac_15 char_XX\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;  \n 1     69 d       9.08  1.98     9.08     1.98   4.59  3      a      \n 2     66 w       8.55  8.42     8.55     8.42   1.02  5      e      \n 3     36 d       6.14 12.8      6.14    12.8    0.480 1      N      \n 4     55 f       5.03 12.7      5.03    12.7    0.397 2      q      \n 5     35 k       6.17  4.53     6.17     4.53   1.36  1      z      \n 6     34 c       5.58  5.36     5.58     5.36   1.04  2      M      \n 7     79 p       5.59  7.83     5.59     7.83   0.713 5      g      \n 8     65 p       7.22  8.23     7.22     8.23   0.877 2      Z      \n 9     10 t       5.68 17.2      5.68    17.2    0.330 5      K      \n10     38 n       8.22  3.36     8.22     3.36   2.44  3      v      \n# ℹ 990 more rows\n\n\nNote that the calculation in mutate() for the third variable use the result of the calculations for the first and second. Because R first calculates the first, then the second you can use their results are known as R starts on the third. In other words, R executes these calculations line by line and not row by row where it first calculates all values for the three operations for the first row and then moves to the second row.\nLet’s calculate add the mean of var1d in a separate column after var1d:\n\ndfm |&gt;\n  mutate(\n    meanvar1d = mean(var1d), \n    .after = var1d\n  )\n\n# A tibble: 1,000 × 7\n   var_1s var_2s var1d meanvar1d var2d fac_15 char_XX\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;  \n 1     69 d       9.08      7.54  1.98 3      a      \n 2     66 w       8.55      7.54  8.42 5      e      \n 3     36 d       6.14      7.54 12.8  1      N      \n 4     55 f       5.03      7.54 12.7  2      q      \n 5     35 k       6.17      7.54  4.53 1      z      \n 6     34 c       5.58      7.54  5.36 2      M      \n 7     79 p       5.59      7.54  7.83 5      g      \n 8     65 p       7.22      7.54  8.23 2      Z      \n 9     10 t       5.68      7.54 17.2  5      K      \n10     38 n       8.22      7.54  3.36 3      v      \n# ℹ 990 more rows\n\n\nNote that here, R uses all observations for var1d to calculate that mean. With a grouped data frame, that wouldn’t the the case: R calculates the mean per group. You can add groups using the .by argument in mutate() or you can do so using group_by. Using the latter often helps to read the code. However, here, we will use the former to illustrate. The grouping variable is fac_15:\n\ndfm |&gt; \n  mutate(\n    meanvar1d = mean(var1d), \n    .by = fac_15, \n    .after = fac_15\n  )\n\n# A tibble: 1,000 × 7\n   var_1s var_2s var1d var2d fac_15 meanvar1d char_XX\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1     69 d       9.08  1.98 3           7.49 a      \n 2     66 w       8.55  8.42 5           7.67 e      \n 3     36 d       6.14 12.8  1           7.63 N      \n 4     55 f       5.03 12.7  2           7.41 q      \n 5     35 k       6.17  4.53 1           7.63 z      \n 6     34 c       5.58  5.36 2           7.41 M      \n 7     79 p       5.59  7.83 5           7.67 g      \n 8     65 p       7.22  8.23 2           7.41 Z      \n 9     10 t       5.68 17.2  5           7.67 K      \n10     38 n       8.22  3.36 3           7.49 v      \n# ℹ 990 more rows\n\n\nThe column meanvar1d now shows different means for every level in the grouping variable. You can use this to calculate e.g. difference between the mean an a value for every group. Using group_by() to define the grouped data frame:\n\ndfm |&gt;\n  group_by(fac_15) |&gt;\n  mutate(\n    diffmin = var1d - mean(var1d), \n    .after = fac_15\n  )\n\n# A tibble: 1,000 × 7\n# Groups:   fac_15 [5]\n   var_1s var_2s var1d var2d fac_15 diffmin char_XX\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1     69 d       9.08  1.98 3        1.59  a      \n 2     66 w       8.55  8.42 5        0.877 e      \n 3     36 d       6.14 12.8  1       -1.49  N      \n 4     55 f       5.03 12.7  2       -2.39  q      \n 5     35 k       6.17  4.53 1       -1.46  z      \n 6     34 c       5.58  5.36 2       -1.83  M      \n 7     79 p       5.59  7.83 5       -2.08  g      \n 8     65 p       7.22  8.23 2       -0.198 Z      \n 9     10 t       5.68 17.2  5       -1.99  K      \n10     38 n       8.22  3.36 3        0.729 v      \n# ℹ 990 more rows\n\n\nTo rank observations per group, you can use e.g. min_rank():\n\ndfm |&gt;\n  group_by(fac_15) |&gt;\n  mutate(\n    ranked = min_rank(var_2s),\n    .after = var_2s\n  ) |&gt;\n  arrange(fac_15, ranked)\n\n# A tibble: 1,000 × 7\n# Groups:   fac_15 [5]\n   var_1s var_2s ranked var1d var2d fac_15 char_XX\n    &lt;int&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;  \n 1     56 a           1  7.30  7.85 1      j      \n 2     49 a           1  8.28  6.01 1      M      \n 3     76 a           1  5.72  5.25 1      y      \n 4     92 a           1  8.13  6.73 1      U      \n 5     22 a           1  7.95  1.80 1      U      \n 6     36 a           1  8.04  8.80 1      Q      \n 7     70 a           1  7.55 -3.00 1      s      \n 8     44 a           1  9.96 -1.47 1      M      \n 9     32 b           9  9.71  1.19 1      q      \n10     66 b           9  6.73  9.07 1      P      \n# ℹ 990 more rows\n\n\nNote the behaviour of min_rank(): the rank of “a” is 1, there are two values in the first factor equal to “a”. The first value of “b” is ranked 3 and not 2. The final rank will be the length of the factor. Using dense_rank, the first rank of “b” would be 2. However, in these case, the highest rank wouldn’t equal the number of values in a factor.\nTo illustrate case_match()let’s recode the factor where 1 will be shown as A, 2 as B, …\n\ndfm |&gt;\n  mutate(\n    recodefact = \n      case_when(\n        fac_15 == 1 ~ \"A\", \n        fac_15 == 2 ~ \"B\", \n        fac_15 == 3 ~ \"C\", \n        fac_15 == 4 ~ \"D\", \n        fac_15 == 5 ~ \"E\",\n        .default = NA\n      )\n  )\n\n# A tibble: 1,000 × 7\n   var_1s var_2s var1d var2d fac_15 char_XX recodefact\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;   &lt;chr&gt;     \n 1     69 d       9.08  1.98 3      a       C         \n 2     66 w       8.55  8.42 5      e       E         \n 3     36 d       6.14 12.8  1      N       A         \n 4     55 f       5.03 12.7  2      q       B         \n 5     35 k       6.17  4.53 1      z       A         \n 6     34 c       5.58  5.36 2      M       B         \n 7     79 p       5.59  7.83 5      g       E         \n 8     65 p       7.22  8.23 2      Z       B         \n 9     10 t       5.68 17.2  5      K       E         \n10     38 n       8.22  3.36 3      v       C         \n# ℹ 990 more rows\n\n\nUsing if_else() to calculate the log of var1d if that variable is larger than 1 and setting the result to 0 is that is not the case, dropping all unused variables:\n\ndfm |&gt; mutate(\n  logvar2d = if_else(var1d &gt; 1, log(var1d), 0),\n  .keep = \"used\"\n) \n\n# A tibble: 1,000 × 2\n   var1d logvar2d\n   &lt;dbl&gt;    &lt;dbl&gt;\n 1  9.08     2.21\n 2  8.55     2.15\n 3  6.14     1.81\n 4  5.03     1.61\n 5  6.17     1.82\n 6  5.58     1.72\n 7  5.59     1.72\n 8  7.22     1.98\n 9  5.68     1.74\n10  8.22     2.11\n# ℹ 990 more rows\n\n\nThe result shows only the variables used in the calculation.\nacross() allows you to perform the same calculations on multiple columns in a data frame. We covered across() in the previous section on summarize(). For instance, using across() to rescale all numeric variables in dfm by dividing their value with their mean:\n\ndfm |&gt; mutate(\n  across(\n    .cols = where(is.numeric), \n    .fns = \\(x) x/mean(x),\n    .names = \"{.col}_rescale\"\n  )\n)\n\n# A tibble: 1,000 × 9\n   var_1s var_2s var1d var2d fac_15 char_XX var_1s_rescale var1d_rescale\n    &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1     69 d       9.08  1.98 3      a                1.33          1.20 \n 2     66 w       8.55  8.42 5      e                1.27          1.13 \n 3     36 d       6.14 12.8  1      N                0.693         0.814\n 4     55 f       5.03 12.7  2      q                1.06          0.667\n 5     35 k       6.17  4.53 1      z                0.674         0.819\n 6     34 c       5.58  5.36 2      M                0.655         0.740\n 7     79 p       5.59  7.83 5      g                1.52          0.741\n 8     65 p       7.22  8.23 2      Z                1.25          0.957\n 9     10 t       5.68 17.2  5      K                0.193         0.753\n10     38 n       8.22  3.36 3      v                0.732         1.09 \n# ℹ 990 more rows\n# ℹ 1 more variable: var2d_rescale &lt;dbl&gt;\n\n\nNote the general setup of this code in across(). First we identify the relevant columns using &lt;tidy-select&gt; syntax. Second, we introduce the function or functions that will be used by mutate() to create new variables. To do so we use a anonymous functions. As al alternative you can use the “~ and .x” approach. Third, we add how R should build the names.\n\n\n\n8.2.6 Per row operations: rowwise()\nUsing rowwise() you can create a new variable that includes multiple colum values. For instance, to calculate the average of var1d and var2d per row:\n\ndfm |&gt;\n  rowwise() |&gt;\n  mutate(aver = mean(c(var1d, var2d)), .keep = \"used\")\n\n# A tibble: 1,000 × 3\n# Rowwise: \n   var1d var2d  aver\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  9.08  1.98  5.53\n 2  8.55  8.42  8.48\n 3  6.14 12.8   9.45\n 4  5.03 12.7   8.85\n 5  6.17  4.53  5.35\n 6  5.58  5.36  5.47\n 7  5.59  7.83  6.71\n 8  7.22  8.23  7.72\n 9  5.68 17.2  11.4 \n10  8.22  3.36  5.79\n# ℹ 990 more rows\n\n\nThe column aver shows the results for the average, per row, of var1d and var2d. Let’s see what would happen without rowwise():\n\ndfm |&gt;\n  mutate(aver = mean(c(var1d, var2d)), .keep = \"used\")\n\n# A tibble: 1,000 × 3\n   var1d var2d  aver\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  9.08  1.98  6.35\n 2  8.55  8.42  6.35\n 3  6.14 12.8   6.35\n 4  5.03 12.7   6.35\n 5  6.17  4.53  6.35\n 6  5.58  5.36  6.35\n 7  5.59  7.83  6.35\n 8  7.22  8.23  6.35\n 9  5.68 17.2   6.35\n10  8.22  3.36  6.35\n# ℹ 990 more rows\n\n\nThe column aver now returns a fixed value for every row. Where does the value come from? Recall that tidy datasets have their variables in columns and observations in rows. Functions such as mutate() and summarize() are means to deal with variables. For instance, summarize() shows the mean per column, the standard deviation per column, … . Likewise, if you use mean(), sd() … mutate will apply these functions per column. This is what happens here: mutate() first calculates the mean for each of the two columns:\n\nave1 &lt;- mean(dfm$var1d)\nave2 &lt;- mean(dfm$var2d)\nave1\n\n[1] 7.536825\n\nave2\n\n[1] 5.162055\n\n\nBecause these means are included in a vector, c(var1d, var2d), mutate() then computes the mean of the values in that vector:\n\nmean(c(ave1, ave2))\n\n[1] 6.34944\n\n\nand adds this result for the tibble. This might seem add at first, but it is predictable: R calculates functions for columns, not for rows. If you need rows, you need to be explicit. You do so by adding rowwise().\nThis functions acts as a group_by() but uses every row as a group. To illustrate, we’ll use:\n\ndfmarks &lt;- tibble(\n  name = c(\"Elisa\", \"Joe\"), \n  macro = c(17, 12), \n  micro = c(16, 14), \n  marketing = c(11, 19)\n)\n\nLet’s apply the usual summarize() function with group_by()\n\ndfmarks |&gt; group_by(name) |&gt; mutate(ave = mean(c(macro, micro, marketing)))\n\n# A tibble: 2 × 5\n# Groups:   name [2]\n  name  macro micro marketing   ave\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Elisa    17    16        11  14.7\n2 Joe      12    14        19  15  \n\n\nHere, I get the result for each individual student: as there is only one individual group and mutate() first calculates the mean per name and then the mean for all variables in the vector c(macro, micro, marketing), I get the average for each student across these three subjects. Now let’s use rowwise(name) with summarize(). Here, R will calculate the summary statistic across rows. In other words, I’ll get the average mark for each student across all subjects:\n\ndfmarks |&gt; rowwise(name) |&gt;\n  summarize(ave = mean(c(macro, micro, marketing)))\n\n`summarise()` has grouped output by 'name'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 2\n# Groups:   name [2]\n  name    ave\n  &lt;chr&gt; &lt;dbl&gt;\n1 Elisa  14.7\n2 Joe    15  \n\n\nAs you can see, R also adds the name of the students. This result is due to addition of name in rowwise(). Second, note that the tibble is “grouped” and has Groups: name[2]. This is also the case for result after using mutate() with group_by(name).\n\n\n\n\n\n\nYour turn: NYC Flights\n\n\n\n\n\nRecall that the dataset includes observations with missing variables: cancelled flights, flights with missing data on arr_delay or air_time and data with values for dep_timebut with missing values for arr_time.\nIdentify the cancelled fligthts, i.e. flights with no data for dep_time, arr_time, dep_delay, arr_delay and air_time.\n\n\nCode\nnycmissing |&gt; filter(is.na(dep_time) & is.na(arr_time) & is.na(dep_delay) & is.na(arr_delay) & is.na(air_time))\n\n\n# A tibble: 10,738 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1       NA           2245        NA       NA            146\n 2  2023     1     1       NA           1655        NA       NA           2012\n 3  2023     1     1       NA            920        NA       NA           1202\n 4  2023     1     1       NA            959        NA       NA           1306\n 5  2023     1     1       NA           2030        NA       NA           2334\n 6  2023     1     2       NA           1000        NA       NA           1219\n 7  2023     1     2       NA           1458        NA       NA           1810\n 8  2023     1     2       NA           1125        NA       NA           1407\n 9  2023     1     2       NA           1930        NA       NA           2234\n10  2023     1     2       NA           2259        NA       NA            346\n# ℹ 10,728 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIdentify the flights with a departure time and arrival time, but with not values for arrival delay or air time.\n\n\nCode\nnycmissing |&gt; filter((!is.na(dep_time) & !is.na(arr_time) & is.na(arr_delay)) | (!is.na(dep_time) & !is.na(arr_time) & is.na(air_time)))\n\n\n# A tibble: 1,081 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      753            730        23     1211            855\n 2  2023     1     1     2000           1959         1     1537           2229\n 3  2023     1     1     2151           2159        -8     1110             12\n 4  2023     1     2      711            715        -4     1313            955\n 5  2023     1     2      738            730         8     1140            919\n 6  2023     1     2      821            800        21     1309           1120\n 7  2023     1     2      831            840        -9     1237           1042\n 8  2023     1     2     1547           1545         2       44           1910\n 9  2023     1     2     1700           1645        15     2359           2035\n10  2023     1     2     2105           2059         6      122           2325\n# ℹ 1,071 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nIdentify the flights with a departure time but without arrival time:\n\n\nCode\nnycmissing |&gt; filter(!is.na(dep_time) & is.na(arr_time))\n\n\n# A tibble: 715 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2023     1     1      949            900        49       NA           1525\n 2  2023     1     2     1857           1800        57       NA           2140\n 3  2023     1     2     2315           1829       286       NA           2139\n 4  2023     1     4      950            940        10       NA           1302\n 5  2023     1     4     1141            845       176       NA           1232\n 6  2023     1    11      703            600        63       NA            720\n 7  2023     1    12     2346           2034       192       NA           2359\n 8  2023     1    19     1817           1622       115       NA           1809\n 9  2023     1    19     1918           1929       -11       NA           2109\n10  2023     1    19     2014           2015        -1       NA           2328\n# ℹ 705 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNow use mutate() to add a variable “status” with value complete if there are no missing values, cancelled it there are missing values for departure and arrival time, departure and arrival delay and air time, imcomplete if there is not value for arrival delay or air time but with values for departure and arrival times, and flagged to indicate that these flights require further analysis as they include a departure time, but not arrival time.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    status = case_when(rowSums(is.na(nycflights)) == 0 ~ \"complete\",\n                       (is.na(dep_time) & is.na(arr_time) & is.na(dep_delay) & is.na(arr_delay) & is.na(air_time)) ~ \"cancelled\", \n                       (!is.na(dep_time) & !is.na(arr_time) & is.na(arr_delay)) | (!is.na(dep_time) & !is.na(arr_time) & is.na(air_time)) ~ \"incomplete\", \n                       !is.na(dep_time) & is.na(arr_time) ~ \"flagged\",\n                       .default = \"NA\"))\n\n\nCheck if these 4 categories include all observations.\n\n\nCode\nnycflights |&gt; \n  group_by(status) |&gt; \n  summarize(n = n()) |&gt; \n  summarize(total = sum(n)) |&gt;\n  ungroup()\n\n\n# A tibble: 1 × 1\n   total\n   &lt;int&gt;\n1 435352\n\n\nShow a summary table which includes the number of observations in each group, as well as the percentage share in each category. Hint: the summarize statement returns a tibble which you can use in subsequent calculations e.g. using mutate():\n\n\nCode\nnycflights |&gt; \n  group_by(status) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(per_n = n/sum(n) * 100) |&gt;\n  ungroup()\n\n\n# A tibble: 4 × 3\n  status          n  per_n\n  &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;\n1 cancelled   10738  2.47 \n2 complete   422818 97.1  \n3 flagged       715  0.164\n4 incomplete   1081  0.248\n\n\nAre there differences in terms of the share of cancelled flights between airports in New York? Calculate the total number of missing flights, store this statistic in n_cancelled and arrange ascending in the number of cancelled flights.\n\n\nCode\nnycflights |&gt;\n  filter(status == \"cancelled\") |&gt;\n  group_by(origin) |&gt;\n  summarize(n_cancelled = n()) |&gt;\n  arrange(n_cancelled) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin n_cancelled\n  &lt;chr&gt;        &lt;int&gt;\n1 JFK           2848\n2 EWR           3633\n3 LGA           4257\n\n\nIs there a difference in the number of flights with incomplete data between airports? Calculate the number of incomplete observations, store this statistics in n_incomplete and arrange in descending order of the number of incomplete observations.\n\n\nCode\nnycflights |&gt;\n  filter(status == \"incomplete\") |&gt;\n  group_by(origin) |&gt;\n  summarize(n_incomplete = n()) |&gt;\n  arrange(desc(n_incomplete)) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  origin n_incomplete\n  &lt;chr&gt;         &lt;int&gt;\n1 LGA             416\n2 JFK             350\n3 EWR             315\n\n\nThe dataset does not allow us to show e.g. the number of flights per weekday, or per hour of actual departure. It includes the day of the month, as a numeric variable, but does not include the day of the week. The last column time_hour shows the data and hour, but only for the scheduled departure. In addition, if you inspect the first rows, you’ll see this variable is wrong for those flights that were bound to depart late December 31st 2022 but did so with a delay into the early minutes or hours of January 1st, 2022. Here, we will add a variable dep_hhmm that shows the actual departure time in POSIXct format and sched_dep_hhmm that shows the (correct) scheduled departure time. To do so, we need the year, the month and the day of departure as well as the (scheduled) hour and minute of departure and include a correction for the observations with a late departure on the last day of 2022. We will proceed in different steps. First, we will use dep_time to extract the actual departure hour and minute. Using those, we create a POSIXct date/time variable. This variable is then use to create sched_dep_hhmm, the scheduled departure hour in POSIXct.\nThe variable dep_time is shown as HHMM with HH = 0 if a flight left after midnight but before 1 AM and, in that case, with MM one or two digits to show the minutes. If the flight departed after 1 AM, that variable shows HHMM with HH as one or two digit hour and a 2 digit minute. Create two new variables, dep_hour and dep_minute which shows the hour and minute of the departure. In other words, the first needs to show the HH part, including 0 and the second the MM part. To check if your code is correct, first assign the result to a data frame test, in the mutate() statement keep only observations that you used. To inspect, draw a sample of 200 observations from test. Hint 1: in Chapter 3 we looked at str_sub to extra parts of a string. Hint 2: here you have a two cases: if HH is 0 or is HH &gt; 0. In the first case, dep_hour will be 0, in the second case, you can extract it from dep_time.\n\n\nCode\ntest &lt;- nycflights |&gt; \n  mutate(\n    dep_hour = case_when(\n      dep_time &lt; 60 ~ \"0\", \n      dep_time &gt;= 60 ~ str_sub(dep_time, -4, -3)), \n    dep_minute = str_sub(dep_time, -2, -1), \n    .keep = \"used\", \n    .after = dep_time\n) |&gt;\n  slice_sample(n = 200)\n\n\nUsing test: inspect if you code works. If that is the case, rerun this code, but now add the variable to nycflights. Using dep_hour and dep_minute we would be able to recalculate dep_time. In other words, you remove this variable from the dataset after completing this operations. Also show the new variables after day.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    dep_hour = case_when(\n      dep_time &lt; 60 ~ \"0\", \n      dep_time &gt;= 60 ~ str_sub(dep_time, -4, -3)), \n    dep_minute = str_sub(dep_time, -2, -1), \n    .keep = \"unused\", \n    .after = day\n  )\n\n\nRepeat this for sched_dep_time and create sched_dep_hour and sched_dep_minute.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    sched_dep_hour = case_when(\n      sched_dep_time &lt; 60 ~ \"0\", \n      sched_dep_time &gt;= 60 ~ str_sub(sched_dep_time, -4, -3)), \n    sched_dep_minute = str_sub(sched_dep_time, -2, -1), \n    .keep = \"unused\", \n    .after = dep_minute\n  )\n\n\nVerify the typeof dep_hour, … . As you can see, these are character variables. This result is due to the fact that you used {stringr}, a package that works with character data. Use across to change the type of dep_hour, dep_minute, sched_dep_hour and sched_dep_minute from character to integer.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    across(.cols = contains(c(\"minute\", \"hour\")), \n           .fns = \\(x) as.integer(x))\n  )\n\n\nIf you look at the dataset, you see that you have a year, a month, a day, an hour and a minute. Recall from Chapter 3 that you can use {lubridate} to create a POSIXct type. Using this package to create a data/time variable with dep_hour and dep_minute and name this variable dep_hhmm. You can use tz = \"UTC\" for the time zone variable (recall from the description of the dataset that all times are in UTC). Include this variable after day. Here, you can keep all the variables used in the calculation.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    dep_hhmm = make_datetime(\n      year = year, \n      month = month, \n      day = day, \n      hour = dep_hour, \n      min = dep_minute, \n      tz = \"UTC\"),\n    .keep = \"all\", \n    .after = day)\n\n\nCan you use the same approach to create a POSIXct data/time variable for the scheduled departure date/time? Try, add sched to the names in the previous code and assign this result to a new data frame test to avoid overwriting nycflights\n\n\nCode\ntest &lt;- nycflights |&gt;\n  mutate(\n    shed_dep_hhmm = make_datetime(\n      year = year, \n      month = month, \n      day = day, \n      hour = sched_dep_hour, \n      min = sched_dep_minute, \n      tz = \"utc\"),\n    .keep = \"all\", \n    .after = day)\n\n\nInspect the first row of test: here you have a flight that departed in 2023 (1 minute passed midnight) but was scheduled to depart in 2022. The result for sched_dep_hhmm shows a scheduled departure for 2023. If you look at the details, the flight would have left 20 hours and 37 minutes before its scheduled departure. To avoid this, calculate the variable sch_dep_hhmm as the difference between the dep_hhmm and dep_delay. Recall that dep_delay is shown in minutes and that dep_hhmm is a POSIXct data/time variable. in Chapter 3, we covered {lubridate}. Return to that section of check {lubridate}’s cheat sheet to see how you can calculate a new POSIXct variable using one POSIXct data and a variable measures in minutes. Store this sched_dep_hhmm after dep_hhmm.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    sched_dep_hhmm = dep_hhmm - minutes(dep_delay), \n    .after = dep_hhmm\n  )\n\n\nThe “hhmm” variables now allow your to further analyse the dataset. Recall that you can use {lubridate} to extract the day of the week, the hour of the day, the quarter of the year, … . Extract the day of the week from dep_hhmm. Name this variable day_week (day of the week) and include it after day. Show abbreviated days of the week and allow weeks to start on Monday. If you want to show the names of the weekdays in English, you can add locale = Sys.setlocale(\"LC_TIME\", \"English\") (if you don’t add this, the days will show in the language of your computer, which is also fine). Add this variable after day.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    day_week = wday(dep_hhmm, week_start = getOption(\"lubridate.week.start\", 1),\n                    label = TRUE, \n                    abbr = TRUE, \n                    locale = Sys.setlocale(\"LC_TIME\", \"English\")),\n    .after = day\n  )\n\n\nRepeat this for sched_dep_hhmm and name this variable sched_day_week.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    sched_day_week = wday(sched_dep_hhmm, week_start = getOption(\"lubridate.week.start\", 1),\n                    label = TRUE, \n                    abbr = TRUE, \n                    locale = Sys.setlocale(\"LC_TIME\", \"English\")),\n    .after = day_week\n  )\n\n\nExtract the hour for both dep_hhmm as well as sched_dep_hhmm. Name this variable hour_day (hour of the day) and sched_hour_day and locate it after sched_dep_hhmm.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    hour_day = hour(dep_hhmm),\n    sched_hour_day = hour(sched_dep_hhmm), \n    .after = sched_dep_hhmm\n  )\n\n\nWe now have a dataset that we can use to e.g. verify when flights depart (day, hour). Let’s check a couple of these facts.\nFor all flights with complete data, show the total number of flights per day of the week. Call this statistic n_flights_day.\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(day_week) |&gt; \n  summarize(n_flights_day = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 7 × 2\n  day_week n_flights_day\n  &lt;ord&gt;            &lt;int&gt;\n1 Mon              62791\n2 Tue              62368\n3 Wed              63315\n4 Thu              63582\n5 Fri              62764\n6 Sat              49080\n7 Sun              58918\n\n\nAdd a column to the previous table showing the the mean departure delay? Are busier days also days with longer departure delays? Call this variable ave_dep_delay.\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(day_week) |&gt; \n  summarize(\n    n_flights_day = n(),\n    ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 7 × 3\n  day_week n_flights_day ave_dep_delay\n  &lt;ord&gt;            &lt;int&gt;         &lt;dbl&gt;\n1 Mon              62791          14.5\n2 Tue              62368          10.8\n3 Wed              63315          10.6\n4 Thu              63582          11.8\n5 Fri              62764          16.4\n6 Sat              49080          15.6\n7 Sun              58918          16.9\n\n\nFor all flights with full data, show the total number of flights per hour of the day that actually departed. Call this statistic n_flights_hour.\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(hour_day) |&gt; \n  summarize(n_flights_hour = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 24 × 2\n   hour_day n_flights_hour\n      &lt;int&gt;          &lt;int&gt;\n 1        0           1297\n 2        1            485\n 3        2            150\n 4        3             42\n 5        4            473\n 6        5          10405\n 7        6          24156\n 8        7          27584\n 9        8          28143\n10        9          28140\n# ℹ 14 more rows\n\n\nFor all flights with full data, show the total number of flights per hour of the day that were scheduled to depart. Call this statistic n_sched_flights_hour\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(sched_hour_day) |&gt; \n  summarize(n_sched_flights_hour = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 19 × 2\n   sched_hour_day n_sched_flights_hour\n            &lt;int&gt;                &lt;int&gt;\n 1              5                 3214\n 2              6                28376\n 3              7                29436\n 4              8                28609\n 5              9                31147\n 6             10                23398\n 7             11                25640\n 8             12                21005\n 9             13                23767\n10             14                25927\n11             15                28369\n12             16                25525\n13             17                26706\n14             18                24975\n15             19                26436\n16             20                23453\n17             21                19696\n18             22                 6874\n19             23                  265\n\n\nCalculate the average delay per hour for scheduled flights: what is the best time to depart if you want to avoid delays? Call this statistic ave_delay_hour.\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(sched_hour_day) |&gt; \n  summarize(ave_delay_hour = mean(dep_delay)) |&gt;\n  ungroup()\n\n\n# A tibble: 19 × 2\n   sched_hour_day ave_delay_hour\n            &lt;int&gt;          &lt;dbl&gt;\n 1              5           6.32\n 2              6           3.33\n 3              7           4.96\n 4              8           6.08\n 5              9           7.14\n 6             10           8.12\n 7             11           8.04\n 8             12           9.77\n 9             13          13.2 \n10             14          15.5 \n11             15          16.9 \n12             16          17.3 \n13             17          21.3 \n14             18          20.9 \n15             19          23.9 \n16             20          23.6 \n17             21          21.9 \n18             22          26.8 \n19             23          32.0 \n\n\nCalculate the variable gain as the difference between the departure delay and the arrival delay. A positive variable indicates that the flight was shorter than planned as the pilot made up some of the lost time while in the air.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay\n  )\n\n\nWhat is the average for this variable for 3 groups of distances: short, medium and long. You can use cut() to generate these groups. Ask for help on how to add labels (?cut). Call his statistic ave_gain.\n\n\nCode\nnycflights |&gt; \n  group_by(dis = cut(distance, 3, labels = c(\"short\", \"medium\", \"long\"))) |&gt;\n  summarize(ave_gain = mean(gain, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  dis    ave_gain\n  &lt;fct&gt;     &lt;dbl&gt;\n1 short      8.68\n2 medium    13.6 \n3 long      12.0 \n\n\nCalculate the mean and the sum of the departure and arrival delay and add this variable to nycflights using ave_delay for the mean and tot_delay for the sum. Here, you need to add a variable using a function that has to work on the rows, and not on the columns. The calculation might take some time to complete.\n\n\nCode\nnycflights &lt;- nycflights |&gt; rowwise() |&gt;\n  mutate(ave_delay = mean(c(dep_delay, arr_delay)), \n         tot_delay = dep_delay + arr_delay) |&gt;\n  ungroup()\n\n\nProduce a table for all flights with complete data showing the average of the mean departure and arrival delay ave_delay_day (average delay per day), the average of the total delay ave_tot_delay, the average gain ave_gain and the number of flights departing on every day of the week n_flights. If you want to avoid a long total delay, which day is the best to travel from one of New York’s airports?\n\n\nCode\nnycflights |&gt; \n  filter(status == \"complete\") |&gt;\n  group_by(day_week) |&gt;\n  summarize(\n    ave_delay_day = mean(ave_delay, na.rm = TRUE), \n    ave_tot_delay = mean(tot_delay, na.rm = TRUE), \n    ave_dain = mean(gain, na.rm = TRUE),\n    n_flights = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 7 × 5\n  day_week ave_delay_day ave_tot_delay ave_dain n_flights\n  &lt;ord&gt;            &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;\n1 Mon               9.80          19.6     9.44     62791\n2 Tue               5.79          11.6     9.93     62368\n3 Wed               5.57          11.1     9.96     63315\n4 Thu               7.22          14.4     9.14     63582\n5 Fri              12.2           24.5     8.33     62764\n6 Sat              10.9           21.7     9.57     49080\n7 Sun              12.3           24.6     9.16     58918",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "08_Transforming_data.html#multiple-datasets",
    "href": "08_Transforming_data.html#multiple-datasets",
    "title": "8  Data transformation",
    "section": "8.3 Multiple datasets",
    "text": "8.3 Multiple datasets\n\n8.3.1 bind_rows()\nWith bind_rows(), you can add rows to a data frame. This functions includes the following arguments:\n\nbind_rows(..., .id = NULL)\n\nR will the data frames included in .... This function returns a data frame that contains all columns included in any of the data frames in .... The .id argument allows you to add a reference to the data frames. To illustrate, we’ll use two data frames, each with two columns, but with only one, A, in common\n\ndfcb1 &lt;- tibble(\n  A = c(\"A\", \"B\"),\n  B = c(\"E\", \"F\")\n)\n\ndfcb2 &lt;- tibble(\n  A = c(\"C\", \"D\"),\n  C = c(\"G\", \"H\")\n)\n\nUsing bind_rows() creates a new data frame. This data frame has 3 columns, A, B and C:\n\nbind_rows(dfcb1, dfcb2, .id = \"dfcb\")\n\n# A tibble: 4 × 4\n  dfcb  A     B     C    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 1     A     E     &lt;NA&gt; \n2 1     B     F     &lt;NA&gt; \n3 2     C     &lt;NA&gt;  G    \n4 2     D     &lt;NA&gt;  H    \n\n\nThis new data frame has missing values for the observations on the third and fourth row of column B (which is missing in the second data frame) and missing values for the first and second observations of column C (which is missing in the first data frame).\n\n\n8.3.2 bind_cols\nThis functions allows you to add the columns of one dataset to those of another dataset. The function binds does so in the other in which the observations appear in each dataset. In other words, it is easy to create data sets that are useless as R will not check if all the observations that are part of one dataset also appear in the another and if they to, if they do on the same rows. For that reason, it is usually much saver to use one of the join functions.\n\n\n8.3.3 Merging data: join functions\nJoining two data frame allows you to add variables that are part of the second to the first. Doing so, you change or mutate the first data frame by adding columns with matching observations from the second.\nSuppose that you have two datasets. One records the date and time of a transactions, the items bought and prices paid, discounts offered as well as a customer id. The second dataset includes data on the customer, including his or her id, age, address, … . Joining both datasets would allow you to add data on the customer (e.g. age) to the data on the transactions. Doing so, you could analyse if, for instance, the age of a customer affects the probability that he or she only bought discounted items, … .\nThere are four functions in R to add variables to a dataset. The key difference between these four functions is how they handle observations in any of the two datasets that are not matched.\n\ninner_join(x, y) returns a dataset that includes only observations with a match in both x and y\nleft_joint(x, y) returns a dataset that keeps all observations in x (the left data frame in (x, y), but drops unmatched observations in y\nright_join(x, y) returns a dataset that keeps all observations in y (the richt data frame in (x, y), but drops unmatched observations in x\nfull_join() returns a dataset that keeps all observations in both x and y\n\nThese four functions have similar arguments:\n\nx_join(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\".x\", \".y\"),\n  ...,\n  keep = NULL\n)\n\nIn these functions, x and y refer to the data frames. To join two data frames, i.e. to add columns, R needs to know which observations in the first dataset match those in the second. In there is match, R will add all the values in the column in y to the values for that same observation in x. In the example: for every customer id in the transactions dataset, R will search for the same value in the customer dataset. If R finds that value in the customer dataset, it will copy the values for age, address … for that customer and add these values in the age, address, … column in the transaction dataset in the row or rows where R found the customer id. The by = argument includes the variables in both dataset where R looks for a match. By default, R uses all variables that both data frames have in common. You can specify these variables explicitly using join_by(). This also allows you to include variables R will use to match but whose name is different across data sets. For instance, join_by(a == b) tells R that it has to look for values in column a in x that have a match with values column b in y. Here, you can add multiple variables, e.g. join_by(iso3c == iso3, date == year). In this code, R will match countries identified by the variable iso3c in x with countries identified by iso3 in y AND year identified by date in x and year in y both have equil values. As R matches on both conditions, observations only match is both the values for the country code and the year match. The suffix = c(\".x\", \".y\") argument shows how R will identify columns with the same name from both datasets. By default, R adds .x to the variable names from x and .y to those from y. The last arguments deals with the join keys. Using join_by(a == b) implies that the value in both columns are identical. By default R doesn’t keep both (keep = NULL). By default, R retains only keys from x (i.e. a). Changing this to TRUE, R will retain both keys (i.e. both a and b).\nTo illustrate these operations, we’ll use\n\n# A data frame with id 501, 502 ... 510\n\ndfj1 &lt;- tibble(\n  id = seq(from = 501, by = 1, length.out = 10),\n  var1 = seq(from = 10, by = 10, length.out = 10))\n\n# A data frame with id 501, 502, 503, 511, 512 and 513\n\ndfj2 &lt;- tibble(\n  id = c(seq(from = 501, by = 1, length.out = 3), seq(from = 511, by = 1, length.out = 3)),\n  var2 = seq(from = 100, by = 100, length.out = 6))\n\nLet’s use dfj1 and dfj2 to join both datasets using inner_join(). Both data frames include a variable id that we can use to join. We can add it to by. However, in this case, as this is the only variable both datasets have in common, R would be default use this column to join. However, as it increases the readability of the code to add this key explicitly, we’ll do so (an in general, this is a good idea). Note that the first data frame includes 10 observations, the second 6 and that observations with keys 501, 502 and 503 are common in both, 504-510 is only included in the first and 511, 512 and 513 is only part of the second. Joining both using inner_join()\n\ninner_join(dfj1, dfj2, by = \"id\")\n\n# A tibble: 3 × 3\n     id  var1  var2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   501    10   100\n2   502    20   200\n3   503    30   300\n\n\nreturns a tibble that only includes the observation that both have in common for id. The column var1 was part of dfj1. This function adds the var2 column from dfj2. All other observations are dropped from the dataset. In the output, you can see that R includes only one id variable (keep = NULL).\nUsing left_join()\n\nleft_join(dfj1, dfj2, by = \"id\")\n\n# A tibble: 10 × 3\n      id  var1  var2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   501    10   100\n 2   502    20   200\n 3   503    30   300\n 4   504    40    NA\n 5   505    50    NA\n 6   506    60    NA\n 7   507    70    NA\n 8   508    80    NA\n 9   509    90    NA\n10   510   100    NA\n\n\nreturns a tibble where all observations from dfj1, the tibble on the left the two tibbles to join, are retained. As R doesn’t have any values for observations 504-510 in dfj1 in dfj2. Recall that the latter doesn’t include these observations. As it doesn’t have the observations, R adds “NA” in column 3 (the column from the tibble on the right of the two tibbles to join). The function is often used in pipes:\n\ndfj1 |&gt; left_join(dfj2, by = \"id\")\n\n# A tibble: 10 × 3\n      id  var1  var2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   501    10   100\n 2   502    20   200\n 3   503    30   300\n 4   504    40    NA\n 5   505    50    NA\n 6   506    60    NA\n 7   507    70    NA\n 8   508    80    NA\n 9   509    90    NA\n10   510   100    NA\n\n\nWith right_join() R will keep the observations from the tibble on the right of the tibbles to join\n\nright_join(dfj1, dfj2, by = \"id\")\n\n# A tibble: 6 × 3\n     id  var1  var2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   501    10   100\n2   502    20   200\n3   503    30   300\n4   511    NA   400\n5   512    NA   500\n6   513    NA   600\n\n\nIn dfj1 there are not values for observations 511, 512 and 513 while there are in dfj2. The latter data frame doesn’t have any values for observations 504-510. As R retains all the observations in dfj2 that are not in dfj1, it add observations 511, 512 and 513. However, as these observations are missing in dfj1, the values for var1, the variable in dfj1 are all missing.\nUsing full_join() retains all observations:\n\nfull_join(dfj1, dfj2, by = \"id\")\n\n# A tibble: 13 × 3\n      id  var1  var2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   501    10   100\n 2   502    20   200\n 3   503    30   300\n 4   504    40    NA\n 5   505    50    NA\n 6   506    60    NA\n 7   507    70    NA\n 8   508    80    NA\n 9   509    90    NA\n10   510   100    NA\n11   511    NA   400\n12   512    NA   500\n13   513    NA   600\n\n\nThe result shows all observations in both dfj1 and dfj2. For observations that are not in dfj1 but are in dfj2 R adds missing in the dfj1 column. In the dfj2 column var2, R adds missing for these observations that are in the left but not in the right tibble.\nIn the previous examples, we always had datasets where all observations appeared once. What is they appear multiple times in at least one of these datasets? To illustrate how R handles these cases, we’ll use two data frames, dfj3 and dfj4. The first includes observations 501, 502, 503 and 505 three times, the latter includes one observations for 501, 502, 503 and 504. Note that observations 504 and 505 are unique to each dataset.\n\n# A data frame with id 501, 501, 501, 502, 502, 502, 503, 503, 503\n\ndfj3 &lt;- tibble(\n  id3 = c(rep(501,3), rep(502,3), rep(503,3), rep(505, 3)), \n  var3 = seq(from = 100, by = 100, length.out = 12))\n\n# A data frame with id 501, 502 and 503\n\ndfj4 &lt;- tibble(\n  id4 = seq(from = 501, by = 1, length.out = 4),\n  var4 = c(\"A\", \"B\", \"C\", \"D\"))\n\nThese datasets are examples of datasets where different observational units are storred in different tables (see Chapter 7). For instance, dfj3 could be a dataset with transactions, dfj4 with customer data. The customer id is id3 in the first and id4 in the second. In this case, we’ll have to include these two variables explicitly. Else, R will not find any matches. Recall that R joins datasets using matches it finds in columns with the same name. Abstant the latter, R will not finds any matches across both dataset.\nLet’s use the 4 join functions to see what they show:\n\ninner_join()\n\n\ninner_join(dfj3, dfj4, by = join_by(id3 == id4))\n\n# A tibble: 9 × 3\n    id3  var3 var4 \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1   501   100 A    \n2   501   200 A    \n3   501   300 A    \n4   502   400 B    \n5   502   500 B    \n6   502   600 B    \n7   503   700 C    \n8   503   800 C    \n9   503   900 C    \n\n\nAs expected, R drops the observations 504 and 505. For all observations in djf3 where it finds a match in dfj4 it add the values in the latter to the data frame in the first. Here, you see for instance, that it includes the value “A” vor var4 in dfj4 three times to the data frame dfj3 as the latter includes observation 501 three times.\n\nleft_join()\n\n\nleft_join(dfj3, dfj4, by = join_by(id3 == id4))\n\n# A tibble: 12 × 3\n     id3  var3 var4 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1   501   100 A    \n 2   501   200 A    \n 3   501   300 A    \n 4   502   400 B    \n 5   502   500 B    \n 6   502   600 B    \n 7   503   700 C    \n 8   503   800 C    \n 9   503   900 C    \n10   505  1000 &lt;NA&gt; \n11   505  1100 &lt;NA&gt; \n12   505  1200 &lt;NA&gt; \n\n\nThe first data frame (the one on the left) includes values for observation 505. However, this observations is not matched in dfj4. R adds missing values for the variable var4 as that variable doesn’t include any values for 505 in the data frame on the right. Because 505 is included in dfj3 three times, there are three missing values.\n\nright_join()\n\n\nright_join(dfj3, dfj4, by = join_by(id3 == id4))\n\n# A tibble: 10 × 3\n     id3  var3 var4 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1   501   100 A    \n 2   501   200 A    \n 3   501   300 A    \n 4   502   400 B    \n 5   502   500 B    \n 6   502   600 B    \n 7   503   700 C    \n 8   503   800 C    \n 9   503   900 C    \n10   504    NA D    \n\n\nThe resulting tibble shows one missing value: R retains all observations from dfj4, the data frame on the right: 501, 502, 503 and 504. Because the last observation doesn’t appear in dfj3 it adds an NA for that observation in the var3 column of dfj3. As dfj4 includes this observation only once, there is one missing value.\n\nfull_join()\n\n\nfull_join(dfj3, dfj4, by = join_by(id3 == id4))\n\n# A tibble: 13 × 3\n     id3  var3 var4 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1   501   100 A    \n 2   501   200 A    \n 3   501   300 A    \n 4   502   400 B    \n 5   502   500 B    \n 6   502   600 B    \n 7   503   700 C    \n 8   503   800 C    \n 9   503   900 C    \n10   505  1000 &lt;NA&gt; \n11   505  1100 &lt;NA&gt; \n12   505  1200 &lt;NA&gt; \n13   504    NA D    \n\n\nFill join retains all observations from both the data frame and the left and the one on the right. The missing values are equal to those in the previous two cases.\nHere, the data frame with multiple observations per id was always on the left the data frame with a single observation on the right. If you switch positions, R will duplicate the values in dfj4 to allow for 3 matching observations in dfj3. For instance:\n\nleft_join(dfj4, dfj3, by = join_by(id4 == id3))\n\n# A tibble: 10 × 3\n     id4 var4   var3\n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1   501 A       100\n 2   501 A       200\n 3   501 A       300\n 4   502 B       400\n 5   502 B       500\n 6   502 B       600\n 7   503 C       700\n 8   503 C       800\n 9   503 C       900\n10   504 D        NA\n\n\nWhat is there are multiple observations in both? To see what R does in that case, let’s use a data frame that includes 3 observations for 501, 502 and 505 and try to join it with dfj3\n\ndfj5 &lt;- tibble(\n  id5 = c(rep(501,3), rep(502,3), rep(505, 3)),\n  var5 = LETTERS[1:9])\n\n\nleft_join(dfj3, dfj5, by = join_by(id3 == id5))\n\nWarning in left_join(dfj3, dfj5, by = join_by(id3 == id5)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 30 × 3\n     id3  var3 var5 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1   501   100 A    \n 2   501   100 B    \n 3   501   100 C    \n 4   501   200 A    \n 5   501   200 B    \n 6   501   200 C    \n 7   501   300 A    \n 8   501   300 B    \n 9   501   300 C    \n10   502   400 D    \n# ℹ 20 more rows\n\n\nHere, R adds all combination of values in both datasets but adds a warning that you are musring a many-to-many(here 3 times 501 in dfj3 and in dfj5). Usually this is the result of a messy dataset and is not the intended outcome. If this is what is want, you can add a relationship = \"many-to-many. Doing so, R\n\nleft_join(dfj3, dfj5, by = join_by(id3 == id5), relationship = \"many-to-many\")\n\n# A tibble: 30 × 3\n     id3  var3 var5 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1   501   100 A    \n 2   501   100 B    \n 3   501   100 C    \n 4   501   200 A    \n 5   501   200 B    \n 6   501   200 C    \n 7   501   300 A    \n 8   501   300 B    \n 9   501   300 C    \n10   502   400 D    \n# ℹ 20 more rows\n\n\n\n\n8.3.4 Set operations\nWe covered these operations in Chapter 4. However, in that chapter, we used base R functions. Loading {dplyr} will override their base R equivalents. The {dplyr} functions return rows in two datasets. To illustrate, we’ll use two tibbles with two columns and five rows. Both tibbles share common observations on rows 1 and 2:\n\ndfi1 &lt;- tibble(\n  var1 = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  var2 = c(\"Q\", \"R\", \"S\", \"T\", \"U\")\n)\n\ndfi2 &lt;- tibble(\n  var1 = c(\"A\", \"B\", \"F\", \"G\", \"H\"),\n  var2 = c(\"Q\", \"R\", \"S\", \"D\", \"E\")\n)\n\n\nintersect(x, y): to find all common observations in x and y that both data frames have in common. There are only two: observations 1 and 2\n\n\nintersect(dfi1, dfi2)\n\n# A tibble: 2 × 2\n  var1  var2 \n  &lt;chr&gt; &lt;chr&gt;\n1 A     Q    \n2 B     R    \n\n\n\nunion(x, y): find all observations in xand y excluding duplicates: there are 8: both data frames have 3 observations that are different but also both that are equal (i.e. duplicates):\n\n\nunion(dfi1, dfi2)\n\n# A tibble: 8 × 2\n  var1  var2 \n  &lt;chr&gt; &lt;chr&gt;\n1 A     Q    \n2 B     R    \n3 C     S    \n4 D     T    \n5 E     U    \n6 F     S    \n7 G     D    \n8 H     E    \n\n\n\nunion_all(x, y): find all observations in x and y allowing for duplicates: here you have 10\n\n\nunion_all(dfi1, dfi2)\n\n# A tibble: 10 × 2\n   var1  var2 \n   &lt;chr&gt; &lt;chr&gt;\n 1 A     Q    \n 2 B     R    \n 3 C     S    \n 4 D     T    \n 5 E     U    \n 6 A     Q    \n 7 B     R    \n 8 F     S    \n 9 G     D    \n10 H     E    \n\n\n\nsetdiff(x, y): find all observations that are in x but not in y: here you have 3 observations where there is data in dfi1 but not in dfi2:\n\n\nsetdiff(dfi1, dfi2)\n\n# A tibble: 3 × 2\n  var1  var2 \n  &lt;chr&gt; &lt;chr&gt;\n1 C     S    \n2 D     T    \n3 E     U    \n\n\n\nsymdiff(x, y): find all observations that are either in x but not in y or in y and not in x: here you have 6 observations that are not shared across these two data frames:\n\n\nsymdiff(dfi1, dfi2)\n\n# A tibble: 6 × 2\n  var1  var2 \n  &lt;chr&gt; &lt;chr&gt;\n1 C     S    \n2 D     T    \n3 E     U    \n4 F     S    \n5 G     D    \n6 H     E    \n\n\n\n\n\n\n\n\nYour turn: NYC Flights\n\n\n\n\n\nThe dataset includes cancelled flights. Suppose that you want to show a table that shows the number of cancelled flights per New York airport as a percentage of the total number of flights. To so so, first create a tibble with the number of cancelled flights per airport. Assign this tibble to n_cancelled and call the statistic n_can_flights.\n\n\nCode\nn_cancelled &lt;- nycflights |&gt; \n  filter(status == \"cancelled\") |&gt;\n  group_by(origin) |&gt;\n  summarize(n_can_flights = n()) |&gt;\n  ungroup()\n\n\nCreate the same tibble, but now include all flights. Assign the result to n_totaland call the statistic n_tot_flights.\n\n\nCode\nn_total &lt;- nycflights |&gt; \n  group_by(origin) |&gt;\n  summarize(n_tot_flights = n()) |&gt;\n  ungroup()\n\n\nMerge these two datasets. The identifier in both is origin: add the n_total to n_cancelled.\n\n\nCode\nn_cancelled &lt;- n_cancelled |&gt; \n  left_join(n_total, by = \"origin\")\n\n# Here, there is a one on one relation between both tibbles. inner_join, \n# full_join or right_join would have shown the same result. \n\n\nUsing n_cancelled, calculate the percentage of cancelled flights: as n_can_flights / n_tot_flights * 100.\n\n\nCode\nn_cancelled &lt;- n_cancelled |&gt; \n  mutate(per_cancelled = n_can_flights / n_tot_flights * 100)\n\n\nDoes the tibble n_cancelled return the table you need? Which one of the NY airports has highest percentage of cancelled flights.\n\n\nCode\nn_cancelled\n\n\n# A tibble: 3 × 4\n  origin n_can_flights n_tot_flights per_cancelled\n  &lt;chr&gt;          &lt;int&gt;         &lt;int&gt;         &lt;dbl&gt;\n1 EWR             3633        138578          2.62\n2 JFK             2848        133048          2.14\n3 LGA             4257        163726          2.60\n\n\nHere we used the fact that {dplyr} functions return a tibble or data frame. In other words, you can add them to in subsequent functions as a standard tibble. Here, we added the summary tables n_cancelled and n_total using left_join() and used mutate() on the resulting table.\nCreate create a table with the total number of complete flights per carrier. Assign this data frame to tot_flights_carrier and call the statistic tot_flights.\n\n\nCode\ntot_flights_carrier &lt;- nycflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(tot_flights = n()) |&gt;\n  ungroup()\n\n\nThis dataset now includes the total number of flights as well as the carrier abbreviation. We would like to add the full carriers name. {nycflights23} include date on the airplane. This dataset is included in nycflights23::airlines and includes both the carrier abbreviation as well as its full name. We can now use this dataset to add the full carrier name to the data frame tot_flights_carrier. Use nycflights::airlines and join with tot_flights_carrier. The common identifier is carrier.\n\n\nCode\ntot_flights_carrier &lt;- tot_flights_carrier |&gt;\n  left_join(nycflights23::airlines, by = \"carrier\")\n\n\nInspect the merged data frame. As you can see, the carrier name is in the last column. Relocate that column to the first, and arrange in descending order of tot_flights.\n\n\nCode\ntot_flights_carrier |&gt; \n  relocate(name, .before = \"carrier\") |&gt;\n  arrange(desc(tot_flights))\n\n\n# A tibble: 14 × 3\n   name                   carrier tot_flights\n   &lt;chr&gt;                  &lt;chr&gt;         &lt;int&gt;\n 1 Republic Airline       YX            88785\n 2 United Air Lines Inc.  UA            79641\n 3 JetBlue Airways        B6            66169\n 4 Delta Air Lines Inc.   DL            61562\n 5 Endeavor Air Inc.      9E            54141\n 6 American Airlines Inc. AA            40525\n 7 Spirit Air Lines       NK            15189\n 8 Southwest Airlines Co. WN            12385\n 9 Alaska Airlines Inc.   AS             7843\n10 SkyWest Airlines Inc.  OO             6432\n11 Frontier Airlines Inc. F9             1286\n12 Allegiant Air          G4              671\n13 Hawaiian Airlines Inc. HA              366\n14 Envoy Air              MQ              357\n\n\n{nycflights23} include date on the airplane. This dataset is included in nycflights23::planes. In the introduction, there is a list of the data included in the data frame for each plane in nycflights. The identifier in both nycflights and planes is tailnum. Add the nycflights23::planes dataset to nycflights.\n\n\nCode\nnycflights &lt;- nycflights |&gt; left_join(nycflights23::planes, by = join_by(tailnum == tailnum))\n\n\nBoth nyflights and planes include a variable year. After the merger, these are shown as year.x for nycflights and year.y for planes. Rename year.x back to year and year.y to year_construction (year of construction).\n\n\nCode\nnycflights &lt;- nycflights |&gt; rename(c(year = year.x, year_construction = year.y))\n\n\nFor flights less than 180 miles, whose airplanes (manufacturer) are most used? Store this statistic in n_manuf_180.\n\n\nCode\nnycflights |&gt; \n  filter(distance &lt; 180) |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(n_models_180 = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 8 × 2\n  manufacturer                  n_models_180\n  &lt;chr&gt;                                &lt;int&gt;\n1 AIRBUS                                   3\n2 AIRBUS INDUSTRIE                         2\n3 BOEING                                   3\n4 BOMBARDIER INC                        2850\n5 EMBRAER                               1788\n6 EMBRAER S A                           1735\n7 EMBRAER-EMPRESA BRASILEIRA DE          787\n8 &lt;NA&gt;                                    21\n\n\nNote that we have missing data for 21 observations. Is that different for flights of more than 2500 miles? Store this statistic in n_manuf_2500.\n\n\nCode\nnycflights |&gt; \n  filter(distance &gt;= 2500) |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(n_models_2500 = n()) |&gt;\n  ungroup()\n\n\n# A tibble: 4 × 2\n  manufacturer     n_models_2500\n  &lt;chr&gt;                    &lt;int&gt;\n1 AIRBUS                    3314\n2 AIRBUS INDUSTRIE          1253\n3 BOEING                    8953\n4 &lt;NA&gt;                       324\n\n\nDetermine the average age of the fleet for every carrier ave_age_fleet and the average departure delay ave_dep_delay. For the former, you can use the difference between 2023 and the year of construction. Show the table descending in ave_age_fleet. Show the full name or the carrier. Write this without storing intermediate results.\n\n\nCode\nnycflights |&gt; \n  mutate(age_fleet = 2023 - year_construction) |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    ave_age_fleet = mean(age_fleet, na.rm = TRUE),\n    ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  arrange(desc(ave_age_fleet)) |&gt;\n  left_join(nycflights23::airlines, by = \"carrier\") |&gt;\n  relocate(name, .before = \"carrier\") |&gt;\n  ungroup()\n\n\n# A tibble: 14 × 4\n   name                   carrier ave_age_fleet ave_dep_delay\n   &lt;chr&gt;                  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 United Air Lines Inc.  UA              16.5          17.6 \n 2 JetBlue Airways        B6              14.3          23.8 \n 3 Delta Air Lines Inc.   DL              13.6          15.1 \n 4 American Airlines Inc. AA              12.9          14.2 \n 5 Republic Airline       YX              12.4           4.21\n 6 Envoy Air              MQ              12.3          10.5 \n 7 Endeavor Air Inc.      9E              12.2           7.44\n 8 Southwest Airlines Co. WN              11.3          16.1 \n 9 Hawaiian Airlines Inc. HA               9.73         22.9 \n10 Spirit Air Lines       NK               7.44         18.2 \n11 Alaska Airlines Inc.   AS               6.78         12.0 \n12 SkyWest Airlines Inc.  OO               6.55         19.8 \n13 Frontier Airlines Inc. F9               4.14         35.7 \n14 Allegiant Air          G4             NaN             3.98\n\n\nYou can calculate the number of seats offers for every route. This capacity or supply on any given routs in terms of the number of seats. Calculate the total number of seats available for every route for every day of the year. Add this variable to nycflights as n_seats_route.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  group_by(origin, dest, month, day) |&gt; \n  mutate(n_seats_route = sum(seats)) |&gt;\n  ungroup()\n\n\nOn what day and route did airlines offer most seats?\n\n\nCode\nnycflights |&gt;\n  filter(n_seats_route == max(n_seats_route, na.rm = TRUE)) |&gt;\n  select(year, month, day, origin, dest, n_seats_route) |&gt;\n  unique()\n\n\n# A tibble: 1 × 6\n   year month   day origin dest  n_seats_route\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;int&gt;\n1  2023    11    26 JFK    LAX           10992\n\n\nThe nycflights23::weather dataset includes hourly observations for the weather in each of the 3 New York airports. This dataset includes e.g. temperature, wind speed, gust and direction, visibility, precipitation, … . In also includes a year, month, day and hour variable. To merge this dataset with the flights data, we need a common identifier. Here are two candidates: the departure hour or the scheduled departure hour. We’ll choose the second. Recall that this variable is acturate up to the minute. The time_hour variable in the weather dataset includes an hourly observations. To be able to merge both, we first need to add a hourly measure of time in nycflights. Recall that {lubridate} includes a number of round functions. Use one of these funtions to round sched_dep_hhmm to the nearest hour and store this variable as sched_dep_hhmm_rounded.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    sched_dep_hhmm_rounded = round_date(sched_dep_hhmm, unit = \"hour\")\n  )\n\n\nJoin nyflightswith nycflights23::weather. Remove the variables year, month, day and hour from this dataset and only add the remaining variables. Try to do so in one join functions (note that you can also do this is multiple steps).\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  left_join(\n    nycflights23::weather |&gt; select(-c(year, month, day, hour)), \n    by = join_by(sched_dep_hhmm_rounded == time_hour, origin == origin))\n# note that here we exploit the fact that the select function returns a tibble\n# doing so, we can use this function within inner, left, full or right_join.\n# Also note: select using a negative index to remove variables\n\n\nLet’s first show a summary of some statistics. Use across to generate a table with the 10th percentile, median and 90th percentile as well as the minimum and maximum of wind_dir, wind_speed, wind_gust, temp, precip and visib. Assign the result to a data frame weather_sum.\n\n\nCode\nweather_sum &lt;- nycflights |&gt;\n  summarize(\n    across(\n      .cols = c(wind_dir, wind_speed, wind_gust, temp, precip, visib), \n      .fns = list(q10 = \\(x) quantile(x, prob = 0.10, na.rm = TRUE),\n                  q50 = \\(x) quantile(x, prob = 0.50, na.rm = TRUE),\n                  q90 = \\(x) quantile(x, prob = 0.90, na.rm = TRUE),\n                  min = \\(x) min(x, na.rm = TRUE), \n                  max = \\(x) max(x, na.rm = TRUE)\n                  ), \n      .names = \"{.fn}_{.col}\"\n    )\n  )\n\n\nSelect the columns that include wind_speed from weather_sum.\n\n\nCode\nweather_sum |&gt; select(contains(\"wind_speed\"))\n\n\n# A tibble: 1 × 5\n  q10_wind_speed q50_wind_speed q90_wind_speed min_wind_speed max_wind_speed\n           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1           2.30           8.06           16.1              0           35.7\n\n\nIs the average departure delays longer is there wind blows stronger? To answer this question, use these values to create a table with the mean departure delay ave_dep_delay.\n\n\nCode\nnycflights |&gt; \n  group_by(wind_speed_cat = cut(wind_speed, breaks = c(0, 2.30, 8.06, 16.1, 35.7))) |&gt;\n  summarize(ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 4 × 2\n  wind_speed_cat ave_dep_delay\n  &lt;fct&gt;                  &lt;dbl&gt;\n1 (2.3,8.06]              15.5\n2 (8.06,16.1]             11.5\n3 (16.1,35.7]             12.9\n4 &lt;NA&gt;                    16.6\n\n\nYou can do a similar analysis with e.g. precipitation:\n\ndetermine the quantiles:\n\n\n\nCode\nweather_sum |&gt; select(contains(\"precip\"))\n\n\n# A tibble: 1 × 5\n  q10_precip q50_precip q90_precip min_precip max_precip\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1          0       0.01       0.04          0       0.64\n\n\n\ncalculate the average delay:\n\n\n\nCode\nnycflights |&gt; \n  group_by(precip_cat = cut(precip, breaks = c(0, 0.01, 0.04, 0.64))) |&gt;\n  summarize(ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 4 × 2\n  precip_cat  ave_dep_delay\n  &lt;fct&gt;               &lt;dbl&gt;\n1 (0,0.01]             19.7\n2 (0.01,0.04]          20.7\n3 (0.04,0.64]          23.9\n4 &lt;NA&gt;                 13.6\n\n\nor visibility:\n\ndetermine the quantiles\n\n\n\nCode\nweather_sum |&gt; select(contains(\"visib\"))\n\n\n# A tibble: 1 × 5\n  q10_visib q50_visib q90_visib min_visib max_visib\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         7        10        10      0.24        10\n\n\n\ncalculate the average:\n\n\n\nCode\nnycflights |&gt; \n  group_by(visib_cat = cut(visib, breaks = c(0.24, 7, 10))) |&gt;\n  summarize(ave_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ungroup()\n\n\n# A tibble: 3 × 2\n  visib_cat ave_dep_delay\n  &lt;fct&gt;             &lt;dbl&gt;\n1 (0.24,7]          22.7 \n2 (7,10]            12.7 \n3 &lt;NA&gt;               8.02\n\n\nThe final dataset we are going to add is nycflights23::airports. This dataset includes the names, location, timezone, UTC offset and a daylight savings time indicator. The common identifier is faa in nycflights::airports and dest in nycflights. Add the former to the latter. Here, you can use nycflights23::airports or, as an alternative, first assign this dataset to another variable and use the latter to merge. Here, to save space, I use the former approach.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  left_join(nycflights23::airports, by = join_by(\"dest\" == \"faa\")) \n\n\nLet’s now try to determine the total length, in minutes, of flights. We’ll calculate the total length as well as the scheduled length. To do so, we need to arrival times and scheduled arrival times. The dataset includes arr_time and sched_arr_time. These are both in UTC, i.e. they are in local times. To calculate the total time, we have to adjust these for the difference in time zones. A final adjustment includes daylight savings time. Here, some parts of the US have DST while others, have not. The airports dataset includes a variable dst which is “N” for those airports who are located in a region where there is no daylight savings time adjustment.\nA note: daylight savings time adjustments/timezone adjustments are not part of the exam. However, the steps that I follow here show that you need a good knowledge of the dataset before you start any analysis.\nLet’s start with arr_time and, using similar steps as with dep_time, create a POSIXct data/time variable arr_time_utc:\n\ncreate two variables, arr_hour_utc and arr_minute_utc extracting that part of the information from arr_time. Add these variables after sched_dep_time.\n\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    arr_hour_utc = case_when(\n      arr_time &lt; 60 ~ \"0\", \n      arr_time &gt;= 60 ~ str_sub(arr_time, -4, -3)), \n    arr_minute_utc = str_sub(arr_time, -2, -1), \n    .keep = \"unused\", \n    .after = sched_dep_minute\n  )\n\n\n\nchange arr_hour_utc and arr_hour_utc in type integer:\n\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    across(.cols = contains(c(\"arr_minute\", \"arr_hour\")), \n           .fns = \\(x) as.integer(x))\n  )\n\n\n\ncreate the POSIXct date/time variable arr_hhmm_utc.\n\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    arr_hhmm_utc = make_datetime(\n      year = year, \n      month = month, \n      day = day, \n      hour = arr_hour_utc, \n      min = arr_minute_utc, \n      tz = \"UTC\"),\n    .keep = \"all\", \n    .after = sched_dep_hhmm)\n\n\nHere, we need a first adjustment. If the departure hour (e.g. 22) is smaller then the arrival hour (e.g. 1), then the arrival day is the next day. In the previous code, we used the departure day day variable. To adust for that, you need to add one day to arr_hhmm_utc if dep_hour &lt; arr_hour_utc. If that condition is not met - departure hour is smaller then or equal to arrival hour, then there is no need for any adjustment. Use case_when() to make this adjustment. Recall that you can use one of {lubridate}’s functions if to add a day, hour, … to a date. In case you don’t recall how you can do so, try {lubridate}’s cheat sheet.\n\n\nCode\nnycflights &lt;- nycflights |&gt; \n  mutate(\n    arr_hhmm_utc = case_when(\n      dep_hour &gt; arr_hour_utc ~ arr_hhmm_utc + days(1), \n      dep_hour &lt;= arr_hour_utc ~ arr_hhmm_utc)\n  )\n\n\nYou can now calculate the time zone corrected arrival date/time. To do so, you can use the variable tz. This shows you the UTC offset in the timezone of the airport. For New York, this offset is -5. If the offset is -8 (e.g. LA), you can add 3 hours to arr_hhmm_utc to arrive at arr_hhmm_tzc, the time zone corrected data/time of arrival. You can locate the new variable after arr_hhmm_utc.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    arr_hhmm_tzc = arr_hhmm_utc + hours(abs(tz) - 5),\n    .keep = \"all\", \n    .after = arr_hhmm_utc)\n\n\nYou can now calculate the scheduled arrival time and the time zone adjusted scheduled arrival time using the difference between the arr_hhmm_utc and the arr_delay and the difference between arr_hhmm_tzc and arr_delay. Locate these variables after arr_hhmm_tzc. Recall that the delays are measured in minutes.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    sched_arr_hhmm_utc = arr_hhmm_utc - minutes(arr_delay), \n    sched_arr_hhmm_tzc = arr_hhmm_tzc - minutes(arr_delay), \n    .after = arr_hhmm_tzc\n  )\n\n\nThere is on final adjustment: daylight savings time. Here I’ll give you the code:\n\n# DST was on in the US on March 12th, 2023\ndst_on &lt;- make_datetime(\n  year = 2023,\n  month = 3,\n  day = 12,\n  hour = 2,\n  tz = \"UTC\")\n\n# DST was off in the US on November 5, 2023\ndst_off &lt;- make_datetime(\n  year = 2023,\n  month = 11,\n  day = 5,\n  hour = 2,\n  tz = \"UTC\")\n\n# DST for dep_hhmm is off before 2023-3-12 and after 2023-11-5 and on in between\nnycflights &lt;- nycflights |&gt; \n  mutate(dst_on_orig = case_when(\n    dep_hhmm &lt; dst_on ~ FALSE,\n    dep_hhmm &lt; dst_off ~ TRUE,\n    dep_hhmm &gt;= dst_off ~FALSE)\n  )\n# DST for dep_hhmm is off before 2023-3-12 and after 2023-11-5 and on in between\n# IS the airport is in a region with DST, else, in a region without DST, DST is OFF.\nnycflights &lt;- nycflights |&gt; \n  mutate(dst_on_dest = case_when(\n    arr_hhmm_utc &lt; dst_on ~ FALSE,\n    arr_hhmm_utc &lt; dst_off & dst == \"A\" ~ TRUE,\n    arr_hhmm_utc &lt; dst_off & dst == \"N\" ~ FALSE,\n    arr_hhmm_utc &gt;= dst_off ~FALSE)\n  )\n\nWe now know for both destination and origin and for every data is Daylight Savings Time was on or off. If DST if on for both origin and destination, no change is required. If this is not the case and DST is on for New York, but not for the destination, the arr_hhmm_tzc and sched_arr_hhmm_tzc need to add 1 hour. Here is the code: try to understand what happens:\n\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    sched_arr_hhmm_tzc = case_when(\n      dst_on_orig == TRUE & dst_on_dest == FALSE ~ sched_arr_hhmm_tzc + hours(1),\n      !(dst_on_orig == TRUE & dst_on_dest == FALSE) ~ sched_arr_hhmm_tzc), \n    arr_hhmm_tzc = case_when(\n      dst_on_orig == TRUE & dst_on_dest == FALSE ~ arr_hhmm_tzc + hours(1),\n      !(dst_on_orig == TRUE & dst_on_dest == FALSE) ~ arr_hhmm_tzc)\n    )\n\nYou can now determine the total lenght of every flight as well as the scheduled length. You can do so in minutes. Use one of {lubridate}’s functions to determine the difference, in minutes, between arr_hhmm_tzc and dep_hhmm and store as total_time and between sched_arr_hhmm_tzc and sched_dep_hhmm as store as total_sched_time. Locate these two variables after air_time.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    total_time = time_length(arr_hhmm_tzc - dep_hhmm, unit = \"minute\"), \n    total_sched_time = time_length(sched_arr_hhmm_tzc - sched_dep_hhmm, unit = \"minute\"),\n    .after = air_time)\n\n\nTaxi time is known as the time in the airplane but on the ground, taxiing from and to the gates. Calculate taxi_time as the difference between total_time and air_time and store after total_time.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(taxi_time = total_time - air_time, \n         .after = total_time)\n\n\nCalculate the actual speed speed, scheduled speed sched_speed and air_speed as the ratio of distance to (total_time / 60), distance to (total_sched_time / 60) and distance to (air_time / 60). This variables equals the number of miles per hour.\n\n\nCode\nnycflights &lt;- nycflights |&gt;\n  mutate(\n    speed = distance / (total_time / 60),\n    air_speed = distance / (air_time / 60), \n    sched_speed = distance / (total_sched_time / 60)\n  )\n\n\nCalculate the average of these speed variable for each of the models that are used for flights &gt; 2500 miles. Name these variables ave_air_speed_2500, ave_speed_2500 and ave_sched_speed_2500. Store this data frame in speed_model.\n\n\nCode\nspeed_model &lt;- nycflights |&gt; \n  filter(distance &gt; 2500) |&gt;\n  group_by(model) |&gt;\n  summarize(\n    ave_air_speed_2500 = mean(air_speed, na.rm = TRUE), \n    ave_speed_2500 = mean(speed, na.rm = TRUE),\n    ave_sched_speed_2500 = mean(sched_speed, na.rm = TRUE)\n  )\n\n\nIf you look at the model, you’ll see that here are various types within a model, e.g. 737-8, 739-890, A320-243, … . Using speed_model, extract the first part of the model (e.g. 737, A320) before the - and assign name this variable model_type. Using this result, show the averages per modeltype (statistically, this is not correct as you should take into account the number of flights with each type to determine the average across types). Use str_extract() and a regular expression to extract the first part of the model.\n\n\nCode\nspeed_model |&gt; \n  mutate(model_type = str_extract(model, pattern = \"^[a-zA-Z0-9]+\")) |&gt;\n  group_by(model_type) |&gt;\n  summarize(\n    ave_air_speed_2500 = mean(ave_air_speed_2500, na.rm = TRUE), \n    ave_speed_2500 = mean(ave_speed_2500, na.rm = TRUE), \n    ave_sched_speed_2500 = mean(ave_sched_speed_2500, na.rm = TRUE) \n  )\n\n\n# A tibble: 9 × 4\n  model_type ave_air_speed_2500 ave_speed_2500 ave_sched_speed_2500\n  &lt;chr&gt;                   &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n1 737                      460.           423.                 400.\n2 757                      454.           411.                 386.\n3 767                      471.           438.                 429.\n4 777                      476.           429.                 412.\n5 787                      472.           426.                 411.\n6 A320                     454.           419.                 401.\n7 A321                     449.           406.                 388.\n8 A330                     452.           422.                 420.\n9 &lt;NA&gt;                     456.           414.                 399.\n\n\n\n\n\n\n\n\n\n\n\nCouch, Simon P. 2025. Anyflights: Query ’Nycflights13’-Like Air Travel Data for Given Years and Airports. https://github.com/simonpcouch/anyflights.\n\n\nHenry, Lionel, and Hadley Wickham. 2024. Tidyselect: Select from a Set of Strings. https://tidyselect.r-lib.org.\n\n\nIsmay, Chester, Simon P. Couch, and Hadley Wickham. 2024. Nycflights23: Flights and Other Useful Metadata for NYC Outbound Flights in 2023. http://github.com/moderndive/nycflights23.\n\n\nWickham, Hadley. 2022. Nycflights13: Flights That Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.",
    "crumbs": [
      "Importing and wrangling data in the tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "09_ggplot.html",
    "href": "09_ggplot.html",
    "title": "9  Data visualization: the grammer of graphics",
    "section": "",
    "text": "9.1 The grammer of graphics\nThere are 7 components to the grammar of graphics. We will illustrate these components using a graph which plots per capita gdp and life expectancy at birth for 2000. This dataset is included in your data &gt; raw folder in the file life_df.csv. You can import it using:\nlife_df &lt;- readr::read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"), show_col_types = FALSE)",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data visualization: the grammer of graphics</span>"
    ]
  },
  {
    "objectID": "09_ggplot.html#the-grammer-of-graphics",
    "href": "09_ggplot.html#the-grammer-of-graphics",
    "title": "9  Data visualization: the grammer of graphics",
    "section": "",
    "text": "9.1.1 The data\nFirst component is the data. Let’s review a sample of the data that we have:\n\nlife_df |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 8\n  iso2c iso3c country       date gdp_capita life_exp      pop region            \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;             \n1 TM    TKM   Turkmenistan  2019      6718.     69.0  5942000 Europe & Central …\n2 MA    MAR   Morocco       1978      1301.     53.5 19059770 Middle East & Nor…\n3 BS    BHS   Bahamas, The  1999     31563.     71.8   294062 Latin America & C…\n4 KW    KWT   Kuwait        1965        NA      62.2       NA Middle East & Nor…\n5 IS    ISL   Iceland       2011     48934.     82.4   319014 Europe & Central …\n\n\nThe data includes variables identifying the country, a variable date which shows the year for each observations, gdp_capita measuring income per capita in constant USD, life_exp which measures average life expectancy at birth, pop which show the total population of a country and region, the location of the country according to the Word Bank’s regional country groupings. The first three variables are continuous (numeric) variables. Region is a categorical or factor variable. It’s levels don’t imply an ordering. In other words it is measured on a nominal scale. The diamonds dataset includes numeric variables, carat, price, x, y, z, table and depth and categorical variables cut, color and clarity. For these three categorical variables, the values imply an ordering. For instance, for cut, ideal is better than premium, premium is better than very good, … .\nRecall that tidy data needs to meet at least 3 conditions: all variables are in the columns, all observations in the rows and each cell represents one value. For life_df as well as diamonds these conditions are satisfied.\n\n\n9.1.2 Aesthetic mappings\nThe second component are the aesthetics mappings or aesthetics. Aesthetics are the features of the visualization and the show where you will see the values of a variable in the graph. For instance, a variable can be mapped on the horizontal or vertical axis or its values can be shown using a different color (hue), a different size or shape of opacity (transparency or intensity). In other words, aesthetic mapping define where you need to look to find the values for a variable. In Figure 9.1, the variable gdp_capita is mapped on the horizontal axis, life_exp on the vertical axis, region on the color aesthetic and pop on the size aesthetic. Note that R doesn’t differentiate between color or colour.\n\n\n\n\n\nFigure 9.1: Life expectancy at birth, per capita GDP, region and population, color aesthetic\n\n\n\n\n\n\n\n\nTo see the values for variable gdp_capita in Figure 9.1, you need to look at the horizontal axis, the values for life_exp are shown on the vertical axis. The values for region show up as different colors in the graph and pop values are shown using the size of the circle.\nAs an alternative, we could have mapped the region on the shape aesthetic. In that case, every different value for region is shown by a different shape.\n\n\n\n\n\nFigure 9.2: Life expectancy at birth, per capita GDP, region and population, shape aesthetic\n\n\n\n\n\n\n\n\nThe shape, color, or size are aesthetics. However, you can also use different shapes, sizes or colors in your graph. This is done in Figure 9.3. In this graph, per capita GDP is mapped on the horizontal axis and life expectancy on the vertical axis. There are no other variables mapped on the other aesthetics such as color or size. The color, size or shape of a “point” on the graph was chosen for lay out purposes: the color is HEX #FF7F50 and the shape is a square and its size is 5. However, the color, shape or size do not reveal an additional information. In other words, if there not no variable mapped on an aesthetic, that shape (or color) is meaningless. Although this could also be the result of an aesthetic mapping because, e.g. all countries in the dataset without missing values are from the same region and would be shown with the same color, usually a single color is inspired by requirements in terms of lay-out, not to represent a value in the graph.\n\n\n\n\n\nFigure 9.3: Life expectancy at birth and per capita GDP\n\n\n\n\n\n\n\n\nIn some case one of the aesthetic mapping is determined by the type of the graph (or, the geometry). In Figure 9.4 the variable cut is mapped on the horizontal axis and the clarity on the color aesthetic. The number of observations, which is a statistic, is mapped on the vertical axis. Here, the choice of the chart - a bar chart - also implies the an aesthetic mapping.\n\n\n\n\n\nFigure 9.4: Cut and clarity: number of observations in the diamonds dataset (sample 10%)\n\n\n\n\n\n\n\n\nSecond, not all aesthetics are suitable for all variables. Say to map a variable on a shape. Doing so, the values are shown using a dot, a cross, a trianble, a cube, … . There is no order in these shapes: a cub is not bigger or smaller than a circle or a filled dot. In other words, using a shape to map an ordinal variable misses to some extent the “point”: you want to show order with your aesthetic, but the choice has no order. A color has order: yellow, orange, red, green, blue, black; lightblue, blue, darkblue, shades of red or grey.\nThe choice of the aesthetic is important to build good visualizations. Human beings can detect differences in line length, shape, orientation, hue or position with little or no conscious effort. This is called pre-attentive processing and is done very fast and in parallel. These attributes are called pre-attentive attributes. In a data visualization, you should map variables on aesthetics using attributes that a person can identify with little or no effort? In other words, if you use orientation, line length, line width, size, shape, color (hue), opacity (intensity) or spatial position, a data visualization will work much better then one where this is not the case. Other attributes are more difficult to process: an area is harder to compare, especially if the differences are subtle. People are usually much better (i.e. faster and with less effort) at detecting differences in position or length than differences in surface area. Likewise, people are usually good at spotting differences in hue but need more effort to interpret differences in intensity. With respect to hue, they are usually much better at identifying differences in a limited set of colors, but require more effort to perceive differences between many shades of the same color. In other words, to compare numeric values, mapping variables on aesthetics that involve positions (e.g. of points in a scatter plot or lines in a line graph) or length (such as the length of a bar in a bar chart) will work better than using variations in color intensity (e.g. a scale from red to green). Although numeric data might include a lot of values, people will process these much after if there is a difference in line length or width or position. This will require much more effort is these variables are mapped on a color aesthetic with a color palette including many shades of green or blue. Mapping categorical variables on color, size of shape aesthetics will make it easier to people (i.e. less conscious effort) to process that graph, as long as the number of colors, shapes or sizes is limited. For examples, see e.g. Stephen Few’s paper. In addition, for some aesthetics, notably color or shape, there are psychological associations that are in part culturally determined. The same holds for some shapes.\n\n\n9.1.3 Geometric object or geometry\nThe third component is the geometric object or geometry that is used to show the data. In Figure 9.1, that geometric object is a point: for every observations, the graph includes a point that corresponds to the country’s life expectancy and per capita GDP. In other words, if you move left from a specific point, you can read the country’s life expectancy. Moving south from a point shows you its per capita GDP on the horizontal axis. With the color aesthetic, the point also shows you in which region the country is located. The size of the point shows you how many people are living in that country. With the shape aesthetic, the point is shown as a circle, triangle, cross here each shape now represents a region … . Recall that colors, shapes and sizes derive their meaning from the variables mapped onto them. In other words “where” you read “what” is determined by the aesthetic mapping. The geometry determines “how” you show the data. The choice of the aesthetic mapping also “hides” some variables. For instance in Figure 9.1 or Figure 9.2 hide the individual countries. A point represents a unique “per capita GDP - Life expectancy - Region - Populations” quartet. If there are two more more countries with identical values for these variables, they will be shown by one point, with the same color and size (the graph will actually include one point for every country with identical values but as they overlap, will show only one). The geometry in Figure 9.4 is a bar. The height of the bar shows the number of observations, which is mapped on the vertical axis. cut, which is mapped on the horizontal axis, shows the level of the cut. clarity is mapped on the color aesthetic. Every bar allows you to determine the number of observations, for each level of cut from the total height of the bar. You can determine, for every level of cut the number of observations for every level of clarity. Other geometries are a line, boxplot, an area, … . All these show the data in a particular way. A line graph essentially is a collection of many points and allows you to determine, for any point on the line, the value of the variable mapped on the vertical axis given a value of the variable mapped on the horizontal axis. In other words, you can see a line graph as an extension of point plot. Whereas point plots show the value pairs for all individual observations (i.e. observed values), line plots show the value for the variable on the vertical axis for every possible (both observed and not observed) value on the horizontal axis. Not all geometries include the same set of aesthetics. A line graph needs a variable to map on the horizontal axis and one on the vertical axis. You can show lines for different categorical variables using e.g. a different color for every line representing a different category, a different line type (solid, dotted, dashed, …) or thickness. However, you can’t add a shape aesthetic. A boxplot, which shows the distribution using the e.g. median, 1st and 2rd quartile, 10th and 90th percentile, the mean and the minimum and maximum values will map these values on the vertical axis. In other words, this geometry does not need a variable to map on the vertical axis. For the boxplot geometry, the values on the y-axis are known from the choice of the geometry. However, use use e.g. the color aesthetic to show a boxplot for different categoeis. Every plot needs at least one geometry. However, you can combine geometries to create more complex plots.\n\n\n9.1.4 Scales\nThe fourth components includes the scales. Each mapping is associated with a scale. A scale shows the position of a value on the aesthetic. This scale is related to the type of the variable mapped. For instance, on the horizontal or vertical axis, the scale for a numeric variable shows the position of a value on that axis. In the life expectancy plot, you can see that the vertical axis has a linear scale while the horizontal axis has a log scale. For the vertical axis, the choice of a linear scale implies that the distance between two values will be equal if the difference in life expectancy is equal in absolute terms. With log scales, that is not the case: the distance between two values will be equal is the relative difference in per capita GDP is equal to a factor 10. You can see this in the plot: the difference between 1000 and 10000 is equal to the difference between 10000 and 100000. The scale of the other continues variable, pop changes with the size of the population: the size of the dot increases with the size of the population. For the categorical variables region, the scale is shown as a different color or shape. If you map a categorical variable on the horizontal axis, the relative position is usually not relevant. For instance, with a bar chart showing average life expectancy at birth for different regions, the distance between these bars is not informative. For numeric variable, the scale is continuous, for categorical variables, it is discrete. If you use a color to represent these scales, the color scale will be continuous for the former (all colors from e.g. green to red) and discrete for the latter (a fixed number of colors in a color palette). In the life expectancy plot, the colors are discrete.\n\n\n9.1.5 Statistics\nIn a plot, you can show the values of a variable, but also a transformation of these variables. If you show the values that implies that the plot doesn’t add any transformation. In other words, the values that you plot are identical to the values that you find in the data and you plot “identities”. The plot can also show transformations or summaries of the data, e.g. the total number of observations in the data for a given variable or categories of variables (count data), a probability that a variable occurs in the data (a density), a (linear) trend, … . Some geometries, notably those that summarize distributions such as a histogram of box plot, show such transformation by construction. However, you can also add statistical information to e.g. a column or point geometry. In the former for instance, you can show count data, in the second, you can add a linear trend. As an example Figure 9.5 shows a scatterplot (point geometry) with carat mapped on the horizontal axis, price and the vertical axis and cut mapped on the shape aesthetic. The figure also includes a statistical transformation: a smooth line connecting the conditional means for price. The line also include a confidence interval.\n\n\n\n\n\nFigure 9.5: Carat and price of diamons (sample 10%)\n\n\n\n\n\n\n\n\nAdding such a smooth line helps to interpret the data: as carat rises, so does the price. If carat low, there is less uncertainty with respect to the price level than when the value for carat is high. You can see this as the dispersion of the individual values around the smooth line widens as well as from the wider confidence intervals at higher levels of carat. In Figure 9.5, we show both the trendline as well as the individual values. In Figure 9.6, we remove the individual values from the graph. Here, the graph only shows a statistical transformation: the smooth line connecting the conditional mean values.\n\n\n\n\n\nFigure 9.6: Carat and price of diamons (sample 10%)\n\n\n\n\n\n\n\n\n\n\n9.1.6 Facets\nFaceting is another way to show differences between categorical variables. You can map them on an aesthetic, but you can also show them in different plots. Faceting shows the same graph for each level of a categorical variable. Figure 9.7 show an example of faceting. The main graph was already shown in Figure 9.5. In that graph, the level of the cut was shown by mapping that variable on the shape aesthetic. In Figure 9.7, we remove the mapping on the shape aesthetic and show one graph with carat mapped on the horizontal axis and price on the vertical axes for each level of the variable cut. Note that we could have dept the shape aesthetic. However, doing so wouldn’t add any value: each subplot would include the same shape. In addition, not that the trend line is now drawn for each level of cut.\n\n\n\n\n\nFigure 9.7: Carat and price of diamons per cut (sample 10%)\n\n\n\n\n\n\n\n\nRelative to using a color, shape of size aesthetic, groups are further apart using faceting and there is no overlap. In other words, faceting makes is more difficult to see small differences across groups. Mapping a categorical variable on e.g. a color aesthetic, will show small difference much clearer. Because of this, it is often usefull to add summary annotations or summary statistics that highlight these differences when usin faceting.\n\n\n9.1.7 Coordinate systems\nThe last component of the grammar of graphics is the coordinate system. There are two types of coordinate systems: linear and non-linear ones. The first includes the default Cartesian coordinate system and is shown in Figure 9.8.\n\n\n\n\n\nFigure 9.8: Cartesian coordinate system\n\n\n\n\n\n\n\n\nThe Cartesian coordinate system represents each point by two numbers (x, y) where x is the x-coordinate and y is the y-coordinate. The former measures the vertical distance from the y-axis and the latter the vertical distance from the x-axis. In Figure 9.8, the point (5, 15) represents a point where x equals 5 and y equals 15. Usually, but that is not always the case, if x and y are related, then the value of the x axis shows the cause and the value on the y-axis the consequence:\n\\[\ny = f(x)\n\\]\nYou can fix the ratio of the length of the x- and y-axis. Doing so with a ratio equal to 1 ensures that both x- and y- axis have equal scales. In other words, 1cm on the x-axis represents the same range as 1cm on the y-axis. Figure 9.9 shows Figure 9.8 but with a fixed ratio equal to 1.\n\n\n\n\n\nFigure 9.9: Cartesian coordinate system with fixed ratio\n\n\n\n\n\n\n\n\nFor some graphs, it can be convenient to flip the coordinates. Doing so with a bar or column chart for instance will show the categories on the vertical axis and not on the horizontal axis. Figure 9.10 show Figure 9.4 but the the coordinates flipped.\n\n\n\n\n\nFigure 9.10: Cut and clarity: number of observations in the diamonds dataset (sample 10%)\n\n\n\n\n\n\n\n\nNon-linear coordinate systems change the shape of the geometry. The first non-linear coordinate system results from a transformation. The example of a non-linear coordinate system is the polar coordinate system. Map projections are a third example of a non-linear coordinate systems. An example of the first includes was already shown in Figure 9.1. The x-axis was a log scale. Polar coordinates are used to create pie charts (see Chapter 2) and radar charts. In this system, a point is identified by the points distance from a reference point (the pole) and point’s direction relative to the direction of the polar axis. Moving from a Cartesian coordinate system to a polar one, requires that every point is represented by a distance from a reference point (or radius) and the point’s direction (or angle).\nTo see how a polar coordinate system changes the shape for the graph, we start from Figure 9.11 which shows a bar chart with the number of observations each category of clarity in the diamonds dataset. The clarity is mapped on the fill aesthetic, the number of observations on the vertical axis. The graph includes one bar: the total number of observations for all diamonds in the sample.\n\n\n\n\n\nFigure 9.11: Cartesian coordinate system\n\n\n\n\n\n\n\n\nFigure 9.12 includes the same data - the number of diamonds per clarity category - as Figure 9.11. However, now we draw the bar chart in a polar coordinate systems. You can now see that the number of observations is measured as a circle, where each circle has the same distance to the origin (the pole). In other words, they all have the same radius. However angle, the second part of a polar coordinate, is different and reflects the number of observations as a percent of the total number of observations: the more observations, the larger the angle and the large the share that clarity category reflects in the total pie.\n\n\n\n\n\nFigure 9.12: Polar coordinate system\n\n\n\n\n\n\n\n\nIf you compare Figure 9.12 with Figure 9.11, you’ll see that the latter is clearer than the former. Usually, a pie chart doesn’t allow you to see subtle differences in values because these show up as different slices of the pie, not as differences in the height or a bar as in e.g. Figure 9.13.\n\n\n\n\n\nFigure 9.13: Cartesian coordinate system\n\n\n\n\n\n\n\n\nMap projections are a third non-linear coordinate systems. To illustrate, Figure 9.14 shows the map of the world in a Cartesian coordinate system. Because the earth is not a Cartesian 2D frame, this graph does not show the earth as we know it.\n\n\n\n\n\nFigure 9.14: world map in a Cartesian coordinate system\n\n\n\n\n\n\n\n\nUsually, the map of the earth uses a Mercator projection. In Figure 9.15, we use this projection to plot a map of the earth.\n\n\n\n\n\nFigure 9.15: world map: Mercator projection\n\n\n\n\n\n\n\n\nChanging this projection, changes the coordinate system. As there are many projections, there are many ways to show the earth as a map.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data visualization: the grammer of graphics</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html",
    "href": "11_Changing_plot_layout.html",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "",
    "text": "11.1 Scales\nIn the previous section, we focused on the geometries and hardly touched upon the scales of e.g. the x and y axis or the color, size or fill aesthetics. In this section we will take a closer look at this component of the grammar of graphics. If you looked at the code for the graph with life expectancy, you saw some lines such as\nscale_x_continuous(\n transform = \"log\",\n breaks = c(100, 1000, 10000, 100000),\n labels = scales::label_currency(prefix = \"$\")) +\nor\nscale_colour_paletteer_c(\"grDevices::Purple-Green\")\nThese were two lines that change the scale of an aesthetic. The first changes the x-axis and the second changes the color used to show different values of life expectancy. In general the scale functions have a similar format: scale_AES_continuous/discrete. The word ‘scale’ is almost always part of the function. It refers to the scales component of the grammar of graphics. The AES part refers to the aesthetic: x, y, color, fill, linewidth, alpha, size, shape. Scales can be continuous or discrete. Not all aesthetics are suitable for continuous variables. For instance, there are only so much shapes as there are available in R. In addition, for some scale, there is also a manual variant. There are also some specific scales, e.g. for data/time variables, and scales to develop specific color palettes. Here, we will start with the continuous scales, we will then cover the discrete scales and end with the manual scales. In addition to the {ggplot2} scale functions, the {scales} package adds further possibilities. If you use that package, we’ll do so using scales::. In that way, it is clear what part of the code is {ggplot2} and what part of {scales}.\nSome scale functions are identical: scale_x_continuous() and scale_y_continuous() or scale_x_discrete() and scale_y_discrete() are identical, the same holds for e.g. scale_color_manual(), scale_fill_manual(), scale_shape_manual(), scale_size_manual(), scale_linewidth_manual() and scale_linetype_manual(). All these scales allow you to set the values for the aesthetic. They only differ in terms of the way to identify that aesthetic: a shape number, a color or size. In other words, although there are many scale functions, a lot of them share many similarities. The fact that you have so many of them is e.g. due to the fact that for each continuous numeric scale, you have an identical x and y scale, for each color scale, you have a discrete, continuous and manual scale.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "09_ggplot.html#the-non-data-parts-of-a-graph-titles-background-legend",
    "href": "09_ggplot.html#the-non-data-parts-of-a-graph-titles-background-legend",
    "title": "9  Data visualization: the grammer of graphics",
    "section": "9.2 The non-data parts of a graph: titles, background, legend",
    "text": "9.2 The non-data parts of a graph: titles, background, legend\nThe grammar of graphics deals with the data part of a graph. Graphs also include non data parts: titles or subtitles or captions; the panel background color, borders, major and minor grid lines, aspect ratio; axis titles and labels; a legend including its title, labels and position and, in case of faceting, the margins between facets - both vertical and horizontal - and the text that shows the various categories for each facet. Some of these components are illustrated in #fig-themes. This chart reproduces Figure 9.1 adds changes to the lay out. Note that the data components didn’t change: the per capita GDP is mapped on the horizontal axis, life expectancy on the vertical axis, the population size on the size aesthetic and the region on the color aesthetic. The geometry is a point graph. The scales of the x-axis are not linear (log), while those on the vertical axis are linear. The coordinate system is Cartesian and the plot isn’t shown in facets: all regions are on the graph. This chart - which is not intended to win a visualizatin beauty contest - allows to show various non-data components.\n\n\n\n\n\nFigure 9.16: Non data components of maps\n\n\n\n\n\n\n\n\nFigure 9.16 shows the various components of a graph. Using {ggplot2} you can adjust most of these components. This allows you to create a graph is very consistent is terms of layout and fits with e.g. your personal style, the style guide of the organisation your work for of the journal/newspaper, … you write the article for. First there is the full plot area. The borders of this area are shown in dark grey. The background color of the full plot area is grey. You can adjust both the background color, border color as well as the line width. This full plot area will be shown in e.g. a powerpoint presentation, a report, an article or a paper or book. Within the full plot area, you have the graph. The graph includes the title and subtitle, both shown at the top, the caption (shown at the bottom in bright green using a size 8 bold/italic font), the legend (shown at the left) and the panel. The latter includes the geometry. The background color of the panel is light yellow. The panel includes major and minor grid lines. Major grid lines separate the axis in major units. Minor grid lines separate the units delineated by major grid lines. The major grid lines for the x-axis is shown in blue and are drawn as a solid line; for the y-axis, they are shown in dark red and are also drawn as solid lines. For every major grid line, there is label on both the horizontal and vertical axis. The minor grid lines are shown in orange on both the horizontal and vertical axis. They are drawn using dotted lines for the horizontal axis and dashed lines for the vertical axis. At the bottom and to the left of the panel, you find the axis. In Figure 9.16, the horizontal axis is shown with a thick black line and the vertical axis with a thick grey line. Both axis include labels and a title. The title of the horizontal axis is shown in blue using a sans font, size 14 in bold and italics. The title of the vertical axis is shown in a size 16 plain (i.e. not bold/italics) mono font. The axis labels for the x-axis are in green using a size 10 italized mono font. They include a “$” sign to show that they are referring to USD. A size 12 bold dark green serif font was used for the labels on the vertical axis. The legend is shown on the left in Figure 9.16. In Figure 9.1, this was on the right side of the panel. You can also position the legend above of below the panel and axis. The background color for the legend is orange. The title of each legend is shown in lia. Both legends are shown in column format: one on top of the other. Using row format would show one next to the other. The title and subtitle are aligned with the panel. You can also align them with the plot (i.e. including the title on the y-axis), align them to the right or center them across the panel or graph. These two are shown in different fonts: a size 18 bold serif font for the title and a size 16 sans font, italized and in grey, for the subtitle. The graph is included within the full plot area with margins equal to 1cm at the bottom and right and 2cm at the right and and left. You can adjust these margin to increase the background area relative to the graph or to reduce that area.\nFigure 9.17 shows another variation of Figure 9.1. Here, the {ggthemes} `the_economist()´ theme was used. This theme adjusts non data components in line with the graphical style guide of the new paper The Economist. Note some key differences. There are no differences between the full plot background color and the background color of the panel. There are no major thick lines on the horizontal axis and those on the vertical one are shown using a white, solid line. The x-axis is highlighted using a black line which isn’t to wide. There is no vertical axis.\n\n\n\n\n\nFigure 9.17: Using the Economist theme\n\n\n\n\n\n\n\n\nHere, the title and subtitle are aligned with the panel. The legend, which is shown next to each other at the top, are aligned with the full plot area.\nWe will cover these layout options more in detail in Chapter 11. {ggplot2} and the many extensions referred to in the introduction allow you to determine the layout of year graph with a extremely high level of detail. Most other packages, e.g. Microsoft Excel, allow you to generate a graph fast but often take a lot of time to get the details of the graph correct. With {ggplot2}, once have your design options fixed, creating a new plot is usually as easy as piping a file into the ggplot() function.\n\n\n\n\n\n\nArnold, Jeffrey B. 2024. Ggthemes: Extra Themes, Scales and Geoms for ’Ggplot2’. https://jrnold.github.io/ggthemes/.\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.\n\n\nEngler, Jan Broder. 2024. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” bioRxiv. https://doi.org/10.1101/2024.11.08.621836.\n\n\nHvitfeldt, Emil. 2021. Paletteer: Comprehensive Collection of Color Palettes. https://github.com/EmilHvitfeldt/paletteer.\n\n\nPedersen, Thomas Lin. 2024a. Ggforce: Accelerating ’Ggplot2’. https://ggforce.data-imaginist.com.\n\n\n———. 2024b. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin, and David Robinson. 2024. Gganimate: A Grammar of Animated Graphics. https://gganimate.com.\n\n\nSlowikowski, Kamil. 2024. Ggrepel: Automatically Position Non-Overlapping Text Labels with ’Ggplot2’.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales: Scale Functions for Visualization. https://scales.r-lib.org.\n\n\nWilkinson, Leland. 2005. The Grammar Op Graphics. 2nd ed. Springer-Verlag, New York. https://doi.org/https://doi.org/10.1007/0-387-28695-0.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data visualization: the grammer of graphics</span>"
    ]
  },
  {
    "objectID": "09_ggplot.html#the-layered-grammer-of-graphics-ggplot.",
    "href": "09_ggplot.html#the-layered-grammer-of-graphics-ggplot.",
    "title": "9  Data visualization: the grammer of graphics",
    "section": "9.3 The layered grammer of graphics: ggplot.",
    "text": "9.3 The layered grammer of graphics: ggplot.\n{ggplot2} is one of the most widely used packages to create data visualizations in R. The “gg” stand for the grammar of graphics. In other words, {ggplot2} is an implementation of the grammar of graphics. It does so in a layered way: you build a plot layer by layer. To add a layer, you need to use a +, not a |&gt;. Using the + nicely shows that you add one layer on top of the other. The overall approach is straightforward: you first select the dataset and possibly the aesthetic mappings. The aesthetic mapping are includes in mapping = aes(). Here you include the aesthetics that will be used by all layers. In other words, subsequent layers inherit the aesthetic mappings. You then add one or more layers with the geometries. In {ggplot2} these are referred to as geom_type, e.g. geom_point, geom_line, … . Within these geometries, you can (further) add aesthetic mappings. However, in this case, these mapping are only relevant for that specific geometry. After the geometries, you define the scales. Depending on the variable mapped on an aesthetic, these scales are either continuous or discrete. For each scale, there are default values. In other words, if you don’t include that layer, {ggplot2} will produce a graph using default values for all scale parameters. Note that often, these default values produce elegant graphs. To change the default values, you use scale_xxx_continuous or scale_xxx_discrete where xxx refers to the aesthetic, e.g x, y, color, fill, .. . For most scales, you also have to option to personalize them using scale_xxx_manual. In addition, you can transform scale using e.g. scale_x_log10 or scale_y_reverse. Using these functions, you can also set limits, or define labels for the axis. The statistics are usually included in the geometry layer. You can also specify the coordinate system (Cartesian, polar, …). Here, you can usually include the minimum and maximum values of e.g. the horizontal axis and vertical axis. The last layer adds the theme. This layer will be covered in Chapter 11 but in short allows you to specify all components that are part of the non data part of a plot. In this chapter we will use a standard theme theme_minimal(). This theme removes all background annotations.\nRecall that you can save a plot (R save the plot as a list, (see Chapter 4)). Here, I will use this to add layers. For instance, suppose that there is a graph developed with ggplot(diamonds, aes(carat, price, color = cut)) + geom_point()? To illustrate the use of function scale_x_log10, we can save the first part e.g. using p1 for plot 1 and use p1 + scale_x_log10. This would be similar to ggplot(diamonds, aes(carat, price, color = cut)) + geom_point() + scale_x_log10.\n\n9.3.1 The data and aesthetic mappings\nTo build a plot, ggplot() needs to know the data. In this first layer, you can also specify the aesthetics. This functions is\n\nggplot(data = NULL, mapping = aes())\n\nThe argument data refers to the default data frame ggplot() will use for the plot. Recall that {ggplot2} is part of the {tidyverse}. In other words, you can use pipe operator |&gt; to “pipe” a dataset in the ggplot() function. If the object in data is not a data frame or tibble, R will try to convert the object into a data frame. If the argument is missing, each geometry has to specify the data frame to use in that geometry. The second argument mapping = aes(x = , y = , color =, size =, shape =, fill =) includes the aesthetic mappings for the plot. This argument too is optional. If this mapping is not specified here, each geometry will needs its own aesthetic mapping. You can partially map variables at this level and add aesthetic mappings in the geometry layer. For instance, here you would map a variable on the x- and y-axis and add a mapping on the color aesthetic in e.g. the point geometry. In that case, the mapping on the x- and y- axis will be used for all geometries in the plot while the color aesthetic will only be used for the point geometry.\nLet’s use a sample of 10% of the diamonds dataset\n\ndia &lt;- diamonds |&gt; slice_sample(prop = 0.10) \n\nand start a plot where we map carat on the horizontal axis and price on the vertical axis:\n\np1 &lt;- ggplot(data = dia, mapping = aes(x = carat, y = price)) + theme_minimal()\np1\n\n\n\n\n\n\n\n\nThis function returns the first layer of the plot: it shows the panel of the plot and the x- and y-axis. In addition, it shows the variables that were mapped on both axis: carat on the horizontal axis and price on the vertical axis. The limits of these axis (minimum and maximum value of the axis) are derived from the data. You can see this from the minimum and maximum values for carat and price:\n\ndia |&gt; summarise(\n  min_carat = min(carat),\n  max_carat = max(carat), \n  min_price = min(price),\n  max_price = max(price)\n)\n\n# A tibble: 1 × 4\n  min_carat max_carat min_price max_price\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;\n1       0.2      5.01       340     18777\n\n\nThe part `aes(x = carat, y = price) includes the aesthetic mapping. To see this, if you run this part separately, R returns the mapping:\n\naes(x = carat, y = price)\n\nAesthetic mapping: \n* `x` -&gt; `carat`\n* `y` -&gt; `price`\n\n\nBecause the aesthetic mapping in ggplot() is second after the data, usually, this part of shortened by eliminating the reference to the argument mapping in aes(). Similarly, because the data argument is always first, data = is usually dropped from the argument. The addition of theme_minimal() removes e.g. the background color from the panel.\nAt this stage, R is not able to add more aesthetic mappings to the output. All R knows at this stage is that the plot will include variable mappings on the x-axis and a y-axis. However, it is not able to show other aesthetic mappings, e.g. on color, size of shape. Here, R needs additional information from the geometry. With a point geometry, ggplot() will show these aesthetic mapping using a different color, size of shape of a point, with a line geometry, R will show these additional mappings by differentiating he lines using their color, width of type. However, if these aesthetic mappings are defined at this stage, R will include them in any subsequent geometry. The output from the aes() argument shows that cut is also mapped on the color. aesthetic.\n\nggplot(dia, aes(x = carat, y = price, color = cut))\n\n\n\n\n\n\n\naes(x = carat, y = price, color = cut)\n\nAesthetic mapping: \n* `x`      -&gt; `carat`\n* `y`      -&gt; `price`\n* `colour` -&gt; `cut`\n\n\n\n\n9.3.2 Geometry\nThe first layer includes the data and the aesthetic mappings that will be used for all geometries in the plot (unless another aesthetic is specified). At this stage, we know which variable R will shows using which aesthetic.We also know where R will find these variables: in the data frame included in the data argument of the ggplot() function. We don’t know how they will shown. To add this component, we need an additional layer: the one that includes the geometry and the statistics. {ggplot2} includes many geometries and I refer to the geometry section in the {ggplot2} and Chang (2025) or to view all these possibilities. Here, you’ll find the often used geometries: point geometries, line geometries, area geometries, bar and column charts and summarizing geometries.\n\n9.3.2.1 Point geometries\nA point geometry is used to show the correlation between two numeric, continuous variables. The first variable is mapped on the horizontal axis, the second on the vertical axis. These two, x and y are required aesthetics. For every observations, a point geometry shows the pair of values for the variable mapped on the x-axis and the variable mapped on the y-axis as a single dot. Note that you shouldn’t interpret “dot” in a literal sense as you can change that representation and use e.g. a cross, … .\n\n9.3.2.1.1 geom_point()\n\n9.3.2.1.1.1 The function\nTo point geometries, let’s use the point geometry geom_point(). The function includes the following arguments:\n\ngeom_point(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe arguments for geom_point() include an aesthetic mapping argument mapping, a data argument data, a stats argument with default identity and a position argument with default identity. The value NULL for the first two arguments means that R will use the default aesthetic mappings included in the ggplot(aes()) call. geom_point() needs at least two mappings: one on the x- and one on the y-axis. If these mappings are not included in the ggplot() call, you have to add them here. You can use the mapping argument in this geometry also to add additional aesthetic mappings or, if no defaults were supplied, define all the aesthetics for this geometry. The inherit.aes = TRUE argument shows that R will use the aesthetic mappings from the ggplot(aes()) call. If you wouldn’t want this, changing this into FALSE will remove these aesthetic mapping for that layer. Doing so, you need to add new aesthetic mappings to this geometry. With FALSE, the point geometry will not add the mapping included in mapping to the mappings in the ggplot(aes()) call. The stats argument allows you to define transformations. The default identity shows the the data as they are. The position argument with default identity shows the points as they are. However in some cases, one point actually masks two ore more points with the same value pair. Adding position = position_jitter() R will add a random variation to every point to the left or right and to the top or bottom. You can define the maximum width and height of that random variation using the arguments width = and height =. A third option for position = is stack. We’ll cover that postion more in depth when we discuss the area geometry geom_area(). In addition to these arguments, you can further specify the way in which R will plots the points, e.g. their color, fill, shape, size, stroke and alpha. You can further add the option na.rm  = TRUE. In that case, ggplot() will remove missing values without a warning. The default FALSE shows a warning. By default, ggplot() will add new mappings in this layer to the legend. If you don’t want to do so, you need to set inherit.aes = FALSE. The ... part allows you to define the settings. For instance, the color, size, shape or transparency of a “dot”. As we’ll see, you shouldn’t confuse setting with mapping. The setting color is part of the lay out of a graph. Using the aesthetic color you map a variable on that aesthetic. In other words, with a mapping the color has a meaning, as a setting, a color doesn’t.\nLet’s see what these arguments do. We start from ggplot(data = dia, mapping = aes(x = carat, y = price)) and add the geom_point() layer, accepting all defaults:\n\np1 + geom_point()\n\n\n\n\n\n\n\n\nRecall that p1 didn’t include the aesthetic mapping on color. In other words, this point geometry only includes the aesthetic mappings on the horizontal and vertical axis. By default, R uses black dots to represent each (carat, price) pair. Every dot shows one observation. The shape of the cloud illustrates the correlation between the variable on the horizontal and the one on the vertical axis. In Figure 9.18, there are six patterns shown in six panel. The first panel, shows no correlation. The point cloud does not show any pattern. The second panel include a pattern where the two variables are correlated, but not in a linear way. Using traditional measure for correlation, the result would suggest the absence of any correlation between variable 1 and variable 2. However, as panel 2 reveals: the correlation is actually strong, but not linear. Panels 3 to 6 show clouds that suggest weak positive correlation (panel 3), weak negative correlation (panel 4) and strong positive (panel 5) and negative (panel 6) correlation. These panel all show a point geometry. In other words, using a point geometry allows you to detect a pattern in the correlation between two variables, even if that correlation is not linear.\n\n\n\n\n\nFigure 9.18: Correlation patterns\n\n\n\n\n\n\n\n\n\n\n9.3.2.1.1.2 Changing the settings\nBefore we add additional aesthetic mappings, let’s first review what the options are for changing the settings of this plot. Recall that settings don’t add any new information to the plot and only change the way the plot is shown. By default, R shows the dots with a black color. You can change the color of the points into e.g. red. You can do so by adding color = \"red\" to geom_point():\n\np1 + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nAs an alternative, you can use “lightyellow”:\n\np1 + geom_point(color = \"lightyellow\")\n\n\n\n\n\n\n\n\nHere, we identify the colors by their name. As an alternative to the name you can also add a color’s HEX code. For instance, the color “lightsteelblue” has hex code “#BEC4DE”. You can enter this hex code to define the color:\n\np1 + geom_point(color = \"#BEC4DE\")\n\n\n\n\n\n\n\n\nHere too, R shows the color. The hex code and name are equivalent:\n\np1 + geom_point(color = \"lightsteelblue\")\n\n\n\n\n\n\n\n\nThere are many color pickers online where you can find these color codes. In addition, most of these color pickers allows you to generate complement colors, shades, … Here for instance, you can find HEX codes. Using their detail, you can find complements, … .\nUsing alpha, you can change the transparency of a point. This value is between 0 (absolute transparency) and 1 (no transparency). To illustrate, using the color red and transparency 1/5:\n\np1 + geom_point(color = \"red\", alpha = 1/5)\n\n\n\n\n\n\n\n\nThe plot now shows for which values the dataset includes a lot of observations. As these single, transparant, dots overlay, they produce a brighter color. Here, you can see that the dataset includes a lot of observations for diamonds with a lighter weight and lower price.\nTo adjust the size of the dots, you can use the size setting. For instance, setting the size = 5 shows much larger dots:\n\np1 + geom_point(color = \"orange\", size = 5)\n\n\n\n\n\n\n\n\nUsing size = 0.75 much smaller\n\np1 + geom_point(color = \"orange\", size = 0.75)\n\n\n\n\n\n\n\n\nIn addition to the color and size, you can also change the shape using shape =. To identify a shape, you can refer to its number (shown in Figure 9.19) or name (shown in Figure 9.20).\n\n\n\n\n\nFigure 9.19: Shapes by number\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.20: Shapes by name\n\n\n\n\n\n\n\n\nLet’s show the “dots” with a cross (shape 4):\n\np1 + geom_point(shape = 4)\n\n\n\n\n\n\n\n\nNote that the default color is the cross is black. Combining both the color (e.g. lightsteelblue) and shape setting:\n\np1 + geom_point(color = \"#BEC4DE\", shape = 4)\n\n\n\n\n\n\n\n\nShapes 21 to 24 can be further adjusted using color, fill and stroke settings. The first, color sets the color of the border, the second, fill, the color of the interior and stroke the width of the border. If you add size, this setting controls the size of the interior part. The total size the shape is the sum of the interior size and the stroke. You can see this in Figure 9.21. For instance, to draw a circle with an orange border, with stroke 2 and a light blue steel interior with size 4.\n\np1 + geom_point( shape = 23, color = \"orange\", fill = \"#BEC4DE\", size = 4, stroke = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.21: Non data components of maps\n\n\n\n\n\n\n\n\n\n\n9.3.2.1.1.3 Aesthetic mappings\nAdding aesthetic mappings, in contract to settings, change the information that is shown in the plot. You add these mappings in the aes() function. Doing so, ggplot() will show different colors, shapes, sizes or strokes for every value of the variable that was mapped on that aesthetic. To illustrate, let’s again start from p1 and map the variable cut on the aesthetic color. To do so, we add aes(color = cut) to the geom_point() function and keep all other default values:\n\np1 + geom_point(aes(color = cut))\n\n\n\n\n\n\n\n\nThe result now show different colors per level of cut: diamonds whose cut is ideal are shown with a yellow dot, premium with a light green dot, … . By default, R adds a legend which shows these values. Adding show.legend = FALSE would remove that legend. However, in that case, it would be difficult to interpret the color scale. Note how the plot shows each aesthetic mapping: the variable that was mapped on the horizontal axis is shown as a label under that axis, the variable that was mapped on the vertical axis is shown as a label with the vertical axis and the variable that was mapped on the color aesthetic is shown in the legend.\nAt this stage, adding the aesthetic mapping in the geom_point() function or in the ggplot(aes()) function shows similar results:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is due to the fact that all layers that follow the ggplot() function inherit the aesthetic mapping defined there. Without any aesthetic mapping in ggplot() you would have to add it in the geom_point() function:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn that case, the mapping is only relevant for the point geometry.\nWhat if you add aesthetic mapping on color as well as set a setting for color? In that case, R overrides the aesthetic mapping and shows all colors in the color defined in the setting. For instance:\n\np1 + geom_point(aes(color = cut), color = \"red\")\n\n\n\n\n\n\n\n\nIn the result, you can see that R shows all dots in red. In addition, it removes the legend. You can however, specify other settings, e.g. \n\np1 + geom_point(aes(color = cut), shape = 6)\n\n\n\n\n\n\n\n\nHere, R maps the variable cut on the aesthetic color and shows the “dots” using a triangle.\nIn addition to the color aesthetic, geom_point() also accepts the fill, shape, size and stroke aesthetic. In addition, you can include more mappings in the aes() function. To illustrate. Let’s add another aesthetic mapping and map color on the shape aesthetic:\n\np1 + geom_point(aes(color = cut, shape = color))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 286 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, R produces a warning: the shape palette can deal with a maximum of 6 discrete values and more than 6 become difficult to differentiate. If you look at the legend, you’ll see that R left of the shape for color “J”. In this case, you could change the mapping and map cut one shape and color on the aesthetic color:\n\np1 + geom_point(aes(color = color, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\nHowever, here too, R shows a warning. What R is essentially telling here is that you are mapping an ordinal variable on a aesthetic that doesn’t allow to show something in an ordered way: a cross is not better or worse than a dot. For ordinal variables, it is better to use another aesthetic that does allow to show the order. Color for instance, can be shown from light to dark and reflect in order in that way. The same holds for size. A better “cut” can be shown with a larger or smaller size. For instance, if you map color on size:\n\np1 + geom_point(aes(color = cut, size = color))\n\n\n\n\n\n\n\n\nthe plot shows color “D” using the smallest dot while “J” is shown using the largest dot. The shape aesthetic allows you to map nominal values. Here, the order is not relevant can R can show these values with different shapes. However, R will restrict the number of nominal values to 6.\nYou can map the same variable on two aesthetics. In that case, R will change both aesthetics in line with the values of the variable that is mapped on both. For instance, if you map cut on both color and shape, ggplot() shows different level of the variable cut using both a different color and shape: “Fair” is sown with a bleu dot, “Premium” with a lightbleu cross, … .\n\np1 + geom_point(aes(color = cut, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\n\n\n9.3.2.1.1.4 Adding jitter\nIn some cases, points are on top of each other. Consider the following graph:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph doesn’t make much sense, but is nicely shows that in some cases the same (clarity - price) pairs are on top of each other. As you can not see all that that are actually on the graph, this graph does not represent the data very will. In that case, you can add a bit a jitter to each plot. You can do so using the argument position =. By default the position is identity. This default tells ggplot() to draw the points in line with their (x-value, y-value) pair, even if there are points with 100% overlap. Using the jitter option, R adds a random noise to a plot. In other words, every dot is now shown using the (x-value, y-value) pair but for both R adds a random noise term. In other words it actually shows (x-value + noise, y-value + noise). As two points with 100% overlap (i.e. the same (x-value, y-value)) both get a different randome noise term, the plot will show them without that overlap. To illustrate:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = \"jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now show more points than without the jitter. This might seem counterintuitive, but plot with jitter actually shows more information than the plot without (see Figure 9.22)\n\n\n\n\n\nFigure 9.22: Adding jitter\n\n\n\n\n\n\n\n\nYou can controle the “amount of jitter” using position_jitter(width = NULL, height = NULL, seed = NA). Here, you can add both the width and height of the jitter. Because jitter is added both positive and negative, the total spread is twice the amount in width and height. The default values (also used if you refer to jitter) are 0.40. Doing so, the jitter occupies 80% of the width of the categorial variable. In Figure 9.22 for instance, you can see that the width of the clarity column in the plot with jitter is 80% of the total width of that column. Adding a value of more than 0.5 doesn’t make sense: in that case the width of one column would overlap with the width of another column. However, setting smaller values, reduces the width of the column with jitter. For instance, using 0.25 for both:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = position_jitter(width = 0.25, height = 0.25)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAdding a see value, allows you to reproduce the graph with “the same” random jitter.\nA third way to add jitter is to use the geometry geom_jitter() and not the geometry geom_point(). In addition to the arguments for the latter, the former also includes width and height from the positin_jitter() function.\n\n\n\n9.3.2.1.2 geom_text() and geom_label()\nA point (or another shape) is one of the ways to show data in a graph. Using text is a second. To illustrate, let’s use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have use this dataset in e.g. Figure 9.1 and Figure 9.2. We will remove some of the aesthetic mapping and scale attributes and start from this plot:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs you can see, we kept the aesthetic mappings on the x- and y-axis and color. However, we dropped the aesthetic mapping on size. We also removed the labels with the axis.\nlife_df includes the name of the country, as well as its ISO3 code. Using geom_text() we can now use these to plot these codes (or names). To do so we add the aesthetic label in the geom_text() function and map the ISO3 code on that aesthetic.\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nIn the results, the dots are now replaced by each the ISO3 code for each country. The aesthetic mapping on color now shows as a different test color for each region in the dataset. Note that you can include that aesthetic mapping also in the ggplot() function. However, given that the labels are usually very specific to this geometry, they are usually added in there and not in the default aesthetics for the entire plot. The function includes a couple of other function that are worthwhile to note:\n\ngeom_text(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  check_overlap = FALSE,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should seem familiar by now. The mapping needs at least a mapping on the x- and y-axis as well as a mapping on the aesthetic label. The can include these mappings in the geom_text() function or define them for all layers in the plot in ggplot(). The options that are new allow you to position to text. The first is check_overlap. By default, this is FALSE. Changing that into TRUE, ggplot() will avoid overlap in the text. nudge_x and nudge_y allow you to add some distance from the text to another geometry. For instance, if you plot also includes geom_point() using nudge_x will allow some horizontal space, while nudge_y some vertical space. Size is measured in mm (size_unit = \"mm\" alternatives are pt, cm, or pica pc.). The ... allow you to add setting, e.g font size, font family, color (in case that aesthetic is not used), angle, … . Let’s add a couple of settings, e.g. reduce the font size to 3, check for overlap, set the font family to “mono” and add a small angle (22.5°)\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(\n    aes(label = iso3c), \n    size = 3, \n    check_overlap = TRUE, \n    family = \"mono\", \n    angle = 22.5) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nAs an alternative to geom_text() you can use geom_label(). Let’s check the result of this geometry:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_label(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nThe function includes the following arguments:\n\ngeom_label(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  label.padding = unit(0.25, \"lines\"),\n  label.r = unit(0.15, \"lines\"),\n  label.size = 0.25,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nIn addition to those for geom_text() these arguments also allow you to adjust the border of the label: the amoung of padding in each label (label.padding), the rounding of the corners label.r and the label size. In addition, you get include setting to e.g fill the label. You can do the latter in the aes() function. Doing so, ggplot() fills the labels per color. To illustrate a couple of options. Here we wil how the label, where iso3c is mapped on the fill aesthetic, the color of the font is white and the labels have straight corners. The font family is serif. Note that we moved the color aesthetic from the ggplot() call to the fill aesthetic in the geom_label() call.\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp)) +\n  geom_label(\n    aes(label = iso3c, fill = region), \n    color = \"white\", \n    family = \"serif\", \n    size = 3, \n    label.r = unit(0, \"lines\")) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nYou can use these geometries together with geom_point(). Doing so, the graph includes a dot for every observations as well as a label:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() +\n  geom_text(\n    aes(label = iso3c), \n    check_overlap = TRUE, \n    nudge_x = 0.2, \n    nudge_y = 0.2, \n    size = 3) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n{ggrepel} allows you to add more detail to labels and text parts of a plot.\n\n\n9.3.2.1.3 geom_smooth()\nAlthough this geometry is not a point geometry, it is often used as an additional layer with a point geometry as it helps in identifying trends in (overplotted) data. To smooth a dataset is to add a function that approximates the important patterns in that dataset leaving out random noise or other rapid changes in the data. The function’s arguments are:\n\ngeom_smooth(\n  mapping = NULL,\n  data = NULL,\n  stat = \"smooth\",\n  position = \"identity\",\n  ...,\n  method = NULL,\n  formula = NULL,\n  se = TRUE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nYou can control more options if you use stat_smooth(). The aesthetics mapping needs at least a mapping on the x- and y- axis. Here, the statistic is smooth. The position = identity shows that the smooth function will show the predicted values for the variable mapped on the y-axis. With respect to the method, you can accept the default values and a analysis of those methods is left for more advanced statistics of econometrics classes. The same holds for the formula argument. By default, geom_smooth() adds a 95 confidence interval. You can change this level using level = 0.90 for a 90% confidence level or 0.99 for a 99% confidence level. If you don’t want that, you can change the default se = TRUE in FALSE. The ... allow you to change the settings e.g. color, … .\nLet’s return to the diamonds dataset where carat is mapped on the horizontal axis and price on the vertical axis (p1) and add geom_smooth().\n\np1 + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe plot shows the relation between carat on the one hand and price on the other. The expected price, given a value for carat, is shown by the blue line. The confidence level is shown as a confidence interval around that blue line. By default, is confidence level is 95%. R also shows the method used to calculate the smooth function. Let’s first change some of the settings, e.g. the line width of the blue line (linewidth =) and the color of both the line (color) and the background of the confidence interval (fill).\n\np1 + \n  geom_smooth(\n    level = 0.90, \n    linewidth = 0.75, \n    linetype = \"solid\", \n    color = \"#F59247\", \n    fill = \"#F3Be96\") +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nOften geom_smooth() is added as a layer in addition to geom_point(). This produces the following graph (where I add some transparency to the points in geom_point():\n\np1 + \n  geom_point(alpha = 1/10) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nLet’s see what happens if we add another aesthetic in the ggplot() call and map cut on the aesthetic color:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now shows 5 smoothed lines, one per level of cut. The lines are in the same color as the color chosen to map cut. Why did ggplot() return one smoothed function per cut? Recall that we the plot defined the aesthetic mappings in the ggplot() call. All other layers inherit this mapping. For geom_smooth() this means that it will plot a smooted line using the x- and y- values, but will do so for every level of every aesthetic. If you would include the size aesthetic for for clarity, geom_smooth() returns 13 smoothed lines: 5 for each level of cut and 8 for each level of clarity. For the latter, geom_smooth() changes the width of the line to show the level of the aesthetic. Even if we set se = FALSE to remove the confidence levels, this plot is hardly interpretable.\n\nggplot(dia, aes(x = carat, y = price, color = cut, linewidth = clarity)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth(se = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo avoid this while keeping the option to add other aesthetic mapping in addition to x and y, there are various options. First, you don’t define any aesthetic mapping at the level of the plot, but keep these for the individual geometries. As mappings in one geometry are not inherited by the others, geom_smooth() will only smooth using the variables mapped in its aesthetics mapping, e.g. only x and y. To illustrate:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  geom_smooth(aes(x = carat, y = price)) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere, we map the carat and price variables on the x- and y- axis and cut on the color aesthetic in the geom_point() call. In geom_smooth() we don’t include the mapping on color. Doing so, geom_smooth() only used the values mapped on the x and y axis to calculate the smoothed line. Note that this also allows you to add other aesthetics in geom_smooth(). For instance, you could draw a smoothed function per level of clarity while the points show the level of cut.\nThe second way to handle this is to include the aesthetic mapping at all layers have on common in the ggplot() call while adding those that are specific for each layer to the geometry for that layer. In the example: you add the aesthetic mapping of cut on color in geom_point() while adding not additional mapping to geom_smooth(). As geom_point() inherits the mappings from the ggplot() function, it it adds one addition mapping. geom_smooth() doesn’t add any other mapping. Doing so, it only used the mappings that it inherits:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere is one method and formula that is worth mentioning: a linear trend. The method to estimate a linear trend is lm of linear model. This is the method you would use to estimate linear regression models. The formula is y ~ x. This too is the formula you would use for bivariate linear regression models. Adding both allow you to add a linear trend:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n9.3.2.2 Line geometries\nIn the previous section, geom_smooth() resulted in a line and is essentially a summary geometry which is shown as a line. Line geometries are ideal to show the evolution of a numeric (double) variable. Examples include a firm’s market capitalization, its sales or gross margin or macro-economic data where evolution is shown with a date/time variable mapped on the x-axis and the value of the series on the y-axis, … There are a couple of different line geometries: geom_line(), geom_step() and geom_path(). These three differ in the way they show the data. The first shows that data in the order of the dataset and uses a continuous line. For instance, is the dataset is ordered per year, the first “point” on the line will show the value of the variable mapped on the y-axis for the earliest year, the second for the second year, … . The second uses the same order to show the data, but uses straight lines. The last, geom_path() shows the data in the order in which they appear on the x-axis. If the variable mapped on the x-axis is time, geom_path() and geom_line() or geom_step() are equivalent. In addition geom_vline(), geom_hline(), geom_abline() allow you to draw line segments. geom_segment() and geom_curve() allow you to connect two points. In general, most of what was written with respect to the aesthetic mapping in the previous section for point geometries, also applies to line geometries and most differences are straightforward: you can define the line width in a line geometry, not in a point geometry, you can have a shape in a point geometry, but not in a line geometry. So, here we’ll focus on what is specific for these geometries.\nYou can think of a line geometry in terms of a point geometry. While the latter shows one dot for each observations, the former implicitly connects these dots using a line. While for point geometry is very usefull to show correlation, the line geometry is very usefull to show evolution.\nTo illustrate, we’ll use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have use this dataset in e.g. Figure 9.1 and Figure 9.2. There we used the dataset for a specific year. Here, we will use all data for Costa Rica and Brazil. We’ll also rename the variable date in year.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nWe will start with geom_line() and geom_step().\n\n9.3.2.2.1 geom_line() and geom_step()\ngeom_line() and geom_step() are usually used to show the evolution of one numeric variable where a date/time is used to map on the x-axis van the value is shown on the y-axis. These two mappings are required. You can map other variables on the color of the line, the line width and line type and group. The difference between both is the way they connect the dots.\n\ngeom_line(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nMost arguments of the geom_line() function should be familiar. We’ll use the position = when we discuss geom_area() where we will show how you can “stack” lines in a plot. The orientation = argument is only used if the orientation of the line is not straightforward from the data. The geom_step() functions add a couple of arguments that are related to the way this functions shows a line: either in a horizontal or vertical segment.\n\ngeom_step(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  direction = \"hv\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nHere, the function needs to know if it first has to move up (vertical) or first horizontal. The default direction = \"hv\" first move right than up or down, using “vh” the first movement is up or down, than right. “Mid” means that the step is taken halfway.\nLet’s illustrate these two function for the dataset with life expectancy at birth for Brazil and Costa Rica. We first show only one country. Doing so allows us to keep the aesthetic mapping simple: we’ll map the variable year on the horizontal axis, life expectancy on the vertical axis. For all other arguments, we accept the default values.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows the evolution of life expectancy at birth since 1960 for Costa Rica. R adds the variables mapped on the horizontal and vertical axis as labels. The default color for the line is black. Adding a point geometry, you can see that a line geometry is often an extension of a point geometry. However, because here we show an evolution, a line geometry works better. Adding dots doesn’t add any additional value.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s change the settings, in other words, the visual presentation of the plot without adding any new information. In addition the color and transparency, you can change the line width, the line type (dotted, dashed, …). With respect to the line type, Figure 9.23 shows the 6 most common types: solid, dashed, dotted, dotdash, longdash and twodash. To illustrate these settings, we’ll plot the data for Costa Rica using a blue dotted line with linewidth 1.5 and a level of transparancy (alpha) equal to 0.50.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line(color = \"blue\", linewidth = 1.5, linetype = \"dotted\", alpha = 1/2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.23: Non data components of maps\n\n\n\n\n\n\n\n\nFor the geometry geom_step() with the exception of the the direction of the steps (hv, vh or mid), all other settings are similar. However, in this case, the result is not a continuous line, but a line that moves in discrete steps from one point to the other. To illustrate the latter, a point geometry shows the exact location of each observation.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(color = \"blue\", direction = \"vh\") +\n  geom_point(color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot also illustrates the direction. With vh chosen here, the movement from one point to the other starts with the vertical movement. As the line reaches the y-level of the next point, the line graph moves in horizontal direction. With ‘hv’, the first movement would be horizontal. Using mid, the vertical movement starts in the middle of the horizontal movement.\nFor both geom_line() and geom_step() and in addition to the required x and y mapping, you can map additional variable on the color, line type and width aesthetics. You can do so in the ggplot() function as well as in the geom_line() or geom_step() functions. The difference between both was discusses in the section on the point geometry. Here, we’ll use the life expectancy dataset with both Brazil and Costa Rica and map these countries on the color aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nthe linetype aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linetype = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nand the linewidth aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linewidth = country)) +\n  theme_minimal()\n\nWarning: Using linewidth for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nThe results would be similar for the geom_line() geometry.\nThe group aesthetic is useful is you want to show the evolution for a large number of values. Suppose that you want to show the evolution of life expectancy for all countries in the dataset. As the number of countries is very large, mapping the country variable on an aesthetic such as color, linewidth or linetype is usually not a good option. First, there are way too many colors to allow for a meaningful difference across countries. This is where the group aesthetic can help you out. To see what it does, in the next figure, we’ll map the variable country on the group aesthetic while we keep the x values for the date and the y variable for life expectancy:\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe result is a line graph with all lines shown in the same (default) color, type and width. There is one line per country. You can combine the group aesthetic with e.g. a color aesthetic. Using the previous graph, you could map, e.g. the region variable on the color aesthetic. In that way, all countries in the same region would be shown using the same color, with difference colors per region.\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(color = region, group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nUsing {[gghighlight] (https://yutannihilation.github.io/gghighlight/index.html)} you can highlight some some countries. For ways to do so, see the documentation (Yutani (2024)).\nIf series are very volatile, adding the geom_smooth() allows you to filter out noise. To illustrate, we’ll add a smooth function for both Brazil and Costa Rica. We’ll accept all default values. Using the linetype of linewidth options, you would change the linetype of e.g. the smoothed line and make it a bit thinner than the line showing the actual data (or vice versa).\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, color = country)) +\n  geom_line() +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#####geom_path()\nThe previous geometries show the data as they appear in the dataset. This is not always the most useful representation of the data. If you want to show co-movement for instance, geom_line() or geom_step() are not the most suitable options. To illustrate, consider the dataset in data_beveridge.csv. The dataset includes quarterly data for the unemployment rate and the vacancy rate (the number of vacancies to the number of jobs). The Beveridge curve shows how both move together: when the economy is solid and growth the good, the unemployment rate should be low and the vacancy rate high. The opposite should be the case in times of recession. For import the data, you can run\n\ndata_beveridge &lt;- read_csv(here::here(\"data\", \"raw\", \"data_beveridge.csv\")) \n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): TIME PERIOD, FREQUENCY\ndbl  (2): vacancy_rate, unemployment_rate\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_beveridge &lt;- data_beveridge |&gt; rename(date = DATE, quarter = `TIME PERIOD`)\ndata_beveridge |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 5\n  date       quarter vacancy_rate unemployment_rate FREQUENCY\n  &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;    \n1 2007-06-30 Q2 2007          1.2              7.65 Q        \n2 2008-12-31 Q4 2008          1.2              8.11 Q        \n3 2021-06-30 Q2 2021          2.4              8.08 Q        \n4 2016-12-31 Q4 2016          1.7              9.82 Q        \n5 2018-03-31 Q1 2018          2.2              8.61 Q        \n\n\nThe data include a variable date showing the last day of the quarter, quarter, showing the quarter and the vacancy_rate and unemployment_rate. One way to show co-movement is to add both to the same line graph:\n\ndata_beveridge |&gt; ggplot(aes(x = date)) +\n  geom_line(aes(y = unemployment_rate),color = \"red\") +\n  geom_line(aes(y = vacancy_rate), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, as you can see from the graph, this is not very helpful. Even if both series are measured in percent both are usually less than 10%-points removed from one another, it is difficult to spot co-movement.\ngeom_path allows you to map one series on the horizontal axis and another on the vertical axis. Doing so, you create a point geometry with one dot for every (x-value, y-value) pair. A line now connects all points in the order in which they appear in the dataset. To see how the graph builds, let’s start form the point geometry with the unemployment rate on the horizontal axis and the vacancy rate on the vertical axis:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is a scatterplot with one dot for every observation in the dataset. We don’t know however how both variables moved together. Starting from any dot in the scatter plot, there is no way to tell which dot came in the quarter after the first dot. As a matter of fact, there is not way to identify the first dot in the data. This is where geom_path() makes the difference: it connects dots as they appear in the data. In other words, it shows which came first, which dot came after the first, which one came after the second, … . In short is shows the path from one point to the other: if you start from the start of the line, you can follow the path until is reaches the end of the line. Here, both the geom_path() and the point geometry are shown. However, this does not need to be the case and you can use the line path geometry as a stand alone layer?\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you add a geom_text or geom_label() layer, you can add additional information to the graph. To do so, you need to map a variable on the aesthetic label. In this case, we could use the variable quarter as it shows in which quarter the unemployment rate and vacancy rate were measured. Adding these label add additional information: you now also now in the movements per quarter. Fitting the label using geom_text() or geom_label() often requires some experimenting with the settings e.g. the size of the font and the nudge left, right, up or down (using the argument nudge_x or nudge_y).\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_text(aes(label = quarter), size = 3, color = \"grey\", nudge_x = 0.2, check_overlap = TRUE) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe label is shown in grey, with the check_overlap = TRUE and a nudge to the right to avoid too much overlap with the line. In addition to the options that are similar to those for the other line geometries, geom_path() includes arguments that allows you to determine how the line ends (lineend =), how various parts connect (line_join =) and e.g. the use of an arrow (arrow =).\n\ngeom_path(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  lineend = \"butt\",\n  linejoin = \"round\",\n  linemitre = 10,\n  arrow = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nFor the arrow, you can change the default options if you add arrow = arrow() which allow you to set the angle of the arrow head (the width of the angle), length of the arrow head measured in “inches” or “mm”, if arrows are needed at the end of the line (“last”), at the beginning (“first”) or at both ends (“both”) and if you would like to arrow head to be open or closed.\n\narrow(angle = 30, length = unit(0.25, \"inches\"),\n      ends = \"last\", type = \"open\")\n\nNote that the {ggrepel} packages includes a geometry greom_text_repel() that allows you to add, e.g. line segments connecting the text and the dot. If you create a graph using geom_path() and you want to add labels, it is worthwhile to look at the options in this package. Using geom_text_repel you can finetune most of the label or text parts (in addition to Slowikowski (2024) you can also use “?ggrepel::geom_text_repel” in the console to check all the options). To show only a limited amount of these options, here is a graph using the geom_text_repel() geometry:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate, label = `quarter`)) +\n  geom_path() +\n  geom_point(color = \"red\") +\n  geom_text_repel(\n    size = 3, \n    color = \"#003049\", \n    min.segment.length = 0, \n    seed = 42, \n    box.padding = 0.5, \n    max.overlaps = getOption(\"ggrepel.max.overlaps\", default = 20)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n9.3.2.2.2 geom_hline(), geom_vline() and geom_abline()\nThese geometries allow you to add horizontal lines, vertical lines and line segments to a plot. The arguments for these three functions are very similar. As an example for geom_hline(), these arguments are\n\ngeom_hline(\n  mapping = NULL,\n  data = NULL,\n  ...,\n  yintercept,\n  na.rm = FALSE,\n  show.legend = NA\n)\n\nIn addition to the usual arguments, this function include the argument yintercept: the value where the horizontal line crosses the vertical axis. The argument is xintercept in geom_vline() equals the value where the vertical line with intersect the horizontal axis. For geom_abline(), you need the intercept, where the line will intersect the vertical axis if the value on the horizontal axis is 0 and the slope, the increase per unit on the horizontal axis. To illustrate the use of these geometries, let’s use the unemployment rate from the data_Beveridge dataset and show this variable using a line geometry:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to add the average for the entire period. The average value equals\n\nave_unemployment &lt;- mean(data_beveridge$unemployment_rate)\n\nWe can add this to the plot using geom_hline().\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  geom_hline(yintercept = ave_unemployment, color = \"lightblue\", linewidth = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_vline() you can also highlight the quarter where the major investment bank Lehman Brothers collapsed (third quarter 2008):\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  geom_vline(xintercept = as.Date(\"2008-09-30\"), color = \"orange\", linewith = 0.5) +\n  theme_minimal()\n\nWarning in geom_vline(xintercept = as.Date(\"2008-09-30\"), color = \"orange\", :\nIgnoring unknown parameters: `linewith`\n\n\n\n\n\n\n\n\n\nUsing geom_abline() you can draw any straight line. Suppose that you have a scatter plot looking like Figure 9.24\n\n\n\n\n\nFigure 9.24: Using geom_abline() in a scatter plot\n\n\n\n\n\n\n\n\nand would like top draw a red line with intercept 0 and slope equal to 10. Using geom_abline() as a addtional layer, you can add this line to the scatter plot:\n\ndf |&gt; ggplot() +\n  geom_point(aes(x = x, y = z1)) +\n  geom_abline(intercept = 0, slope = 10, color = \"red\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAll lines where drawn crossing the entire dataset. Using geom_segment() and geom_curve() you can draw straight lines and curves that are shorter.\n\n\n9.3.2.2.3 geom_segment() and geom_curve()\nWith geom_segment() and geom_curve you can connect two points in a graph. To illustrate both, we’ll use the life expectancy dataset and filter observations for 1980 and 2020. Using both geom_segment() and geom_curve() we will connect two points. For every country, the first point is the (per capita gdp, life expectancy) pair in 1980. The second point for each country is are the values for the same variables in 2020. The required aesthetic mappings for both geometries are x, the first observations for the variable mapped on the horizontal axis, y, the first observation for the variable mapped on the vertical axis and xend and yend, the last observations of the variables mapped on the horizontal and vertical axis. For the life expectancy dataset, x = gpd per capita in 1980, y = life expectancy in 1980, xend = gdp per capita in 2020 and yend = life expectancy in 2020. Other aesthetic mappings include the color of the line, its width and type and, for geom_curve() also the curvature.\nLet’s first prepare the dataset: filter the observations of 1980 and 2020 the life_df. In addition, we need a variable for gdp in 1980 and one for gdp in 2020. The same holds for life expectancy. To create these variables, we need to pivot the dataset wider (Chapter 8):\n\nlife_seg &lt;- life_df |&gt; \n  filter(date == 1980 | date == 2020) |&gt; \n  pivot_wider(\n    names_from = c(date), \n    values_from = c(gdp_capita, life_exp, pop),\n    names_glue = \"{date}_{.value}\")\n\nWe can now use life_seg to illustrate geom_segment(). To show the data, we kept the log-transformation on the horizontal axis and added the aesthetic color to show lines in different colors per region:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region)) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nUsually, to show direction, you would add arrows. You can do so using the arrow = argument in both geom_segment() and geom_curve(). Here, the arrow head is closed and is 2.5 mm long, with an angle equal to 11 degrees:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\") )) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nAnother way to use geom_segment() is to combine it with a point geometry and connect two points. Using the life expectancy data for 1980 and 2020, we can use a point geometry to how both values. We will do so for a sample of 40 countries and first remove observations with missing data for life expectancy in 1980. We map the countries on the vertical axis. We sort them i descending order of life expectancy at birth in 1980. To do so, we use {forcats} fct_reorder() function (Chapter 4). In that way, the countries will be ordered in descending order of life expectancy at birth in 1980. We use geom_point() to show the observations. To do so, we map life expectancy at birth on the horizontal axis and also on the size and color aesthetics. Doing so, higher values for life expectancy in will show up through a larger dot as well as the color of the dot. We drop the legend.\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE)\n\n\n\n\n\n\n\n\nWe now have two dots. One with life expectancy at birth in 1980 and one in 2020. Using geom_segment() we can now connect these dots. In the aesthetics, we map life expectancy in 1980 on the x aesthetic, live expectancy in 2020 on the xend aesthetic. We also map the latter on the color aesthetic. Doing so, the segment will have the same color as the dots for life expectancy in 2020. In the last line, we change the color:\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_segment(\n    aes(x = `1980_life_exp`, xend = `2020_life_exp`, color = `2020_life_exp`), \n    arrow = arrow(angle = 12, type = \"closed\", length = unit(2, \"mm\")),\n    show.legend = FALSE) +\n  scale_colour_paletteer_c(\"grDevices::Purple-Green\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is some further layout work, e.g. the labels of the axis, a caption, … . We would add the map of the country next to its name, … . But here, you have the basic dumbell chart: a combination of two point geometries and one segment.\ngeom_curve() shows a similar plot, but adds a curved line. You can control the level of the curvature using the curvature = argument. A positive value will produce a right hand curve, a negative value a left and curvature and 0 shows a straight line. In the example, the curvature is -0.5. The plot also includes arrows.\n\nlife_seg |&gt;\n  ggplot() +\n  geom_curve(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\")), \n    curvature = -0.50) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_curve()`).\n\n\n\n\n\n\n\n\n\nGetting the curvature right usually requires some experimenting with various numbers, both positive and negative, usually starting from the default value and moving closer to 0 to reduce curvature or futher from 1 to add to curvature.\nYou can use both geometries also to add a line or curve segment on a plot. To do so, you can use the aes() mapping. For any segment or curve you would like to add as a layer to the graph, you add a value for x, xend, y and yend in the aesthetic mapping in geom_segment() or geoom_curve() and these geometries will add a line or curve between the points (x, y) and (xend, yend). Using the unemployment data in the Beverdidge dataset, the next graph adds a curves line stressing the rise in the unemployment rate and a segment to stress the fall in the unemployment rate. The x- values x and xend are dates. For both the curve and the segment the end is shown with an arrow:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  geom_curve(\n    aes(x = as.Date(\"2008-03-31\"),\n        y = 7.5, \n        xend = as.Date(\"2013-03-31\"), \n        yend = 12.5), \n    curvature = -0.25, \n    color = \"red\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  geom_segment(\n    aes(x = as.Date(\"2013-06-30\"),\n        y = 12.5, \n        xend = as.Date(\"2025-03-30\"), \n        yend = 6.5), \n    color = \"#2ECC71\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  theme_minimal()\n\nWarning in geom_curve(aes(x = as.Date(\"2008-03-31\"), y = 7.5, xend = as.Date(\"2013-03-31\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = as.Date(\"2013-06-30\"), y = 12.5, xend = as.Date(\"2025-03-30\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nNote that you can use geom_hline(), geom_vline(), geom_abline(), geom_segment() and geom_curve() to draw graphs such as a supply and demand diagram.\n\n\n\n\n\n\nSupply and demand\n\n\n\n\n\nTo keep it as simple as possible and use as many line geoms as possible, let’s start from the y-axis, which we will call “Price”. We generete this series as a simple sequence from 0 to 100:\n\nsup_dem &lt;- data.frame(\n  price = seq(0, 100, by = 1))\n\nWe can now add demand\n\\[\nQ_D = 800 - 8 * P\n\\]\nand, using the inverse supply curve, supply\n\\[\nP = \\frac{1}{8} * Q_S =&gt; Q_S = 8 * P\n\\]\n\nsup_dem$demand = 800 - 8*sup_dem$price\nsup_dem$supply = 8 * sup_dem$price\n\nUsing the data in sum_dem we can now build the supply and demand plot:\n\nsup_dem |&gt; ggplot(aes(y = price)) + # Price on the vertical axis\n  # Show quanity on the horizontal axis, add some color and linetype and width (as a setting)\n  geom_line(aes(x = demand), color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  geom_line(aes(x = supply),  color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  \n  # Dotted lines for the equilibruim (note that you can calculate the equilibrium \n  # using the demand and supply parameters and add them here)\n  geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  \n  # Create x and y-axis \n  geom_hline(yintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  \n  # Set the labels for the axis\n  labs(\n    x = \"Quanity\", \n    y = \"Price\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 95\n  # here the text is \"Supply\"\n  annotate(\"text\", x = 700, y = 95, label = \"Supply\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 5\n  # here the text is \"Demand\"\n  annotate(\"text\", x = 700, y = 5, label = \"Demand\") +\n  \n  # Remove all ticks and labels for the x and y axis\n  theme(axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      \n  # Show on a white background\n      panel.background = element_rect(fill = \"white\"))\n\nWarning in geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3.2.3 Area geometries\nThere are two area geometries that are often used: geom_area() and geom_ribbon().\n\n9.3.2.3.1 geom_area()\ngeom_area() allows you to show the composition of a variables changes over time, e.g. the share of various components in GDP, the share of all household expenditures, … . To illustrate this geometry, we’ll use the life_expectancy at birth dataset for Brazil and Costa Rica and the AMECO database with data on age structure of the population in France: the population ages 0-14, aged 15-64 and 65 and over.\nFor the first dataset, if you haven’t used it yet, you can run the following code\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRows: 13671 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): iso2c, iso3c, country, region\ndbl (4): date, gdp_capita, life_exp, pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nIn Chapter 7, you used the AMECO dataset to learn some tidying skills. In the next box, I’ll create the dataset from the data in data &gt; raw &gt; ameco_xlsx folder. We’ll need AMEC01.XLSX.\n\n\n\n\n\n\nFrench population data\n\n\n\n\n\nFirst import the dataset\n\nlibrary(readxl)\n\nameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nWe’ll store two series: one with data (ameco1) and one with data labels (ameco2):\n\nameco2 &lt;- ameco1 |&gt; select(CODE, TITLE, UNIT...11)\nameco1 &lt;- ameco1 |&gt; select(CODE, COUNTRY, matches(\"\\\\d{4}\"))\n\nTo tidy the dataset, we need to pivot, first longer followed by a pivto wider\n\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = matches(\"[0-9]{4}\"), names_to = \"year\", values_to = \"value\")\nameco1 &lt;- ameco1 |&gt; pivot_wider(names_from = \"CODE\", values_from = \"value\")\n\nAfter seleting the series we need, we need to pivot the data back to longer format: the population structure will be used as one variable with three level: one for each age category:\n\n# Select the variables we need\nameco1 &lt;- ameco1 |&gt; select(COUNTRY, year, NPTN, NPCN, NPAN, NPON)\n\n# year is a character variable: need to change in numeric\nameco1$year &lt;- as.numeric(ameco1$year)\n\n# Pivot longer to crate one variable, \"age_group\" with 3 values, one for each age category\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = starts_with(\"NP\"), names_to = \"age_group\", values_to = \"pop\")\n\nClean the labels dataset and join with the data. This will allows us to generate e.g. nice legends:\n\nameco2 &lt;- ameco2 |&gt; filter(CODE == \"NPTN\" | CODE == \"NPCN\" | CODE == \"NPAN\" | CODE == \"NPON\")\nameco2 &lt;- unique(ameco2)\nameco1 &lt;- ameco1 |&gt; left_join(ameco2, by = join_by(age_group == CODE))\n\nTidy the names and select France for all years to 2022:\n\nameco1 &lt;- ameco1 |&gt; rename(unit = UNIT...11, country = COUNTRY, population = TITLE)\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s check the data:\n\npop_fra |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 6\n  country  year age_group    pop population                 unit        \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                      &lt;chr&gt;       \n1 France   1971 NPAN      32751. Population: 15 to 64 years 1000 persons\n2 France   2009 NPAN      41860. Population: 15 to 64 years 1000 persons\n3 France   1964 NPAN      30840. Population: 15 to 64 years 1000 persons\n4 France   1999 NPAN      39303. Population: 15 to 64 years 1000 persons\n5 France   2008 NPCN      11881. Population: 0 to 14 years  1000 persons\n\n\n\n\n\nLet’s first use an area graph with one variable. Using geom_area() will map the variable year on the horizontal axis and life expectancy at birth for Brazil on the vertical one. We will then use geom_area() and accept all default values:\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWith one variable, geom_area() essentially creates a line geometry but fills the area between the horizontal axis and the vertical axis. Using the settings, you can change the color or transparency and using geom_line() you can show the line as an additional layer. For instance,\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area(fill = \"blue\", alpha = 0.20) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the dataset, we have two countries, Brazil and Costa Rica. If you map the variable country on the fill aesthetic, ggplot() will stack the life expectancy data. In other words, it add life expectancy in Brazil to life expectancy in Costa Rica and shows the stacked outcome.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis outcome, which doesn’t make sencse, is due to the default values in the geom_area() function:\n\ngeom_area(\n  mapping = NULL,\n  data = NULL,\n  stat = \"align\",\n  position = \"stack\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...,\n  outline.type = \"upper\"\n)\n\nThe mapping, data, na.rm, orientation, show.legend, inherit.aes and ... should be familiar. Although we already met stat and position, for the first we always kept identity and for the latter, we kept identity or jitter. The first, stat = \"align\", shows what will happen is two series are stacked, but how no common x-coordinates. By default, R will interpolate these values and align the values for each series it stacks with a common x-value. Using stack  = \"identiy\" changes this default. The second, position = \"stack\" implies that R show stacked values. To illustrate, in the graph with life expectancy for Brazil and Costa Rica, geom_area() first plotted life expectancy for Costa Rica. The second series was the sum of life expectancy in Costa Rica and Brazil. If there would have been a thrid country, the third line would have shown the sum of life expectancy in all three these countries. Changing these defaults, R will plot two curves. However, as life expectancy in Costa Rica was always higher than in Brazil, the area for Costa Rico will overlap and mask the area for Brazil.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo deal with these issues, you can add transparency using the alpha setting and add a line per country, change the order in which they appear, … .\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\", alpha = 0.10) +\n  geom_line(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_area() with multiple variables, stacking them should make sense. This is the case for the French population dataset. In pop_fra we have a dataset which includes the number of people ages 0-14, 15-64 and 65 and older. The sum of these three population groups equals the total population in France. geom_area() plots these three series and stacks their values. Doing so, it creates three new series: one with the number of people ages 0-14, a second with the number of people ages 0-65 (the sum of those ages 0-14 and those ages 15-64) and a third series with the sum across age categories. To show these series, it fills the area below the first series and the x-asis, between the first and second series and between the second and last series with a color. Doing so, the size of the area shows the size of each subgroup.\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the graph makes sense: it shows the evolution of the total population in France as the sum of three components: those ages 0-14, 15-64 and 65 and over. The size of each cohort is shown mapped on the vertical axis. However, there is one issue. R shows the order of the categories starting with the largest: NPAN of those ages 15-64, then the young (or children) NPCN and then the “old” or NPON. This order doesn’t make a lot of sense as these categories imply an order and the graph should preferably show the young at the bottom, those ages 15-64 in the middle and those ages 65 and over at the top. There are a couple of ways to deal with this. The first is to order the values in age_group (an ordered factor). Here, we will first order the ameco1 dataset and then filter the data for France. Doing so, you can create an area graph for other countries by changing the filter:\n\nameco1$age_group &lt;- factor(ameco1$age_group, \n                           levels = c(\"NPTN\", \"NPON\", \"NPAN\", \"NPCN\"), \n                           ordered = TRUE)\n\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s now see the result of the plot:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area()+ \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that R ordered the area’s in line with the age categories. It also shows a different color. This is due to the fact that is will choose another color scale for ordered factors as apposed to unordered factors. The yellow at the bottom is lighter than the dark blue at the top.\nThere are other ways to change the order. For see how they work let’s create a geom_area() but now map the variabel population on the fill aesthetic:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the legend, you can see that the plot shows to order from young to old. This is due to the fact that the order of the categories in population in alfabetical order happens to put the category with the youngest first and with the oldest last. If you want to reverse the order, you can add the option position = position_stack(reverse = TRUE):\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe problem is not fully solved: the plot is fine, but the legend isn’t. Here, you need to add scales_fill_manual and set the legend labels manually:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  scale_fill_discrete(breaks=c(\"Population: 65 years and over\", \"Population: 15 to 64 years\", \"Population: 0 to 14 years\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we used geom_line() we kept the default position = \"identity\". Given how close geom_area() and geom_line() are connected, it shouldn’t come as a complete surprise that you can add position = \"stack\" as an argument in geom_line(). Doing so, shows the stacked lines:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, color = age_group)) +\n  geom_line(position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph is a bit misleading: a lot of people will interpret this graph to mean that the largest population group in France are the oldest and that there are hardly any young people, as they will not immediatly notice that this is a stack line chart, not one with position = identity. This the main reason why this chart is never used as a chart on its own. However, you can use it to add as a layer to the geom_area() chart. Doing so, allows you to add a line between the areas in the area chart. If you add some transparency to the areas, these lines will stand out:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop)) +\n  geom_area(aes(fill = age_group), alpha = 0.20) +\n  geom_line(aes(color = age_group), position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n9.3.2.3.2 geom_ribbon()\ngeom_ribbon() is a special case of geom_area() It allows you to plot a “ribbon” defined by minimum and maximum values for the variable mapped on the vertical axis. The required aesthetics for this geometry are the variables mapped on the x-axis and those on the ymin and ymax aesthetics. Note that you can also map on the y-axis and maps the variables includin the minimum and maximum values on the xmin and xmax aesthetic. To illustrate, we’ll use the diamonds dataset, filter diamands with carat &lt;= 2.5, calculate the standard deviation and mean price per level of carat and show the result using a ribbon where the maximum value is defined as the mean price plus 1.96 times the standard deviation and the minimum value is defined as the mean price minus 1.96 times the standard deviation. Let’s first create the summary data frame:\n\ndiamonds_sum &lt;- diamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n  group_by(carat) |&gt; \n  summarize(\n    min_price = min(price), \n    max_price = max(price), \n    mean_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))\n\nUsing geom_ribbon() and accepting all default values for the settings:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis ribbon shows the carat on the horizontal axis. The vertical axis shows two variables: the mean price + 1.96 the standard deviation of the price (per carat group) and the mean prices - 1.96 times the standard deviation (per carat group). Using e.g. fill you can change the color of the ribbon, using alpha the transparency of the ribbon and using the setting color you can change the color of the lines that show the minimum and maximum values. In addition, you can set the line width and line type, For instance:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightblue\", color = \"blue\", linewidth = 0.50, linetype = \"solid\", alpha = 0.20) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you want to also the mean values, you add a line geometry:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightgrey\", color = \"darkgrey\", alpha = 0.50) +\n  geom_line(aes(y = mean_price), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_ribbon() is a special case of geom_area(). To see this, let’s change the minimum value in the previous graph to ymin = 0 and the maximum value, ymax = mean_price:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = 0, \n    ymax = mean_price)) +\n  geom_ribbon(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is equal the the one you would have gotten with geom_area():\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    y = mean_price)) +\n  geom_area(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause of these similaries, it shouldn’t come as a surprise that the arguments for geom_ribbon() and geom_area() are very similar:\n\ngeom_ribbon(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"both\"\n)\n\nWith the exception of the default that are specific to geom_area() - stat and position - all others are very similar.\n\n\n\n9.3.2.4 Bar or column geometries\nThere are two bar geometries: geom_bar() and geom_col(). By default, first shows the number of observations in each category of the variable that is mapped on the x-axis. The second allows you to show values for a variable mapped on the vertical axis, for every value of the variable mapped on the x-axis. geom_bar() has one required aesthetic: the x-axis. This variable is a discrete variable (nominal or ordinal)\n\n9.3.2.4.1 geom_bar()\nFor geom_bar() the arguments are\n\ngeom_bar(\n  mapping = NULL,\n  data = NULL,\n  stat = \"count\",\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nHere the stat argument has the value \"count\". Because geom_bar() shows the total number of observations on the vertical axis, there is only one required aesthetic: the variable for whose values the count will happen. The alternative, \"identity\" shows the values of the variable mapped on the y-axis. Before moving on to the position and stat argument, a short work about some of the others. The argument just positions the bar over the major gridline for the value on the x-asis. The default value centers the bars over that grid line. Alternative values are 0 or 1 to align left or right. The width argument - which is be default 0.90 - measures how much space the bars will take up in the plot.\nThe position argument with default \"stack\" matters in case the aesthetics include e.g. a mapping on e.g. fill or color. Mapping a variable on the fill aesthetic will show the number of observations for each variable on the horizontal axis, but wil differentiate between various values of the variable mapped on the fill aesthetic using different colors. The color aesthetic shows similar values, but does so using a different color for the line between various subgroups. The position argument as two alternatives: \"fill\" and \"dodge\" or \"dodge2\". The first rescales the vertical axis from 0 to 100% and shows the various subcategories for each category on the x-axis as a percent of total values for the x-category. The second, \"dodge\" or \"dodge2\" shows various subgroups next to each other and not stacked.\nTo illustrate, we’ll use the diamonds dataset. To show the number of observations per level of cut, we map the cut variable on the x-axis and accept all defaults. Recall that geom_bar() will plot the count on the vertical axis. In other words, one aesthetic mapping, x = cut, is sufficient:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see from the plot, geom_bar() adds the label “count” to the vertical axis. As cut is an ordered factor, the values on the x-axis are shown in that order. As usual, you can change the setting (i.e. layout options) using e.g. the fill (fill or the bar), color (color of the line around a bar), … by specifying those in geom_bar():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar(fill = \"lightblue\", color = \"blue\", linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to now the number of observations for each level cut-clarity combination. To do so, you can use the fill aesthetic. Doing so, geom_bar() will show the number of observations per level of and will do so using different colors to fill the bars, where each color show one value of clarity:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are other aesthetics that you can use to map other discrete variable, e.g. color or linewidth. The color shows the various subcategories using a different color for a line:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, color = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, except when you set the color of the bars to white, the different lines reveal little on their own. The same holds for the other aesthetic mappings.\nBy default, geom_bar() stacks (position = stack) the various values of the variable mapped on the fill aesthetic. To show them next to one another, you add position = \"dodge\" or position = \"dodge2\":\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every level of cut, geom_bar() now shows 8 bars: one per level of clarity. If you use position = \"dodge2\", you can add further details using\n\nposition_dodge2(\n  width = NULL,\n  preserve = \"total\",\n  padding = 0.1,\n  reverse = FALSE\n)\n\nIf you use position = \"dodge\", you can specify the width and preserve. The first, width is relevant is you have different geometries with different width, for instance, point geometry and a bar geometry. The second preserve is relevant in case not all values of the variable mapped on the x-axis have the same number of subcategories for the variable mapped on the fill aesthetic. By default, R preserves the total width of the of all bars for each value on the horizontal axis. The alternative, \"single\" preserves the width of the subcategories. To illustrate, here are two examples taken from Chang (2025):\n\n\n\n\n\nFigure 9.25: the preserve argument\n\n\n\n\n\n\n\n\nThe padding argument in position_dodge2() allows to to specify the distance between two bars. The default value is 0.1. If you increase that value, the distance between two bars at the same x position widens:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = 0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA negative value causes overlap:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe argument reverse = FALSE keeps the order of the subcategories. Changing this into TRUE, reverses the order of the subcategories:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing position = fill, geom_bar() will show proportions. For every value of the variable mapped on the horizontal axis, the total is set equal to 100. The subcategories are then shown in a percentage on the number of observations for each of the subcategories in relation to the total number of observations for their category shown on the x-axis:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#####geom_count()\ngeom_count() allows you to show values for the variable mapped on the vertical axis for every value of the variable mapped on the horizontal axis. This geometry needs at least two aesthetics: a discrete variable to map on the x-axis and the variable to map on the y-axis. This geometry allows you to show, e.g. the average for a variable (e.g. price), for every value of a discrete variable (e.g. cut). Here, we’ll illustrate this geometry to visualize summary data.\nThe argument of this function are very similar to the arguments for the geom_bar() geometry:\n\ngeom_col(\n  mapping = NULL,\n  data = NULL,\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nSuppose you want to visualize the average price per cut. Using {dplyr}’s group_by() and summarize() functions, you can calculate these averages using diamonds |&gt; group_by(cut) |&gt; summarize(ave_price = mean(price, na.rm = TRUE)) (see Chapter 8). Because these function return a data frame, we can connect their output in a pipe with ggplot(). In the next code, the first two lines calculate the average price per cut. We map the cut, the discrete variable, on the horizontal axis and the average price on the vertical axis. Note that the names of these variables are found in the tibble returned by the summarize() function. This data frame is use by ggplot() to find the variables. Here, we accept all default values.\n\ndiamonds |&gt; group_by(cut) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every value of cut, the variable mapped on the horizontal axis, geom_col() shows the variable ave_price, the mean of the diamond’s prices for each category of cut. Note that here ggplot(data, aes() ...) uses the data frame returned by {dplyr}’s functions as the data.\nUsing the other aesthetics, you can add other categories. As was the case with geom_bar(), usually on the the fill aesthetic is used. Other aesthetics are not visually appealing to differentate across categories. Here, we’ll show the average price for every value of clarity and cut. Doing so, we first need to calculate these values. Again, we do so using group_by() and summarize(). We map cut on the horizontal axis van use the fill aesthetic to map the variable clarity\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nNote that the output doesn’t make sense. The previous graph showed that the average price for fair cut diamonds was a little over 4000. Here, the average price is over 30000. This result is due to the fact that geom_col() by default stacks the values. In this case, this is not how it should be done. Stacking values per category is only relevant is the sum of these values is relevant. Here, this is not the case. Changing the position to dodge2 shows the various averages next to each other, grouped by level of cut. The position dodge or dodge2 can be modified in the same way as in geom_bar().\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n9.3.2.4.2 Using geom_bar() as geom_col() and vice versa\nBoth geometries are closely related. In the arguments for geom_bar(), there was stat = \"count\". Changing this into stat = \"identity\" tells this geometry to plot values, not counts. Using this value, you can create a bar chart using geom_bar() that is identical to the one created with geom_col(). To see, let’s use geom_bar() in stead of geom_col() in the previous plot and use the stat = \"identity\" argument:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_bar(stat = \"identity\", position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nBecause we used stat = \"identity\", geom_bar() now plots the values for the variable mapped on the vertical axis as values that that axis. In other words, using this argument, you can often use geom_bar() as geom_col().\ngeom_bar() shows counts. However, you can also use geom_col() to show counts. Let’s use this graph created with geom_bar()\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo create the same chart in geom_col() we need a data frame that includes the number of observations but value for cut. Using summarize(n = n()), this is what we can do. Using the result of these function in ggplot() with geom_col() shows a geom_bar() type of output:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n9.3.2.4.3 coordinate flip\nWe already touched upon coordinates. There, I illustrated coordinate flip or coord_flip(). For bar and column charts, the effect is that variable mapped on the horizontal axis is shown on the vertical axis and the values shown on the vertical axis are measured along the horizontal axis. Note that you don’t have to change the aesthetics mapping. If is sufficient to add coord_flip():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou would have had the same result if you mapped the discrete variable on the vertical axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, the advantage of coord_flip() is that is doesn’t change e.g. values for scales. As the discrete variable is mapped on the x-axis and is only shown on the y-axis, scale_x_discrete() … refer to the right variable. You can also flip coordinaties with geom_col() and with more than one category where you reverse the order of the subcategories:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  coord_flip() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n9.3.2.4.4 Adding text\nSometimes it is useful to add values as text to a bar or column chart. Recall that you can add text using, e.g. geom_text() or geom_label(). Adding this text layer to a bar or column plot allows you to add e.g. the values they represent. Here, the values are added in white to a bar chart in blue. The values are nudged down 200 units. As the units on the vertical axis are measured from 0 to over 20000, a 200 nudge down brings these values within the bars. Adding a positive nudge would add them to the top.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col(fill = \"blue\") +\n  geom_text(aes(label = n), nudge_y = -200, color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI refer to the section on point geometries for further possibilities with geom_text() and similar geometries.\n\n\n\n9.3.2.5 Geometries for distributions\nThere are multiple ways to show the distribution of a variable. geom_histrogram() and geom_density() visualize the full distribution. Using geom_boxplot() or geom_violin() you can show the distribution, using summary statistics such as the mean, various quantiles and percentiles, minimum and maximum. For two continuous variables, you can use geom_bin2d(). geom_density_2d() or geom_density_2d_filled(). Here we will not go into detail on all of these geometries. We’ll focus on those for one variable and show, as an illustration, how you can extend them to two variables.\n\n9.3.2.5.1 geom_histrogram() and geom_density()\ngeom_histrogram() divides the full range of possible values for a variable into bins. For every bin, geom_histrogram() shows the number of observation. Recall that is is also what geom_bar() did. In other words, you would be able to use geom_bar() or geom_col() to generate this plot. The arguments of geom_histrogram() are:\n\ngeom_histogram(\n  mapping = NULL,\n  data = NULL,\n  stat = \"bin\",\n  position = \"stack\",\n  ...,\n  binwidth = NULL,\n  bins = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should be familiar. The binwidth = allows you to specify the width of a bin, e.g. 50 or 25. bins allows you to set the number of bins. By default this value of 30. There is one other way to change the bins and that is to define them using breaks. Using this argument, you can include a vector with bins or use, e.g. seq() to generate the bins. There is only one required aesthetic: the variable to map on the x-axis. To illustrate this function, we’ll visualize the distribution for price in the diamonds dataset. Accepting all defaults:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou can change the settings of the plot in the usual way: change the fill, change the color of the lines, … . Experimenting with the number of bins is often a good idea. Doing so, you can see what number is best for the data. Let’s change the number of bins in three ways:\n\nincrease the number of bins from 30 to 50:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nset the bin width to 250:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 250) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nuse seq() to set bins every 500:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(breaks = seq(from = 0, to = 20000, by = 500)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you map another variable on e.g. the aesthetic fill, geom_historgram() will show the count for each level of cut in every bin using different colors. Like with the bar geometries, other aesthetics are much less suitable to use as aesthetic mapping. Using the fill aesthetic to map cut, the histogram shows all observations for each level of cut within each bin:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFor two variables, geom_hex() extends geom_histogram(). Both variables are divided into bins (default 30) and the graph shows the number of observations per bin. The function requires a mapping on the x-axis and one on the y-axis. To illustrate, I’ll use a data frame with random draws from a bivariate normal distribution\n\nmat1 &lt;- matrix(rnorm(2000, 0, 1), nrow = 1000, ncol = 2)\ncov &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2, ncol = 2)\nmat1 &lt;- mat1 %*% cov\ndf_mat1 &lt;- as.data.frame(mat1)\ndf_mat1 &lt;- df_mat1 |&gt; rename(var1 = V1, var2 = V2)\n\nWith 50 bins, this is how the histograms of these two variables, var1 and var2 look like:\n\nplothist1 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var1)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist2 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var2)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist1 + plothist2\n\n\n\n\n\n\n\n\nUsing geom_hex() you plot both in one plot. Here, for every of 2500 bins (50 for var1 and 50 for var1) this plot shows the number observations in each and every one of these bins:\n\ndf_mat1 |&gt; \n  ggplot(aes(x = var1, y = var2)) +\n  geom_hex(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere color scale here, shows a higher number of observations with a lighter blue color. Consistent with the histograms for the individual series var1 and var2, geom_hex() show higher counts in bins closer to 0.\ngeom_histrogram() shows the count by default. You can change that in a density (i.e. probability) using an after_stat() value. Using after_state(\"density\"\" and geom_histrogram() maps these estimated probabilities on the y-axis. With 50 bins, this generates this plot:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo show a density, you can also use geom_density(). The arguments of this geometry are:\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\nThe stats = \"density\" by default calculates densities. Changing this into “count” generate a geom_histromgram(). geom_density() uses a by default a Gaussian kernel density estimator to smooth the estimates of the probabilities. For instance, the density estimate for the price of diamonds:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing the full aesthetic, you can show density plots for every value of the variable mapped on that aesthetic. For instance, to plot these price density for various values of cut, you map the latter on the fill aesthetic:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHer problem here is that the density plot for e.g. “Ideal” with the one for e.g. “Fair”. To show all density plots, you can add some transparency using the alpha setting. If, in addition, you map cut also on the color aesthetic, geom_density() will add lines at the top of each density:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut, color = cut)) +\n  geom_density(alpha = 1/10, linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo plot a density with 2 variables, you can either use geom_density2d() or geom_density2d_filled(). The first shows the bivariate density using countour plots, the second using filled contour plots. To illustrate, we’ll use the random data in df_mat1. The individual densities look like this:\n\nplotmat1 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var1)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat2 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var2)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat1 + plotmat2 \n\n\n\n\n\n\n\n\nUsing geom_density2d(), the result is\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe contour lines reveal higher probabilities the more they are circled with other countour lines. Here, consistent with the univariate densities, the probability that you’ll see a pair with values for var1 and var2 increases as the values for both these variables close in on 0. With geom_density2d_filled() the plot is filled:\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d_filled() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the color reveals information on the probabilities. For this plot, value pairs with a high probability are drawn in yellow. Ast he color scale moves from yellow to blue, the probability of a value pair in that range falls.\n\n\n9.3.2.5.2 geom_boxplot() and geom_violin()\nA boxplot shows the distribution of a variable using a box, which shows the 1st and 3rd quartile as its outer ranges, a whisker up and down, both with a length of 1.50 times the interquartile range (or the value of the 3rd minus the value of the 1st quartile). In addition, the box shows the median. You can adapt the way in which geom_boxplot() shows the outliers. Outliers are values outside of the range of the boxplot. To do so, you need to change the arguments of the function:\n\ngeom_boxplot(\n  mapping = NULL,\n  data = NULL,\n  stat = \"boxplot\",\n  position = \"dodge2\",\n  ...,\n  outliers = TRUE,\n  outlier.colour = NULL,\n  outlier.color = NULL,\n  outlier.fill = NULL,\n  outlier.shape = 19,\n  outlier.size = 1.5,\n  outlier.stroke = 0.5,\n  outlier.alpha = NULL,\n  notch = FALSE,\n  notchwidth = 0.5,\n  staplewidth = 0,\n  varwidth = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe stat = \"boxplot\" defines the default values. If you want to change these, you need to add, e.g. coef = 1.75 to extend the whiskers to 1.75 the IQR. The treatment of outliers is governed by the arguments starting with outlier. The first, outliers = TRUE by default shows the outliers. Changing this into FALSE hides the outliers. The axis of the plot will be adjusted accordingly, unless you add oulier.shape = NA. If outliers are shown, you can change their color, fill, shape, size, stroke and alpha setting. Here, I refer to the point geometries to see how these affect the plot. notch = FALSE makes a traditional boxplot. Setting this to TRUE, R will show a notched boxplot, where the notches allow you to compare the medians across groups. You can set the width of the notch using notchwidth, by default this value equals 0.50, i.e. the notch covers half of the box width. The staples mark the end of the whiskers. The staple width, which is by default 0, allows you to specify the width of these staples. The last argument, varwidth = FALSE allows you to change the width of the box. By default, this is not the case. If changed to TRUE, the width of the box will be proportional to the square root of observations.\nLet’s first draw a boxplot for one variable, price. We’ll map that variable on the y-axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the x-asis in these case has no real meaning, so you can actually leave it blank. Let’s add a notch with width 0.25, increase the length of the whiskers to 2 the IQR, change the way outliers are shown (color = red) and add a staple at the end of the whisker. Note that you use additional settings to change, e.g. the color of the lines, the fill of the box, …\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot(\n    coef = 2, \n    notch = TRUE, \n    notchwidth = 0.25,\n    outlier.color = \"red\",\n    staplewidth = 0.10) +\n  theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nTo compare distributions across values of another variable, we need to map this variable on the x-aesthetic. If you map the same variable on the fill aesthetic, the color of the boxes will change with these values the plot will also show a legend. As such, it is not necessary as the box plot will show the values on of the variables mapped on the x-asis as label. However, doing so, you can show a legend and remove the x-axis. To start, let’s map cut on the x-axis and price on the y-axis. With the exception of notch, which we’ll set to TRUE, all default valules apply:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_boxplot() now shows one boxplot per value of cut. The notches suggest that here is a significant difference in the median value between the price for “Ideal” cut diamonds and the price for “Premium” cut diamonds. Whether this is also the case for the values “Fair” and “Good” is not immediately visible as the notches seem to overlap. Using varwidth = TRUE we can adjust the width of the box using the square root of the number of observations as criterium. To see this:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE, varwidth = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe width of each box is now proportional to the square root of the number of observations. With 1610 observations in “Fair” and 21551 in “Ideal”:\n\ndiamonds |&gt; group_by(cut) |&gt; summarize(n = n()) |&gt; mutate(sqrtn = round(sqrt(n), 2))\n\n# A tibble: 5 × 3\n  cut           n sqrtn\n  &lt;ord&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Fair       1610  40.1\n2 Good       4906  70.0\n3 Very Good 12082 110. \n4 Premium   13791 117. \n5 Ideal     21551 147. \n\n\nthe width of the “Ideal” box is\n\\[\n\\frac{\\sqrt{21551}}{\\sqrt{1610}} = 3.65\n\\] 3.65 times wider then the width of the “Fair” box.\nTo illustrate these use of the aesthetic fill, and some other setting for the outliers, let’s use the fill aesthetic to map cut and add staples with width equal to 25% of the box width:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_boxplot(\n    notch = TRUE, \n    varwidth = TRUE, \n    staplewidth = 0.25, \n    outlier.colour = \"red\", \n    outlier.alpha = 1/5) +\n   theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nViolin plots are closely related to boxplots, but reveal fore information about the distribution of the variable. They are meanth to show the distribution of one continious variable per value of a discrete variable. The arguments in geom_violin() should be largely familiar. The argument draw_quantiles = FALSE allows you to add horizontal lines for every quantile. The scale argument determines the area of the violin. By default, this area is equal. Changing this default value into “count” will creates violins where areas are scaled proportionally to the number of observations while “width” keeps the width of the violins equal across groups:\n\ngeom_violin(\n  mapping = NULL,\n  data = NULL,\n  stat = \"ydensity\",\n  position = \"dodge\",\n  ...,\n  draw_quantiles = NULL,\n  trim = TRUE,\n  bounds = c(-Inf, Inf),\n  scale = \"area\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nLet’s use a violin plot to show the distribution of price for every value of cut. To do so, we map the variable price on the vertical axis and cut on the horizontal axis:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe violins for every value of cut show you where the observations are. The very thin upper part shows that thera are almost no observations for high price levels. At the lower part of the price levels, there seems to be more spread for “Fair” cut diamonds that for e.g. “Ideal” cut diamonds. For the latter, the distribution is very wide at the lowest part of the price range, suggesting that here are a lot of observations for these price cut pairs. To see this more in detail, let’s add the a sample of the observations of the diamonds dataset using a point geometry with a bit of jitter and tranparency:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  geom_point(\n    data = diamonds |&gt; slice_sample(prop = 0.10),\n    aes(x = cut, y = price), \n    position = position_jitter(width = 0.20, height = 0.20), alpha = 1/5, \n    color = \"red\", \n    size = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that indeed, there are a lot of observations in the lower price range for “Ideal” cut diamonds.\nAs is the case with geom_histogram() you can map the same variable on both the x-axis and e.g. fill aesthetic. Doing so, geom_violin() will fill the violins and the color to the legend. If you want to add quantiles, e.g. the 10th and 90th percentile and the first and third quartile, you can specify those using draw_quartiles = c(0.10, 0.25, 0.75, 0.90). Here, we use the color white to draw these lines. By default R uses black and this color wouln’t show up in the “Fair” cut category:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_violin(draw_quantiles = c(0.10, 0.25, 0.75, 0.90), color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n9.3.3 Scales\nIn the previous section, we focused on the geometries and hardly touched upon the scales of e.g. the x and y axis or the color, size or fill aesthetics. In this section we will take a closer look at this component of the grammar of graphics. If you looked at the code for the graph with life expectancy, you saw some lines such as\n\nscale_x_continuous(\n transform = \"log\",\n breaks = c(100, 1000, 10000, 100000),\n labels = scales::label_currency(prefix = \"$\")) +\n\nor\n\nscale_colour_paletteer_c(\"grDevices::Purple-Green\")\n\nThese were two lines that change the scale of an aesthetic. The first changes the x-axis and the second changes the color used to show different values of life expectancy. In general the scale functions have a similar format: scale_AES_continuous/discrete. The scale arguments is almost always part of the function. It refers to the scales component of the grammar of graphics. The AES part refers to the aesthetic: x, y, color, fill, linewidth, alpha, size, shape. Scales can be continuous or discrete. Not all aesthetics are suitable for continuous variables. For instance, there are only so much shapes as there are available in R. In addition, for some scale, there is also a manual variant. There are also some specific scales, e.g. for data/time variables, and scales to develop specific color palettes. Here, we will start with the continuous scales, we will then cover the discrete scales and end with the manual scales. In addition to the {ggplot2} scale functions, the {scales} package adds further possibilities. If you use that package, we’ll do so using scales::. In that way, it is clear what part of the code is {ggplot2} and what part of {scales}.\nSome scale functions are identical: scale_x_continuous() and scale_y_continuous() or scale_x_discrete() and scale_y_discrete() are identical, the same holds for e.g. scale_color_manual(), scale_fill_manual(), scale_shape_manual(), scale_size_manual(), scale_linewidth_manual() and scale_linetype_manual(). All these scales allow you to set the values for the aesthetic. They only differ in terms of the way to identify that aesthetic: a shape number, a color or size. In other words, although there are many scale functions, a lot of them share many similarities. The fact that you have so many of them is e.g. due to the fact that for each x scale, you have an similar y scale.\n\n9.3.3.1 Continuous scales\n\n\n\n\n\n\nArnold, Jeffrey B. 2024. Ggthemes: Extra Themes, Scales and Geoms for ’Ggplot2’. https://jrnold.github.io/ggthemes/.\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.\n\n\nEngler, Jan Broder. 2024. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” bioRxiv. https://doi.org/10.1101/2024.11.08.621836.\n\n\nHvitfeldt, Emil. 2021. Paletteer: Comprehensive Collection of Color Palettes. https://github.com/EmilHvitfeldt/paletteer.\n\n\nPedersen, Thomas Lin. 2024a. Ggforce: Accelerating ’Ggplot2’. https://ggforce.data-imaginist.com.\n\n\n———. 2024b. Patchwork: The Composer of Plots. https://patchwork.data-imaginist.com.\n\n\nPedersen, Thomas Lin, and David Robinson. 2024. Gganimate: A Grammar of Animated Graphics. https://gganimate.com.\n\n\nSlowikowski, Kamil. 2024. Ggrepel: Automatically Position Non-Overlapping Text Labels with ’Ggplot2’.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales: Scale Functions for Visualization. https://scales.r-lib.org.\n\n\nWilkinson, Leland. 2005. The Grammar Op Graphics. 2nd ed. Springer-Verlag, New York. https://doi.org/https://doi.org/10.1007/0-387-28695-0.\n\n\nYutani, Hiroaki. 2024. Gghighlight: Highlight Lines and Points in ’Ggplot2’. https://yutannihilation.github.io/gghighlight/.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data visualization: the grammer of graphics</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html",
    "href": "10_Plot_types.html",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "",
    "text": "10.1 The layered grammer of graphics: ggplot.\n{ggplot2} is one of the most widely used packages to create data visualizations in R. The “gg” stand for the grammar of graphics. In other words, {ggplot2} is an implementation of the grammar of graphics. It does so in a layered way: it builds a plot layer by layer. To add a layer, you need to use a +, not a |&gt;. Using the + nicely shows that you add one layer on top of the other. The overall approach is straightforward: you first select the dataset and possibly the aesthetic mappings. The aesthetic mapping are included in mapping = aes(). Here you include the aesthetics that will be used by all layers. In other words, subsequent layers inherit the aesthetic mappings. You then add one or more layers with the geometries. In {ggplot2} these are referred to as geom_type, e.g. geom_point, geom_line, … . Within these geometries, you can (further) add aesthetic mappings. However, in this case, these mapping are only relevant for that specific geometry. After the geometries, you define the scales. Depending on the variable mapped on an aesthetic, these scales are either continuous or discrete. For each scale, there are default values. In other words, if you don’t include that layer, {ggplot2} will produce a graph using default values for all scale parameters. Note that often, these default values produce elegant graphs. To change the default values, you use scale_xxx_continuous or scale_xxx_discrete where xxx refers to the aesthetic, e.g x, y, color, fill, .. . For most scales, you also have to option to set them manually using scale_xxx_manual. In addition, you can transform scales using a log transformation, using the reverse of reciprocal. Using these functions, you can also set limits, or define labels for the axis. The statistics are usually included in the geometry layer. For instance, a bar charts by default shows the total number of observations per group, a density the probability, … . You can also specify the coordinate system (Cartesian, polar, …). Here, you can usually include the minimum and maximum values of e.g. the horizontal axis and vertical axis. The last layer adds the theme. This layer will be covered in Chapter 11 but in short allows you to specify all components that are part of the non data part of a plot. In this chapter we will use a standard theme theme_minimal(). This theme removes all background annotations.\nMastering the technicalities of data visualization is one thing, designing good quality visualizations is another. Here we’ll focus on the former and cover how you can use {ggplot2} to create visualizations. For the latter you can use e.g. Claus O. Wilke’s [Fundamentals of Data Visualization] (https://clauswilke.com/dataviz/) or the [BBC Visual and Data Journalism cookbook for R graphics] (https://bbc.github.io/rcookbook/#how_to_create_bbc_style_graphics) which show the BBC’s R visualization cookbook. The BBC published its {[bbplot] (https://github.com/bbc/bbplot)} package which includes the BBC styleguide.\nRecall that you can save a plot (R save the plot as a list, (see Chapter 4)). Here, I will use this to add layers. For instance, suppose that there is a graph developed with ggplot(diamonds, aes(carat, price, color = cut)) + geom_point()? To illustrate the use of function scale_x_continious, we can save the first part e.g. using p1 for plot 1 and use p1 + scale_x_continuous. This would be similar to ggplot(diamonds, aes(carat, price, color = cut)) + geom_point() + scale_x_continuous.\nBefore we start, we need to load some packages:",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html#the-layered-grammer-of-graphics-ggplot.",
    "href": "10_Plot_types.html#the-layered-grammer-of-graphics-ggplot.",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "",
    "text": "10.1.1 The data and aesthetic mappings\nTo build a plot, ggplot() needs to know “what” to plot. In other words, you need to tell R where the data is (in which data frame) and which variables in that data frame will be used in the visualization. This is the first layer of a plot. To add this first layer, you use the function ggplot(). The arguments of this function are:\n\nggplot(data = NULL, mapping = aes())\n\nThe argument data refers to the default data frame ggplot() use to search for the variables. Recall that {ggplot2} is part of the {tidyverse}. In other words, you can use pipe operator |&gt; to “pipe” a dataset in the ggplot() function. If the object in data is not a data frame or tibble, R will try to convert the object into a data frame. If the argument is missing, each geometry has to specify the data frame to use in that geometry. The second argument mapping = aes(x = , y = , color =, size =, shape =, fill = linewidth = , linetype = ) includes the aesthetic mappings for the plot. This argument too is optional. If this mapping is not specified here, each geometry will needs its own aesthetic mapping. You can partially map variables at this level and add aesthetic mappings in the geometry layer. For instance, here you would map a variable on the x- and y-axis and add a mapping on the color aesthetic in e.g. the point geometry. In that case, the mapping on the x- and y- axis will be used for all geometries in the plot while the color aesthetic will only be used for the point geometry.\nLet’s use a sample of 10% of the diamonds dataset\n\ndia &lt;- diamonds |&gt; slice_sample(prop = 0.10) \n\nand start a plot where we map carat on the horizontal axis and price on the vertical axis:\n\np1 &lt;- ggplot(data = dia, mapping = aes(x = carat, y = price)) + theme_minimal()\np1\n\n\n\n\n\n\n\n\nThis function returns the first layer of the plot: it shows the panel of the plot and the x- and y-axis. In addition, it shows the variables that were mapped on both axis: carat on the horizontal axis and price on the vertical axis. The limits of these axis (minimum and maximum value of the axis) are derived from the data. You can see this from the minimum and maximum values for carat and price:\n\ndia |&gt; summarise(\n  min_carat = min(carat),\n  max_carat = max(carat), \n  min_price = min(price),\n  max_price = max(price)\n)\n\n# A tibble: 1 × 4\n  min_carat max_carat min_price max_price\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;\n1       0.2      4.01       348     18791\n\n\nThe part `aes(x = carat, y = price) includes the aesthetic mapping. To see this, if you run this part separately, R returns the mapping:\n\naes(x = carat, y = price)\n\nAesthetic mapping: \n* `x` -&gt; `carat`\n* `y` -&gt; `price`\n\n\nBecause the aesthetic mapping in ggplot() is second after the data, usually, the part mapping = aes() is shortened by eliminating the reference to the argument mapping and written as aes(). Similarly, because the data argument is always first, data = is usually dropped from the argument and the function call is either ggplot(data, aes() or data |&gt; ggplot(aes()). In the code, theme_minimal() removes e.g. the background color from the panel. We add it here, to remove colors from the background and panel.\nAt this stage, R is not able to add more aesthetic mappings to the output. All R knows at this stage is that the plot will include variable mappings on the x-axis and a y-axis. However, it is not able to show other aesthetic mappings, e.g. on color, size of shape. Here, R needs additional information from the geometry. With a point geometry, ggplot() will show these aesthetic mapping using a different color, size of shape of a point, with a line geometry, R will show these additional mappings by differentiating he lines using their color, width of type. However, if these aesthetic mappings are defined at this stage, R will include them in any subsequent geometry. The output from the aes() argument shows that cut is also mapped on the color. aesthetic.\n\nggplot(dia, aes(x = carat, y = price, color = cut, size = clarity, shape = color))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\n\n\n\n\n\n\naes(x = carat, y = price, color = cut, size = clarity, shape = color)\n\nAesthetic mapping: \n* `x`      -&gt; `carat`\n* `y`      -&gt; `price`\n* `colour` -&gt; `cut`\n* `size`   -&gt; `clarity`\n* `shape`  -&gt; `color`\n\n\nYou can map the same variable on two ore more aesthetics. For instance, mapping cut on size and shape will show every level of but with a different size and a different shape.\n\n\n10.1.2 Geometry\nThe first layer includes the data and the aesthetic mappings that will be used for all geometries in the plot (unless another aesthetic is specified). At this stage, we know which variable R will show using which aesthetics. We also know where R will find these variables: in the data frame included in the data argument of the ggplot() function. We don’t know how these variables will shown. To add this component, we need an additional layer: the one that includes the geometry and the statistics. In other words, we need a plot type. {ggplot2} includes many geometries or types of plots and I refer to the geometry section in the {ggplot2} and Chang (2025) or to view all these possibilities. Here, you’ll find the often used geometries: point geometries (e.g. scatter plots), line geometries (a line graph), area geometries, bar and column charts and geometries that summarize the data using e.g. a boxplot or a density.\nNote that there are many ways to visualize the same data. The same aesthetics (x- and y-axis, color, fill, line type of width, shape, size or alpha) can be used with many geometries. However, not all geometries allow for the same aesthetics. A point geometry - where you use a “dot” to show combinations of values for the variables mapped on both axis - allows you to include color, shape, size of transparency. However light width of type are not relevant. For a bar chart, you can include fill, color, transparency, but shape would be irrelevant.\nSecond, every geometry is a layer. This allows you to add geometries on top of one another. Doing so, you can add a line geometry to a point geometry or text geometry. Doing so, allows you to build complex data visualizations. Here, we will not focus too much on how you can do so, but once you master the individual geometries, adding additional ones is straightforward. Usually, you can show the data in numerous ways. To illustrate, here are [100 visualizations] (https://100.datavizproject.com/) of the same dataset: a small table showing the number of World Heritage sites in Denmark, Norway and Sweden for 2004 and 2022. In other words, there are 100 ways to visualize data in a table with 2 rows and 6 columns. Selecting the best geometry to visualize your data is very important. Here are many guides to help you select the appropriate geometry, e.g. [from Data to Viz] (https://www.data-to-viz.com/), [Visual Vocabulary] (https://ft-interactive.github.io/visual-vocabulary/) or the guide from the [UK’s Office for national statistics] (https://service-manual.ons.gov.uk/data-visualisation/chart-types). If you add various geometries as separate layers, you have to think about the order in which they are shown: a line geometry followed by a point geometry will show the points on top of the lines.\n\n10.1.2.1 Point geometries\nA point geometry is used to show the correlation between two numeric, continuous variables. The first variable is mapped on the horizontal axis, the second on the vertical axis. These two, x and y, are required aesthetics. For every observations, a point geometry shows the pair of values for the variable mapped on the x-axis and the variable mapped on the y-axis as a single dot. Note that you shouldn’t interpret “dot” in a literal sense as you can change that representation and use e.g. a cross, … .\n\n10.1.2.1.1 geom_point()\nMost geometries share a large number of arguments. So, we will discuss this geometry in detail. Doing so, when we introduce other geometries we can focus on those arguments that are particular for a geometry.\n\n10.1.2.1.1.1 The function\nTo illustrate point geometries, let’s use the point geometry geom_point(). The function includes the following arguments:\n\ngeom_point(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe arguments for geom_point() include an aesthetic mapping argument mapping, a data argument data, a stats argument with default identity and a position argument with default identity. The value NULL for the first two arguments means that R will use the default aesthetic mappings included in the ggplot(aes()) call. geom_point() needs at least two mappings: one on the x- and one on the y-axis. If these mappings are not included in the ggplot() call, you have to add them here. You can use the mapping argument in this geometry also to add additional aesthetic mappings. The inherit.aes = TRUE argument shows that R will use the aesthetic mappings from the ggplot(aes()) call. If you wouldn’t want this, changing this into FALSE will remove these aesthetic mapping for that layer. Doing so, you need to add new aesthetic mappings to this geometry. With FALSE, the point geometry will not add the mapping included in mapping to the mappings in the ggplot(aes()) call. The stats argument allows you to define transformations. The default identity shows the the data as they are. The position argument with default identity shows the points as they are in the data: every “dot” is shown in the panel exactly on value pair spot. However in some cases, one point actually masks two ore more points with the same value pair. Adding position = position_jitter() R will add a random variation to every point to the left or right and to the top or bottom. You can define the maximum width and height of that random variation using the arguments width = and height =. A third option for position = is stack. We’ll cover that position more in depth when we discuss the area geometry geom_area().\nIn addition to these arguments, you can further specify the way in which R will plot the points, e.g. their color, fill, shape, size, stroke and alpha. You can further add the option na.rm  = TRUE. In that case, ggplot() will remove missing values without a warning. The default FALSE shows a warning. By default, ggplot() will add new mappings in this layer to the guide or legend. If you don’t want to do so, you need to set inherit.aes = FALSE. The ... part allows you to define the settings: for instance, the color, size, shape or transparency of a “dot”. As we’ll see, you shouldn’t confuse setting with mapping. Both use the same reference: color, size, shape, linewidth, fill, … However, a setting such as color is part of the lay out of a graph. It tells R which color to use to show a dot. This color can be red, blue, yellow, grey or any other color that is avaiable. R will show all dots using the same color. Using the aesthetic color you map a variable on that aesthetic. Here R will show a different color for every value in the variable that you map on the aesthetic color. In other words, with a mapping the color has a meaning: different colors show different values. As a setting, a color doesn’t offer any additional information with respect to the data as all dots are shown in the same color.\nLet’s see what these arguments do. We start from ggplot(data = dia, mapping = aes(x = carat, y = price)) and add the geom_point() layer, accepting all defaults:\n\np1 + geom_point()\n\n\n\n\n\n\n\n\nRecall that p1 didn’t include the aesthetic mapping on color. In other words, this point geometry only includes the aesthetic mappings on the horizontal and vertical axis. By default, R uses black dots to represent each (carat, price) pair. Every dot shows one observation. The shape of the cloud illustrates the correlation between the variable on the horizontal and the one on the vertical axis. In Figure 10.1, there are six patterns shown in six panel. The first panel, shows no correlation. The point cloud does not show any pattern. The second panel include a pattern where the two variables are correlated, but not in a linear way. Using traditional measure for correlation, the result would suggest the absence of any correlation between variable 1 and variable 2. However, as panel 2 reveals: the correlation is actually strong, but not linear. Panels 3 to 6 show clouds that suggest weak positive correlation (panel 3), weak negative correlation (panel 4) and strong positive (panel 5) and negative (panel 6) correlation. These panel all show a point geometry. In other words, using a point geometry allows you to detect a pattern in the correlation between two variables, even if that correlation is not linear.\n\n\n\n\n\nFigure 10.1: Correlation patterns\n\n\n\n\n\n\n\n\n\n\n10.1.2.1.1.2 Changing the settings\nBefore we add additional aesthetic mappings, let’s first review what the options are for changing the settings of this plot. The options for the settings are usually also available for the mappings. Recall that settings don’t add any new information to the plot and only change the way the plot is shown. By default, R shows the dots with a black color. You can change the color of the points into e.g. red. You can do so by adding color = \"red\" to geom_point():\n\np1 + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nAs an alternative, you can use “lightyellow”:\n\np1 + geom_point(color = \"lightyellow\")\n\n\n\n\n\n\n\n\nHere, we identify the colors by their name. As an alternative to the name you can also add a color’s HEX code. For instance, the color “lightsteelblue” has HEX code “#BEC4DE”. You can enter this hex code to define the color:\n\np1 + geom_point(color = \"#BEC4DE\")\n\n\n\n\n\n\n\n\nHere too, R shows the color. The hex code and name are equivalent:\n\np1 + geom_point(color = \"lightsteelblue\")\n\n\n\n\n\n\n\n\nTo find a color, you can use online color pickers. These usually allow you to identify a color on a wheel or picture and return various color codes: HEX, RGB, … If you copy paste the HEX code, R will show exactly that colo. In addition, most of these color pickers allows you to generate complement colors, shades, … Here for instance, you can find HEX codes. Using their detail, you can find complements, … .\nUsing alpha, you can change the transparency of a point. This value is between 0 (absolute transparency) and 1 (no transparency). To illustrate, using the color red and transparency 1/5:\n\np1 + geom_point(color = \"red\", alpha = 1/5)\n\n\n\n\n\n\n\n\nThe plot now shows for which values the dataset includes a lot of observations. As these single, transparent, dots overlay, they produce a brighter color. Here, you can see that the dataset includes a lot of observations for diamonds with a lighter weight and lower price.\nTo adjust the size of the dots, you can use the size setting. For instance, setting the size = 5 shows much larger dots:\n\np1 + geom_point(color = \"orange\", size = 5)\n\n\n\n\n\n\n\n\nUsing size = 0.75 much smaller\n\np1 + geom_point(color = \"orange\", size = 0.75)\n\n\n\n\n\n\n\n\nIn addition to the color and size, you can also change the shape using shape =. To identify a shape, you can refer to its number (shown in Figure 10.2) or name (shown in Figure 10.3).\n\n\n\n\n\nFigure 10.2: Shapes by number\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Shapes by name\n\n\n\n\n\n\n\n\nLet’s show the “dots” with a cross (shape 4):\n\np1 + geom_point(shape = 4)\n\n\n\n\n\n\n\n\nNote that the default color is the cross is black. Combining both the color (e.g. lightsteelblue) and shape setting:\n\np1 + geom_point(color = \"#BEC4DE\", shape = 4)\n\n\n\n\n\n\n\n\nShapes 21 to 24 can be further adjusted using color, fill and stroke settings. The first, color sets the color of the border, the second, fill, the color of the interior and stroke the width of the border. If you add size, this setting controls the size of the interior part. The total size the shape is the sum of the interior size and the stroke. You can see this in Figure 10.4. For instance, to draw a circle with an orange border, with stroke 2 and a light blue steel interior with size 4.\n\np1 + geom_point( shape = 23, color = \"orange\", fill = \"#BEC4DE\", size = 4, stroke = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.4: Size and stroke\n\n\n\n\n\n\n\n\nIn addition to the shapes that are shown in Figure 10.2, you can add unicode UTF shapes. All characters, shapes, symbols, … have unicode “code”. You can find all codes for [geometric shapes] (https://www.w3schools.com/charsets/ref_utf_geometric.asp). Suppose I you want to use a white diamond containing a black small diamaod as a shape. The unicode is “25C8”. You can add this code using “u25C7”\n\np1 + geom_point( shape = \"\\u25C7\", color = \"blue\", size = 2)\n\n\n\n\n\n\n\n\nThe list of unicode symbols include math symbols, currency symbols, weather symbols, emoji’s, … .\nSettings can be used to highlight specific values. Suppose for instance that you want to highlight a specific level of cut (e.g. “Premium”) in the price - carat plot. To do so, we first plot the plot and use the color settings to show these points in light grey. You can then add a second point geometry, but use a filtered dataset. To do so , you introduce a new dataset in this second point geometry using filter(dia, cut == \"Premium\"). Recall from Chapter 8, that this function returns a dataset. The second point geometry inherits the x and y values from the ggplot() call. In other words, the second point geometry map price and carat on the x-axis and y-axis. However, this dataset only includes the data for cut == \"Premium\" diamonds. Adding a different color as a setting, these points will be shown in another color. For instance:\n\np1 +\n  geom_point(color = \"lightgrey\") +\n  geom_point(data = filter(dia, cut == \"Premium\"), color = \"steelblue\")\n\n\n\n\n\n\n\n\nAs we will see, you can now add an annotation:\n\np1 +\n  geom_point(color = \"lightgrey\") +\n  geom_point(data = filter(dia, cut == \"Premium\"), color = \"steelblue\") +\n  annotate(\"text\", x = 0.5, y = 15000, label = \"Premium cut diamonds \\n shown in color\", color = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n10.1.2.1.1.3 Aesthetic mappings\nAdding aesthetic mappings, in contract to settings, change the information that is shown in the plot. You add these mappings in the aes() function. Doing so, ggplot() will show different colors, shapes, sizes or strokes for every value of the variable that was mapped on that aesthetic. To illustrate, let’s again start from p1 and map the variable cut on the aesthetic color. To do so, we add aes(color = cut) to the geom_point() function and keep all other default values:\n\np1 + geom_point(aes(color = cut))\n\n\n\n\n\n\n\n\nThe result now show different colors per level of cut: diamonds whose cut is ideal are shown with a yellow dot, premium with a light green dot, … . By default, R adds a guide or legend which shows these values. Adding show.legend = FALSE would remove that legend. However, in that case, it would be difficult to interpret the color scale. Note how the plot shows each aesthetic mapping: the variable that was mapped on the horizontal axis is shown as a label under that axis, the variable that was mapped on the vertical axis is shown as a label with the vertical axis and the variable that was mapped on the color aesthetic is shown in the legend.\nAt this stage, adding the aesthetic mapping in the geom_point() function or in the ggplot(aes()) function shows similar results:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is due to the fact that all layers that follow the ggplot() function inherit the aesthetic mapping defined there. Without any aesthetic mapping in ggplot() you would have to add it in the geom_point() function:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn that case, the mapping is only relevant for the point geometry.\nWhat if you add aesthetic mapping on color as well as set a setting for color? In that case, R overrides the aesthetic mapping and shows all colors in the color defined in the setting. For instance:\n\np1 + geom_point(aes(color = cut), color = \"red\")\n\n\n\n\n\n\n\n\nIn the result, you can see that R shows all dots in red. In addition, it removes the legend. You can however, specify other settings, e.g. if you map cut on the aesthetic color, you can still change the setting shape. R will now show every point using the same shape and will use various colors to show the values of the variable cut:\n\np1 + geom_point(aes(color = cut), shape = 6)\n\n\n\n\n\n\n\n\nHere, R maps the variable cut on the aesthetic color and shows the “dots” using a triangle.\nIn addition to the color aesthetic, geom_point() also accepts the fill, shape, size and stroke aesthetic. In addition, you can include more mappings in the aes() function. To illustrate. Let’s add another aesthetic mapping and map the variable in the diamonds dataset color (not to be confused with the aesthetic in R, the variable and aesthetic both happen to have the same name) on the shape aesthetic:\n\np1 + geom_point(aes(color = cut, shape = color))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 276 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, R produces a warning: the shape palette can deal with a maximum of 6 discrete values and more than 6 become difficult to differentiate. If you look at the legend, you’ll see that R didn’t include the shape for color “J”. In this case, you could change the mapping and map cut one shape and color on the aesthetic color:\n\np1 + geom_point(aes(color = color, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\nHowever, here too, R shows a warning. What R is essentially telling here is that you are mapping an ordinal variable on a aesthetic that doesn’t allow to show something in an ordered way: a cross is not better or worse than a dot. For ordinal variables, it is better to use another aesthetic that does allow to show the order. Color for instance, can be shown from light to dark and reflect in order in that way. The same holds for size. A better “cut” can be shown with a larger or smaller size. For instance, if you map color on size:\n\np1 + geom_point(aes(color = cut, size = color))\n\n\n\n\n\n\n\n\nthe plot shows color “D” using the smallest dot while “J” is shown using the largest dot. The shape aesthetic allows you to map nominal values. Here, the order is not relevant can R can show these values with different shapes. However, R will restrict the number of nominal values to 6.\nYou can map the same variable on two aesthetics. In that case, R will change both aesthetics in line with the values of the variable that is mapped on both. For instance, if you map cut on both color and shape, ggplot() shows different level of the variable cut using both a different color and shape: “Fair” is sown with a bleu dot, “Premium” with a lightbleu cross, … .\n\np1 + geom_point(aes(color = cut, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\nIn Chapter 11, we see how you can change the colors, size and shape … in the mapping and show them in the color of your choice and not in the default color values.\n\n\n10.1.2.1.1.4 Adding jitter\nIn some cases, points are on top of each other. Consider the following graph:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph doesn’t make much sense, but is nicely shows that in some cases the same (clarity - price) pairs are on top of each other. As you can not see all that that are actually on the graph, this graph does not represent the data very will. In that case, you can add a bit a jitter to each plot. You can do so using the argument position =. By default the position is identity. This default tells ggplot() to draw the points in line with their (x-value, y-value) pair, even if there are points with 100% overlap. Using the jitter option, R adds a random noise to a plot. In other words, every dot is now shown using the (x-value, y-value) pair but for both R adds a random noise term. In other words it actually shows (x-value + noise, y-value + noise). As two points with 100% overlap (i.e. the same (x-value, y-value)) both get a different random noise term, the plot will show them without that overlap. To illustrate:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = \"jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now show more points than without the jitter. This might seem counter-intuitive, but plots with jitter actually show more information than the plot without (see Figure 10.5)\n\n\n\n\n\nFigure 10.5: Adding jitter\n\n\n\n\n\n\n\n\nYou can control the “amount of jitter” using position_jitter(width = NULL, height = NULL, seed = NA). Here, you can add both the width and height of the jitter. Because jitter is added both positive and negative, the total spread is twice the amount in width and height. The default values (also used if you refer to jitter) are 0.40. Doing so, the jitter occupies 80% of the width of the categorical variable. In Figure 10.5 for instance, you can see that the width of the clarity column in the plot with jitter is 80% of the total width of that column. Adding a value of more than 0.5 doesn’t make sense: in that case the width of one column would overlap with the width of another column. However, setting smaller values, reduces the width of the column with jitter. For instance, using 0.25 for both:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = position_jitter(width = 0.25, height = 0.25)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAdding a seed value, allows you to reproduce the graph with “the same” random jitter.\nA third way to add jitter is to use the geometry geom_jitter() and not the geometry geom_point(). In addition to the arguments for the latter, the former also includes width and height from the position_jitter() function.\n\n\n\n10.1.2.1.2 geom_text() and geom_label()\nA point (or another shape) is one of the ways to show data in a graph. Using text is a second. To illustrate, let’s use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have used this dataset in e.g. Figure 9.1 and Figure 9.2. We will remove some of the aesthetic mapping and scale attributes and start from this plot:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs you can see, we kept the aesthetic mappings on the x- and y-axis and color. However, we dropped the aesthetic mapping on size. We also removed the labels from the axis.\nlife_df includes the name of the country, as well as its ISO3 code. Using geom_text() we can now use these to plot these codes (or names). To do so we add the aesthetic label in the geom_text() function and map the ISO3 code on that aesthetic. Here, the aesthetic label tells where where the values are it needs to use to show the “dots”. In this case, they are in the variable iso3c:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nIn the results, the “dots” are now replaced by each the ISO3 code for each country. The aesthetic mapping on color now shows as a different test color for each region in the dataset. Note that you can include that aesthetic mapping also in the ggplot() function. However, given that the labels are usually very specific to this geometry, they are usually added there and not in the default aesthetics for the entire plot. The function includes a couple of other function that are worthwhile to note:\n\ngeom_text(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  check_overlap = FALSE,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should seem familiar by now. The mapping needs at least a mapping on the x- and y-axis as well as a mapping on the aesthetic label. You can include these mappings in the geom_text() function or define them for all layers in the plot in ggplot(). The options that are new allow you to position the text. The first is check_overlap. By default, this is FALSE. Changing that into TRUE, ggplot() will avoid overlap in the text. nudge_x and nudge_y allow you to add some distance from the text to another geometry. For instance, if your plot also includes geom_point() then using nudge_x will allow some horizontal space, while nudge_y some vertical space between the dot and the text. Size is measured in mm (size_unit = \"mm\" alternatives are pt, cm, or pica pc.). The ... allow you to add setting, e.g font size, font family, color (in case that aesthetic is not used), angle, … . Let’s add a couple of settings, e.g. reduce the font size to 3, check for overlap, set the font family to “mono” or one of the other supported font families (serif, …) or add fontface (plain, bold or italic) or add an angle (22.5°). To illustrate, let’s add the country’s ISO code to the life expectancy plot, using the font family “mono” in italics and with a small angle of 22.5°:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(\n    aes(label = iso3c), \n    size = 3, \n    check_overlap = TRUE, \n    family = \"mono\",\n    fontface = \"italic\",\n    angle = 22.5) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nIn addition to you can use string functions to generate text labels from the aesthetic mappings. For instance, to show the values as (x, y) pairs, you can use paste0 with \"(\", x, \",\", y, \") to show a (, the value from the variable mapped on the x-aesthetic, a “,”, the value mapped on the y-aesthetic and a closing bracket. For life_df, where per capita gdp is mapped on the x-axis, life expectancy at birth on the y-axis - and adding rounding - the code to generate a plot with (x, y) pairs is:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(\n    aes(label = paste0(\"(\", round(gdp_capita, 0), \",\", round(life_exp, 1), \")\")), \n    size = 3, \n    check_overlap = TRUE, \n    family = \"mono\") +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nAs an alternative for geom_text() you can use geom_label(). Let’s check the result of this geometry:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_label(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nBy default, R includes values for the variable mapped on the label aesthetic but does so in a frame. The function includes the following arguments:\n\ngeom_label(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  label.padding = unit(0.25, \"lines\"),\n  label.r = unit(0.15, \"lines\"),\n  label.size = 0.25,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nIn addition to those for geom_text() these arguments also allow you to adjust the border of the label: the amount of padding in each label (label.padding), the rounding of the corners label.r and the label size. In addition, you can include setting to e.g fill the label. You can do the latter in the aes() function. Doing so, ggplot() fills the labels per color. To illustrate a couple of options. Here we will show the label, where iso3c is mapped on the fill aesthetic, the color of the font is white and the labels have straight corners. The font family is serif. Note that we moved the color aesthetic from the ggplot() call to the fill aesthetic in the geom_label() call.\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp)) +\n  geom_label(\n    aes(label = iso3c, fill = region), \n    color = \"white\", \n    family = \"serif\", \n    size = 3, \n    label.r = unit(0, \"lines\")) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nYou can use these geometries together with geom_point(). Doing so, the graph includes a dot for every observations as well as a label:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() +\n  geom_text(\n    aes(label = iso3c), \n    check_overlap = TRUE, \n    nudge_x = 0.2, \n    nudge_y = 0.2, \n    size = 3) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n{ggrepel} (Slowikowski (2024)) allows you to add more details to labels and text parts of a plot.\n\n\n10.1.2.1.3 geom_smooth()\nAlthough this geometry is not a point geometry, it is often used as an additional layer with a point geometry as it helps in identifying trends in (overplotted) data. To smooth a dataset is to add a function that approximates the important patterns in that dataset leaving out random noise or other rapid changes in the data. The function’s arguments are:\n\ngeom_smooth(\n  mapping = NULL,\n  data = NULL,\n  stat = \"smooth\",\n  position = \"identity\",\n  ...,\n  method = NULL,\n  formula = NULL,\n  se = TRUE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nYou can control more options if you use stat = stat_smooth(). The aesthetics mapping needs at least a mapping on the x- and y- axis. Here, the statistic is smooth. The position = identity shows that the smooth function will show the predicted values for the variable mapped on the y-axis. With respect to the method, you can accept the default values. An analysis of those methods is left for more advanced statistics or econometrics classes. The same holds for the formula argument. By default, geom_smooth() adds a 95 confidence interval. You can change this level using level = 0.90 for a 90% confidence level or 0.99 for a 99% confidence level. This arguments is not a formal part of the geom_smooth() function, but will be used by stat_smooth() if your refer to it to change the stat argument. If you don’t want that, you can change the default se = TRUE in FALSE. The ... allow you to change the settings e.g. color, … .\nLet’s return to the diamonds dataset where carat is mapped on the horizontal axis and price on the vertical axis (p1) and add geom_smooth().\n\np1 + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe plot shows the relation between carat on the one hand and price on the other. The expected price, given a value for carat, is shown by the blue line. The confidence level is shown as a confidence interval around that blue line. By default, is confidence level is 95%. R also shows the method used to calculate the smooth function. Let’s first change some of the settings, e.g. the line width of the blue line (linewidth =) and the color of both the line (color) and the background of the confidence interval (fill).\n\np1 + \n  geom_smooth(\n    level = 0.90, \n    linewidth = 0.75, \n    linetype = \"solid\", \n    color = \"#F59247\", \n    fill = \"#F3Be96\") +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nOften geom_smooth() is added as a layer in addition to geom_point(). This produces the following graph (where I add some transparency to the points in geom_point():\n\np1 + \n  geom_point(alpha = 1/10) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nLet’s see what happens if we add another aesthetic in the ggplot() call and map cut on the aesthetic color:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now shows 5 smoothed lines, one per level of cut. The lines are in the same color as the color chosen to map cut. Why did ggplot() return one smoothed function per cut? Recall that the plot defines the aesthetic mappings in the ggplot() call. All other layers inherit this mapping. For geom_smooth() this means that it will plot a smoohted line using the x- and y- values, but will do so for every level of every aesthetic. If you would include the size aesthetic for for clarity, geom_smooth() returns 13 smoothed lines: 5 for each level of cut and 8 for each level of clarity. For the latter, geom_smooth() changes the width of the line to show the level of the aesthetic. Even if we set se = FALSE to remove the confidence levels, this plot is hardly interpretable.\n\nggplot(dia, aes(x = carat, y = price, color = cut, linewidth = clarity)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth(se = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo avoid this while keeping the option to add other aesthetic mapping in addition to x and y, there are various options. First, you don’t define any aesthetic mapping at the level of the plot, but keep these for the individual geometries. As mappings in one geometry are not inherited by the others, geom_smooth() will only smooth using the variables mapped in its aesthetics mapping, e.g. only x and y. To illustrate:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  geom_smooth(aes(x = carat, y = price)) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere, we map the carat and price variables on the x- and y- axis and cut on the color aesthetic in the geom_point() call. In geom_smooth() we don’t include the mapping on color. Doing so, geom_smooth() only used the values mapped on the x and y axis to calculate the smoothed line. Note that this also allows you to add other aesthetics in geom_smooth(). For instance, you could draw a smoothed function per level of clarity while the points show the level of cut.\nThe second way to handle this is to include the aesthetic mapping at all layers have on common in the ggplot() call while adding those that are specific for each layer to the geometry for that layer. In the example: you add the aesthetic mapping of cut on color in geom_point() while adding not additional mapping to geom_smooth(). As geom_point() inherits the mappings from the ggplot() function, it it adds one addition mapping. geom_smooth() doesn’t add any other mapping. Doing so, it only used the mappings that it inherits:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThere is one method and formula that is worth mentioning: a linear trend. The method to estimate a linear trend is lm of linear model. This is the method you would use to estimate linear regression models. The formula is y ~ x. This too is the formula you would use for bivariate linear regression models. Adding both allow you to add a linear trend:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.2 Line geometries\nIn the previous section, geom_smooth() resulted in a line and is essentially a summary geometry which is shown as a line. Line geometries are ideal to show the evolution of a numeric (double) variable. Examples include a firm’s market capitalization, its sales or gross margin or macro-economic data where “evolution” is shown with a date/time variable mapped on the x-axis and the value of the series on the y-axis, … . Every (x,y) pair - e.g. (year, sales) - represents a “dot”. These dots are not shown by they are connected using a line. There three different line geometries: geom_line(), geom_step() and geom_path(). These three differ in the way they show the data. The first shows that data in the order of the dataset and uses a continuous line. For instance, if the dataset is ordered per year, the first “point” on the line will show the value of the variable mapped on the y-axis for the earliest year, the second for the second year, … . The second uses the same order to show the data, but uses straight lines to connect the “dots”. The last, geom_path() shows the data in the order in which they appear on the x-axis. If the variable mapped on the x-axis is time, geom_path() and geom_line() or geom_step() are equivalent. However, if the variable mapped on the x-axis is e.g. “gross margin” and the variable on the y-axis is “market capitalization”, geom_path() will use the lowest value for (gross margin, marketcapialization) as the first “dot”. It then connect that dot with the second lowest value for (gross margin, market capitalization), … . In addition geom_vline(), geom_hline(), geom_abline() allow you to draw line segments: a vertical line, a horizontal line or a line with a slope covering the full plot panel. Using geom_segment() and geom_curve() you can draw a line that connects two points within the panel. In general, most of what was written with respect to the aesthetic mapping in the previous section for point geometries, also applies to line geometries and most differences are straightforward: you can define the line width in a line geometry, not in a point geometry, you can have a shape in a point geometry, but not in a line geometry. So, here we’ll focus on what is specific for these geometries.\nYou can think of a line geometry in terms of a point geometry. While the latter shows one dot for each observations, the former implicitly connects these dots using a line. While for point geometry is very usefull to show correlation, the line geometry is very usefull to show evolution.\nTo illustrate, we’ll use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have use this dataset in e.g. Figure 9.1 and Figure 9.2. There we used the dataset for a specific year. Here, we will use all data for Costa Rica and Brazil. We’ll also rename the variable date in year.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nWe will start with geom_line() and geom_step().\n\n10.1.2.2.1 geom_line() and geom_step()\ngeom_line() and geom_step() are usually used to show the evolution of one numeric variable where a date/time is mapped on the x-axis and the value is mapped on the y-axis. These two mappings, x and y, are required. You can map other variables on the color of the line, the line width and line type. In addition you can use values of other variables to group lines. The difference between geom_line() and geom_step() is the way they connect the dots. For geom_line() the arguments are:\n\ngeom_line(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nMost arguments of the geom_line() function should be familiar. We’ll use the position = when we discuss geom_area() where we will show how you can “stack” lines in a plot. The orientation = argument is only used if the orientation of the line is not straightforward from the data. The geom_step() functions adds a couple of arguments that are related to the way this functions shows a line: either in a horizontal or vertical segment.\n\ngeom_step(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  direction = \"hv\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nHere, the function needs to know if it first has to move up (vertical) or first moves rights (horizontal). The default direction = \"hv\" first move right than up or down, using “vh” the first movement is up or down, than right. “Mid” means that the step is taken halfway.\nLet’s illustrate these two function for the dataset with life expectancy at birth for Brazil and Costa Rica:\n\ndf_life_cribra &lt;- life_df |&gt; filter(iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nWe first show only one country (Costa Rica). Doing so allows us to keep the aesthetic mapping simple: we’ll map the variable year on the horizontal axis and life expectancy on the vertical axis. For all other arguments, we accept the default values.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows the evolution of life expectancy at birth since 1960 for Costa Rica. R adds the variables mapped on the horizontal and vertical axis as labels. The default color for the line is black. Adding a point geometry, you can see that a line geometry is often an extension of a point geometry. However, because here we show an evolution, a line geometry works better. Adding dots doesn’t add any additional value.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s change the settings, in other words, the visual presentation of the plot without adding any new information. In addition the color and transparency, you can change the line width, the line type (dotted, dashed, …). With respect to the line type, Figure 10.6 shows the 6 most common types: solid, dashed, dotted, dotdash, longdash and twodash. To illustrate these settings, we’ll plot the data for Costa Rica using a blue dotted line with linewidth 1.5 and a level of transparancy (alpha) equal to 0.50.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line(color = \"blue\", linewidth = 1.5, linetype = \"dotted\", alpha = 1/2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.6: Non data components of maps\n\n\n\n\n\n\n\n\nFor the geometry geom_step() with the exception of the the direction of the steps (hv, vh or mid), all other settings are similar. However, in this case, the result is not a continuous line, but a line that moves in discrete steps from one point to the other. To illustrate the latter, a point geometry shows the exact location of each observation.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(color = \"blue\", direction = \"vh\") +\n  geom_point(color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot also illustrates the direction. With vh chosen here, the movement from one point to the other starts with the vertical movement. As the line reaches the y-level of the next point, the line graph moves in horizontal direction. With ‘hv’, the first movement would be horizontal. Using mid, the vertical movement starts in the middle of the horizontal movement.\nFor both geom_line() and geom_step() and in addition to the required x and y mapping, you can map additional variable on the color, line type and width aesthetics. You can do so in the ggplot() function as well as in the geom_line() or geom_step() functions. The difference between both was discusses in the section on the point geometry. Here, we’ll use the life expectancy dataset with both Brazil and Costa Rica and map these countries on the color aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nthe linetype aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linetype = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nand the linewidth aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linewidth = country)) +\n  theme_minimal()\n\nWarning: Using linewidth for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nThe results would be similar for the geom_line() geometry. In Chapter 11, we see how you can change the colors, line width and type, … in the mapping. There, you’ll see how you can change e.g. the values used to set the line width or determine the color of the line.\nThe group aesthetic is useful is you want to show the evolution for a large number of values. Suppose that you want to show the evolution of life expectancy for all countries in the dataset. As the number of countries is very large, mapping the country variable on an aesthetic such as color, linewidth or linetype is usually not a good option. First, there are way too many colors to allow for a meaningful difference across countries. This is where the group aesthetic can help you out. To see what it does, in the next figure, we’ll map the variable country on the group aesthetic while we keep the x values for the date and the y variable for life expectancy:\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe result is a line graph with all lines shown in the same (default) color, type and width. There is one line per country. You can combine the group aesthetic with e.g. a color aesthetic. Using the previous graph, you could map, e.g. the region variable on the color aesthetic. In that way, all countries in the same region would be shown using the same color, with differences colors per region.\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(color = region, group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nUsing {[gghighlight] (https://yutannihilation.github.io/gghighlight/index.html)} you can highlight some some countries. For ways to do so, see the documentation (Yutani (2024)).\nIf series are very volatile, adding the geom_smooth() allows you to filter out noise. To illustrate, we’ll add a smooth function for both Brazil and Costa Rica. We’ll accept all default values. Using the linetype of linewidth options in the geom_smooth() geometry, you could change the linetype of e.g. the smoothed line and make it a bit thinner than the line showing the actual data (or vice versa).\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, color = country)) +\n  geom_line() +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#####geom_path()\nThe previous geometries show the data as they appear in the dataset. This is not always the most useful representation of the data. If you want to show co-movement for instance, geom_line() or geom_step() are not the most suitable options. To illustrate, consider the dataset in data_beveridge.csv. The dataset includes quarterly data for the unemployment rate (the number of unemployed to the number of employed + the number of unemployed) and the vacancy rate (the number of vacancies to the number of jobs in a country). The Beveridge curve shows how both move together: when the economy is solid and growth the good, the unemployment rate should be low and the vacancy rate high. The opposite should be the case in times of recession. The dataset is includes in data &gt; raw &gt; data_beveridge.csv. To import the data, you can run\n\ndata_beveridge &lt;- read_csv(here::here(\"data\", \"raw\", \"data_beveridge.csv\")) \n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): TIME PERIOD, FREQUENCY\ndbl  (2): vacancy_rate, unemployment_rate\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_beveridge &lt;- data_beveridge |&gt; rename(date = DATE, quarter = `TIME PERIOD`)\ndata_beveridge |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 5\n  date       quarter vacancy_rate unemployment_rate FREQUENCY\n  &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;    \n1 2019-06-30 Q2 2019          2.3              7.63 Q        \n2 2014-12-31 Q4 2014          1.5             11.6  Q        \n3 2019-12-31 Q4 2019          2.3              7.46 Q        \n4 2010-06-30 Q2 2010          1.2             10.3  Q        \n5 2013-03-31 Q1 2013          1.4             12.2  Q        \n\n\nThe data include a variable date showing the last day of the quarter, quarter, showing the quarter and the vacancy_rate and unemployment_rate. One way to show co-movement is to add both to the same line graph. Do do so, we add two line geometries: one for the unemployment rate and one for the vacancy rate.\n\ndata_beveridge |&gt; ggplot(aes(x = date)) +\n  geom_line(aes(y = unemployment_rate),color = \"red\") +\n  geom_line(aes(y = vacancy_rate), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, as you can see from the graph, this is not very helpful. Even if both series are measured in percent and both are usually less than 10%-points removed from one another (i.e. the are close enough to compare), it is difficult to spot co-movement. The vacancy rate’s volatility is much higher than the volatility of the unemployment rate.\ngeom_path allows you to map one series on the horizontal axis and another on the vertical axis. Doing so, you create a point geometry with one dot for every (x-value, y-value) pair. A line now connects all points in the order in which they appear in the for the variable mapped on the horizontal axis. To see how the graph builds, let’s start form the point geometry with the unemployment rate on the horizontal axis and the vacancy rate on the vertical axis:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is a scatterplot with one dot for every observation in the dataset. We don’t know however how both variables moved together. Starting from any dot in the scatter plot, there is no way to tell which dot came in the quarter after that dot. As a matter of fact, there is not way to identify the first dot in the data. This is where geom_path() makes the difference: it connects dots as they appear in the data. In other words, it shows which “dot” came first, which dot came after the first, which one came after the second, … . In short is shows the path from one point to the other: if you start from the start of the line, you can follow the path until is reaches the end of the line. Here, both the geom_path() and the point geometry are shown. However, this does not need to be the case and you can use the line path geometry as a stand alone layer.\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you add a geom_text or geom_label() layer, you can add additional information to the graph. To do so, you need to map a variable on the aesthetic label. In this case, we could use the variable quarter as it shows in which quarter the unemployment rate and vacancy rate were measured. Adding this label adds additional information: you see the movements per quarter. Fitting the label using geom_text() or geom_label() often requires some experimenting with the settings e.g. the size of the font and the nudge left, right, up or down (using the argument nudge_x or nudge_y) or the color of the font:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_text(aes(label = quarter), size = 3, color = \"grey\", nudge_x = 0.2, check_overlap = TRUE) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe label is shown in grey, with the check_overlap = TRUE and a nudge to the right to avoid too much overlap with the line. In addition to the options that are similar to those for the other line geometries, geom_path() includes arguments that allows you to determine how the line ends (lineend =), how various parts connect (line_join =) and e.g. the use of an arrow (arrow =).\n\ngeom_path(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  lineend = \"butt\",\n  linejoin = \"round\",\n  linemitre = 10,\n  arrow = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nFor the arrow, changing arrow = NULL in TRUE, add an arrow. You can change the default options if you add arrow = arrow() which allow you to set the angle of the arrow head (the width of the angle), length of the arrow head measured in “inches” or “mm”, if arrows are needed at the end of the line (“last”), at the beginning (“first”) or at both ends (“both”) and if you would like to arrow head to be open or closed.\n\narrow(angle = 30, length = unit(0.25, \"inches\"),\n      ends = \"last\", type = \"open\")\n\nYou can now add an arrow at the end. Here, we draw an arrow with a closed arrow head with length 6 mm and angle 22.5°:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path(\n    arrow = arrow(angle = 22.5, length = unit(6, \"mm\"), ends = \"last\", type = \"closed\")) +\n  geom_text(aes(label = quarter), size = 3, color = \"grey\", nudge_x = 0.2, check_overlap = TRUE) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the {ggrepel} packages includes a geometry greom_text_repel() that allows you to add, e.g. line segments connecting the text and the dot. If you create a graph using geom_path() and you want to add labels, it is worthwhile to look at the options in this package. Using geom_text_repel you can fine tune most of the label or text parts (in addition to Slowikowski (2024) you can also use “?ggrepel::geom_text_repel” in the console to check all the options). To show only a limited set of possibilities with this package, here is a graph using the geom_text_repel() geometry:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate, label = `quarter`)) +\n  geom_path() +\n  geom_point(color = \"red\") +\n  geom_text_repel(\n    size = 3, \n    color = \"#003049\", \n    min.segment.length = 0, \n    seed = 42, \n    box.padding = 0.5, \n    max.overlaps = getOption(\"ggrepel.max.overlaps\", default = 20)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.1.2.2.2 geom_hline(), geom_vline() and geom_abline()\nThese geometries allow you to add horizontal lines, vertical lines and line segments to a plot. Doing so, you can identify e.g. moments in time or or show average values for the y-axis over a longer period of time. The arguments for these three functions are very similar. As an example for geom_hline(), these arguments are\n\ngeom_hline(\n  mapping = NULL,\n  data = NULL,\n  ...,\n  yintercept,\n  na.rm = FALSE,\n  show.legend = NA\n)\n\nIn addition to the usual arguments, this function include the argument yintercept: the value where the horizontal line crosses the vertical axis. The argument is xintercept in geom_vline() equals the value where the vertical line with intersect the horizontal axis. For geom_abline(), you need the intercept, where the line will intersect the vertical axis if the value on the horizontal axis is 0 and the slope, the increase per unit on the horizontal axis. To illustrate the use of these geometries, let’s use the unemployment rate from the data_Beveridge dataset and show this variable using a line geometry:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to add the average for the entire period. The average value equals\n\nave_unemployment &lt;- mean(data_beveridge$unemployment_rate)\n\nWe can add this to the plot using geom_hline().\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  geom_hline(yintercept = ave_unemployment, color = \"lightblue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_vline() you can also highlight the quarter where the major investment bank Lehman Brothers collapsed (third quarter 2008):\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  geom_vline(xintercept = as.Date(\"2008-09-30\"), color = \"orange\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_abline() you can draw any straight line. Suppose that you have a scatter plot looking like Figure 10.7\n\n\n\n\n\nFigure 10.7: Using geom_abline() in a scatter plot\n\n\n\n\n\n\n\n\nand would like top draw a red line with intercept 0 and slope equal to 10. Using geom_abline() as a additional layer, you can add this line to the scatter plot:\n\ndf |&gt; ggplot() +\n  geom_point(aes(x = x, y = z1)) +\n  geom_abline(intercept = 0, slope = 10, color = \"red\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAll lines where drawn crossing the entire plot panel. Using geom_segment() and geom_curve() you can draw straight lines and curves that are shorter.\n\n\n10.1.2.2.3 geom_segment() and geom_curve()\nWith geom_segment() and geom_curve you can connect two points in a graph. To illustrate both, we’ll use the life expectancy dataset and filter observations for 1980 and 2020. Using both geom_segment() and geom_curve() we will connect two points. For every country, the first point is the (per capita gdp, life expectancy) pair in 1980. The second point for each country are the values for the same variables in 2020. The required aesthetic mappings for both geometries are x, the first observations for the variable mapped on the horizontal axis, y, the first observation for the variable mapped on the vertical axis and xend and yend, the last observations of the variables mapped on the horizontal and vertical axis. For the life expectancy dataset, x = gpd per capita in 1980, y = life expectancy in 1980, xend = gdp per capita in 2020 and yend = life expectancy in 2020. Other aesthetic mappings include the color of the line, its width and type and, for geom_curve(), also the curvature.\nLet’s first prepare the dataset: filter the observations of 1980 and 2020 the life_df. In addition, we need a variable for gdp in 1980 and one for gdp in 2020. The same holds for life expectancy. To create these variables, we need to pivot the dataset wider (Chapter 8):\n\nlife_seg &lt;- life_df |&gt; \n  filter(date == 1980 | date == 2020) |&gt; \n  pivot_wider(\n    names_from = c(date), \n    values_from = c(gdp_capita, life_exp, pop),\n    names_glue = \"{date}_{.value}\")\n\nWe can now use life_seg to illustrate geom_segment(). To show the data, we kept the log-transformation on the horizontal axis and added the aesthetic color to show lines in different colors per region:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region)) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nUsually, to show direction, you would add arrows. You can do so using the arrow = argument in both geom_segment() and geom_curve(). Here, the arrow head is closed and is 2.5 mm long, with an angle equal to 11 degrees:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\") )) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nFor every country, can now spot the evolution of both variables between 1980 and 2020. An line and arrow which points to the top right, suggests that both per capita gdp and life expectancy improved over that time period, a line pointing top left suggests that per capita gdp fell (left) but life expectancy rose (top). If the line and arrow suggest a moment down, life expectancy fell while per capita gdp rose (line facing right) or fell (life facing left).\nAnother way to use geom_segment() is to combine it with a point geometry and connect two points. Using the life expectancy data for 1980 and 2020, we can use a point geometry to show both values. We will do so for a sample of 40 countries and first remove observations with missing data for life expectancy in 1980. We map the countries on the vertical axis. We sort them in descending order of life expectancy at birth in 1980. To do so, we use {forcats} fct_reorder() function (Chapter 4). In that way, the countries will be ordered in descending order of life expectancy at birth in 1980. We use geom_point() to show the observations. To do so, we map life expectancy at birth on the horizontal axis and also on the size and color aesthetics. Doing so, higher values for life expectancy in will show up through a larger dot as well as the color of the dot. We drop the legend. We need two point geometries: one for 1980 and one for 2020.\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE)\n\n\n\n\n\n\n\n\nWe now have two dots. One with life expectancy at birth in 1980 and one in 2020. Using geom_segment() we can now connect these dots. In the aesthetics, we map life expectancy in 1980 on the x aesthetic, live expectancy in 2020 on the xend aesthetic. We also map the latter on the color aesthetic. Doing so, the segment will have the same color as the dots for life expectancy in 2020. In the last line, we change the color:\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_segment(\n    aes(x = `1980_life_exp`, xend = `2020_life_exp`, color = `2020_life_exp`), \n    arrow = arrow(angle = 12, type = \"closed\", length = unit(2, \"mm\")),\n    show.legend = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is some further layout work, e.g. the choice of colors for the dots and lines, the labels of the axis, a caption, … . We could add the flag of the country next to its name, … . But here, you have the basic “dumbell chart”: a combination of two point geometries and one segment.\ngeom_curve() shows a similar plot, but adds a curved line. You can control the level of the curvature using the curvature = argument. A positive value will produce a right hand curve, a negative value a left and curvature and 0 shows a straight line. In the example, the curvature is -0.5. The plot also includes arrows.\n\nlife_seg |&gt;\n  ggplot() +\n  geom_curve(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\")), \n    curvature = -0.50) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_curve()`).\n\n\n\n\n\n\n\n\n\nGetting the curvature right usually requires some experimenting with various numbers, both positive and negative, usually starting from the default value and moving closer to 0 to reduce curvature or futher from 1 to add to curvature.\nYou can use both geometries also to add a line or curve segment on a plot. To do so, you can use the aes() mapping. For any segment or curve you would like to add as a layer to the graph, you add a value for x, xend, y and yend in the aesthetic mapping in geom_segment() or geoom_curve() and these geometries will add a line or curve between the points (x, y) and (xend, yend). Using the unemployment data in the Beverdidge dataset, the next graph adds a curve line stressing the rise in the unemployment rate and a segment to stress the fall in the unemployment rate. The x- values x and xend are dates. For both the curve and the segment the end is shown with an arrow:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  geom_curve(\n    aes(x = as.Date(\"2008-03-31\"),\n        y = 7.5, \n        xend = as.Date(\"2013-03-31\"), \n        yend = 12.5), \n    curvature = -0.25, \n    color = \"red\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  geom_segment(\n    aes(x = as.Date(\"2013-06-30\"),\n        y = 12.5, \n        xend = as.Date(\"2025-03-30\"), \n        yend = 6.5), \n    color = \"#2ECC71\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  theme_minimal()\n\nWarning in geom_curve(aes(x = as.Date(\"2008-03-31\"), y = 7.5, xend = as.Date(\"2013-03-31\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = as.Date(\"2013-06-30\"), y = 12.5, xend = as.Date(\"2025-03-30\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nAs you can see, R suggests to use another function annotate() to draw these lines. We cover annotate() Chapter 11.\nNote that you can use geom_hline(), geom_vline(), geom_abline(), geom_segment() and geom_curve() to draw graphs such as a supply and demand diagram.\n\n\n\n\n\n\nSupply and demand\n\n\n\n\n\nTo keep it as simple as possible and use as many line geoms as possible, let’s start from the y-axis, which we will call “Price”. We generete this series as a simple sequence from 0 to 100:\n\nsup_dem &lt;- data.frame(\n  price = seq(0, 100, by = 1))\n\nWe can now add demand\n\\[\nQ_D = 800 - 8 * P\n\\]\nand, using the inverse supply curve, supply\n\\[\nP = \\frac{1}{8} * Q_S =&gt; Q_S = 8 * P\n\\]\n\nsup_dem$demand = 800 - 8*sup_dem$price\nsup_dem$supply = 8 * sup_dem$price\n\nUsing the data in sum_dem we can now build the supply and demand plot:\n\nsup_dem |&gt; ggplot(aes(y = price)) + # Price on the vertical axis\n  # Show quanity on the horizontal axis, add some color and linetype and width (as a setting)\n  geom_line(aes(x = demand), color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  geom_line(aes(x = supply),  color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  \n  # Dotted lines for the equilibruim (note that you can calculate the equilibrium \n  # using the demand and supply parameters and add them here)\n  geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  \n  # Create x and y-axis \n  geom_hline(yintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  \n  # Set the labels for the axis\n  labs(\n    x = \"Quanity\", \n    y = \"Price\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 95\n  # here the text is \"Supply\"\n  annotate(\"text\", x = 700, y = 95, label = \"Supply\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 5\n  # here the text is \"Demand\"\n  annotate(\"text\", x = 700, y = 5, label = \"Demand\") +\n  \n  # Remove all ticks and labels for the x and y axis\n  theme(axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      \n  # Show on a white background\n      panel.background = element_rect(fill = \"white\"))\n\nWarning in geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.3 Area geometries\nThere are two area geometries that are often used: geom_area() and geom_ribbon().\n\n10.1.2.3.1 geom_area()\ngeom_area() allows you to show the composition of a variables changes over time, e.g. the share of various components in GDP, the share of items in household expenditures, the share of one product or region in total sales … . To illustrate this geometry, we’ll use the life_expectancy at birth dataset for Brazil and Costa Rica and the AMECO database with data on age structure of the population in France: the population ages 0-14, aged 15-64 and 65 and over.\nFor the first dataset, if you haven’t used it yet, you can run the following code\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRows: 13671 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): iso2c, iso3c, country, region\ndbl (4): date, gdp_capita, life_exp, pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nIn Chapter 7, you used the AMECO dataset to learn some tidying skills. In the next box, I’ll create the dataset from the data in data &gt; raw &gt; ameco_xlsx folder. We’ll need AMEC01.XLSX.\n\n\n\n\n\n\nFrench population data\n\n\n\n\n\nFirst import the dataset\n\nlibrary(readxl)\n\nameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nWe’ll store two series: one with data (ameco1) and one with data labels (ameco2):\n\nameco2 &lt;- ameco1 |&gt; select(CODE, TITLE, UNIT...11)\nameco1 &lt;- ameco1 |&gt; select(CODE, COUNTRY, matches(\"\\\\d{4}\"))\n\nTo tidy the dataset, we need to pivot, first longer followed by a pivto wider\n\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = matches(\"[0-9]{4}\"), names_to = \"year\", values_to = \"value\")\nameco1 &lt;- ameco1 |&gt; pivot_wider(names_from = \"CODE\", values_from = \"value\")\n\nAfter selecting the series we need, we need to pivot the data back to longer format: the population structure will be used as one variable with three level: one for each age category:\n\n# Select the variables we need\nameco1 &lt;- ameco1 |&gt; select(COUNTRY, year, NPTN, NPCN, NPAN, NPON)\n\n# year is a character variable: need to change in numeric\nameco1$year &lt;- as.numeric(ameco1$year)\n\n# Pivot longer to crate one variable, \"age_group\" with 3 values, one for each age category\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = starts_with(\"NP\"), names_to = \"age_group\", values_to = \"pop\")\n\nTo get nice labels, we first identify the unique labels and merge those with the data. This will allows us to generate e.g. nice legends:\n\nameco2 &lt;- ameco2 |&gt; filter(CODE == \"NPTN\" | CODE == \"NPCN\" | CODE == \"NPAN\" | CODE == \"NPON\")\nameco2 &lt;- unique(ameco2)\nameco1 &lt;- ameco1 |&gt; left_join(ameco2, by = join_by(age_group == CODE))\n\nTidy the names and select France for all years to 2022:\n\nameco1 &lt;- ameco1 |&gt; rename(unit = UNIT...11, country = COUNTRY, population = TITLE)\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s check the data:\n\npop_fra |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 6\n  country  year age_group    pop population                    unit        \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;       \n1 France   1974 NPON       7085. Population: 65 years and over 1000 persons\n2 France   1982 NPCN      12394. Population: 0 to 14 years     1000 persons\n3 France   2018 NPCN      12147. Population: 0 to 14 years     1000 persons\n4 France   1962 NPCN      12771. Population: 0 to 14 years     1000 persons\n5 France   1992 NPAN      38496. Population: 15 to 64 years    1000 persons\n\n\n\n\n\nLet’s first use an area graph with one variable. Using geom_area() will map the variable year on the horizontal axis and life expectancy at birth for Brazil on the vertical one. These two aesthetics are required. We will then use geom_area() and accept all default values:\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWith one variable, geom_area() essentially creates a line geometry but fills the area between the line, the horizontal axis and the vertical axis. Using the settings, you can change the color or transparency and using geom_line() you can show the line as an additional layer. For instance,\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area(fill = \"blue\", alpha = 0.20) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the dataset, we have two countries, Brazil and Costa Rica. If you map the variable country on the fill aesthetic, ggplot() will stack the life expectancy data. In other words, it add life expectancy in Brazil to life expectancy in Costa Rica and shows the stacked outcome.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis outcome, which doesn’t make sense, is due to the default values in the geom_area() function:\n\ngeom_area(\n  mapping = NULL,\n  data = NULL,\n  stat = \"align\",\n  position = \"stack\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...,\n  outline.type = \"upper\"\n)\n\nThe mapping, data, na.rm, orientation, show.legend, inherit.aes and ... should be familiar. Although we already met stat and position, for the first we always kept identity and for the latter, we kept identity or jitter. The first, stat = \"align\", shows what will happen is two series are stacked, but have no common x-coordinates. By default, R will interpolate these values and align the values for each series. If there are two series, R stacks the values: in other words, it adds the values for the first series to those of the second to show the second series, the sum of the values of the first two are added to the values of the third to show the third series, … . This is the default: position = stack. Using stack  = \"identiy\" changes this default. To illustrate, in the graph with life expectancy for Brazil and Costa Rica, geom_area() first plotted life expectancy for Costa Rica. The second series was the sum of life expectancy in Costa Rica and Brazil. If there would have been a third country, the third line would have shown the sum of life expectancy in all three these countries. Changing these defaults, R will plot two curves. However, as life expectancy in Costa Rica was always higher than in Brazil, the area for Costa Rico will overlap and mask the area for Brazil.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo deal with these issues, you can add transparency using the alpha setting and add a line per country, change the order in which they appear, … .\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\", alpha = 0.10) +\n  geom_line(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_area() with multiple variables, stacking them should make sense. Else there is little reason why you wouldn’t use geom_line. This is the case for the French population dataset. In pop_fra we have a dataset which includes the number of people aged 0-14, 15-64 and 65 and older. The sum of these three population groups equals the total population in France. geom_area() plots these three series and stacks their values. Doing so, it creates three new series: one with the number of people ages 0-14, a second with the number of people ages 0-65 (the sum of those ages 0-14 and those ages 15-64) and a third series with the sum across age categories. To show these series, it fills the area below the first series and the x-axis, between the first and second series and between the second and last series with a color. Doing so, the size of the area shows the size of each subgroup.\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the graph makes sense: it shows the evolution of the total population in France as the sum of three components: those ages 0-14, 15-64 and 65 and over. The size of each cohort is shown on the vertical axis. However, there is one issue. R shows the order of the categories starting with the largest: NPAN (15-64), then the young (or children) NPCN and then the “old” or NPON. Dhis is standard as R orders to values to stack first from low to high. This order doesn’t make a lot of sense as these categories imply an order and the graph should preferably show the young at the bottom, those aged 15-64 in the middle and those aged 65 and over at the top. There are a couple of ways to deal with this. The first is to order the values in age_group (an ordered factor). Here, we will first order the ameco1 dataset and then filter the data for France. Doing so, you can create an area graph for other countries by changing the filter:\n\nameco1$age_group &lt;- factor(ameco1$age_group, \n                           levels = c(\"NPTN\", \"NPON\", \"NPAN\", \"NPCN\"), \n                           ordered = TRUE)\n\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s now see the result of the plot:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area()+ \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that R ordered the area’s in line with the age categories. It also shows a different color. This is due to the fact that R chooses another color scale for ordered factors as apposed to unordered factors. The yellow at the bottom is lighter than the dark blue at the top: in other words, the hue implies an order: lighter colors for smaller values (in this cases ordered per age group).\nThere are other ways to change the order. For see how they work let’s create a geom_area() but now map the variable population on the fill aesthetic:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the legend, you can see that the plot shows to order from young to old. This is due to the fact that the order of the categories in population in alphabetical order happens to put the category with the youngest first and with the oldest last. If you want to reverse the order, you can add the option position = position_stack(reverse = TRUE):\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe problem is not fully solved: the plot is fine, but the legend isn’t. Here, you need to add scales_fill_manual and set the legend labels manually (see Chapter 11):\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  scale_fill_discrete(breaks=c(\"Population: 65 years and over\", \"Population: 15 to 64 years\", \"Population: 0 to 14 years\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we used geom_line() we kept the default position = \"identity\". Given how close geom_area() and geom_line() are connected, it shouldn’t come as a complete surprise that you can add position = \"stack\" as an argument in geom_line(). Doing so, shows the stacked lines:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, color = age_group)) +\n  geom_line(position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph is a bit misleading: a lot of people will interpret this graph to mean that the largest population group in France are the oldest and that there are hardly any young people, as they will not immediately notice that this is a stacked line chart, not one with position = identity. This the main reason why you should use an area chart: this because it fills the areas, it also suggests that categories are stacked. Note that you can also combine geom_area() and geom_line() with stacked lines; Doing so, allows you to add a line between the areas in the area chart. If you add some transparency to the areas, these lines will stand out:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop)) +\n  geom_area(aes(fill = age_group), alpha = 0.20) +\n  geom_line(aes(color = age_group), position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.1.2.3.2 geom_ribbon()\ngeom_ribbon() is a special case of geom_area() It allows you to plot a “ribbon” defined by minimum and maximum values for the variable mapped on the vertical axis. The required aesthetics for this geometry are the variables mapped on the x-axis and those on the ymin and ymax aesthetics. The latter two define the boundaries of the ribbon. To illustrate, we’ll use the diamonds dataset, filter diamonds with carat &lt;= 2.5, calculate the standard deviation and mean price per level of carat and show the result using a ribbon where the maximum value is defined as the mean price plus 1.96 times the standard deviation and the minimum value is defined as the mean price minus 1.96 times the standard deviation. Let’s first create the summary data frame:\n\ndiamonds_sum &lt;- diamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n  group_by(carat) |&gt; \n  summarize(\n    min_price = min(price), \n    max_price = max(price), \n    mean_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))\n\nUsing geom_ribbon() and accepting all default values for the settings:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis ribbon shows the carat on the horizontal axis. The vertical axis shows two variables: the mean price + 1.96 the standard deviation of the price (per carat group) and the mean prices - 1.96 times the standard deviation (per carat group). Using e.g. fill you can change the color of the ribbon, using alpha the transparency of the ribbon and using the setting color you can change the color of the lines that show the minimum and maximum values. In addition, you can set the line width and line type, For instance:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightblue\", color = \"blue\", linewidth = 0.50, linetype = \"solid\", alpha = 0.20) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you want to also the mean values, you add a line geometry:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightgrey\", color = \"darkgrey\", alpha = 0.50) +\n  geom_line(aes(y = mean_price), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_ribbon() is a special case of geom_area(). To see this, let’s change the minimum value in the previous graph to ymin = 0 and the maximum value, ymax = mean_price:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = 0, \n    ymax = mean_price)) +\n  geom_ribbon(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is equal the the one you would have gotten with geom_area():\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    y = mean_price)) +\n  geom_area(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause of these similarities, it shouldn’t come as a surprise that the arguments for geom_ribbon() and geom_area() are very similar:\n\ngeom_ribbon(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"both\"\n)\n\nWith the exception of the default that are specific to geom_area() - stat and position - all others are very similar.\n\n\n\n10.1.2.4 Bar or column geometries\nThere are two bar geometries: geom_bar() and geom_col(). By default, first shows the number of observations in each category of the variable that is mapped on the x-axis. The second allows you to show values for a variable mapped on the vertical axis, for every value of the variable mapped on the x-axis. geom_bar() has one required aesthetic: the x-axis. This variable is a discrete variable (nominal or ordinal)\n\n10.1.2.4.1 geom_bar()\nFor geom_bar() the arguments are\n\ngeom_bar(\n  mapping = NULL,\n  data = NULL,\n  stat = \"count\",\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nHere the stat argument has the value \"count\". In other words geom_bar() by default shows the total number of observations on the vertical axis. This also means that there is only one required aesthetic: the variable for whose values the count will happen. The alternative, \"identity\" shows the values of the variable mapped on the y-axis. Before moving on to the position and stat argument, a short word about some of the others. The argument just positions the bar over the major grid line for the value on the x-axis. The default value centers the bars over that grid line. Alternative values are 0 or 1 to align left or right. The width argument - which is be default 0.90 - measures how much space the bars will take up in the plot. By default this is 90% of the total area. If you reduce this to e.g. 0.75, the sum of the area or the bars will take up 75% of the total panel area.\nThe position argument with default \"stack\" matters in case the aesthetics include a mapping on e.g. fill or color. Mapping a variable on the fill aesthetic will show the number of observations for each variable on the horizontal axis, but will differentiate between various values of the variable mapped on the fill aesthetic using different colors. The color aesthetic shows similar values, but does so using a different color for the line between various subgroups. The position argument as two alternatives: \"fill\" and \"dodge\" or \"dodge2\". The first rescales the vertical axis from 0 to 100% and shows the various subcategories for each category on the x-axis as a percent of total values for the x-category. The second, \"dodge\" or \"dodge2\" shows various subgroups next to each other and not stacked.\nTo illustrate, we’ll use the diamonds dataset. To show the number of observations per level of cut, we map the cut variable on the x-axis and accept all defaults. Recall that geom_bar() will plot the count on the vertical axis. In other words, one aesthetic mapping, x = cut, is sufficient:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see from the plot, geom_bar() adds the label “count” to the vertical axis. As cut is an ordered factor, the values on the x-axis are shown in that order. As usual, you can change the setting (i.e. layout options) using e.g. the fill (fill or the bar), color (color of the line around a bar), … by specifying those in geom_bar():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar(fill = \"lightblue\", color = \"blue\", linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to now the number of observations for each level cut-clarity combination. To do so, you have to map the variable clarity on an aesthetic. You can use the fill aesthetic. Doing so, geom_bar() will show the number of observations per level of and will do so using different colors to fill the bars, where each color show one value of clarity:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are other aesthetics that you can use to map other discrete variable, e.g. color or line width. The color shows the various subcategories using a different color for a line:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, color = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, except when you set the color of the bars to white, the different lines reveal little on their own. The same holds for the other aesthetic mappings. In other words, the preferred aesthetic to other variables is fill.\nBy default, geom_bar() stacks (position = stack) the various values of the variable mapped on the fill aesthetic. To show them next to one another, you add position = \"dodge\" or position = \"dodge2\":\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every level of cut, geom_bar() now shows 8 bars: one per level of clarity. If you use position = \"dodge2\", you can add further details using\n\nposition_dodge2(\n  width = NULL,\n  preserve = \"total\",\n  padding = 0.1,\n  reverse = FALSE\n)\n\nIf you use position = \"dodge\", you can specify the width and preserve. The first, width is relevant is you have different geometries with different width, for instance, point geometry and a bar geometry. The second preserve is relevant in case not all values of the variable mapped on the x-axis have the same number of subcategories for the variable mapped on the fill aesthetic. By default, R preserves the total width of the of all bars for each value on the horizontal axis. The alternative, \"single\" preserves the width of the subcategories. To illustrate, here are two examples taken from Chang (2025):\n\n\n\n\n\nFigure 10.8: the preserve argument\n\n\n\n\n\n\n\n\nThe padding argument in position_dodge2() allows to to specify the distance between two bars. The default value is 0.1. If you increase that value, the distance between two bars at the same x position widens:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = 0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA negative value causes overlap:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe argument reverse = FALSE keeps the order of the subcategories. Changing this into TRUE, reverses the order of the subcategories:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing position = fill, geom_bar() will show proportions. For every value of the variable mapped on the horizontal axis, the total is set equal to 100. The subcategories are then shown in a percentage on the number of observations for each of the subcategories in relation to the total number of observations for their category shown on the x-axis:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#####geom_count()\ngeom_count() allows you to show values for the variable mapped on the vertical axis for every value of the variable mapped on the horizontal axis. This geometry needs at least two aesthetics: a discrete variable to map on the x-axis and the variable to map on the y-axis. This geometry allows you to show, e.g. the average for a variable (e.g. price), for every value of a discrete variable (e.g. cut). Here, we’ll illustrate this geometry to visualize summary data.\nThe argument of this function are very similar to the arguments for the geom_bar() geometry:\n\ngeom_col(\n  mapping = NULL,\n  data = NULL,\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nSuppose you want to visualize the average price per cut. Using {dplyr}’s group_by() and summarize() functions, you can calculate these averages using diamonds |&gt; group_by(cut) |&gt; summarize(ave_price = mean(price, na.rm = TRUE)) (see Chapter 8). Because these function return a data frame, we can connect their output in a pipe with ggplot(). In the next code, the first two lines calculate the average price per cut. We map the cut, the discrete variable, on the horizontal axis and the average price on the vertical axis. Note that the names of these variables are found in the tibble returned by the summarize() function. This data frame is use by ggplot() to find the variables. Here, we accept all default values.\n\ndiamonds |&gt; group_by(cut) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every value of cut, the variable mapped on the horizontal axis, geom_col() shows the variable ave_price, the mean of the diamond’s prices for each category of cut on the vertical axis. Note that here ggplot(data, aes() ...) uses the data frame returned by {dplyr}’s functions as the data.\nUsing the other aesthetics, you can add other categories. As was the case with geom_bar(), usually only the fill aesthetic is used. Other aesthetics are not visually appealing to differentiate across categories. Here, we’ll show the average price for every value of clarity and cut. Doing so, we first need to calculate these values. Again, we do so using group_by() and summarize(). We map cut on the horizontal axis van use the fill aesthetic to map the variable clarity\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nNote that the output doesn’t make sense. The previous graph showed that the average price for fair cut diamonds was a little over 4000. Here, the average price is over 30000. This result is due to the fact that geom_col() by default stacks the values. In this case, this is not how it should be done. Stacking values per category is only relevant is the sum of these values is relevant. Here, this is not the case. Changing the position to dodge2 shows the various averages next to each other, grouped by level of cut. The position dodge or dodge2 can be modified in the same way as in geom_bar().\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.4.2 Using geom_bar() as geom_col() and vice versa\nBoth geometries are closely related. In the arguments for geom_bar(), there was stat = \"count\". Changing this into stat = \"identity\" tells this geometry to plot values, not counts. Using this value, you can create a bar chart using geom_bar() that is identical to the one created with geom_col(). To see this, let’s use geom_bar() in stead of geom_col() in the previous plot and use the stat = \"identity\" argument:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_bar(stat = \"identity\", position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nBecause we used stat = \"identity\", geom_bar() now plots the values for the variable mapped on the vertical axis as values that that axis. In other words, using this argument, you can often use geom_bar() as geom_col().\ngeom_bar() shows counts. However, you can also use geom_col() to show counts. Let’s use this graph created with geom_bar()\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo create the same chart in geom_col() we need a data frame that includes the number of observations but value for cut. Using summarize(n = n()), this is what we can do. Using the result of these function in ggplot() with geom_col() shows a geom_bar() type of output:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.1.2.4.3 Coordinate flip\nWe already touched upon coordinates. There, I illustrated coordinate flip or coord_flip(). For bar and column charts, the effect is that variable mapped on the horizontal axis is shown on the vertical axis and the values shown on the vertical axis are measured along the horizontal axis. Note that you don’t have to change the aesthetics mapping. If is sufficient to add coord_flip():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou would have had the same result if you mapped the discrete variable on the vertical axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, the advantage of coord_flip() is that is doesn’t change e.g. values for scales. As the discrete variable is mapped on the x-axis and is only shown on the y-axis, scale_x_discrete() … refer to the right variable. You can also flip coordinates with geom_col() and with more than one category where you reverse the order of the subcategories:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  coord_flip() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.4.4 Adding text\nSometimes it is useful to add values as text to a bar or column chart. Recall that you can add text using, e.g. geom_text() or geom_label(). Adding this text layer to a bar or column plot allows you to add e.g. the values they represent. Here, the values are added in white to a bar chart in blue. To do so, add geom_text() and map the sum n to the label aesthetic. This geometry inherits the x- and y- mappings. In other words, geom_tex() knows the values on the x-asis and the height along the y-axis. The values are nudged down 200 units. As the units on the vertical axis are measured from 0 to over 20000, a 200 nudge down brings these values within the bars. This might require a bit of experimenting to get right. Adding a positive nudge would add them to the top.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col(fill = \"lightsteelblue\") +\n  geom_text(aes(label = n), nudge_y = -200, color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI refer to the section on point geometries for further possibilities with geom_text() and similar geometries.\n\n\n\n10.1.2.5 Geometries for distributions\nThere are multiple ways to show the distribution of a variable. geom_histrogram() and geom_density() visualize the full distribution. Using geom_boxplot() or geom_violin() you can show the distribution, using summary statistics such as the mean, various quantiles and percentiles, minimum and maximum. For two continuous variables, you can use geom_bin2d(). geom_density_2d() or geom_density_2d_filled(). Here we will not go into detail on all of these geometries. We’ll focus on those for one variable and show, as an illustration, how you can extend them to two variables.\n\n10.1.2.5.1 geom_histrogram() and geom_density()\ngeom_histrogram() divides the full range of possible values for a variable into bins. For every bin, geom_histrogram() shows the number of observation. Recall that is is also what geom_bar() did. In other words, you would be able to use geom_bar() or geom_col() to generate this plot. The arguments of geom_histrogram() are:\n\ngeom_histogram(\n  mapping = NULL,\n  data = NULL,\n  stat = \"bin\",\n  position = \"stack\",\n  ...,\n  binwidth = NULL,\n  bins = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should be familiar. The binwidth = allows you to specify the width of a bin, e.g. 50 or 25. bins allows you to set the number of bins. By default this value of 30. There is one other way to change the bins and that is to define them using breaks. Using this argument, you can include a vector with bins or use, e.g. seq() to generate the bins. There is only one required aesthetic: the variable to map on the x-axis. To illustrate this function, we’ll visualize the distribution for price in the diamonds dataset. Accepting all defaults:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou can change the settings of the plot in the usual way: change the fill, change the color of the lines, … . Experimenting with the number of bins is often a good idea. Doing so, you can see what number is best for the data. Let’s change the number of bins in three ways:\n\nincrease the number of bins from 30 to 50:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nset the bin width to 250:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 250) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nuse seq() to set bins every 500:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(breaks = seq(from = 0, to = 20000, by = 500)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you map another variable on e.g. the aesthetic fill, geom_historgram() will show the count for each level of cut in every bin using different colors. As with the bar geometries, other aesthetics are much less suitable to use as aesthetic mapping. Using the fill aesthetic to map cut, the histogram shows all observations for each level of cut within each bin:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFor two variables, geom_hex() extends geom_histogram(). Both variables are divided into bins (default 30) and the graph shows the number of observations per crossed bin. The function requires a mapping on the x-axis and one on the y-axis. To illustrate, I’ll use a data frame with random draws from a bivariate normal distribution\n\nmat1 &lt;- matrix(rnorm(2000, 0, 1), nrow = 1000, ncol = 2)\ncov &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2, ncol = 2)\nmat1 &lt;- mat1 %*% cov\ndf_mat1 &lt;- as.data.frame(mat1)\ndf_mat1 &lt;- df_mat1 |&gt; rename(var1 = V1, var2 = V2)\n\nWith 50 bins, this is how the histograms of these two variables, var1 and var2 look like:\n\nplothist1 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var1)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist2 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var2)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist1 + plothist2\n\n\n\n\n\n\n\n\nUsing geom_hex() you plot both in one plot. Here, for every of 2500 bins (50 for var1 and 50 for var1) this plot shows the number observations in each and every one of these bins:\n\ndf_mat1 |&gt; \n  ggplot(aes(x = var1, y = var2)) +\n  geom_hex(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere color scale here, shows a higher number of observations with a lighter blue color. Consistent with the histograms for the individual series var1 and var2, geom_hex() show higher counts in bins closer to 0.\ngeom_histrogram() shows the count by default. You can change that in a density (i.e. probability) using an after_stat() value for the stat argument. Using after_state(\"density\"), geom_histrogram() maps these estimated probabilities on the y-axis. With 50 bins, this generates this plot:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo show a density, you can also use geom_density(). The arguments of this geometry are:\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\nThe stats = \"density\" by default calculates densities. Changing this into “count” generate a geom_histromgram(). geom_density() uses a by default a Gaussian kernel density estimator to smooth the estimates of the probabilities. For instance, the density estimate for the price of diamonds:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing the full aesthetic, you can show density plots for every value of the variable mapped on that aesthetic. For instance, to plot these price density for various values of cut, you map the latter on the fill aesthetic:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHer problem here is that the density plot for e.g. “Ideal” with the one for e.g. “Fair”. To show all density plots, you can add some transparency using the alpha setting. If, in addition, you map cut also on the color aesthetic, geom_density() will add lines at the top of each density:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut, color = cut)) +\n  geom_density(alpha = 1/10, linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo plot a density with 2 variables, you can either use geom_density2d() or geom_density2d_filled(). The first shows the bivariate density using contour plots, the second using filled contour plots. To illustrate, we’ll use the random data in df_mat1. The individual densities look like this:\n\nplotmat1 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var1)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat2 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var2)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat1 + plotmat2 \n\n\n\n\n\n\n\n\nUsing geom_density2d(), the result is\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe contour lines reveal higher probabilities the more they are circled with other countour lines. Here, consistent with the univariate densities, the probability that you’ll see a pair with values for var1 and var2 increases as the values for both these variables close in on 0. With geom_density2d_filled() the plot is filled:\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d_filled() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the color reveals information on the probabilities. For this plot, value pairs with a high probability are drawn in yellow. Ast he color scale moves from yellow to blue, the probability of a value pair in that range falls.\n\n\n10.1.2.5.2 geom_boxplot() and geom_violin()\nA boxplot shows the distribution of a variable using a box, which shows the 1st and 3rd quartile as its outer ranges, a whisker up and down, both with a length of 1.50 times the interquartile range (or the value of the 3rd minus the value of the 1st quartile). In addition, the box shows the median. You can adapt the way in which geom_boxplot() shows the outliers. Outliers are values outside of the range of the boxplot. To do so, you need to change the arguments of the function:\n\ngeom_boxplot(\n  mapping = NULL,\n  data = NULL,\n  stat = \"boxplot\",\n  position = \"dodge2\",\n  ...,\n  outliers = TRUE,\n  outlier.colour = NULL,\n  outlier.color = NULL,\n  outlier.fill = NULL,\n  outlier.shape = 19,\n  outlier.size = 1.5,\n  outlier.stroke = 0.5,\n  outlier.alpha = NULL,\n  notch = FALSE,\n  notchwidth = 0.5,\n  staplewidth = 0,\n  varwidth = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe stat = \"boxplot\" defines the default values. If you want to change these, you need to add, e.g. coef = 1.75 to extend the whiskers to 1.75 the IQR. The treatment of outliers is governed by the arguments starting with outlier. The first, outliers = TRUE by default shows the outliers. Changing this into FALSE hides the outliers. The axis of the plot will be adjusted accordingly, unless you add oulier.shape = NA. If outliers are shown, you can change their color, fill, shape, size, stroke and alpha setting. Here, I refer to the point geometries to see how these affect the plot. notch = FALSE makes a traditional boxplot. Setting this to TRUE, R will show a notched boxplot, where the notches allow you to compare the medians across groups. You can set the width of the notch using notchwidth, by default this value equals 0.50, i.e. the notch covers half of the box width. The staples mark the end of the whiskers. The staple width, which is by default 0, allows you to specify the width of these staples. The last argument, varwidth = FALSE allows you to change the width of the box. By default, this is not the case. If changed to TRUE, the width of the box will be proportional to the square root of observations.\nLet’s first draw a boxplot for one variable, price. We’ll map that variable on the y-axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the x-asis in these case has no real meaning, so you can actually leave it blank. Let’s add a notch with width 0.25, increase the length of the whiskers to 2 times the IQR, change the way outliers are shown (color = red) and add a staple at the end of the whisker. Note that you use additional settings to change, e.g. the color of the lines, the fill of the box, …\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot(\n    coef = 2, \n    notch = TRUE, \n    notchwidth = 0.25,\n    outlier.color = \"red\",\n    staplewidth = 0.10) +\n  theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nTo compare distributions across values of another variable, we need to map this variable on the x-aesthetic. If you map the same variable on the fill aesthetic, the color of the boxes will change with these values the plot will also show a legend. As such, it is not necessary as the box plot will show the values on of the variables mapped on the x-asis as label. However, doing so, you can show a legend and remove the x-axis. To start, let’s map cut on the x-axis and price on the y-axis. With the exception of notch, which we’ll set to TRUE, all default valules apply:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_boxplot() now shows one boxplot per value of cut. The notches suggest that here is a significant difference in the median value between the price for “Ideal” cut diamonds and the price for “Premium” cut diamonds. Whether this is also the case for the values “Fair” and “Good” is not immediately visible as the notches seem to overlap. Using varwidth = TRUE we can adjust the width of the box using the square root of the number of observations as criterium. To do this, you set varwidth = TRUE:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE, varwidth = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe width of each box is now proportional to the square root of the number of observations. With 1610 observations in “Fair” and 21551 in “Ideal”:\n\ndiamonds |&gt; group_by(cut) |&gt; summarize(n = n()) |&gt; mutate(sqrtn = round(sqrt(n), 2))\n\n# A tibble: 5 × 3\n  cut           n sqrtn\n  &lt;ord&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Fair       1610  40.1\n2 Good       4906  70.0\n3 Very Good 12082 110. \n4 Premium   13791 117. \n5 Ideal     21551 147. \n\n\nthe width of the “Ideal” box is\n\\[\n\\frac{\\sqrt{21551}}{\\sqrt{1610}} = 3.65\n\\] 3.65 times wider then the width of the “Fair” box.\nTo illustrate these we use of the aesthetic fill to map cut and some other setting for the outliers, let’s use the fill aesthetic to map cut and add staples with width equal to 25% of the box width:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_boxplot(\n    notch = TRUE, \n    varwidth = TRUE, \n    staplewidth = 0.25, \n    outlier.colour = \"red\", \n    outlier.alpha = 1/5) +\n   theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nViolin plots are closely related to boxplots, but reveal fore information about the distribution of the variable. They are meant to show the distribution of one continuous variable per value of a discrete variable. The arguments in geom_violin() should be largely familiar. The argument draw_quantiles = FALSE allows you to add horizontal lines for every quantile. The scale argument determines the area of the violin. By default, this area is equal across violins. Changing this default value into “count” will creates violins where areas are scaled proportionally to the number of observations while “width” keeps the width of the violins equal across groups:\n\ngeom_violin(\n  mapping = NULL,\n  data = NULL,\n  stat = \"ydensity\",\n  position = \"dodge\",\n  ...,\n  draw_quantiles = NULL,\n  trim = TRUE,\n  bounds = c(-Inf, Inf),\n  scale = \"area\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nLet’s use a violin plot to show the distribution of price for every value of cut. To do so, we map the variable price on the vertical axis and cut on the horizontal axis:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe violins for every value of cut show you where the observations are. The very thin upper part shows that there are almost no observations for high price levels. At the lower part of the price levels, there seems to be more spread for “Fair” cut diamonds that for e.g. “Ideal” cut diamonds. For the latter, the distribution is very wide at the lowest part of the price range, suggesting that here are a lot of observations for these price cut pairs. To see this more in detail, let’s add the a sample of the observations of the diamonds dataset using a point geometry with a bit of jitter and transparency:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  geom_point(\n    data = diamonds |&gt; slice_sample(prop = 0.10),\n    aes(x = cut, y = price), \n    position = position_jitter(width = 0.20, height = 0.20), alpha = 1/5, \n    color = \"red\", \n    size = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that indeed, there are a lot of observations in the lower price range for “Ideal” cut diamonds.\nAs is the case with geom_histogram() you can map the same variable on both the x-axis and e.g. fill aesthetic. Doing so, geom_violin() will fill the violins and the color to the legend. If you want to add quantiles, e.g. the 10th and 90th percentile and the first and third quartile, you can specify those using draw_quartiles = c(0.10, 0.25, 0.75, 0.90). Here, we use the color white to draw these lines. By default R uses black and this color wouln’t show up in the “Fair” cut category:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_violin(draw_quantiles = c(0.10, 0.25, 0.75, 0.90), color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n10.1.2.6 Other geometries\n{ggplot2} includes many other geometries. However, you can create most of these graphs with the geometries that we have covered here. To illustrate, geom_linerange(), geom_pointrange() and geom_errorbar() allow you to draw a line in a specific range, a line with a range including a dot in the middle, or a line with a bar at both ends. These geometries allow you to e.g. show the range of values in a dataset. Here, you can define the range using the mean and standard deviation, median and interquartile distance, … . However, you can recreate all three geometries using geom_segment() and geom_point(). To illustrate, we’ll show the price range for various categories of clarity in the diamonds using each of the two range geometries and the errorbar geometry was well as the segment and point geometry. The range if always defined as one standard deviation below and above the mean.\n\ngeom_linerange()\n\n\nplotlinerange1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))|&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_linerange(aes(ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n  labs(\n    title = \"geom_linerange()\"\n  ) +\n  theme_minimal()\n\nplotlinerange2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))|&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n  labs(\n    title = \"geom_segment()\"\n  ) +\n  theme_minimal()\n\nplotlinerange1 + plotlinerange2\n\n\n\n\n\n\n\n\n\ngeom_pointrange()\n\n\nplotpointrange1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity)) +\n  geom_pointrange(aes(y = ave_price, ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 1, size = 2) +\n  coord_flip() +\n    labs(\n    title = \"geom_pointrange()\"\n  ) +\n  theme_minimal()\n\nplotpointrange2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))|&gt;\n  ggplot(aes(x = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 1, show.legend = FALSE) +\n  geom_point(aes(y = ave_price, color = clarity), size = 8, shape = 16) +\n  coord_flip() +\n    labs(\n    title = \"geom_segment and geom_point()\"\n  ) +\n  theme_minimal()\n\nplotpointrange1 + plotpointrange2\n\n\n\n\n\n\n\n\n\ngeom_errorbar()\n\n\nploterror1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))|&gt;\n  ggplot(aes(x = clarity)) +\n  geom_errorbar(aes(ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n    labs(\n    title = \"geom_errorbar\"\n  ) +\n  theme_minimal()\n\nploterror2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))|&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 2) +\n  geom_point(aes(y = ave_price - sd_price, color = clarity), shape = \"\\uFE31\", size = 10, stroke = 2, show.legend = FALSE) +\n  geom_point(aes(y = ave_price + sd_price, color = clarity), shape = \"\\uFE31\", size = 10, stroke = 2, show.legend = FALSE) +\n  coord_flip() +\n    labs(\n    title = \"geom_segment and geom_point()\"\n  ) +\n  theme_minimal()\n\nploterror1 + ploterror2\n\n\n\n\n\n\n\n\nHere, I used a unicode character “uFE31” to end each line segment.\nIn addition there are many other packages that were designed with specific application in mind. For instance using {[ggwordloud] (https://lepennec.github.io/ggwordcloud/index.html)} (Le Pennec and Slowikowski (2024)) you can build word clouds showing which words occurred most in a text by increase the size of the font, {[treemapify] (https://wilkox.org/treemapify/index.html)} makes it easier to create treemaps and show, e.g. the relative importance product or market in total sales or exports, {[ggradar] (https://github.com/ricardo-bion/ggradar)} is very useful to design radar charts to compare how various products, countries for firms score on a fixed set of characteristics or{[ggbump] (https://github.com/davidsjoberg/ggbump)} can be used to create bump charts to visualize the change in ranking over time.\n\n\n\n10.1.3 Annotations\nUsing geom_text(), geom_label(), geom_hline(), geom_vline(), geom_abline(), geom_segment() or geom_curve() you can add visual annotations to a graph: a label with the values for a bar chart, a horizontal or vertical line showing a specific data or an average, … . Using annotations() there is another way to add additional information to a plot. The main arguments of the annotation() function are:\n\nannotate(\n  geom,\n  x = NULL,\n  y = NULL,\n  xmin = NULL,\n  xmax = NULL,\n  ymin = NULL,\n  ymax = NULL,\n  xend = NULL,\n  yend = NULL,\n  ...,\n  na.rm = FALSE\n)\n\nThe first argument geom refers to the geometry to use e.g. text, rectangle, line segment, point range:\n\n“point for a point: x, y\n“text” for a text annotation: x, y, label\n“rect” for a rectangle: xmin, xmax, ymin, ymax\n“segment” to add a line segment: x, y, xend, yend\n“pointrange” to add a point range: x, y, ymin, ymax\n\nEvery geometry needs a minimum of values: a point needs a position (x, y), a text needs a position, defined by the (xmin, ymin) coordinate and a text to add (label), a rectangle needs to four corners, a line segment needs a start and end position, both defined by (xmin, ymin) and (xmax, ymax) and a pointrange - a vertical line with a midpoint - needs an x value, a value where the line starts and ends (ymin and ymax) as well the position where to add a point. For all these geometries, you can add further settings in line with that geometry. For instance, for a text, you can add a font family, fontface, size or the horizontal and vertical alignment (0 = left/top, 1 = right/bottom, 0.5 = center), annotations that include lines, you can add setting for line types or width and a rectangle can be filled. To illustrate these annotations, we’ll use the vacancy rate in the Beverdige dataset. If you haven’t imported it yet, you can do so here:\n\ndata_beveridge &lt;- read_csv(here::here(\"data\", \"raw\", \"data_beveridge.csv\")) \ndata_beveridge &lt;- data_beveridge |&gt; rename(date = DATE, quarter = `TIME PERIOD`)\n\nHere, we’ll map the date on the horizontal axis and the vacancy rate on the vertical axis and use a simple line geometry to show the data. In addition, we use annotate() to add 3 rectangles: one to highlight the financial crisis, one to highlight the Euro area debt crisis and one to highlight the pandemic. We’ll fill these reactangles with a different color. In addition, we’ll add a text annotation to include the reference to the crisis, aligh this text left of the rectangle and use the font family “serif”. To set the limits for the rectangle, we’ll first set the maximum value for the vacancy rate. The full code for this plot is:\n\nym &lt;- max(data_beveridge$vacancy_rate)\n\ndata_beveridge |&gt; select(date, quarter, vacancy_rate) |&gt;\n  ggplot(aes(x = date, y = vacancy_rate)) +\n  geom_line() +\n  annotate(\"rect\", \n           xmin = as.Date(\"2008-03-31\"), \n           xmax = as.Date(\"2009-08-31\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"lightgrey\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2008-03-31\"),\n           y = ym - 0.5, \n           label = \"Financial crisis\", \n           color = \"darkgrey\", \n           hjust = 0, \n           family = \"serif\") +\n  annotate(\"rect\", \n           xmin = as.Date(\"2011-06-30\"), \n           xmax = as.Date(\"2013-03-31\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"lightsteelblue1\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2011-06-30\"),\n           y = ym - 0.5, \n           label = \"Euro area debt crisis\", \n           color = \"steelblue\", \n           hjust = 0, \n           family = \"serif\") +\n  annotate(\"rect\", \n           xmin = as.Date(\"2020-01-31\"), \n           xmax = as.Date(\"2021-09-30\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"sienna2\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2020-03-31\"),\n           y = ym - 0.5, \n           label = \"Covid pandemic\", \n           color = \"sienna4\", \n           hjust = 0,\n           family = \"serif\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing annotations can add value to a plot. Recall for instance that we use such as annotation to highlight “Premium” cut diamonds in a point geometry.\n\n\n\n\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.\n\n\nLe Pennec, Erwan, and Kamil Slowikowski. 2024. Ggwordcloud: A Word Cloud Geom for ’Ggplot2’. https://github.com/lepennec/ggwordcloud.\n\n\nSlowikowski, Kamil. 2024. Ggrepel: Automatically Position Non-Overlapping Text Labels with ’Ggplot2’.\n\n\nYutani, Hiroaki. 2024. Gghighlight: Highlight Lines and Points in ’Ggplot2’. https://yutannihilation.github.io/gghighlight/.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html#scales",
    "href": "11_Changing_plot_layout.html#scales",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "",
    "text": "11.1.1 Continuous scales\nWe first start with numeric scales. Usually, these are for the x- and y-axis. The scales for the horizontal and vertical axis are position scales: they show a individual value will be mapped on the axis. Other aesthetics that include continuous values are e.g. color, fill or size. For color or fill, you have diverging scales, where colors change from e.g. green to red or from blue to yellow and sequential scales (various shades of e.g. green). We first start with the continuous numeric scales, we then move on to the other scales. We’ll discuss the first scale in detail. For other scales, we’ll only highlight arguments that are specific for these scales.\n\n11.1.1.1 Position scales: scale_x_continuous() and scale_y_continuous()\nPosition scales show the position of a value on the horizontal and vertical axis. In other words, they determine where you have to look to find the values and how these values will be shown. In {ggplot2} scale_x_continuous and scale_y_continuous are function to set the characteristics of the scales of the x- and y-axis. The reference to continuous implies that these scales are used for continuous data. Because both these function are identical, we will discuss only one, scale_x_continuous(). The arguments of this function - i.e. the scale parameters that you can adjust - are\n\nscale_x_continuous(\n  name = waiver(),\n  breaks = waiver(),\n  minor_breaks = waiver(),\n  n.breaks = NULL,\n  labels = waiver(),\n  limits = NULL,\n  expand = waiver(),\n  oob = censor,\n  na.value = NA_real_,\n  transform = \"identity\",\n  trans = deprecated(),\n  guide = waiver(),\n  position = \"bottom\",\n  sec.axis = waiver()\n)\n\nWe’ll use Figure 11.1 to illustrate these arguments.\n\n\n\n\n\nFigure 11.1: Scales\n\n\n\n\n\n\n\n\n\n11.1.1.1.1 The title of the axis:\nBy default, the name of the scale is taken from the aesthetic. For scale_x_continuous() that is the variable mapped on the x-asis. If you add a name = your_name, this name will be shown as the title on the axis. You can set these names also using the labs() function. Using labs(x = \"name of x\") for instance will use “name of x” as the title for the x-axis. Is you set this to NULL in scale_x_continuous, the name will dropped, even if you specify a name in the labs(x= \"name of x\") function. You can drop these titles via labs() using labs(x = \"\"). In Figure 11.1, for scale_x_continious() that means that “GDP per capita (USD, log scale)” would not be shown on the plot. For scale_y_continuous() setting names = NULL would drop the name (the name of the variable or the name given using labs() from the y-axis). We’ll meet this function for other scale_*_x() functions as well. For most of these, the variables mapped on the * scale (e.g. color, fill or size) are shown in the legend. Setting name = \"name of aesthetic\" add the “name of aesthetic” to the legend. Using name = NULL drops the title from the legend, but not the legend itself. In Figure 11.1, the names for the color and size aesthetic are “Region” and “Population”. Here too, you can set those using the labs() function e.g. labs(color = \"Region\", size = \"Population\") adds both titles to the legend. Using labs(color = \"\", size = \"\") would remove these titles.\n\n\n11.1.1.1.2 Breaks and labels\nThe breaks and minor_breaks sets the major and minor breaks for the axis. In ?fig-scale_x_cont, the major breaks are 1000, 10000 and 100000 on the x-axis and 50, 60, 70 and 80 on the y-axis. If you specify in the themes() function (see Chapter 11) that R needs to add major grid lines, R will do so in line with the breaks and will add minor grid lines in line with the minor breaks. In ?fig-scals_x_cont, the major grid lines are shown with dashed grey lines; the minor grid lines with dotted grey lines. The default values, for both are waiver() and allow R to determine both breaks from the data. By default, R adds one minor break point between each of the major breakpoints. Changing waiver() into NULL, will remove all major and/or minor break points. A third option allows you to set your own breakpoints using a vector such as c(1000, 10000, 100000) or a function that generates these breakpoints e.g. seq(from = x, to = y, by = z). The {scales} packages allows you to set the number of major breaks for the most often used cases:\n\nscales::breaks_extended(n = ...) with n the number of major breaks,\nscales::breaks_width(width, offset = 0) with width the width between major breaks and offset if you don’t want breaks to start at e.g. 0 but at e.g. -2 (offset = -2)\nscales::breaks_log(n = 5, base = 10) to set nice breaks for log-axis (as integer powers of base),\n\nFor minor breaks, {scales} includes minor_breaks_width() and minor_breaks_n(). Using scale_x_continuous(), you can set the number of breaks also using n.breaks =. However, this is only possible is breaks equals its default waiver().\nYou can add labels to major breaks. In Figure 11.1, these labels are “50”, “60”, “70,”80” on the vertical axis and “$1,000.00”, “$10.000,00” and “$100.000,00” on the horizontal axis. By default, ggplot() will add the labels from the data. With labels = NULL, you remove the labels. A third options is to add a vector with labels. As there is one label per major break, this vector should have the same length as the number of breaks. You can also add a function, e.g. paste0, paste, … . Here the function needs to output the same number of labels as there are major breaks. For instance using\n\npaste(\"$\", seq(from = 10000, to = 70000, by = 10000), sep = \"\")\n\n[1] \"$10000\" \"$20000\" \"$30000\" \"$40000\" \"$50000\" \"$60000\" \"$70000\"\n\n\nfor labels and\n\nseq(from = 10000, to = 70000, by = 10000)\n\n[1] 10000 20000 30000 40000 50000 60000 70000\n\n\nfor breaks with set major breaks at 10000, 20000, … 70000 and add labels $10000, $20000, … $70000.\nAs with breaks, {scales} includes a number of ways to deal with labels. This package includes, e.g. \n\nscales::label_number() to show numbers as numbers and not in scientific format. You can define the rounding (accuracy), scale factor to use for multiplication before the label is added (e.g. if you want to show values in 1,000,000 of e.g. the population of a country, use a scaling factor 1/1,000,000 will divide all values by 1,000,000 and show these as a label), a suffix or prefix, a decimal and thousands mark, details to show positive and negative values (e.g. with a + and -, a minus for negative values, nothing for positive, parenthesis of a unicode figure space or minus sign)\nscales::label_currency() generate labels include a currency prefix or suffix, including the option to set the decimal and thousand mark and a scaling factor that will be used to multiple the values before adding the label (e.g. with scaling factor x = 1/1000, values are in thousands of currency, e.g. the value €25.000 is shown as €25)\nscales::label_percent() with the optional scaling factor, which is by default 100 (if percentages in the dataset are 0.01, they will be multiplied with 100 to add as label), a prefix of suffix and decimal and thousands mark,\nscales::label_scientific() to show labels using scientific notation (e.g. 10e02, 5e-04),\nscales::label_ordinal() allowing you to add a suffix or prefix, e.g. 2nd, 3th with rules defined by default for ordinal_english() but optional also available for ordinal_french(gender = c(\"masculine\", \"feminine\"), plural = FALSE) and ordinal_spanish()\n\nBefore we continue with the other options, let’s see what these options do. To illustrate, we’ll use the diamonds dataset, filter the observations with carat &lt;= 3 and take a sample of 20% of these observations. Mapping price on the vertical axis and carat on the horizontal axis and using lightsteelblue as a setting the for color of the dots: the base graph looks like:\n\npl_scale_base &lt;- diamonds |&gt; filter(carat &lt;= 3) |&gt;  slice_sample(prop = 0.20) |&gt;\n  ggplot(aes(x = carat, y = price)) +\n  geom_point(color = \"lightsteelblue\") +\n  theme_minimal()\npl_scale_base\n\n\n\n\n\n\n\n\nWe will use both scale_x_continuous and scale_y_continuous to change the plot. Doing so, we can show multiple aspects at the time. Let’s change the number of breaks on the horizontal axis using a function show breaks starting at 0, ending at 3 but in steps of 0.5 and, using vector with 3 values and show breaks for the vertical axis using a vector and set them at 0, 7500 and 15000. We will also change the name of the vertical axis and add a reference to USD in the title.\n\npl_scale_base +\n  scale_x_continuous(breaks = seq(from = 0, to = 3, by  = 0.5)) +\n  scale_y_continuous(\n    name = \"price, in USD\",\n    breaks = c(0, 7500, 15000))\n\n\n\n\n\n\n\n\nNotice how ggplot() also adjusted the minor breaks and adds one in the middle of the major breaks. Let’s remove the minor breaks from the horizontal axis and add minor breaks every 1000 USD on the vertical axis, but only between 5000 and 15000. Starting from the base plot:\n\npl_scale_base +\n  scale_x_continuous(minor_breaks = NULL) +\n  scale_y_continuous(minor_breaks = seq(5000, 15000, by = 1000))\n\n\n\n\n\n\n\n\nUsing {scales} we can set the width of the breaks on the horizontal axis equal to 0.25 and set the number of breaks equal to 10 on the vertical axis:\n\npl_scale_base +\n  scale_x_continuous(breaks = scales::breaks_width(0.25)) +\n  scale_y_continuous(breaks = scales::breaks_extended(n = 10))\n\n\n\n\n\n\n\n\nTo keep the default number of major breaks for the x-axis but change the number of minor breaks to 15:\n\npl_scale_base +\n  scale_x_continuous(minor_breaks = scales::minor_breaks_n(n = 15))\n\n\n\n\n\n\n\n\nLet’s now turn to the labels and add “USD” to the labels on the vertical axis and “ct” to the labels on the horizontal axis. To so do, we will set the breaks to ensure that the number of labels equals the number of breaks:\n\npl_scale_base +\n  scale_x_continuous(\n    breaks = seq(0, 3, by = 1), \n    labels = paste(seq(0, 3, by = 1), \"ct\", sep = \"\")) +\n  scale_y_continuous(\n    breaks = c(0, 5000, 10000, 15000), \n    labels = paste0(\"$\", c(0, 5000, 10000, 15000)))\n\n\n\n\n\n\n\n\nLet’s now use {scales} and show the dollar data in euro data using a scale factor EUR = 1.10 USD, add a euro sign, show carat in mg (1ct = 200mg) and add mg. We’ll add a space for the thousands separator and a dot for the decimal mark on the vertical axis:\n\npl_scale_base +\n  scale_x_continuous(\n    labels = scales::label_number(\n      scale = 200,\n      suffix = \"mg\")) +\n  scale_y_continuous(\n    labels = scales::label_currency(\n      scale = 1/1.10,\n      prefix = \"€\",\n      big.mark = \" \",\n      decimal.mark = \".\"))\n\n\n\n\n\n\n\n\nNote that R shows does not adjust the values on the vertical axis. To do so, we need to adjust the breaks as well. Because we divide the labels, we multiply the breaks:\n\npl_scale_base +\n  scale_x_continuous(\n    labels = scales::label_number(\n      scale = 200,\n      suffix = \"mg\")) +\n  scale_y_continuous(\n    breaks = seq(0, 15000 * 1.10, by = 5000 * 1.10), \n    labels = scales::label_currency(\n      scale = 1/1.10,\n      prefix = \"€\",\n      big.mark = \" \",\n      decimal.mark = \".\"))\n\n\n\n\n\n\n\n\n\n\n11.1.1.1.3 Setting a ranges\nexpand = waiver() allows you to add some space between the data and the axis. By default, R expands the scale by 5% for continuous variables and 0.6 unit on each side for discrete variables. Using expansion(mult = 0, add = 0) you can change these values. If you supply a vector with two values for mult, R will add more space equal the the first component on the lower limit and equal to the second component of that vector to the upper limit. If you include one value, R adds space to equal to that value on both sides. Similarly, for add, adding two values in a vector allows you to add space on both sides, with one value, that value if used to add the same space on both sides. To illustrate, let’s add 1 unit of space on the lower end of the horizontal axis and 2 on the upper end. Here, one unit is 1 carat. We’ll expand the vertical axis with 15% on the lower end and 25% on the upper end:\n\npl_scale_base +\n  scale_x_continuous(expand = expansion(add = c(1, 2))) +\n  scale_y_continuous(expand = expansion(mult = c(0.15, 0.25)))\n\n\n\n\n\n\n\n\nThe arguments limits = NULL, oob = censor and na.value = NA_real_ deal with the value and the range of the values shown on the axis. Using limits you can set the range of values shown on a axis. By default (NULL), ggplot() shows all the values in the range. Adding a vector with limits, reduces the range of the axis to the limits in the vector. You can also specify a function that returns an upper and a lower value for the range. The question is what happens to the values outside of the range? This is what oob determines. By default, censor sets all the out of bounds (oob) values to NA. You can change that using {scales} to squish which replaces out of bounds values with their nearest range limit and keep which keeps the values. The way in which you handle out of bounds values in case you set limits on the range has important implications for other geoms. Using the default censor R sets these values to NA for the plot. R does so before the plot is drawn. Any other geometry will not be able to access these values. For instance, geom_smooth() will only be able to smooth data that is not missing, i.e. data that is within the limits of the range. Using keep avoids this. There is another alternative: you can set the limits in the coordinate component of the grammar. Doing so, keeps the values of the out or bounds values as they are. Setting limits in the coordinate component zooms in on the data. To see the difference, let’s set limits on the data range to 0.5 - 2.5 on the carat variable. We’ll do so withing the scale_x_continuous functions using limits = c(0.5, 2.5). We’ll also use the default value for the out of bounds values: censor and create a second plot with the keep option from {scales}. Third, we’ll set the range in the coordinate. Here, we use xlim = c(0.5, 2.5). The final plot uses all values in the dataset.\n\npl_scale_1 &lt;- pl_scale_base + scale_x_continuous(limits = c(0.5, 2.5), oob = scales::oob_censor) + geom_smooth() + labs(title = \"oob_censor\", x = \"\")\npl_scale_2 &lt;- pl_scale_base + scale_x_continuous(limits = c(0.5, 2.5), oob = scales::oob_keep) + geom_smooth() + labs(title = \"oob_keep\", x = \"\")\npl_scale_3 &lt;- pl_scale_base + coord_cartesian(xlim = c(0.5, 2.5)) + geom_smooth() + labs(title = \"xlim\")\npl_scale_4 &lt;- pl_scale_base + geom_smooth() + labs(title = \"full dataset\")\n\npl_scale_1 + pl_scale_2 + pl_scale_3 + pl_scale_4\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 3601 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3601 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere you can see the effect of oob = oob_censor. Relative to the options where R keeps all out of bounds values (oob_keep and xlim), oob_censor doesn’t use all data to estimate the smoothed line. The other two show a clear downpart part at the end. You can see that this downward part is part of the “true” smoothed line in the plot that uses the full dataset. In other words, using the limits argument in scale_x/y_continuous to limit the range of values on the x or y axis (or both) comes with a warning: you can do so, but only if the geometries do not use the data to calculate e.g. a smoothed line, a density, … . Here, these geometries will use a restricted dataset. If you do so, be explicit on the treatment of out of bounds values and always add either scales::oob_censor or scales::oob_keep. Or, set the ranges using the xlim or ylim arguments. You can do so both within but also outside of the coordinate argument. In other these are both equivalent\n\npl_scale_5 &lt;- pl_scale_base + coord_cartesian(xlim = c(0.5, 2.5)) + labs(title = \"coordinate(xlim = c(0.5, 2.5))\")\npl_scale_6 &lt;- pl_scale_base + xlim(0.5, 2.5) + labs(title = \"xlim(0.5, 2.5)\")\n\npl_scale_5 + pl_scale_6\n\nWarning: Removed 3601 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can also extend the range. This might be useful is you want to include 0 as a value or if you want to explicitly show values larger than the maximum. This can be useful if you compare graphs. For instance, let’s first create a plot that uses only prices &lt; 10000 and carat &lt; 3 and compare that plot with the base plot:\n\npl_scales_7 &lt;- diamonds |&gt; filter(price &lt; 10000 & carat &lt; 3) |&gt; ggplot(aes(x = carat, y = price)) + geom_point(color = \"lightsteelblue\") + theme_minimal()\n\npl_scales_7 + pl_scale_base\n\n\n\n\n\n\n\n\nNotice how the y-axis in both plots differs. In other words, comparing both is difficult. To deal with this, it is useful to expand the limits on the first plot and set them equal to those on the second. To so so, we will set both limits equals to 0 - 20000:\n\npl_scales_7 &lt;- pl_scales_7 + scale_y_continuous(limits = c(0, 20000)) + labs(title = \"Price &lt; 10000, carat &lt; 3\")\npl_scale_base2 &lt;- pl_scale_base + scale_y_continuous(limits = c(0, 20000)) + labs(title = \"carat &lt; 3\")\n\npl_scales_7 + pl_scale_base2\n\n\n\n\n\n\n\n\nAs you can see, it is no easy to compare both plots as the y-axis are similar.\nThe argument na.value = allows you to add a value that will replace missing values.\n\n\n11.1.1.1.4 Transformations\nThe argument transform = \"identity\" by default shows the values on the axis. {ggplot2} and {scales} include a wide variate of ways to transform the variables mapped on the x- and y- axis, e.g. log10, exp, log, logit, sqrt, reverse, reciprocal, boxcox, logit, probit … . In addition, {scales} allows you to create your own transformation. Let’s illustrate a tranformation using the log10 transformation. Using transform = log10 R will compute the log with base 10 of the variable mapped on the axis and show the plot using the log-tranformed variable. However, the labels will show the un-tranformed values. To see how this works, let’s return to the life expectancy dataset.\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = region, size = pop)) +\n  geom_point() +\n  theme_minimal()\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, the variable gdp_capita is showing a very wide range: the richest economy is more than 100 times richer than the poorest one. A log10 transformation can reduce this range: To show this, log(100,000) = 5 and the log(10) = 1. In other words using a log transformation reduces the range from 1 in 10,000 to 1 in 5. One way to do so would be to calculate the log of the variables in the dataset and use these to plot:\n\nlife_df |&gt; filter(date == 2000) |&gt; mutate(loggdp = log(gdp_capita, base = 10)) |&gt;\n  ggplot(aes(x = loggdp, y = life_exp, color = region, size = pop)) +\n  geom_point() +\n  theme_minimal()\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, as you can see in the graph: the labels on the horizontal axis are not 3, 4, 5, … . These are not informative in terms of the level of per capita GDP shown. You first have to raise these to the power 10, to see what level of per capita GDP they represent. You could deal with that using breaks and labels in scale_x_continuous():\n\nlife_df |&gt; filter(date == 2000) |&gt; mutate(loggdp = log(gdp_capita, base = 10)) |&gt;\n  ggplot(aes(x = loggdp, y = life_exp, color = region, size = pop)) +\n  geom_point() +\n  scale_x_continuous(breaks = c(3, 4, 5), labels = 10^(3:5)) +\n  theme_minimal()\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing format(c(10^(3:5)), scientific = FALSE) we can even change the scientific notation. However, as you can see, this takes a lot of code. This is where the transformation enter. Using transform = \"log10\", it is sufficient to add this to your code to get show the plot as if you had plotted it after a mutate(logvar = log(var, base = 10)) command:\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = region, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  theme_minimal()\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can now set the breaks and labels, and the plot is finished. Using tranform = \"reverse\" you change the order of the axis. In other words, the values on the x-axis are shown, moving from the left to right, from large to small and, on the y-axis and moving from bottom to top, from high to low. Reversing the order on the y-axis, shows lower levels of life expectancy at birth higher up the axis:\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = region, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_y_continuous(transform = \"reverse\") +\n  theme_minimal()\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis transformation can be useful is you have two plots side by side. The first measures something: high is good (life expectancy at birth), the second measures something: high is bad (infant mortality rates). If you reverse the axis for the latter, it is show high is good and low is bad. Doing so, makes it often easier to compare charts.\nFor some of these transformation, you can use specific scales: scale_x/y_log10, scale_x/y_reverse and scale_x/y_sqrt. Doing so, you can use scale_x/y_log10() as a substitute for scale_x/y_continuous(transform = \"log10\").\n\n\n11.1.1.1.5 Guide, position and secundary axis\nThe guid = argument allows you to specify the legend. Here, for numeric continuous variables mapped on the x- and y-axis this is less relevant. As you usually wouldn’t include a legend for these axis. Usually, the title and labels on the axis are sufficient as a guide. The position = argument determines where the axis will be shown: for the vertical axis: left or right and for the horizontal axis: bottom or top. For the x-axis, the default of “bottom” and for the y-axis, the default is left. Changing this into “top” and/or “right” changes this:\n\npl_scale_base + \n  scale_x_continuous(position = \"top\") +\n  scale_y_continuous(position = \"right\")\n\n\n\n\n\n\n\n\nYou can add a secondary axis to your plot. In that way, you have two axis. dup_axis() copies the primary x- or y-axis. sec_axis() allows for a monotonic transformation from the values on the primary axis to the values on the secondary axis. Using dup_axis() you can add a name, but the breaks and labels are those on the primary axis. For sec_axis() you can also include a monotonic transformation (i.e. a one-on-one transformation with the values on the primary axis.) To illustrate the first, let’s add a duplicate secondary vertical axis to the diamonds plot:\n\npl_scale_base + scale_y_continuous(sec.axis = dup_axis())\n\n\n\n\n\n\n\n\nTo remove the label from the secondary axis, you can include name = NULL. Using breaks = or labels =, you can also change the breaks and labels on the second vertical axis. To add these, you can use similar methods than those for the primary axis:\n\npl_scale_base + \n  scale_y_continuous(\n    sec.axis = dup_axis(\n      name = NULL,\n      breaks = c(5000, 15000), \n      labels = seq(5000, 15000, by = 10000)))\n\n\n\n\n\n\n\n\nUsing dus_axis() you can highlight other value ranges on the secondary axis than those on the primary axis. In the plot, observations on the left are relatively low on price, while those on the right are relatively high. You can stress this in the plot by setting different breaks for both axis: for the primary axis, you include more breaks and labels on the lower and of price while you add more breaks on labels on the secondary axis at higher price ranges:\n\npl_scale_base + \n  scale_y_continuous(\n    breaks = c(seq(0, 10000, by = 2500), seq(10000, 20000, by = 5000)),\n    labels = paste(\"$\", c(seq(0, 10000, by = 2500), seq(10000, 20000, by = 5000)), sep = \"\"),\n    sec.axis = dup_axis(\n      name = NULL,\n      breaks = c(seq(0, 10000, by = 5000), seq(10000, 20000, by = 2500)), \n      labels = paste(\"$\", c(seq(0, 10000, by = 5000), seq(10000, 20000, by = 2500)), sep = \"\")))\n\n\n\n\n\n\n\n\nUsing sec_axis() you can add a secondary axis with a different scale. However, the scale of the secondary axis must be a monotonic transformation of the first. This allows you to show three variables on three axis: one variable mapped on the x-axis, one on the primary y-axis and one on the secondary y-axis. Although this is something that is often done, it is not recommended. A “double” line graph for instance, with one variable mapped on the primary vertical axis and another on the secondary vertical axis suggests correlation, even if the correlation is completely spurious. To illustrate, consider Figure 11.2, which was taken from [Tyler Vigen] (https://tylervigen.com/spurious-correlations)] and shows the distance between the planet Neptune and the earth and the number of burglaries in Kansas. The plot suggests that we should find a way to get Neptune as closes as possible to the earth. Tyler Vigen’s site includes many more of these examples and often includes a GenIA generate motivation why these correlations might exist.\n\n\n\n\n\nFigure 11.2: Does Neptune cause burglaries in Kansas?\n\n\n\n\n\n\n\n\nTo show correlation, you can use other geometries, e.g. point geometries (where you maps one variable on the horizontal axis and another on the vertical one, Figure 10.1), a path geometry to show co-movement, … .\nIn some cases, a secondary y-axis is necessary. The first case is where you want to show the same variable but measured in two units (e.g. carat and mg, usd and euro, kilometer and miles, …), where you want to to show a date/time variable but shown in two different time zones. Second, sometimes you want to show two unrelated variables in one plot. Although here you could show two plots, one per variable, often two vertical axis are used to do so. In that case, you need a monotonic transformation from the first variable to the second. I refer to the discussion on [stackoverflow] (https://stackoverflow.com/questions/3099219/ggplot-with-2-y-axes-on-each-side-and-different-scales) for solutions to these with this transformation. To illustrate the case, let’s add a secondary x-axis and a secondary y-axis to the diamonds plot. The first will show the weight of a diamond in mg at the top, the second the price in euro on the right:\n\npl_scale_base +\n  scale_x_continuous(\n    sec.axis = sec_axis(\n      transform = ~. * 200,\n      name = \"weight in mg\")) +\n  scale_y_continuous(\n    sec.axis = sec_axis(\n      transform = ~. /1.10,\n      name = \"price in euro\", \n      labels = scales::label_currency(\n        scale = 1/1.10,\n        prefix = \"€\")))\n\n\n\n\n\n\n\n\n\n\n\n11.1.1.2 scale_x/y_date() and scale_x/y_datetime() and scale_x/y_datetime()\nRecall from Chapter 3 that date and time variables are continuous numeric variables using the number of days since January 1st 1970 of the number of seconds since midnight that day. Date/time variables are often used in data science: each transaction you do in a store has a date/time stamp, sales data is available per day or per month and stock market data is usually available at intervals measured in (parts of) seconds. There are three date/time scale functions: _date() for class data, _datatime() for POSIXct and _time() for data measured in hours, minutes and seconds. Here, I’ll use the scale_x/y_datetime() and focus on the difference with the previous scale. The arguments of this function are\n\nscale_x_datetime(\n  name = waiver(),\n  breaks = waiver(),\n  date_breaks = waiver(),\n  labels = waiver(),\n  date_labels = waiver(),\n  minor_breaks = waiver(),\n  date_minor_breaks = waiver(),\n  timezone = NULL,\n  limits = NULL,\n  expand = waiver(),\n  oob = censor,\n  guide = waiver(),\n  position = \"bottom\",\n  sec.axis = waiver()\n)\n\nThere are a couple of arguments that are specific for POSIXct variables: date_breaks, date_labels, data_minor_breaks and timezone. The other arguments are similar to those for the scale_x/y_continuous scale. Using date_breaks can specify an interval using “sec”, “min”, “hour”, “day”, “week”, “month” or “year” (optionally follows by an s, e.g. “hours”, “days”). The the notation follows “n hours”, “n days”, … . For instance, using date_breaks = \"1 month\" will return major break points that are one month apart. breaks = for instance is similar to the equivelent argument in scale_x/y_continuous: you can define breaks using a vector or a function. Here, given that the data are POSIXct, the vector should be a date/time vector and the function should return a date/time variable, e.g. seq.Date() or seq.POSIXt. In addition you can use {scales} to add specif date/time break functions:\n\nscales::breaks_width(width, offset = 0) with width the width between major breaks using “n hours” and offset if you don’t want breaks to start at e.g. January 1st but at e.g. seven days later (offset = \"7 days\")\nscales::breaks_pretty(n = 5) to set n breaks for date/time variables and works in a similar way to scales::extended_breaks(n = ) for continuous variables\nscales::timespan(unit = c(\"secs\", \"mins\", \"hours, \"days\", \"weeks\"), n = 5) to set breaks using time intervals for date/time variables where unit is used to interpret the unit of the timespan (e.g. hours) and n is the desired number of breaks.\n\nNote that date_breaks = \"1 month\" is equal to using breaks = scales::breaks_width(\"1 month\") (note that the first argument is date_breaks and the second is breaks. You can define minor breaks in a similar way, although the minor break needs to fit within the major breaks (e.g. weeks fit in a month).\nTo set the labels, you have two options: date_labels or labels. With date_labels, you can use the notation referred to in Table 3.1 and Table 3.2. In other words, date_labels = \"%Y\" will show 4-digit years as labels, date_labels = \"%b\" abbreviated months, … In addition, using “”, the notation for “next line” in regular expressions, you can show e.g. months above years. For instance, using date_labels = \"%B\\n%Y\" will show the labels on two rows: the first the full name of the month and on the second row the 4-digit year. Using labels you can add a character vector with labels or a function that generates these labels (paste, paste0, … ). In addition you can use {scales} functions:\n\nscales:: label_date(format = \"%Y-%m-%d\", tz = \"UTC\" , locale = ) to set the format, time zone and locale (by default the current locale, use locale = Sys.setlocale(\"LC_TIME\", \"English\") to show months and days in English)\nscales::label_date_short(format = c(\"%Y\", \"%b\", \"%d\", \"%H:%M\"), sep = \"\\n\") to generate labels on two rows with the first label the e.g. the day or the month and the second the month or the year where the second row is only added on the first and last day of the month or month of the year\nscales::label_time(format = \"%H:%M:%S\", tz = \"UTC\", locale = NULL) to set the format for time, including time zone and locale\nscales::label_timespan(unit = c(\"secs\", \"mins\", \"hours\", \"days\", \"weeks\"), space = FALSE, ...) the set a timespan in seconds, minutes, hours, … . You can add a space before the time unit and … allowing you to specify the e.g. accuracy.\n\nIf you specify date_breaks, date_labels or date_minor_breaks as well as breaks or labels or minor_breaks the former will be used.\nTo illustrate, we’ll use the nycflights dataset which you can import from the data &gt; raw directory as nycflights.csv. This dataset is the result of the mutations completed in the “your turn” in Chapter 8.\n\nnyclfights &lt;- read_csv(here::here(\"data\", \"raw\", \"nycflights.csv\"))\n\nRows: 435352 Columns: 36\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (12): dep_day, sched_dep_day, dep_month, carrier, tailnum, origin, dest...\ndbl  (16): year, month, day, dep_delay, arr_delay, gain, flight, air_time, t...\nlgl   (2): dst_on_orig, dst_on_dest\ndttm  (6): dep_hhmm, sched_dep_hhmm, arr_hhmm_utc, arr_hhmm_tzc, sched_arr_h...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe will show a plot with the number of flights per day. To do so, we only use the flights with all available data. Recall that the variable status records if an observation has missing data, or not. Using status ==  complete we filter the data. Using {lubridate}’s floor_date (see Chapter 3), we round the scheduled departure time in POSIXct to the day. Using summarize, we can then calculate the total flights per day. To limit the sample, we use slice_sample(prop = 0.10) and use 10% of the observations. Although we could pipe the resulting data frame in ggplot, we wil first assign it to nyc_day:\n\nnyc_day &lt;- nyclfights |&gt; \n  filter(status == \"complete\") |&gt;\n  slice_sample(prop = 0.10) |&gt; \n  mutate(round_date = floor_date(sched_dep_hhmm, unit = \"day\")) |&gt;\n  group_by(round_date) |&gt;\n  summarize(n_flights = n()) |&gt;\n  ungroup()\n\nUsing a line geometry with the rounded data mapped on the x-axis and the number of flights on the y-axis and accepting all default values, ggplot() shows this graph:\n\nnyc_day |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe x-axis shows 5 dates: “Jan 2023”, “Apr 2023”, “Jul 2023”, “Oct 2020” and “Jan 2024”. Let’s remove the title of the axis and show labels for every month. To do so, we set breaks = scales::breaks_width(\"1 month\") to set the breaks and labels = scales::label_date_short() to set the labels. If you don’t need to change the language of the days, months, … you can leave the function at its default and use labels = scales::label_date_short(). Here, we add a format (format = c(\"%Y\", \"%b\", locale = Sys.setlocale(\"LC_TIME\", \"English\"))) to show dates in English. Change this into, e.g. Spanish, and you’ll get Spanish months.\n\nnyc_day |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  scale_x_datetime(\n    name = NULL,\n    breaks = scales::breaks_width(\"1 month\"), \n    labels = scales::label_date_short(format = c(\"%Y\", \"%b\", locale = Sys.setlocale(\"LC_TIME\", \"English\")), sep = \"\\n\"), \n    limits = c(as.POSIXct(\"2023-01-01 00:00:00\", tz = \"UTC\"), as.POSIXct(\"2023-12-31 23:59:59\", tz = \"UTC\"))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, the every major break now show the month. The first and last month also include the year. Doing so, it is easy to read from the axis when date/time variables change from one year to the other, from one month to the other, … . Let’s change the breaks using date_breaks and set labels using date_labels. We’ll set major breaks per 3 months, add minor breaks per month and show labels using the full name of the month on the first row and the 2-digit year on the second. In addition, we will add a title to the y-axis and remove minor breaks from that axis:\n\nnyc_day |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  scale_x_datetime(\n    name = NULL,\n    date_breaks = \"3 months\", \n    date_labels = \"%B\\n%y\",\n    date_minor_breaks = \"1 month\") +\n  scale_y_continuous(\n    name = \"Number of flights\", \n    minor_breaks = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s now focus on 1 day and show the number of flights on July 4, 2023 per hour. To select this day, we use month() and mday() from {lubridate}:\n\nnyc_july4 &lt;- nyclfights |&gt; \n  filter(status == \"complete\" & month(sched_dep_hhmm) == 7 & mday(sched_dep_hhmm) == 4) |&gt;\n  mutate(round_date = floor_date(sched_dep_hhmm, unit = \"hour\")) |&gt;\n  group_by(round_date) |&gt;\n  summarize(n_flights = n()) |&gt;\n  ungroup()\n\nLet’s see what ggplot() returns by default:\n\nnyc_july4 |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nR shows 3 times, including the month and day. To change this, let’s add a major break every hour and show the time as e.g. “5 AM”, “3 PM”. To do so, we use date_breaks = \"1 hour\" and date_labels = \"%l %p\" where %l (small L) refers to the day in a 12 hour clock starting at 1 and ending at 12 (%L would start at 01 and end at 12) and %p refers to AM/PM. We add a space between the hour and AM/PM.\n\nnyc_july4 |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  scale_x_datetime(\n    name = NULL,\n    date_breaks = \"1 hour\",\n    date_labels = \"%l %p\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can add limits using limits =. Recall that they have to be of type POSIXct. For instance, starting the graph an hour earlier and ending it one hour later:\n\nnyc_july4 |&gt;\n  ggplot(aes(x = round_date, y = n_flights)) +\n  geom_line() +\n  scale_x_datetime(\n    name = NULL,\n    date_breaks = \"1 hour\",\n    date_labels = \"%l %p\", \n    limits = \n      c(as.POSIXct(\"2023-07-04 04:00:00\", tz = \"UTC\"), \n        as.POSIXct(\"2023-07-04 23:00:00\", tz = \"UTC\"))) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.1.1.3 Color, fill, size and transparency\nThe color, fill and size aesthetic can be used to show continuous variables. In Figure 9.1, size was used to show the size of the population and color was used to show the region. In that graph, population is a continuous variable, region a discrete variable. Using the aesthetic size to map the population, ggplot() scales the area of the geometry - here a point geometry - to the size of the populations. For continuous variables mapped on the color or fill aesthetic, ggplot() will use a continuous color scale to show the values of that variable. For each of these aesthetics, {ggplot2} adds a legend.\nWith respect to the color or fill aesthetics, selecting the appropriate colors is not easy. Here, we will not cover the “color theory” but refer to e.g. Lisa Charlotte Muth’ [A detailed guide to colors in data vis style guides] (https://www.datawrapper.de/blog/colors-for-data-vis-style-guides) or Datawrapper’s section on [color in data viz] (https://www.datawrapper.de/blog/category/color-in-data-vis). In addition, not all colors are interpreted in the same way across cultures. Second, people with color blindness will see “colors” differently. In other words, the choice of one or multiple colors to show your data is a lot more difficult than choosing the colors that you happen to like.\nIn general, there are two types of continuous color scales: sequential and diverging. The first includes one hue and change the “shade” of the color from, e.g. light grey to dark grey or from light red to dark red. Diverging colors use two or more distinct hues, e.g. green and red with a dark or light midpoint. To show continuous variables with e.g. bar of column geometries, there are “binned” versions of continuous color scales. These binnend version assign a discrete color scale to continuous variables.\nBy default, {ggplot2} uses scale_color/fill_continuous() for continuous color scales. Because the color and fill aesthetics use similar functions, we refer to both at the same time using color/fill as a shortcut to scale_color_continuous() or scale_fill_continuous(). These functions include the following arguments:\n\nscale_colour_continuous(..., type = )\n\n# use ... to set e.g.: \n\n  name = waiver()\n  breaks = waiver()\n  minor_breaks = waiver()\n  n.breaks = NULL\n  labels = waiver()\n  limits = NULL\n  rescaler = rescale\n  oob = censor\n  expand = waiver()\n  na.value = NA_real_\n  transform = \"identity\"\n  guide = \"legend\"\n  position = \"left\"\n\nUsing ... you can set e.g. breaks, limits and labels. The type argument refers to the type of the color scale. By default, this value is gradient. Other options include viridis or a function that returns a continuous color scale. The default gradient implies that {ggplot2} actually defaults to the scale_color/fill_gradient(). In other words using scale_color/fill_continuous() (accepting all defaults) is equivalent to scale_color_gradient() (accepting all defaults). However, using scale_color_gradient() there are more options to set the color scale. This function uses two hues to develop a continuous scale. The arguments of this function are\n\nscale_colour_gradient(\n  name = waiver(),\n  ...,\n  low = \"#132B43\",\n  high = \"#56B1F7\",\n  space = \"Lab\",\n  na.value = \"grey50\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\n\nwhere ... allow you to set breaks, label, minor breaks, …; low is the hue to show low values and is by default “#132B43” and high is the hue to show high values with default “#56B1F7”. Missing values are shown using “grey50”. If you change the low and high values with your own colors, scale_color_gradient() will develop a color scale using those two colors for the low and high values. You can include the colors using their name. R has 657 color names. Using ?colors() in the console, you can see all of them. In addition, you can add a color HEX code. The guide arguments refers to the way the color scale is shown in the legend. Using guide_colorbar() you can modify the way the legend looks and determine e.g. its position. However, these lay-out items can also be set in the non-data parts of the plot. Here, the color scale is shown using a color bar. There are two variations on the gradient scale: scale_color_gradient2() and scale_color_gradientn(). The first\n\nscale_colour_gradient2(\n  name = waiver(),\n  ...,\n  low = muted(\"red\"),\n  mid = \"white\",\n  high = muted(\"blue\"),\n  midpoint = 0,\n  space = \"Lab\",\n  na.value = \"grey50\",\n  transform = \"identity\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\n\nincludes two additional arguments: mid and midpoint. Here R will use a scale starting at the color for low and moving to the color for high but will use the color mid to show midpoint values. By default, the color for low is “red”, the color for high is “blue” and the color for midpoints (with default value 0) is “white”. You need to define the midpoint from the data if the median or mean is not equal to 0. In other words, by default, the color scale will start at red for low value, change to white to show values around the midpoint and add shades of blue af the values increase. The second variant, scale_color_gradientn()\n\nscale_colour_gradientn(\n  name = waiver(),\n  ...,\n  colours,\n  values = NULL,\n  space = \"Lab\",\n  na.value = \"grey50\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\n\nallows you to add many colors in a vector for the colours or color argument.\nLet’s return to scale_x_continuous(). The default value for type is gradient but you can also use viridis. [Viridis] (https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html#usage) scales will also generate good black/white scale (e.g. if printed). If the option viridis is chosen, R actually uses the scale_color_viridis_c() to show continuous viridis scales\n\nscale_colour_viridis_c(\n  name = waiver(),\n  ...,\n  alpha = 1,\n  begin = 0,\n  end = 1,\n  direction = 1,\n  option = \"D\",\n  values = NULL,\n  space = \"Lab\",\n  na.value = \"grey50\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\n\nIn the scale name, the _c stands for “continuous”. The option allows you to choose one of the viridis color map. You can refer to those by their name of by a letter. By default the color map is “D” or “viridis. The alpha argument allows you to specify the transparency of the scale. The arguments begin and end include the color for low values and high values. You can change the direction (i.e. begin show high values, end shows low values). Using a higher value for begin will exclude the first proportion of colors from the color map. Lowering the value for end excludes the last proportion of colors from a color map. The full range of color maps includes A to H as well as an illustration of the alpha, begin and end values are shown in Figure 11.3. For these last options, the figure always uses option”J” as a reference.\n\n\n\n\n\nFigure 11.3: Viridis color scales\n\n\n\n\n\n\n\n\nscale_color/fill_distiller() uses the [ColorBrewer] (https://colorbrewer2.org/#) scales to create continious scales. The ColorBrewer scales were designed to create attractive maps but are widely used in other applications. The ColorBrewer scales are shown in Figure 11.4:\n\n\n\n\n\nFigure 11.4: ColorBrewer scales\n\n\n\n\n\n\n\n\nTo use these scales as continuous scale, scale_color/fill_distiller() allows you to select the the ColorBrewer palette in the palette argument using it number of name (Figure 11.4). The type argument allows you to specify if you need a sequential (\"seq\") or diverging (\"div\") scale.\n\nscale_colour_distiller(\n  name = waiver(),\n  ...,\n  type = \"seq\",\n  palette = 1,\n  direction = -1,\n  values = NULL,\n  space = \"Lab\",\n  na.value = \"grey50\",\n  guide = \"colourbar\",\n  aesthetics = \"colour\"\n)\n\nFor instance, to use the “Pastel2” scale, you can use scale_color_distiller(palette = \"Pastel2\").\nThere are many other packages that define color scales. {paletteer} collects all those scales. If you install this package, you can all these scales using scale_color_paletteer_c() and include the name of the package and the name of the scale. In addition, you can add arguments for breaks, labels, … For instance, to use {ggthemes} scale “Orange-Blue Diverging”, you use scale_colour_paletteer_c(\"ggthemes::Orange-Blue Diverging\"). The number of scales, which is very large, is shown at [The Paletteer Gallery] (https://pmassicotte.github.io/paletteer_gallery/#discrete-palettes) and includes scales based on Harry Potter, Vincent Van Gogh, Star Trek, … .\nNote that selecting a good color scale is very important for any visualization. Colors often represent values, emotions, … that might differ from one culture to the other. In addition, color blind people will not see all color scales in the same way. To illustrate, Figure 11.5 shows you have people with various types of color blindness see the rainbow:\n\n\n\n\n\nFigure 11.5: How color blind see the rainbow\n\n\n\n\n\n\n\n\nA notes on the names breaks, limits and labels arguments that you can use in these color and fill functions. These arguments work in a similar way as they did for scale_x/y_continious(). However, in the latter, they show op as breaks, limits and labels on the x- or y-axis. Here, they will show up as names, breaks, limits and labels in the legend. Using name you can add a name to the legend. Adding breaks = c(1000, 2000) for a color scale, will show these two breaks in the legend, just as the breaks argument for the x- and y- axis set the major breaks. Adding limits = c(0, 10000) will create a continuous starting at 0 and ending at 10000 even if the range of the data is different. With labels you can set labels as you did for continuous scales on the horizontal or vertical axis, e.g. using {scales} to show currencies or percentages.\nFor the size and alpha aesthetic, you can use the scale_size() or scale_alpha. If you use scale_size/alpha_continuous, R will use scale_size/alpha() and scale the area or the transparency. The arguments of this function are similar to the one for previous functions, with the exception of range:\n\nscale_size(\n  name = waiver(),\n  breaks = waiver(),\n  labels = waiver(),\n  limits = NULL,\n  range = c(1, 6),\n  transform = \"identity\",\n  trans = deprecated(),\n  guide = \"legend\"\n)\n\nBy default, range sets the size of the smallest value equal to 1 and the size of the largest value equal to 6. For the alpha aesthetic, this range is 0.1 to 1. You can change this if you alter the these numbers. The interpretation of names, breaks, labels and limits is identical: names adds a name to the legend, breaks determine the breaks to show on the legend, labels change the labels on the legend and can be adapted using e.g. {scales} and limits restricts the values that will be shown.\nNote that in all these scales, transform is set to identity. Changing this into e.g. log10 will transform the variables and apply the color, size or fill scale to these transformed variables. In addition, you can use line width to map a continuous variable. However, this is rarely done and this aesthetic is used primarily for discrete variables.\n\n\n\n11.1.2 Discrete scales\nDiscrete scales map discrete variable on the x- and y- axis or on other aesthetics such as color, size, shape, alpha, line type or line width. These function are very similar to their equivalents for continuous variables. The position scales scale_x/y_discrete() are used to show the position of the values of the variable mapped on the horizontal and/or vertical axis. The arguments of the scale_x_discrete() function are:\n\nscale_x_discrete(\n  name = waiver(),\n  ...,\n  expand = waiver(),\n  guide = waiver(),\n  position = \"bottom\"\n)\n\n# Use ... to set e.g. : \n  \n  breaks = waiver()\n  labels = waiver()\n  limits = NULL\n  expand = waiver()\n  na.translate = TRUE\n  na.value = NA\n  drop = TRUE\n  guide = \"legend\"\n  position = \"left\"\n\nMost of the arguments were covered for the continuous variants of the scales. The arguments that are new are specific for discrete scales. First, discrete variables allow you to show missing values. By default, na.translate = TRUE shows these missing values. Changing this into FALSE will remove these missing values. na.value = NA shows the missing values on the x- and y-axis on the right hand side. For other aesthetics, you can fill e.g. the color, shape, size, line type of width or alpha to use to show these variables. The argument drop = TRUE is relevant for factor variables. If a factor level is not represented in the data, it will not be shown by default. Changing this into FALSE shows all factor levels. If they are shown but have no values, a bar or column graph will have zero height.\nFor the labels = argument, you can include a named vector. For instance, in the nycflights dataset, “LGA” is “LaGuardia”, “EWR” is “Newark International Airport” and “JFK” is “John F. Kennedy International Airport” . If you use labels = c(\"EWR\" = \"Newark Liberty Int.\", \"JFK\" = \"John F. Kennedy Int.\", \"LGA\" = \"LaGuardia\") the legend will show the full names, not the FAA abbreviation.\nscale_color/fill_discrete() by default use scale_color/fill_hue() which uses evenly spaced colors on the color wheel. For instance, if map a variable with 6 values on the fill aesthetic in a bar chart, scale_fill_hue() will default to\n\nshow_col(pal_hue()(6))\n\n\n\n\n\n\n\n\nYou can change the luminescence or lightness of the color\n\nshow_col(pal_hue(l = 20)(6))\n\n\n\n\n\n\n\n\nor the location where this scale will start along the color wheel\n\nshow_col(pal_hue(h.start = 90)(6))\n\n\n\n\n\n\n\n\nHowever, you can not set the chroma or intensity of a color.\nUsually, for discrete color/fill scales, the colors are determined from a palette, e.g. ColorBrewer or set manually. For the use, you use scale_color/fill_brewer()\n\nscale_colour_brewer(\n  name = waiver(),\n  ...,\n  type = \"seq\",\n  palette = 1,\n  direction = 1,\n  aesthetics = \"colour\")\n\nand use a ColorBrewer palette shown in Figure 11.4. In addition you can use the {paletteer} package’s scale_color/fill_paletteer_d() and select one of the many discrete color scales in this package.\nNote that scale_shape() maps discrete variable to only 6 shapes. Here, there are two shortcuts among the arguments: solid = TRUE shows filled shapes; setting this value to false uses open shapes. For scale_linewidth() the key argument is range = c(1, 6) and shows the width of the line showing the largest value relative to the width of the line showing the smallest value.\nUsually, the discrete variables that are mapped on color, fill, size, shape, alpha, line width or type is small and you can set them manually. To do so, you can use the scale_aes_manual() function (where aes refers to the aesthetic: color, fill, shape, … ). These functions all have the same arguments:\n\nscale_colour_manual(\n  ...,\n  values,\n  aesthetics = \"colour\",\n  breaks = waiver(),\n  na.value = \"grey50\"\n)\n\n# Use ... to set e.g. : \n  \n  breaks = waiver()\n  labels = waiver()\n  limits = NULL\n  expand = waiver()\n  na.translate = TRUE\n  na.value = NA\n  drop = TRUE\n  guide = \"legend\"\n  position = \"left\"\n\nThe values argument allows you to set the values for the aesthetic: the color (color and fill), the shape, line width or type, size or alpha. To do so, you add these values in a vector. For instance, to set the values of a fill or color scale, you can add the names of colors or the HEX codes: values = c(\"lightsteelblue\", \"steelblue\", \"blue\") or c(\"#7CAE00\", \"#00B4F0\", \"#FF64B0\"), for shapes and lines you can refer to the number (Figure 10.3, Figure 10.6). The values will be matched with the breaks if specified or in the order of the factor.\nIn addition, you can also use a named vector, where you specify a color/shape/size, … for every value of the variables that is mapped on the aesthetic. For instance, nycflights includes 3 origin airports: EWR, JFK and LGA. Using color_airports &lt;- c(\"EWR\" = \"#00BFC4\", \"JFK\" = \"#F564E3\", \"LGA\" = \"#00BA38\") you can define for every airport a color. It is now sufficient to add values = color_airports and the values will be shown in the selected color. As you can reuse the names vector, you’ll have a consistent color use. As you can reuse this vector in other code, this consistency will show in all other graphs. Doing so, you will always use the same color for e.g. produce names, brands, countries, regions, … . In addition, it also allows you to be consistent in the use of an organisation’s of brand style guide. For instance, KU Leuven’s blues used in the logo are #52BDEC and #00407A, for print, the brand style includes a fixed set of other colors. Using these, KU Leuven can consistently use the same colors in all its print, but also in graphs and tables.\nBinned scales are used to when you want to show continuous variables in a plot that shows discrete variables. The variable price in the diamonds dataset for instance is a continuous variable. If you want to use this variable in a column or bar chart to show, e.g. the number of observations. To illustrate the binned position scales, we’ll use scale_x_binned():\n\nscale_x_binned(\n  name = waiver(),\n  n.breaks = 10,\n  nice.breaks = TRUE,\n  breaks = waiver(),\n  labels = waiver(),\n  limits = NULL,\n  expand = waiver(),\n  oob = squish,\n  na.value = NA_real_,\n  right = TRUE,\n  show.limits = FALSE,\n  transform = \"identity\",\n  trans = deprecated(),\n  guide = waiver(),\n  position = \"bottom\"\n)\n\nThe binned scales main arguments are n.breaks = and nice.breaks = TRUE. The first allows you to determine the number of bins. Recall that for a historgram geometry, this was also the case. The default here is 10. The second argument, nice.breaks by default will try to put breaks at “nice” values instead of evenly spread within the limits. Changing this to FALSE will put the breaks at evenly spaced intervals. Two other arguments are right and show.limits. Bins can be open or closed on the right and left. An interval is closed on the right is the last value is included in the interval; it is closed on the left if the first value is included. Here, the first and last value refer to the values at break positions. For instance, suppose that the breaks are 0-2 and 2-4. By default, the last values, 2 and 4 are part of the lower bin: the first bin includes 2 and the second bin includes 4. If a bin is open on the right, the last value is includes in the next bin. In other words, 2 would be part of the second bin and 4 would be part of the third bin. The last argument includes the option to show the limits of the scale as ticks. By default this is not the case.\nFor color/fill scales, the binned version scale_color/fill_binned()is comparable to the discrete versions scale_color/fill_discrete(). As was the case with the latter, the former will default to another scale: scale_color/fill_steps(), scale_color/filled_steps2(), scale_color/filled_stepsn() where these function have the same interpretations as the scale_color/fill_gradient() variations, to scale_color/fill_viridis_b() to if you set the type to “viridis” or any other scale, e.g. from the {paletteer} package using scale_color_paletteer_binned().",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html#guides-legends",
    "href": "11_Changing_plot_layout.html#guides-legends",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "11.2 Guides (legends)",
    "text": "11.2 Guides (legends)\nIn almost all scale functions, there is an argument guide. With the exception of x- and y- axis, a graph needs a add a guide that show the relation between the values and the aesthetic. Doing so, the guide shows which variables are mapped on the aesthetic and what a color, shape or size represents in terms of values. The scale functions all include an argument guide that lets you change the default values for the guides. As an alternative, you can specify the guides as an independent layer using guides(aesthetic = guide_function), e.g. guides(x = guide_axis()), `guides(color = guide_colorbar)\nFor continuous and discrete x- and y- axis the guide_axis() includes the following arguments:\n\nguide_axis(\n  title = waiver(),\n  theme = NULL,\n  check.overlap = FALSE,\n  angle = waiver(),\n  n.dodge = 1,\n  minor.ticks = FALSE,\n  cap = \"none\",\n  order = 0,\n  position = waiver()\n)\n\nThe main arguments are check.overlap, angle and n.dodge = 1. Sometimes, the labels on the x- of y-axis are very wide and overlap. Consider for instance this example of a column graph with the average life expectancy per region.\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col()\n\n\n\n\n\n\n\n\nHere you can see that the labels on the horizontal axis overlap. To deal with that, you can organize them on 2 or more rows. To do so, you can use the n.dodge = argument. By default, the labels are shown on one row, however, setting that level to 2 or 3 changes shows labels on two or three rows. Using 2, there is not overlap left\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  guides(x = guide_axis(n.dodge = 2))\n\n\n\n\n\n\n\n\nChanging the value of check.overlap to TRUE isn’t always helpful. For instance, for this plot, this will cause R to drop the value for “Latin America & Caribbean”.\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  guides(x = guide_axis(check.overlap = TRUE))\n\n\n\n\n\n\n\n\nAs a last option, you can change the angle. For instance, setting an angle of 22.5 would solve the overlap. Note that an angle of 90 puts the labels in a vertical position.\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  guides(x = guide_axis(angle = 22.5))\n\n\n\n\n\n\n\n\nThe last argument, position allows you to set the labels at the bottom (default), top or for y-axis, left or right. For instance, setting the axis at the top:\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  guides(x = guide_axis(n.dodge = 2, position = \"top\"))\n\n\n\n\n\n\n\n\nRecall that you can set the position of the scale in the scale functions. For instance, using scale_x_discrete(), you could set position = \"top\". However, if you then add a guide layer with position at the bottom, R will show the guide at the bottom:\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  scale_x_discrete(position = \"top\") +\n  guides(x = guide_axis(n.dodge = 2, position = \"bottom\"))\n\n\n\n\n\n\n\n\nThe title argument allows you to set the title of the axis. Note that this is also something that you can do in scale_x_discrete or using labs(x = , y = ).\n\nlife_df |&gt; group_by(region) |&gt; summarize(ave_life = mean(life_exp, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = region, y = ave_life)) +\n  geom_col() +\n  guides(x = guide_axis(title = \"World Bank Regional Grouping\",  n.dodge = 2))\n\n\n\n\n\n\n\n\nThe guide for continuous colors is guide_colorbar:\n\nguide_colourbar(\n  title = waiver(),\n  theme = NULL,\n  nbin = NULL,\n  display = \"raster\",\n  raster = deprecated(),\n  alpha = NA,\n  draw.ulim = TRUE,\n  draw.llim = TRUE,\n  position = NULL,\n  direction = NULL,\n  reverse = FALSE,\n  order = 0,\n  available_aes = c(\"colour\", \"color\", \"fill\"),\n  ...\n)\n\nThe last argument shows for which aesthetics this guide function can be used: color or fill. To illustrate, let’ use a simple plot using the 2000 data for life expectancy. Here the per capita gdp is mapped on the x-axis, life expectancy on the y-axis as well as on the color aesthetic and the population on the size aesthetic. The x-scale is “log10” transformed.\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = life_exp, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_color_viridis_c(option = \"magma\")\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe main arguments include position, direction, reverse and order. The first determine the position of the guide: “top”, “bottom”, “left” or “right”. The second the direction. By default, the guide is shown vertical in the left or right position and horizontal in the bottom and top positions. You can change that default by setting the desired direction. The argument reverse allows you to change the order of the colorbar. By default, R shows the highest values at the top. In the example, the colors associated with higher life expectancy at birth are at the top, those with lower levels at the bottom. Setting reverse to TRUE changes that order. The order argument allows you to determine the order of the legends. Here, we have two: one for life expectancy and one for population size. If you would like to show population first and live expectency second, you need to add a guide for size and set the order to 1 for size and the order equal to 2 for the color guide.\nLet’s illustrate a couple of these options:\n\nAdding a title and putting the guide at the bottom\n\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = life_exp, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_color_viridis_c(option = \"magma\") +\n  guides(color = guide_colorbar(title = \"Life expectancy at birth\", position = \"bottom\"))\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote two things: first R only shows the colorbar at the bottom. The guide for size stays at the right. Second, note that R changed the direction: the colorbar is now shown horizontally.\n\nReversing the order in the legend\n\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = life_exp, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_color_viridis_c(option = \"magma\") +\n  guides(color = guide_colorbar(reverse = TRUE))\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that R doesn’t change the colors in the plot, only the order in the guide. Low values for life expectancy are now shown first, higher values last. Changing the direction\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = life_exp, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_color_viridis_c(option = \"magma\") +\n  guides(color = guide_colorbar(direction = \"horizontal\"))\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAgain note that R only changes the orientation of the color guide, not the size guide.\nFor the other aesthetics, that are usually to show discrete values guide_legend allows you to set the options\n\nguide_legend(\n  title = waiver(),\n  theme = NULL,\n  position = NULL,\n  direction = NULL,\n  override.aes = list(),\n  nrow = NULL,\n  ncol = NULL,\n  reverse = FALSE,\n  order = 0,\n  ...\n)\n\nHere, there are 3 relevant arguments: nrow and ncol and override.aes = list(). Lets return to the previous graph. Population is shown with 5 levels. Using direction, you can change the direction from vertical to horizontal. However, using nrow and ncol you can show the various values on nrow rows and ncol columns. For instance to show the population legend (aesthetic size) on 3 rows of 2 columns:\n\nlife_df |&gt; filter(date == 2000) |&gt;\n  ggplot(aes(x = gdp_capita, y = life_exp, color = life_exp, size = pop)) +\n  geom_point() +\n  scale_x_continuous(transform = \"log10\") +\n  scale_color_viridis_c(option = \"magma\") +\n  guides(size = guide_legend(nrow = 3, ncol = 2))\n\nWarning: Removed 23 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo illustrate the override.aes argument, we’ll use the diamonds dataset and show the price-carat plot with cut mapped on the color aesthetic. We’ll add settings to show the points: their size should be one and the alpha value 1/2:\n\ndiamonds |&gt; slice_sample(prop = 0.10) |&gt;\nggplot(aes(x = carat, y = price, color = cut)) +\n  geom_point(size = 1, alpha = 1/4) +\n  scale_color_viridis_d(option = \"magma\") \n\n\n\n\n\n\n\n\nNote that R uses these size and alpha values also in the guide. To change that, you need to override the aesthetics using override.aes = list. The list should then include the alternative settings to use in the guide, e.g. size = 4 and alpha = 1\n\ndiamonds |&gt; slice_sample(prop = 0.10) |&gt;\nggplot(aes(x = carat, y = price, color = cut)) +\n  geom_point(size = 1, alpha = 1/4) +\n  scale_color_viridis_d(option = \"magma\") +\n  guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))\n\n\n\n\n\n\n\n\nNow, the legend shows points with size 5 and without any adjustment for transparancy.\nNote that here too, you can change the position, add a title, … .",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html#faceting",
    "href": "11_Changing_plot_layout.html#faceting",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "11.3 Faceting",
    "text": "11.3 Faceting\nGraphs where you map variables on 3 of 4 aesthetics are not always easy to read. For instance, a plot that includes a color for product category, a shape to show “premium”, “medium” of “budget” price ranges and a size to represent sales volumes contains too much information to process. One way to deal with this is to split them up into subplots. This is what faceting does. To illustrate, let’s reuse the following plot:\n\ndiamonds |&gt; slice_sample(prop = 0.10) |&gt;\n  ggplot(aes(x = carat, y = price, color = cut)) + \n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are three facet functions: facet_null() which doesn’t show facts and is the default, facet_wrap() and facet_grid(). The latter is ideal if you have multiple faceting variable and want to show one in the rows and the other in the columns. The former is ideal is you have one factor. Before we start, let’s see what facet_wrap() does:\n\ndiamonds |&gt; slice_sample(prop = 0.10) |&gt;\n  ggplot(aes(x = carat, y = price, color = cut)) + \n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(cut))\n\n\n\n\n\n\n\n\nHere we have 5 subplots: one for every level of cut. The subplots retain the color. Note also that the x-axis on the first two plots in the top row is removed and that the same holds for the y-axis in the plots in the second and third column. The value for each level of cut is included at the top of each subplot. There are many aspects that you can control using the facet functions. To see which ones, let’s start with facet_wrap(). The arguments include:\n\nfacet_wrap(\n  facets,\n  nrow = NULL,\n  ncol = NULL,\n  scales = \"fixed\",\n  shrink = TRUE,\n  labeller = \"label_value\",\n  as.table = TRUE,\n  switch = deprecated(),\n  drop = TRUE,\n  dir = \"h\",\n  strip.position = \"top\",\n  axes = \"margins\",\n  axis.labels = \"all\"\n)\n\nThe first argument defines the faceting groups. Using vars(var1, var2, ...) you can add one or multiple variables. The second and third argument allow you to specify the number of row and columns. In the example, facet_wrap() used 2 rows and three columns. In the example the scales are fixed: all plots have the same range for both price and carat. The other extreme is free. Using that option, all scales will depend on the range of values within that single subplot. In between these two extremes, free_x or free_y allow one of both scales to depend on the range of values while the other is fixed. The default option makes most sense as it maximizes the comparability among subplots. In case you also plot summary statistics, the shrink arguments will, by default, shrink the range of the scales to fit the range of the statistics. Setting these value to FALSE, the output will show the range in the data. This might be relevant is you use geom_smooth() without a point of line geometry. By default, R will shrink the range of the y-axis to fit the values of the smoothed function. The labeller argument allows you to specify the labels on top of each subplot. To illustrate, let’s create a second factor using the table variable in diamonds with two levels, map this factor to the shape aesthetic and use facet_wrap() with the default value for labeller: \"label_value\":\n\ndia &lt;- diamonds |&gt; mutate(table_fac = case_when(table &lt; 57 ~ 0, table &gt;= 57 ~ 1))\ndia$table_fac &lt;- as.factor(dia$table_fac)\n\n\ndia |&gt; ggplot(aes(x = carat, y = price, color = cut, shape = table_fac)) +\n  geom_point() +\n  facet_wrap(\n    vars(cut, table_fac), \n    labeller = \"label_value\")\n\n\n\n\n\n\n\n\nThe plot shows the values for both factors, but doens’t show the factor names. To see those, you have to look at the legend. Using labeller = \"label_both\" shows both the name of the factor as well as the value:\n\ndia |&gt; ggplot(aes(x = carat, y = price, color = cut, shape = table_fac)) +\n  geom_point() +\n  facet_wrap(\n    vars(cut, table_fac), \n    labeller = \"label_both\")\n\n\n\n\n\n\n\n\nThe as.table argument defines the order of the subplots. By default, the largest value is shown “as a table”: in the bottom right part of the plot. Setting this value to FALSE, show the subplots in the order in which you would expect them in a plot: the higest values are shown in the top right corner. drop determines what happens in case there are factor values that do not appear in the plot. By default, these are dropped from the subplots. In other words, if there wouldn’t be any values for e.g. Premium cut diamonds, that subplot would not be included in the facet. Chaning this to FALSE will show an empty plot. R fills the “matrix” in horizontal direction: dir = \"h\". Using \"v\" will first fill in vertical direction. The axis = margins arguments will by default draw the axis at the margins of the plot. In the examples: the y-axis is shown only on the left side and the x-axis only at the bottom of a column. Using all every subplot will include both x- and y-axis. Using all_x or all_y one of the two axis is added to all plots. In case axis is all, the axis.labels = \"all\" argument determines how many labels are shown: all interior labels are shown. If axis = all_x/y R draws labels on the interior x- of y-axis.\nTo illustrate facet_grid, let’s use the diamonds dataset with the second factor for table:\n\ndia |&gt; ggplot(aes(x = carat, y = price, color = cut, shape = table_fac)) +\n  geom_point() +\n  facet_grid(\n    vars(cut, table_fac))\n\n\n\n\n\n\n\n\nNotice how R, by default, plots all subplots in vertical direction and fixes all axis, both horizontal as well as vertical. The subplots are ordered in both factors: starting with the first lowest levels for cut and for each level of cut the values for the second table_fac factor. The arguments of the facet_grid() function are\n\nfacet_grid(\n  rows = NULL,\n  cols = NULL,\n  scales = \"fixed\",\n  space = \"fixed\",\n  shrink = TRUE,\n  labeller = \"label_value\",\n  as.table = TRUE,\n  switch = NULL,\n  drop = TRUE,\n  margins = FALSE,\n  axes = \"margins\",\n  axis.labels = \"all\",\n  facets = deprecated()\n)\n\nMost are familiar from facet_wrap with one important exception: there is no facets argument (it can be used but is is deprecated) and nrows and ncols have been replaced with rows and cols. The reason is that the rows and cols arguments are means to include the vars() argument. R will then show the facets in a matrix format with the col showing the values of the factor in the columns and rows showing the values of the factor in the rows. Using the two factor example:\n\ndia |&gt; ggplot(aes(x = carat, y = price, color = cut, shape = table_fac)) +\n  geom_point() +\n  facet_grid(\n    rows = vars(table_fac), \n    cols = vars(cut))\n\n\n\n\n\n\n\n\nR now shows the matrix format for the facet. By default all subplots have the same size: space = \"fixed\". Using free both the height and width of the subpanels will differ. Using free_x or free_y allows a different width or height but not both. All other arguments are simular to these in facet_wrap.\nNote that you don’t need to include the faceting variable in the aesthetics. You can also use the variable only in the facet function. Doing so will loose the aesthetic used to show the factor. For instance, if we remove color = cut and include cut in facet_wrap():\n\ndiamonds |&gt; slice_sample(prop = 0.10) |&gt;\n  ggplot(aes(x = carat, y = price)) + \n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(cut))\n\n\n\n\n\n\n\n\nR shows the facets, but as there is not mapping on the color aesthetic, does so in the default color.\nAnother useful option is to show all the data in each panel, but to do so in a lighter color. To do so, you need to create a dataset without the factor. Using that dataset, you create geometry which only includes the x- and y-axis. In the second geometry, you use the original dataset to add the color aesthetic which you also use to facet. With the diamonds dataset, let’s first create a data frame without cut:\n\ndiamonds2 &lt;- diamonds |&gt; select(-cut)\n\nNow we use that dataframe in the first layer:\n\nggplot() +\n  geom_point(data = diamonds2, aes(x = carat, y = price), color = \"lightgrey\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe now add the second layer and use facet_wrap():\n\nggplot() +\n  geom_point(data = diamonds2, aes(x = carat, y = price), color = \"lightgrey\") +\n  geom_point(data = diamonds, aes(x = carat, y = price, color = cut)) +\n  theme_minimal() +\n  facet_wrap(vars(cut)) \n\n\n\n\n\n\n\n\nNow every subplot shows all variables. Each subplot uses the color aesthetic to show the values that match the value of the factor. Here, you can nicely see if there are any major differences between the full dataset on the one hand and the data in the subplots on the other.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html#coordinates",
    "href": "11_Changing_plot_layout.html#coordinates",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "11.4 Coordinates",
    "text": "11.4 Coordinates\nBy default R uses the Cartesian coordinate system shown in fig-cartesian. We covered most of the aspects of coordinate systems in the previous parts, so here, we will recap the most important parts. The coord_cartesian() function includes the following arguments:\n\ncoord_cartesian(\n  xlim = NULL,\n  ylim = NULL,\n  expand = TRUE,\n  default = FALSE,\n  clip = \"on\")\n\nThe arguments xlim, ylimand expand were covered. However, it is useful to stress that xlim and ylim, in contrast to the limits argument in scales_x/y_continuous do not remove the values from the dataset when you include statistics in your plot, e.g. a smoothed line. In other words, it is often useful to set the limits in this layer rather than the scales layer. By default, R wil create a warning message if the coordinate system is replaced. Turning default = TRUE changes this. The clip = on means that the drawing is clipped to the size of the panel. This is the most useful option. If this is set to off, it allows to draw points and lines anywhere on the plot, even outside of the panel. In other words, here you could have points in the margins.\nTo fix the ratio between the y and x axis, you can use coord_fixed(). By default, this ratio is 1: a one unit in change on the horizontal axis equals the same unit change on the vertical one. You can change this to e.g. 2. In that case a one unit increase on the horizontal axis represents a two unit increase on the vertical axis.\n\ncoord_fixed(\n  ratio = 1, \n  xlim = NULL, \n  ylim = NULL, \n  expand = TRUE, \n  clip = \"on\")\n\nPolar coordinates are used to e.g. draw pie charts. The arguments of the coord_polar() function are:\n\ncoord_polar(\n  theta = \"x\", \n  start = 0, \n  direction = 1, \n  clip = \"on\")\n\nHere theta refers to the variable that will be used for the angle in the polar charts, start determines the start position and direction sets the direction around the polar coordinate with 1 equal to clockwise and -1 equal to counter clockwise. To illustrate the code for a pie charts, we will use a sample of the diamonds dataset. The first part sets of the “normal” bar chart. Here, we include a variable factor(1) for the x-axis and use the aesthetic fill to map the clarity. Note that the bar chart shows the number of observations on the vertical axis. We also add a scales layer and, using labs() the title.\n\ndia10 &lt;- diamonds |&gt; slice_sample(prop = 0.10)\n\ndia10 |&gt;\nggplot(aes(x = factor(1), fill = clarity)) +\n  geom_bar(width = 1)  +\n  scale_fill_manual(values = c(\"#DFFF00\", \"#FFBF00\", \"#FF7F50\", \"#9FE2BF\", \"#40E0D0\", \"#6495ED\",\"#CCCCFF\",\"#E8CCD4\" )) +\n  labs(\n    title = \"Number of observations for each clarity group\",\n    x = \"\",\n    y = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe then use polar coordinates to change the bar charts into a pie chart. To remove the axis, we use theme_void():\n\ndia10 |&gt;\n  ggplot(aes(x = factor(1), fill = clarity)) +\n  geom_bar(width = 1)  +\n  scale_fill_manual(values = c(\"#DFFF00\", \"#FFBF00\", \"#FF7F50\", \"#9FE2BF\", \"#40E0D0\", \"#6495ED\",\"#CCCCFF\",\"#E8CCD4\" )) +\n  labs(\n    title = \"Number of observations for each clarity group\",\n    x = \"\",\n    y = \"\") +\n  coord_polar(theta = \"y\", start = 0, direction = 1) +\n  theme_void() \n\n\n\n\n\n\n\n\nIf you are interested and would like to see how polar coordinate work: check the “Polar coordinates” box.\n\n\n\n\n\n\nPolar coordinates\n\n\n\n\n\nTo start, let’s look at two points in a Cartesian coordinate system Figure 11.6\n\n\n\n\n\nFigure 11.6: Points in a cartesian coordinate system\n\n\n\n\n\n\n\n\nIn Figure 11.6, you can see that there are two ways to represent a point. The first, using the cartesian coordinates, refers to the x and y values. The first, blue point is given by the pair (750, 400) and the second - the green point - is given by the pair (500, 866). Using these pairs, we can identify every point in the cartesian coordinate system. But you can see that there is also a second way to do so. Starting the the origin, every point can be represented by an angle and a radius. For the first point, the angle is 28.072° and the radius is 850; the second point’s angle is 60° and the radius is 1000. Note that all points on the red circle have a radius of 1000 (if one point on the circle has a radius of 1000, so must all others). The fact that you can represent a point using the angle and the radius is important for polar coordinates. Note that here we measure angle counterclockwise.\nRecall that a circle is 360°. The part of the y-axis above the horizontal axis is at 90°, the part of the x-axis on the left of the vertical axis at 180°, the part of the y-axis below the x-asis at 270° and the part of the x-axis or the right of the vertical axis is at 0° or 360°. For angles larger than 360, you move around the circle more than once. For instance, 450° equals one time around the circle (360°) + 90° and will be shown on the part of the y-axis above the horizontal axis. In other words, all dots with the same radius but with angles equal to 360, 720, … are on the same spot and on the same part of the x-axis, all dots with the same radius but angles 45°, 405°, 765° are also on the same spot.\nLet’s now consider the straight line with Cartesian coordinates in Figure 11.7. The x-axis starts at 0 and end at 1000. The range of the y-axis is 50 to 1050. In other words, the equation for the straight line is\n\\[\ny = 50 + x\n\\]\nThe major breaks are set every 180 on the horizontal axis and every 250 on the vertical axis. The major breaks on the horizontal axis are shown in blue, thoese on the vertical axis are shown in red.\n\n\n\n\n\nFigure 11.7: A line in a cartesian coordinate system\n\n\n\n\n\n\n\n\nA circle is 360°, half a circle is 180°, … . In other words we can rescale the horizontal axis to “times 360°” (?fig-pointsinangleradium). Doing so, we can look at these points as “angles”: with x = 180, 180 is half a circle and represents an angle of 180. Likewise, with x = 360, 360 is a full circle and represents an angle of 360° or 0°. If x is larger than 360, e.g. 540, this represents 1.5 times 360°. In other words, is equals full movement around the circle + half a movement. In other words, this point will be shown at a 180° angle. 720 is two times 360, or two movements around the circle. In other words, the angle is 0°. ?fig-pointsinangleradius also re-interprets the y-axis: this now show the radium.\n\n\n\n\n\nFigure 11.8: A line in a cartesian coordinate system with degrees on the horizontal axis an radius on the vertical axis\n\n\n\n\n\n\n\n\nDoing so, we can reinterpret every point on the line in terms of an angle - measures on the horizontal axis - and a radius - measured on the vertical axis. The circles show points where the angle is 180° or 0°. Given the equation for the straight line: if x = 180 and y = 230 we would interpret that purple point as a point with an angle of 180° and a radius of 230, the blue point (x = 360, y = 410) shows a combination of a point with angle 0° and radius 410, (540, 590) a point with angle 180° (one time around the circle + 180°) and radius 590, … . fig-angleradius180360 shows these points. Here you can see that a point such as 180° with radius 230 is shown on the horizontal axis, with radius 230. Doing a full circle, a point such as (360, 410) is shown on the horizontal axis, … .\n\n\n\n\n\nFigure 11.9: Points with angle 180° or 360°\n\n\n\n\n\n\n\n\nIf you would want to connect these dots on the order in which they appear on the line in ?fig-pointsinangleradius, you would have to draw expanding circles.\nThe triangles in Figure 11.8 show points with an angle equal to 60. On the x-axis, these points are 60, 420 and 780 (60°, once times the circle + 60°, two times around the circle + 60°). The diamonds show points with an angle of 28.072° (28.072, 388.072, …). Figure 11.10 shows these points.\n\n\n\n\n\nFigure 11.10: Points with angle 60° or 28.072°\n\n\n\n\n\n\n\n\nNotice how all these points lie on the same straight line out of the origin: as all points with x-values of 60, 420, … in ?fig-pointsangleradium represents points that are on a line in the circle with an angle of 60°, they are all on that line. The only thing that makes them different is their radius. The same holds for the other line: all points with x-values in ?fig-pointsangleradius of 28.072, 388.072, … all all on a line with angle 28.072°. They differ as they have a different radius. Note that here too, connecting all points in the order in which they appear on the line in ?fig-pointsangleradius would requires you to draw expaning circles. ?fig-angleradiusall slows all points in both Figure 11.9 and Figure 11.10\n\n\n\n\n\nFigure 11.11: Points with angle radius\n\n\n\n\n\n\n\n\nLet’s now see what polar coordinates do. The is shown in Figure 11.12. Recall that the major break lines on the y-axis were shown in read in ?fig-pointsangleradius. There, you can see these lines as circles round the origin. The outer circle shows the “angle” using the full range of x-values. In other words, starting from 0 and moving counterclockwise, every dot is now shown by and angle starting in the origin towards to outer circle where the angle is shown and ending at the radius for that point.\n\n\n\n\n\nFigure 11.12: Polar coordinates\n\n\n\n\n\n\n\n\nFigure 11.12 is the same as Figure 11.7 but using the angle and radius on the circle and not on a Cartesian way.\nLet’s now use diamonds to construct a pie chart that shows the relative share of every level of clarity for diamands with cut = “very good”:\n\ndiamonds |&gt; filter(cut == \"Very Good\") |&gt;\n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing polar coordinates, the pie chart is\n\ndiamonds |&gt; filter(cut == \"Very Good\") |&gt;\n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  coord_polar(theta = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHow do you get to this plot. For what we actually want to show is the relative share of each level of clarity in the “Very good” cut category:\n\ndiamonds |&gt; filter(cut == \"Very Good\") |&gt;\n  group_by(clarity) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(perc_n = n/sum(n)) |&gt;\n  mutate(n_360 = perc_n * 360)\n\n# A tibble: 8 × 4\n  clarity     n  perc_n n_360\n  &lt;ord&gt;   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 I1         84 0.00695  2.50\n2 SI2      2100 0.174   62.6 \n3 SI1      3240 0.268   96.5 \n4 VS2      2591 0.214   77.2 \n5 VS1      1775 0.147   52.9 \n6 VVS2     1235 0.102   36.8 \n7 VVS1      789 0.0653  23.5 \n8 IF        268 0.0222   7.99\n\n\nHere, we have the percentage share of for every level clarity in the column perc_n. That percentage is the “share” of the pie in a pie chart. How does R move get to this percentage? The inuittion is this. Let’s show this summary table in a column geometry and add the column n_360 as a label. This column show the relative share of the pie that a level of clarity would take using the “angle” defined as the 360 * the percentage share. In other words, the category “I1” would start from a zero angle and continue to an angle equal to 2.5. The second cateogry would start at an angle of 2.5 and end at and angle of 65.07. The third would start at 65.07 and end at an angle of 161.61, … .\n\ndiamonds |&gt; filter(cut == \"Very Good\") |&gt;\n  group_by(clarity) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(perc_n = n/sum(n)) |&gt;\n  mutate(n_360 = perc_n * 360) |&gt;\n  ggplot(aes(x = clarity, fill = clarity)) +\n  geom_col(aes(y = n_360)) +\n  geom_text(aes(y = n_360, label = round(n_360, 2)), nudge_y = 2)\n\n\n\n\n\n\n\n\nBecause we have only one value for “cut” the radius is always 1. In other words, R will show scale every category of clarity with an angle defined by n_360 and a radius of 1. To show this pie, we need to tell R where the radius is. In this case, it is on the y-axis of the bar chart: the number of observations per group. To do so, we add \"theta = \"y\". This tells R that the radius is on the horizontal axis (which is 1) and the angle on the vertical axis:\n\ndiamonds |&gt; filter(cut == \"Very Good\") |&gt;\n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  coord_polar(theta = \"y\") \n\n\n\n\n\n\n\n\nThe pie chart shows in the outer circle (the angle) the number of observations. The angle for the first is very small: 85 of 2.5. Adding these observations along the outer circle, R includes a new category at every new “angle”: 65.07, 161.60 (2.5+ 62.6+ 96.5). Adding all these angles the circle will be 360 and the slices show the relative importance of every category.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "11_Changing_plot_layout.html#themes-the-non-data-parts-of-your-graph.",
    "href": "11_Changing_plot_layout.html#themes-the-non-data-parts-of-your-graph.",
    "title": "11  ggplot 2: the appearance and lay out of a plot",
    "section": "11.5 Themes: the non-data parts of your graph.",
    "text": "11.5 Themes: the non-data parts of your graph.\n\n11.5.1 Themes\nThe examples almost always include a layer theme_minimal(). Doing so, the panel of the plot was white, axis grey, … . There are various themes build in {ggplot2}: theme_minimal() is just one example. [Here] (https://ggplot2.tidyverse.org/reference/ggtheme.html) on in the [complete theme section] (https://ggplot2-book.org/themes#sec-themes) of Chang (2025) you can find examples of plots using these themes. In addition, {[ggthemes] (https://jrnold.github.io/ggthemes/index.html)} (Arnold (2024)) includes about 20 themes, in addition to color, shape and line type scales.\nBy default, R uses a standard theme. You can get that standard theme using theme_get(). Here the code is not evaluated as R returns a nested list of 136. However if you run that code in the console you’ll see all list components. Themes are, in other words, lists.\n\ntheme_get()\n\nYou can set the theme - R will override all themes you use in your code - using the theme_set() function.\n\ntheme_set(theme_light())\n\nUsing an existing theme, you can add modifications to specific components using the theme_update() functions. To do so, it is a good idea to first make a copy of the theme you want to change. To do so, you can assing the “old” theme to a new theme name and use theme_set(new_name_theme). All updates will now happen with the copy of the old theme. Doing so, you can always reuse that old theme. Using theme_update() you can not modify specific parts of the theme. All other theme elements are unaffected. In other words, this function is ideal is you want to add a couple of modifications to some theme elements, but are satisfied with the other ones. To use this function, you set a new theme name and add the changes to the elements. For instance, suppose that you want to change the background color of the panel to white, add margins between the panels and the background if 1 cm on each side, set the font size and families for the title, labels, captions, change the major grid lines, remove minor gridlines, … but you are satisfied with all other elements of the “old” theme. You can run:\n\nnew_name_theme &lt;- theme_update(\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    plot.margin = margin(1, 1, 1, 1, \"cm\"), \n    plot.title = element_text(size = 12, family = \"serif\", face = \"bold\", color = \"darkgrey\"), \n    plot.title.position = \"panel\",\n    plot.subtitle = element_text(size = 10, family = \"sans\", face = \"italic\", color = \"grey\"), \n    plot.caption = element_text(size = 8, face = \"bold.italic\", color = \"lightgrey\"),\n    panel.background = element_rect(fill = \"white\"), \n    panel.grid.major.x = element_line(color = \"#F2F1F1\", linetype = \"solid\", linewidth = 0.5),\n    panel.grid.major.y = element_line(color = \"#F2F1F1\", linetype = \"solid\", linewidth = 0.5),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.line.x = element_line(color = \"#D6D5D5\", linewidth = 1),\n    axis.line.y = element_line(color = \"#D6D5D5\", linewidth = 1),\n    axis.title.x = element_text(color = \"darkgrey\", size = 10, family = \"serif\", face = \"italic\"), \n    axis.title.y = element_text(color = \"darkgrey\", size = 10, family = \"serif\", face = \"italic\"), \n    axis.text.x = element_text(color = \"lightgrey\", size = 10, family = \"serif\", face = \"plain\"), \n    axis.text.y = element_text(color = \"lightgrey\", size = 10, family = \"serif\", face = \"plain\"), \n    legend.background = element_rect(fill = \"white\"), \n    legend.title = element_text(size = 10, face = \"bold\", color = \"#CCCCFF\"), \n    legend.key = element_rect(fill = \"white\"),\n    legend.byrow = FALSE, \n    legend.position = \"right\")\n\nDoing so, you update the theme with your personal style. If you add + new_name_theme() after the plot, these values will apply.\nHowever, this function requires that you know which elements you can actually change. This is what we”ll show next. Note that you don’t have to know all these. It is actually sufficient to now that you can change a lot and how you can do that in case you need to. Using theme_update() you start from an existing theme and update same of the values. To design a theme from scratch, you can use\n\n\n11.5.2 Theme elements\nThemes change the elements of the plot using 4 functions:\n\nelement_text() allows you to set the font (family, face, color, size (in points)) and the adjustment (hjust, vjust, angle) . There are three font families that work everywhere: “sans” (the default), “serif” and “mono”. For other families, the font in one system (e.g. Windows) will not always show in the same way on another system (e.g. Mac). The face is “plain”, “bold”, “italic” and “bold.italic”. For the font size, which is measured in points (1pt = 0.35 mm). We alreay covered the justification: hjust and vjust: you can set them using (hjust) 0 for left, 0.5 for center and 1 for right and vertical alignment you have 1 for top, 0.5 for middle and 1 for bottom. The angle is measured in degrees.\nelement_line() allows you to set parameters for all line elements in the themes. Here, you can set color, and line width and line type.\nelement_rect is used to set parameters for all surfaces and their borders, e.g. the panel background, plot background, … . You can set the fill and border colors (fill and color), line width and line type.\nelement_blank() to draw noting, e.g. to remove labels from the axis.\n\nTo set all text elements, you can use theme(text = element_text()). Doing so allows you to specify e.g. the font family, the size of the basic font, … all subsequent uses of element_text() will inherit these values. Doing so, you only need to specify the changes relative to these default values. In some cases, you can even include rel(1.5) to set the size of a specific text component equal to 1.5 times the default size. The same holds for line, rect and title (all titles including axis, …).\nLet’s look at the various components of the non-data parts. First there are the axis. Most theme elements include elements such as axis.title and axis.ticks but also axis.title.x , axis.title.y, axis.title.x.top and axis.title.x.bottom or axis.ticks.x and axis.ticks.y, … If you specify the elements for axis.title of axis.ticks you do so for all axis titles or ticks. If you need a different style of text or tick, you have to use axis.title.x and axis.title.y.\nA plot includes a panel, axis including labels, a title, subtitle, a caption and tags and the guides. These are plotted against a background. The plot.background refers to the entire area of the plot. Using element_rect() you can change the fill color and the border (line color, width and type). Within the plot background, you find the other components. Using plot_margin you can set the top, right, bottom and left (starting at the top and moving clockwise) in units. These two elements set the background, border and margins:\n\nplot.background  \nplot.margin\n\nYou have now defined the background of the plot: the background color, a border and the area within which the plot will be shown.\nThe title, subtitle, captions are set using element_text() and their position using two values: “panel” (default) and “plot”. The latter aligns their position with the plot, the former with the panel. For the tag (e.g. Fig. 1), the location is “panel”, “plot” or “margin” (in the panel area, in the plot area or in the margins) and the position is “topleft”, “top”, “topright”, “left”, “right”, … . You can set tags using the labs() function.\n\nplot.title\nplot.title.position\nplot.subtitle\nplot.caption\nplot.caption.position\nplot.tag\nplot.tag.position\nplot.tag.location\n\nThe next component is the panel area which includes the panel, as well as axis, axis labels, … . With respect to the panel you can set the background color and border as well as the major and minor grid lines. For the first you need element_rect(), for the grid lines, you need element_line(). You can set the following elements of the panel:\n\npanel.background\npanel.border\npanel.grid\npanel.grid.major\npanel.grid.minor\npanel.grid.major.x\npanel.grid.major.y\npanel.grid.minor.x\npanel.grid.minor.y\n\nWith multiple panels (facets) you can also set the distance between subpanels in units():\n\npanel.spacing\npanel.spacing.x\npanel.spacing.y\n\nWith respect to the axis titles, you use element_text() to set the font family, face, size and alignment. You can set these for the following axis elements:\n\naxis.title\naxis.title.x\naxis.title.x.top\naxis.title.x.bottom\naxis.title.y\naxis.title.y.left\naxis.title.y.right\naxis.text\naxis.text.x\naxis.text.x.top\naxis.text.x.bottom\naxis.text.y\naxis.text.y.left\naxis.text.y.right\naxis.text.theta\naxis.text.r\n\nTo adjust the axis lines, you use element_line() to adjust line width, line type, … . If you use element_blank() no lines are shown. The axis lines include:\n\naxis.line\naxis.line.x\naxis.line.x.top\naxis.line.x.bottom\naxis.line.y\naxis.line.y.left\naxis.line.y.right\naxis.line.theta\naxis.line.r\n\nYou can change the color, … of the ticks and minor ticks using `element_line(). These tick elements are\n\naxis.ticks\naxis.ticks.x\naxis.ticks.x.top\naxis.ticks.x.bottom\naxis.ticks.y\naxis.ticks.y.left\naxis.ticks.y.right\naxis.ticks.theta\naxis.ticks.r\naxis.minor.ticks.x.top\naxis.minor.ticks.x.bottom\naxis.minor.ticks.y.left\naxis.minor.ticks.y.right\naxis.minor.ticks.theta\naxis.minor.ticks.r\n\nYou can set tick mark lengths using (unit(x, \"cm\") or unit(y, \"mm\") with default points:\n\naxis.ticks.length\naxis.ticks.length.x\naxis.ticks.length.x.top\naxis.ticks.length.x.bottom\naxis.ticks.length.y\naxis.ticks.length.y.left\naxis.ticks.length.y.right\naxis.ticks.length.theta\naxis.ticks.length.r\naxis.minor.ticks.length\naxis.minor.ticks.length.x\naxis.minor.ticks.length.x.top\naxis.minor.ticks.length.x.bottom\naxis.minor.ticks.length.y\naxis.minor.ticks.length.y.left\naxis.minor.ticks.length.y.right\naxis.minor.ticks.length.theta\naxis.minor.ticks.length.r\n\nWe can now move to the guides (legends). First you can set the position of the legend (legend.position) at the “left”, “right” “bottom” or “top” of the panel or “inside” the panel. In the latter case, you need to specify a vector with the x,y position of the legend in legend.position.inside. The orientation of the legend “vertical” or “horizontal” is set using legend.direction. With multiple legends, these can be shown by row or by column. Using legend.byrow = FALSE multiple legends are organized in one column. You can set the spacing between legends and the margins around the legends.\nUsing element_text() you can change the titles (legend.title) and the legend item labels (legend.text). You can also change the position of the title (legend.title.position) relative to the main title and the position of the legend text (legend.text.position) relative to the legend keys (“top”, “right”, “bottom”, “left”. You can adjust the background color of both the legend as well as the background underneath the legend keys. You can set multiple dimensions of the legend keys.\nThe position the legend relative to the panel of the plot, you can use the legend justification elements. A box adds a box around all legends.\n\nlegend.position\nlegend.position.inside\nlegend.direction\nlegend.byrow\nlegend.margin\nlegend.spacing\nlegend.spacing.x\nlegend.spacing.y\nlegend.text\nlegend.text.position\nlegend.title\nlegend.title.position\nlegend.background\nlegend.key\nlegend.key.size\nlegend.key.height\nlegend.key.width\nlegend.key.spacing\nlegend.key.spacing.x\nlegend.key.spacing.y\nlegend.frame\nlegend.ticks\nlegend.ticks.length\nlegend.axis.line\nlegend.justification\nlegend.justification.top\nlegend.justification.bottom\nlegend.justification.left\nlegend.justification.right\nlegend.justification.inside\nlegend.location\nlegend.box\nlegend.box.just\nlegend.box.margin\nlegend.box.background\nlegend.box.spacing\n\nWith facets, you have further options to define the layout of these facts, including, e.g. the color and background of the value strips and the text labels for the x- and y- axis.\n\n strip.background\n strip.background.x\n strip.background.y\n strip.clip\n strip.placement\n strip.text\n strip.text.x\n strip.text.x.bottom\n strip.text.x.top\n strip.text.y\n strip.text.y.left\n strip.text.y.right\n strip.switch.pad.grid\n strip.switch.pad.wrap\n\n\n\n11.5.3 Branding your visualization\nMany organizations have a logo, a brand name, … you can add these to your visualization. To see how, I refer to e.g. [You Need to Start Branding Your Graphs. Here’s How, with ggplot!] (https://rpubs.com/mclaire19/ggplot2-custom-themes) for a simple illustration of how you can add a logo to your visualization.\n\n\n\n\n\n\nArnold, Jeffrey B. 2024. Ggthemes: Extra Themes, Scales and Geoms for ’Ggplot2’. https://jrnold.github.io/ggthemes/.\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot 2: the appearance and lay out of a plot</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html#first-things-first-saving-a-plot",
    "href": "10_Plot_types.html#first-things-first-saving-a-plot",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "10.2 First things first: saving a plot",
    "text": "10.2 First things first: saving a plot\nIn this and the next chapter, we will design plots. At the end of your work, you will probably want to save your visualizatin to use in a powerpoint, add in a paper, … . There are many ways to do so, e.g. add your plot to a powerpoint from within R, but often it is a good idea to save the plot. To do so, you first assign the plot to a name using plotname &lt;- ggplot() .... R will not show the output but you’ll see plotname in your environment as a list. Using ggsave() you can now save this plot. This function has the following arguments:\n\nggsave(\n  filename,\n  plot = last_plot(),\n  device = NULL,\n  path = NULL,\n  scale = 1,\n  width = NA,\n  height = NA,\n  units = c(\"in\", \"cm\", \"mm\", \"px\"),\n  dpi = 300,\n  limitsize = TRUE,\n  bg = NULL,\n  create.dir = FALSE,\n  ...\n)\n\nThe filename refers to the name of the file where you will store your plot. You can save a plot as a “png”, “eps”, “ps”, “pdf”, “tex”, “jpeg”, “tiff”, “bmp”, “svg” of “wmf” file. By default, R will save the last plot. Here, you can add the name of your plot, e.g. plot = plotname. Using {here} you can set the path where the fill will be stored. The device allows you to specify the device that will be used to turn your plot in an “pgn”, “eps”, … file. If the name of the file includes and extension, the device will be the device for that extension. If you specify a device, you can add further arguments in the [device function] (https://rdrr.io/r/grDevices/unix/png.html)/. You can also add the path using {here} in the filename. The other parameter determine the size of your plot. By default R sets the width and height using your current graphics device. DPI is the resolution of the plot. By default R used 300 dpi which is suitable for most printers, for web pages, you can reduce the resolution; for high quality print, you my need to set this to 600. If you directory doesn’t exist, changing create.dir to TRUE allows R to create a directory.\nUsually, you can save a plot using\n\nggsave(here::here(\"reports\", \"name_of_plot_file.pgn\", plot = plotname))\n\nIf this doesn’t work on your system, try another format. There might be some differences between Windows, Linux and Mac. Usually, you’ll fine a solution quite fast on e.g. stackoverflow if you keep running into issues after saving the file multiple times in a differen format. Note that sometimes opening the plot by clicking on the name in the R Files pane will not show a good result. Usually, to see the end result, opening outside of R or import it in e.g. your word processor or presentation software. Further, if you happen to use Latex (pronounce: latech) e.g. using Overleaf, you can save a plot as tex file and import it into your report, presentation of paper.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html#the-data-and-aesthetic-mappings",
    "href": "10_Plot_types.html#the-data-and-aesthetic-mappings",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "10.3 The data and aesthetic mappings",
    "text": "10.3 The data and aesthetic mappings\nWe will use a various datasets to illustrate the visualizations. In addition to the data in {nycflights23}, and Life expectancy at birth from the World Bank’s data development indicators database (Chapter 2), we will also use the diamonds dataset which comes with your {ggplot2} installation. This dataset includes the price of 53940 diamonds, measured in USD, including their attributes: carat, cut, color and clarity. The attribute carat refers to the weight of the diamond (with 1 carat of ct equal to 0.2 grams). The cut, color and clarity measure the quality of the cut (fair, good, very good, premium or ideal). The cut determines the brightness of a diamond, the dispersion of light and sparkle. The color is graded from D (best) to J (worst) and shows how colorless (best) a diamond is. The clarity show how clear a diamond is and is measures drom IF (best), VVS1, VVS2, VS1, VS2, SI1, SI2 and I1. The size of the diamond is measured as the length (x), width (y) and depth or height of the diamond (z) as well as the table, which measures the width of the top of the diamond relative to the widest point and the depth, a the relative height of the diamond to its widest point relative to its total height. You can find more information on these measures online. Because it is often not necessary to show all the data in the diamonds dataset, we’ll often use a sample of 10%. The data for life expectancy is in the data &gt; raw folder in a file life_df.csv. You can import that file and assign it to life_df.\n\nlife_df &lt;- readr::read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"), show_col_types = FALSE)\n\nTo build a plot, ggplot() needs to know “what” to plot. In other words, you need to tell R where the data is (in which data frame) and which variables in that data frame will be used in the visualization. This is the first layer of a plot. To add this first layer, you use the function ggplot(). The arguments of this function are:\n\nggplot(data = NULL, mapping = aes())\n\nThe argument data refers to the default data frame ggplot() use to search for the variables. Recall that {ggplot2} is part of the {tidyverse}. In other words, you can use pipe operator |&gt; to “pipe” a dataset in the ggplot() function. If the object in data is not a data frame or tibble, R will try to convert the object into a data frame. If the argument is missing, each geometry has to specify the data frame to use in that geometry. The second argument mapping = aes(x = , y = , color =, size =, shape =, fill = linewidth = , linetype = ) includes the aesthetic mappings for the plot. This argument too is optional. If this mapping is not specified here, each geometry will needs its own aesthetic mapping. You can partially map variables at this level and add aesthetic mappings in the geometry layer. For instance, here you would map a variable on the x- and y-axis and add a mapping on the color aesthetic in e.g. the point geometry. In that case, the mapping on the x- and y- axis will be used for all geometries in the plot while the color aesthetic will only be used for the point geometry.\nLet’s use a sample of 10% of the diamonds dataset\n\ndia &lt;- diamonds |&gt; slice_sample(prop = 0.10) \n\nand start a plot where we map carat on the horizontal axis and price on the vertical axis:\n\np1 &lt;- ggplot(data = dia, mapping = aes(x = carat, y = price)) + theme_minimal()\np1\n\n\n\n\n\n\n\n\nThis function returns the first layer of the plot: it shows the panel of the plot and the x- and y-axis. In addition, it shows the variables that were mapped on both axis: carat on the horizontal axis and price on the vertical axis. The limits of these axis (minimum and maximum value of the axis) are derived from the data. You can see this from the minimum and maximum values for carat and price:\n\ndia |&gt; summarise(\n  min_carat = min(carat),\n  max_carat = max(carat), \n  min_price = min(price),\n  max_price = max(price)\n)\n\n# A tibble: 1 × 4\n  min_carat max_carat min_price max_price\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;\n1       0.2       3.5       336     18804\n\n\nThe part `aes(x = carat, y = price) includes the aesthetic mapping. To see this, if you run this part separately, R returns the mapping:\n\naes(x = carat, y = price)\n\nAesthetic mapping: \n* `x` -&gt; `carat`\n* `y` -&gt; `price`\n\n\nBecause the aesthetic mapping in ggplot() is second after the data, usually, the part mapping = aes() is shortened by eliminating the reference to the argument mapping and written as aes(). Similarly, because the data argument is always first, data = is usually dropped from the argument and the function call is either ggplot(data, aes() or data |&gt; ggplot(aes()). In the code, theme_minimal() removes e.g. the background color from the panel. We add it here, to remove colors from the background and panel.\nAt this stage, R is not able to add more aesthetic mappings to the output. All R knows at this stage is that the plot will include variable mappings on the x-axis and a y-axis. However, it is not able to show other aesthetic mappings, e.g. on color, size of shape. Here, R needs additional information from the geometry. With a point geometry, ggplot() will show these aesthetic mapping using a different color, size of shape of a point, with a line geometry, R will show these additional mappings by differentiating he lines using their color, width of type. However, if these aesthetic mappings are defined at this stage, R will include them in any subsequent geometry. The output from the aes() argument shows that cut is also mapped on the color. aesthetic.\n\nggplot(dia, aes(x = carat, y = price, color = cut, size = clarity, shape = color))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\n\n\n\n\n\n\naes(x = carat, y = price, color = cut, size = clarity, shape = color)\n\nAesthetic mapping: \n* `x`      -&gt; `carat`\n* `y`      -&gt; `price`\n* `colour` -&gt; `cut`\n* `size`   -&gt; `clarity`\n* `shape`  -&gt; `color`\n\n\nYou can map the same variable on two ore more aesthetics. For instance, mapping cut on size and shape will show every level of but with a different size and a different shape.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html#geometry",
    "href": "10_Plot_types.html#geometry",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "10.4 Geometry",
    "text": "10.4 Geometry\nThe first layer includes the data and the aesthetic mappings that will be used for all geometries in the plot (unless another aesthetic is specified). At this stage, we know which variable R will show using which aesthetics. We also know where R will find these variables: in the data frame included in the data argument of the ggplot() function. We don’t know how these variables will shown. To add this component, we need an additional layer: the one that includes the geometry and the statistics. In other words, we need a plot type. {ggplot2} includes many geometries or types of plots and I refer to the geometry section in the {ggplot2} and Chang (2025) or to view all these possibilities. Here, you’ll find the often used geometries: point geometries (e.g. scatter plots), line geometries (a line graph), area geometries, bar and column charts and geometries that summarize the data using e.g. a boxplot or a density.\nNote that there are many ways to visualize the same data. The same aesthetics (x- and y-axis, color, fill, line type of width, shape, size or alpha) can be used with many geometries. However, not all geometries allow for the same aesthetics. A point geometry - where you use a “dot” to show combinations of values for the variables mapped on both axis - allows you to include color, shape, size of transparency. However light width of type are not relevant. For a bar chart, you can include fill, color, transparency, but shape would be irrelevant.\nSecond, every geometry is a layer. This allows you to add geometries on top of one another. Doing so, you can add a line geometry to a point geometry or text geometry. Doing so, allows you to build complex data visualizations. Here, we will not focus too much on how you can do so, but once you master the individual geometries, adding additional ones is straightforward. Usually, you can show the data in numerous ways. To illustrate, here are [100 visualizations] (https://100.datavizproject.com/) of the same dataset: a small table showing the number of World Heritage sites in Denmark, Norway and Sweden for 2004 and 2022. In other words, there are 100 ways to visualize data in a table with 2 rows and 6 columns. Selecting the best geometry to visualize your data is very important. Here are many guides to help you select the appropriate geometry, e.g. [from Data to Viz] (https://www.data-to-viz.com/), [Visual Vocabulary] (https://ft-interactive.github.io/visual-vocabulary/) or the guide from the [UK’s Office for national statistics] (https://service-manual.ons.gov.uk/data-visualisation/chart-types). If you add various geometries as separate layers, you have to think about the order in which they are shown: a line geometry followed by a point geometry will show the points on top of the lines.\n\n10.4.0.1 Point geometries\nA point geometry is used to show the correlation between two numeric, continuous variables. The first variable is mapped on the horizontal axis, the second on the vertical axis. These two, x and y, are required aesthetics. For every observations, a point geometry shows the pair of values for the variable mapped on the x-axis and the variable mapped on the y-axis as a single dot. Note that you shouldn’t interpret “dot” in a literal sense as you can change that representation and use e.g. a cross, … .\n\n10.4.0.1.1 geom_point()\nMost geometries share a large number of arguments. So, we will discuss this geometry in detail. Doing so, when we introduce other geometries we can focus on those arguments that are particular for a geometry.\n\n10.4.0.1.1.1 The function\nTo illustrate point geometries, let’s use the point geometry geom_point(). The function includes the following arguments:\n\ngeom_point(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe arguments for geom_point() include an aesthetic mapping argument mapping, a data argument data, a stats argument with default identity and a position argument with default identity. Recall from Chapter 9 that one of the elements of the grammer of graphics refers to the statistics. Here stat = \"identity\" means that R will plot the values of the series: the “statitics” R plots are identical to the values in the dataset. For other geometries, we’ll see for instance stat = \"count\". Here, R will not plot the values, but will “count” the number of observations. From position = \"identity\", you can see that R will also plot the values on their exact location. In other words, a pair (1.5, 2.5) will be shown in the location that corresponds with 1.5 on the horizontal axis and 2.5 on the vertical axis. The exact location is where the line starting in 1.5 on the x-axis and moving up crosses the line starting in 2.5 on the y-axis and moving left. We’ll see other position values, e.g. “jitter”, where R add a random component to both the x and y value, “stack” telling R to “stack” the values onen for one, … .\nThe value NULL for the first two arguments means that R will use the default aesthetic mappings included in the ggplot(aes()) call. geom_point() needs at least two mappings: one on the x- and one on the y-axis. If these mappings are not included in the ggplot() call, you have to add them here. You can use the mapping argument in this geometry also to add additional aesthetic mappings. The inherit.aes = TRUE argument shows that R will use the aesthetic mappings from the ggplot(aes()) call. If you wouldn’t want this, changing this into FALSE will remove these aesthetic mapping for that layer. Doing so, you need to add new aesthetic mappings to this geometry. With FALSE, the point geometry will not add the mapping included in mapping to the mappings in the ggplot(aes()) call. The stats argument allows you to define transformations. The default identity shows the the data as they are. The position argument with default identity shows the points as they are in the data: every “dot” is shown in the panel exactly on value pair spot. However in some cases, one point actually masks two ore more points with the same value pair. Adding position = position_jitter() R will add a random variation to every point to the left or right and to the top or bottom. You can define the maximum width and height of that random variation using the arguments width = and height =. A third option for position = is stack. We’ll cover that position more in depth when we discuss the area geometry geom_area().\nIn addition to these arguments, you can further specify the way in which R will plot the points, e.g. their color, fill, shape, size, stroke and alpha. You can further add the option na.rm  = TRUE. In that case, ggplot() will remove missing values without a warning. The default FALSE shows a warning. By default, ggplot() will add new mappings in this layer to the guide or legend. If you don’t want to do so, you need to set inherit.aes = FALSE. The ... part allows you to define the settings: for instance, the color, size, shape or transparency of a “dot”. As we’ll see, you shouldn’t confuse setting with mapping. Both use the same reference: color, size, shape, linewidth, fill, … However, a setting such as color is part of the lay out of a graph. It tells R which color to use to show a dot. This color can be red, blue, yellow, grey or any other color that is avaiable. R will show all dots using the same color. Using the aesthetic color you map a variable on that aesthetic. Here R will show a different color for every value in the variable that you map on the aesthetic color. In other words, with a mapping the color has a meaning: different colors show different values. As a setting, a color doesn’t offer any additional information with respect to the data as all dots are shown in the same color.\nLet’s see what these arguments do. We start from ggplot(data = dia, mapping = aes(x = carat, y = price)) and add the geom_point() layer, accepting all defaults:\n\np1 + geom_point()\n\n\n\n\n\n\n\n\nRecall that p1 didn’t include the aesthetic mapping on color. In other words, this point geometry only includes the aesthetic mappings on the horizontal and vertical axis. By default, R uses black dots to represent each (carat, price) pair. Every dot shows one observation. The shape of the cloud illustrates the correlation between the variable on the horizontal and the one on the vertical axis. In Figure 10.1, there are six patterns shown in six panel. The first panel, shows no correlation. The point cloud does not show any pattern. The second panel include a pattern where the two variables are correlated, but not in a linear way. Using traditional measure for correlation, the result would suggest the absence of any correlation between variable 1 and variable 2. However, as panel 2 reveals: the correlation is actually strong, but not linear. Panels 3 to 6 show clouds that suggest weak positive correlation (panel 3), weak negative correlation (panel 4) and strong positive (panel 5) and negative (panel 6) correlation. These panel all show a point geometry. In other words, using a point geometry allows you to detect a pattern in the correlation between two variables, even if that correlation is not linear.\n\n\n\n\n\nFigure 10.1: Correlation patterns\n\n\n\n\n\n\n\n\n\n\n10.4.0.1.1.2 Changing the settings\nBefore we add additional aesthetic mappings, let’s first review what the options are for changing the settings of this plot. The options for the settings are usually also available for the mappings. Recall that settings don’t add any new information to the plot and only change the way the plot is shown. By default, R shows the dots with a black color. You can change the color of the points into e.g. red. You can do so by adding color = \"red\" to geom_point():\n\np1 + geom_point(color = \"red\")\n\n\n\n\n\n\n\n\nAs an alternative, you can use “lightyellow”:\n\np1 + geom_point(color = \"lightyellow\")\n\n\n\n\n\n\n\n\nHere, we identify the colors by their name. As an alternative to the name you can also add a color’s HEX code. For instance, the color “lightsteelblue” has HEX code “#BEC4DE”. You can enter this hex code to define the color:\n\np1 + geom_point(color = \"#BEC4DE\")\n\n\n\n\n\n\n\n\nHere too, R shows the color. The hex code and name are equivalent:\n\np1 + geom_point(color = \"lightsteelblue\")\n\n\n\n\n\n\n\n\nTo find a color, you can use online color pickers. These usually allow you to identify a color on a wheel or picture and return various color codes: HEX, RGB, … If you copy paste the HEX code, R will show exactly that colo. In addition, most of these color pickers allows you to generate complement colors, shades, … Here for instance, you can find HEX codes. Using their detail, you can find complements, … .\nUsing alpha, you can change the transparency of a point. This value is between 0 (absolute transparency) and 1 (no transparency). To illustrate, using the color red and transparency 1/5:\n\np1 + geom_point(color = \"red\", alpha = 1/5)\n\n\n\n\n\n\n\n\nThe plot now shows for which values the dataset includes a lot of observations. As these single, transparent, dots overlay, they produce a brighter color. Here, you can see that the dataset includes a lot of observations for diamonds with a lighter weight and lower price.\nTo adjust the size of the dots, you can use the size setting. For instance, setting the size = 5 shows much larger dots:\n\np1 + geom_point(color = \"orange\", size = 5)\n\n\n\n\n\n\n\n\nUsing size = 0.75 much smaller\n\np1 + geom_point(color = \"orange\", size = 0.75)\n\n\n\n\n\n\n\n\nIn addition to the color and size, you can also change the shape using shape =. To identify a shape, you can refer to its number (shown in Figure 10.2) or name (shown in Figure 10.3).\n\n\n\n\n\nFigure 10.2: Shapes by number\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Shapes by name\n\n\n\n\n\n\n\n\nLet’s show the “dots” with a cross (shape 4):\n\np1 + geom_point(shape = 4)\n\n\n\n\n\n\n\n\nNote that the default color is the cross is black. Combining both the color (e.g. lightsteelblue) and shape setting:\n\np1 + geom_point(color = \"#BEC4DE\", shape = 4)\n\n\n\n\n\n\n\n\nShapes 21 to 24 can be further adjusted using color, fill and stroke settings. The first, color sets the color of the border, the second, fill, the color of the interior and stroke the width of the border. If you add size, this setting controls the size of the interior part. The total size the shape is the sum of the interior size and the stroke. You can see this in Figure 10.4. For instance, to draw a circle with an orange border, with stroke 2 and a light blue steel interior with size 4.\n\np1 + geom_point( shape = 23, color = \"orange\", fill = \"#BEC4DE\", size = 4, stroke = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.4: Size and stroke\n\n\n\n\n\n\n\n\nIn addition to the shapes that are shown in Figure 10.2, you can add unicode UTF shapes. All characters, shapes, symbols, … have unicode “code”. You can find all codes for [geometric shapes] (https://www.w3schools.com/charsets/ref_utf_geometric.asp). Suppose I you want to use a white diamond containing a black small diamond as a shape. The unicode is “25C8”. You can add this code using “u25C7”\n\np1 + geom_point( shape = \"\\u25C7\", color = \"blue\", size = 2)\n\n\n\n\n\n\n\n\nThe list of unicode symbols include math symbols, currency symbols, weather symbols, emoji’s, … .\nSettings can be used to highlight specific values. Suppose for instance that you want to highlight a specific level of cut (e.g. “Premium”) in the price - carat plot. To do so, we first plot the plot and use the color settings to show these points in light grey. You can then add a second point geometry, but use a filtered dataset. To do so , you introduce a new dataset in this second point geometry using filter(dia, cut == \"Premium\"). Recall from Chapter 8, that this function returns a dataset. The second point geometry inherits the x and y values from the ggplot() call. In other words, the second point geometry map price and carat on the x-axis and y-axis. However, this dataset only includes the data for cut == \"Premium\" diamonds. Adding a different color as a setting, these points will be shown in another color. For instance:\n\np1 +\n  geom_point(color = \"lightgrey\") +\n  geom_point(data = filter(dia, cut == \"Premium\"), color = \"steelblue\")\n\n\n\n\n\n\n\n\nAs we will see, you can now add an annotation:\n\np1 +\n  geom_point(color = \"lightgrey\") +\n  geom_point(data = filter(dia, cut == \"Premium\"), color = \"steelblue\") +\n  annotate(\"text\", x = 0.5, y = 15000, label = \"Premium cut diamonds \\n shown in color\", color = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n10.4.0.1.1.3 Aesthetic mappings\nAdding aesthetic mappings, in contract to settings, change the information that is shown in the plot. You add these mappings in the aes() function. Doing so, ggplot() will show different colors, shapes, sizes or strokes for every value of the variable that was mapped on that aesthetic. To illustrate, let’s again start from p1 and map the variable cut on the aesthetic color. To do so, we add aes(color = cut) to the geom_point() function and keep all other default values:\n\np1 + geom_point(aes(color = cut))\n\n\n\n\n\n\n\n\nThe result now show different colors per level of cut: diamonds whose cut is ideal are shown with a yellow dot, premium with a light green dot, … . By default, R adds a guide or legend which shows these values. Adding show.legend = FALSE would remove that legend. However, in that case, it would be difficult to interpret the color scale. Note how the plot shows each aesthetic mapping: the variable that was mapped on the horizontal axis is shown as a label under that axis, the variable that was mapped on the vertical axis is shown as a label with the vertical axis and the variable that was mapped on the color aesthetic is shown in the legend.\nAt this stage, adding the aesthetic mapping in the geom_point() function or in the ggplot(aes()) function shows similar results:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is due to the fact that all layers that follow the ggplot() function inherit the aesthetic mapping defined there. Without any aesthetic mapping in ggplot() you would have to add it in the geom_point() function:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn that case, the mapping is only relevant for the point geometry.\nWhat if you add aesthetic mapping on color as well as set a setting for color? In that case, R overrides the aesthetic mapping and shows all colors in the color defined in the setting. For instance:\n\np1 + geom_point(aes(color = cut), color = \"red\")\n\n\n\n\n\n\n\n\nIn the result, you can see that R shows all dots in red. In addition, it removes the legend. You can however, specify other settings, e.g. if you map cut on the aesthetic color, you can still change the setting shape. R will now show every point using the same shape and will use various colors to show the values of the variable cut:\n\np1 + geom_point(aes(color = cut), shape = 6)\n\n\n\n\n\n\n\n\nHere, R maps the variable cut on the aesthetic color and shows the “dots” using a triangle.\nIn addition to the color aesthetic, geom_point() also accepts the fill, shape, size and stroke aesthetic. In addition, you can include more mappings in the aes() function. To illustrate. Let’s add another aesthetic mapping and map the variable in the diamonds dataset color (not to be confused with the aesthetic in R, the variable and aesthetic both happen to have the same name) on the shape aesthetic:\n\np1 + geom_point(aes(color = cut, shape = color))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 283 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, R produces a warning: the shape palette can deal with a maximum of 6 discrete values and more than 6 become difficult to differentiate. If you look at the legend, you’ll see that R didn’t include the shape for color “J”. In this case, you could change the mapping and map cut one shape and color on the aesthetic color:\n\np1 + geom_point(aes(color = color, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\nHowever, here too, R shows a warning. What R is essentially telling here is that you are mapping an ordinal variable on a aesthetic that doesn’t allow to show something in an ordered way: a cross is not better or worse than a dot. For ordinal variables, it is better to use another aesthetic that does allow to show the order. Color for instance, can be shown from light to dark and reflect in order in that way. The same holds for size. A better “cut” can be shown with a larger or smaller size. For instance, if you map color on size:\n\np1 + geom_point(aes(color = cut, size = color))\n\n\n\n\n\n\n\n\nthe plot shows color “D” using the smallest dot while “J” is shown using the largest dot. The shape aesthetic allows you to map nominal values. Here, the order is not relevant can R can show these values with different shapes. However, R will restrict the number of nominal values to 6.\nYou can map the same variable on two aesthetics. In that case, R will change both aesthetics in line with the values of the variable that is mapped on both. For instance, if you map cut on both color and shape, ggplot() shows different level of the variable cut using both a different color and shape: “Fair” is sown with a bleu dot, “Premium” with a lightbleu cross, … .\n\np1 + geom_point(aes(color = cut, shape = cut))\n\nWarning: Using shapes for an ordinal variable is not advised\n\n\n\n\n\n\n\n\n\nIn Chapter 11, we see how you can change the colors, size and shape … in the mapping and show them in the color of your choice and not in the default color values.\n\n\n10.4.0.1.1.4 Adding jitter\nIn some cases, points are on top of each other. Consider the following graph:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph doesn’t make much sense, but is nicely shows that in some cases the same (clarity - price) pairs are on top of each other. As you can not see all that that are actually on the graph, this graph does not represent the data very will. In that case, you can add a bit a jitter to each plot. You can do so using the argument position =. By default the position is identity. This default tells ggplot() to draw the points in line with their (x-value, y-value) pair, even if there are points with 100% overlap. Using the jitter option, R adds a random noise to a plot. In other words, every dot is now shown using the (x-value, y-value) pair but for both R adds a random noise term. In other words it actually shows (x-value + noise, y-value + noise). As two points with 100% overlap (i.e. the same (x-value, y-value)) both get a different random noise term, the plot will show them without that overlap. To illustrate:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = \"jitter\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now show more points than without the jitter. This might seem counter-intuitive, but plots with jitter actually show more information than the plot without (see Figure 10.5)\n\n\n\n\n\nFigure 10.5: Adding jitter\n\n\n\n\n\n\n\n\nYou can control the “amount of jitter” using position_jitter(width = NULL, height = NULL, seed = NA). Here, you can add both the width and height of the jitter. Because jitter is added both positive and negative, the total spread is twice the amount in width and height. The default values (also used if you refer to jitter) are 0.40. Doing so, the jitter occupies 80% of the width of the categorical variable. In Figure 10.5 for instance, you can see that the width of the clarity column in the plot with jitter is 80% of the total width of that column. Adding a value of more than 0.5 doesn’t make sense: in that case the width of one column would overlap with the width of another column. However, setting smaller values, reduces the width of the column with jitter. For instance, using 0.25 for both:\n\nggplot(dia, aes(x = clarity, y = price)) +\n  geom_point(position = position_jitter(width = 0.25, height = 0.25)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAdding a seed value, allows you to reproduce the graph with “the same” random jitter.\nA third way to add jitter is to use the geometry geom_jitter() and not the geometry geom_point(). In addition to the arguments for the latter, the former also includes width and height from the position_jitter() function.\n\n\n\n10.4.0.1.2 geom_text() and geom_label()\nA point (or another shape) is one of the ways to show data in a graph. Using text is a second. To illustrate, let’s use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have used this dataset in e.g. Figure 9.1 and Figure 9.2. We will remove some of the aesthetic mapping and scale attributes and start from this plot:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs you can see, we kept the aesthetic mappings on the x- and y-axis and color. However, we dropped the aesthetic mapping on size. We also removed the labels from the axis.\nlife_df includes the name of the country, as well as its ISO3 code. Using geom_text() we can now use these to plot these codes (or names). To do so we add the aesthetic label in the geom_text() function and map the ISO3 code on that aesthetic. Here, the aesthetic label tells where where the values are it needs to use to show the “dots”. In this case, they are in the variable iso3c:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nIn the results, the “dots” are now replaced by each the ISO3 code for each country. The aesthetic mapping on color now shows as a different test color for each region in the dataset. Note that you can include that aesthetic mapping also in the ggplot() function. However, given that the labels are usually very specific to this geometry, they are usually added there and not in the default aesthetics for the entire plot. The function includes a couple of other function that are worthwhile to note:\n\ngeom_text(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  check_overlap = FALSE,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should seem familiar by now. The mapping needs at least a mapping on the x- and y-axis as well as a mapping on the aesthetic label. You can include these mappings in the geom_text() function or define them for all layers in the plot in ggplot(). The options that are new allow you to position the text. The first is check_overlap. By default, this is FALSE. Changing that into TRUE, ggplot() will avoid overlap in the text. nudge_x and nudge_y allow you to add some distance from the text to another geometry. For instance, if your plot also includes geom_point() then using nudge_x will allow some horizontal space, while nudge_y some vertical space between the dot and the text. Size is measured in mm (size_unit = \"mm\" alternatives are pt, cm, or pica pc.). The ... allow you to add setting, e.g font size, font family, color (in case that aesthetic is not used), angle, … . Let’s add a couple of settings, e.g. reduce the font size to 3, check for overlap, set the font family to “mono” or one of the other supported font families (serif, …) or add fontface (plain, bold or italic) or add an angle (22.5°). To illustrate, let’s add the country’s ISO code to the life expectancy plot, using the font family “mono” in italics and with a small angle of 22.5°:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(\n    aes(label = iso3c), \n    size = 3, \n    check_overlap = TRUE, \n    family = \"mono\",\n    fontface = \"italic\",\n    angle = 22.5) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nIn addition to you can use string functions to generate text labels from the aesthetic mappings. For instance, to show the values as (x, y) pairs, you can use paste0 with \"(\", x, \",\", y, \") to show a (, the value from the variable mapped on the x-aesthetic, a “,”, the value mapped on the y-aesthetic and a closing bracket. For life_df, where per capita gdp is mapped on the x-axis, life expectancy at birth on the y-axis - and adding rounding - the code to generate a plot with (x, y) pairs is:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_text(\n    aes(label = paste0(\"(\", round(gdp_capita, 0), \",\", round(life_exp, 1), \")\")), \n    size = 3, \n    check_overlap = TRUE, \n    family = \"mono\") +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nAs an alternative for geom_text() you can use geom_label(). Let’s check the result of this geometry:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_label(aes(label = iso3c)) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nBy default, R includes values for the variable mapped on the label aesthetic but does so in a frame. The function includes the following arguments:\n\ngeom_label(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  parse = FALSE,\n  nudge_x = 0,\n  nudge_y = 0,\n  label.padding = unit(0.25, \"lines\"),\n  label.r = unit(0.15, \"lines\"),\n  label.size = 0.25,\n  size.unit = \"mm\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nIn addition to those for geom_text() these arguments also allow you to adjust the border of the label: the amount of padding in each label (label.padding), the rounding of the corners label.r and the label size. In addition, you can include setting to e.g fill the label. You can do the latter in the aes() function. Doing so, ggplot() fills the labels per color. To illustrate a couple of options. Here we will show the label, where iso3c is mapped on the fill aesthetic, the color of the font is white and the labels have straight corners. The font family is serif. Note that we moved the color aesthetic from the ggplot() call to the fill aesthetic in the geom_label() call.\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp)) +\n  geom_label(\n    aes(label = iso3c, fill = region), \n    color = \"white\", \n    family = \"serif\", \n    size = 3, \n    label.r = unit(0, \"lines\")) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_label()`).\n\n\n\n\n\n\n\n\n\nYou can use these geometries together with geom_point(). Doing so, the graph includes a dot for every observations as well as a label:\n\nggplot(filter(life_df, date == 2000), aes(x = gdp_capita, y = life_exp, color = region)) +\n  geom_point() +\n  geom_text(\n    aes(label = iso3c), \n    check_overlap = TRUE, \n    nudge_x = 0.2, \n    nudge_y = 0.2, \n    size = 3) + \n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n   theme_minimal()\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n{ggrepel} (Slowikowski (2024)) allows you to add more details to labels and text parts of a plot.\n\n\n10.4.0.1.3 geom_smooth()\nAlthough this geometry is not a point geometry, it is often used as an additional layer with a point geometry as it helps in identifying trends in (overplotted) data. To smooth a dataset is to add a function that approximates the important patterns in that dataset leaving out random noise or other rapid changes in the data. The function’s arguments are:\n\ngeom_smooth(\n  mapping = NULL,\n  data = NULL,\n  stat = \"smooth\",\n  position = \"identity\",\n  ...,\n  method = NULL,\n  formula = NULL,\n  se = TRUE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nYou can control more options if you use stat = stat_smooth(). The aesthetics mapping needs at least a mapping on the x- and y- axis. Here, the statistic is smooth. The position = identity shows that the smooth function will show the predicted values for the variable mapped on the y-axis. With respect to the method, you can accept the default values. An analysis of those methods is left for more advanced statistics or econometrics classes. The same holds for the formula argument. By default, geom_smooth() adds a 95 confidence interval. You can change this level using level = 0.90 for a 90% confidence level or 0.99 for a 99% confidence level. This arguments is not a formal part of the geom_smooth() function, but will be used by stat_smooth() if your refer to it to change the stat argument. If you don’t want that, you can change the default se = TRUE in FALSE. The ... allow you to change the settings e.g. color, … .\nLet’s return to the diamonds dataset where carat is mapped on the horizontal axis and price on the vertical axis (p1) and add geom_smooth().\n\np1 + geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe plot shows the relation between carat on the one hand and price on the other. The expected price, given a value for carat, is shown by the blue line. The confidence level is shown as a confidence interval around that blue line. By default, is confidence level is 95%. R also shows the method used to calculate the smooth function. Let’s first change some of the settings, e.g. the line width of the blue line (linewidth =) and the color of both the line (color) and the background of the confidence interval (fill).\n\np1 + \n  geom_smooth(\n    level = 0.90, \n    linewidth = 0.75, \n    linetype = \"solid\", \n    color = \"#F59247\", \n    fill = \"#F3Be96\") +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nOften geom_smooth() is added as a layer in addition to geom_point(). This produces the following graph (where I add some transparency to the points in geom_point():\n\np1 + \n  geom_point(alpha = 1/10) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nLet’s see what happens if we add another aesthetic in the ggplot() call and map cut on the aesthetic color:\n\nggplot(dia, aes(x = carat, y = price, color = cut)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot now shows 5 smoothed lines, one per level of cut. The lines are in the same color as the color chosen to map cut. Why did ggplot() return one smoothed function per cut? Recall that the plot defines the aesthetic mappings in the ggplot() call. All other layers inherit this mapping. For geom_smooth() this means that it will plot a smoohted line using the x- and y- values, but will do so for every level of every aesthetic. If you would include the size aesthetic for for clarity, geom_smooth() returns 13 smoothed lines: 5 for each level of cut and 8 for each level of clarity. For the latter, geom_smooth() changes the width of the line to show the level of the aesthetic. Even if we set se = FALSE to remove the confidence levels, this plot is hardly interpretable.\n\nggplot(dia, aes(x = carat, y = price, color = cut, linewidth = clarity)) +\n  geom_point(alpha = 1/2) +\n  geom_smooth(se = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo avoid this while keeping the option to add other aesthetic mapping in addition to x and y, there are various options. First, you don’t define any aesthetic mapping at the level of the plot, but keep these for the individual geometries. As mappings in one geometry are not inherited by the others, geom_smooth() will only smooth using the variables mapped in its aesthetics mapping, e.g. only x and y. To illustrate:\n\nggplot(dia) +\n  geom_point(aes(x = carat, y = price, color = cut)) +\n  geom_smooth(aes(x = carat, y = price)) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere, we map the carat and price variables on the x- and y- axis and cut on the color aesthetic in the geom_point() call. In geom_smooth() we don’t include the mapping on color. Doing so, geom_smooth() only used the values mapped on the x and y axis to calculate the smoothed line. Note that this also allows you to add other aesthetics in geom_smooth(). For instance, you could draw a smoothed function per level of clarity while the points show the level of cut.\nThe second way to handle this is to include the aesthetic mapping at all layers have on common in the ggplot() call while adding those that are specific for each layer to the geometry for that layer. In the example: you add the aesthetic mapping of cut on color in geom_point() while adding not additional mapping to geom_smooth(). As geom_point() inherits the mappings from the ggplot() function, it it adds one addition mapping. geom_smooth() doesn’t add any other mapping. Doing so, it only used the mappings that it inherits:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThere is one method and formula that is worth mentioning: a linear trend. The method to estimate a linear trend is lm of linear model. This is the method you would use to estimate linear regression models. The formula is y ~ x. This too is the formula you would use for bivariate linear regression models. Adding both allow you to add a linear trend:\n\nggplot(dia, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n10.4.0.2 Line geometries\nIn the previous section, geom_smooth() resulted in a line and is essentially a summary geometry which is shown as a line. Line geometries are ideal to show the evolution of a numeric (double) variable. Examples include a firm’s market capitalization, its sales or gross margin or macro-economic data where “evolution” is shown with a date/time variable mapped on the x-axis and the value of the series on the y-axis, … . Every (x,y) pair - e.g. (year, sales) - represents a “dot”. These dots are not shown by they are connected using a line. There three different line geometries: geom_line(), geom_step() and geom_path(). These three differ in the way they show the data. The first shows that data in the order of the dataset and uses a continuous line. For instance, if the dataset is ordered per year, the first “point” on the line will show the value of the variable mapped on the y-axis for the earliest year, the second for the second year, … . The second uses the same order to show the data, but uses straight lines to connect the “dots”. The last, geom_path() shows the data in the order in which they appear on the x-axis. If the variable mapped on the x-axis is time, geom_path() and geom_line() or geom_step() are equivalent. However, if the variable mapped on the x-axis is e.g. “gross margin” and the variable on the y-axis is “market capitalization”, geom_path() will use the lowest value for (gross margin, marketcapialization) as the first “dot”. It then connect that dot with the second lowest value for (gross margin, market capitalization), … . In addition geom_vline(), geom_hline(), geom_abline() allow you to draw line segments: a vertical line, a horizontal line or a line with a slope covering the full plot panel. Using geom_segment() and geom_curve() you can draw a line that connects two points within the panel. In general, most of what was written with respect to the aesthetic mapping in the previous section for point geometries, also applies to line geometries and most differences are straightforward: you can define the line width in a line geometry, not in a point geometry, you can have a shape in a point geometry, but not in a line geometry. So, here we’ll focus on what is specific for these geometries.\nYou can think of a line geometry in terms of a point geometry. While the latter shows one dot for each observations, the former implicitly connects these dots using a line. While for point geometry is very useful to show correlation, the line geometry is very useful to show evolution.\nTo illustrate, we’ll use the life_df dataset. In case you haven imported it yet, you can do so using:\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRecall that we have use this dataset in e.g. Figure 9.1 and Figure 9.2. There we used the dataset for a specific year. Here, we will use all data for Costa Rica and Brazil. We’ll also rename the variable date in year.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nWe will start with geom_line() and geom_step().\n\n10.4.0.2.1 geom_line() and geom_step()\ngeom_line() and geom_step() are usually used to show the evolution of one numeric variable where a date/time is mapped on the x-axis and the value is mapped on the y-axis. These two mappings, x and y, are required. You can map other variables on the color of the line, the line width and line type. In addition you can use values of other variables to group lines. The difference between geom_line() and geom_step() is the way they connect the dots. For geom_line() the arguments are:\n\ngeom_line(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nMost arguments of the geom_line() function should be familiar. We’ll use the position = when we discuss geom_area() where we will show how you can “stack” lines in a plot. The orientation = argument is only used if the orientation of the line is not straightforward from the data. The geom_step() functions adds a couple of arguments that are related to the way this functions shows a line: either in a horizontal or vertical segment.\n\ngeom_step(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  direction = \"hv\",\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...\n)\n\nHere, the function needs to know if it first has to move up (vertical) or first moves rights (horizontal). The default direction = \"hv\" first move right than up or down, using “vh” the first movement is up or down, than right. “Mid” means that the step is taken halfway.\nLet’s illustrate these two function for the dataset with life expectancy at birth for Brazil and Costa Rica:\n\ndf_life_cribra &lt;- life_df |&gt; filter(iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nWe first show only one country (Costa Rica). Doing so allows us to keep the aesthetic mapping simple: we’ll map the variable year on the horizontal axis and life expectancy on the vertical axis. For all other arguments, we accept the default values.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot shows the evolution of life expectancy at birth since 1960 for Costa Rica. R adds the variables mapped on the horizontal and vertical axis as labels. The default color for the line is black. Adding a point geometry, you can see that a line geometry is often an extension of a point geometry. However, because here we show an evolution, a line geometry works better. Adding dots doesn’t add any additional value.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nLet’s change the settings, in other words, the visual presentation of the plot without adding any new information. In addition the color and transparency, you can change the line width, the line type (dotted, dashed, …). With respect to the line type, Figure 10.6 shows the 6 most common types: solid, dashed, dotted, dotdash, longdash and twodash. To illustrate these settings, we’ll plot the data for Costa Rica using a blue dotted line with linewidth 1.5 and a level of transparancy (alpha) equal to 0.50.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_line(color = \"blue\", linewidth = 1.5, linetype = \"dotted\", alpha = 1/2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.6: Line shapes\n\n\n\n\n\n\n\n\nFor the geometry geom_step() with the exception of the the direction of the steps (hv, vh or mid), all other settings are similar. However, in this case, the result is not a continuous line, but a line that moves in discrete steps from one point to the other. To illustrate the latter, a point geometry shows the exact location of each observation.\n\ndf_life_cribra |&gt; filter(iso3c == \"CRI\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(color = \"blue\", direction = \"vh\") +\n  geom_point(color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot also illustrates the direction. With vh chosen here, the movement from one point to the other starts with the vertical movement. As the line reaches the y-level of the next point, the line graph moves in horizontal direction. With ‘hv’, the first movement would be horizontal. Using mid, the vertical movement starts in the middle of the horizontal movement.\nFor both geom_line() and geom_step() and in addition to the required x and y mapping, you can map additional variable on the color, line type and width aesthetics. You can do so in the ggplot() function as well as in the geom_line() or geom_step() functions. The difference between both was discusses in the section on the point geometry. Here, we’ll use the life expectancy dataset with both Brazil and Costa Rica and map these countries on the color aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nthe linetype aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linetype = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nand the linewidth aesthetic:\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp)) +\n  geom_step(aes(linewidth = country)) +\n  theme_minimal()\n\nWarning: Using linewidth for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nThe results would be similar for the geom_line() geometry. In Chapter 11, we see how you can change the colors, line width and type, … in the mapping. There, you’ll see how you can change e.g. the values used to set the line width or determine the color of the line.\nThe group aesthetic is useful is you want to show the evolution for a large number of values. Suppose that you want to show the evolution of life expectancy for all countries in the dataset. As the number of countries is very large, mapping the country variable on an aesthetic such as color, linewidth or linetype is usually not a good option. First, there are way too many colors to allow for a meaningful difference across countries. This is where the group aesthetic can help you out. To see what it does, in the next figure, we’ll map the variable country on the group aesthetic while we keep the x values for the date and the y variable for life expectancy:\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe result is a line graph with all lines shown in the same (default) color, type and width. There is one line per country. You can combine the group aesthetic with e.g. a color aesthetic. Using the previous graph, you could map, e.g. the region variable on the color aesthetic. In that way, all countries in the same region would be shown using the same color, with differences colors per region.\n\nlife_df |&gt;\n  ggplot(aes(x = date, y = life_exp)) +\n  geom_line(aes(color = region, group = country)) +\n  theme_minimal()\n\nWarning: Removed 534 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nUsing {[gghighlight] (https://yutannihilation.github.io/gghighlight/index.html)} you can highlight some some countries. For ways to do so, see the documentation (Yutani (2024)).\nIf series are very volatile, adding the geom_smooth() allows you to filter out noise. To illustrate, we’ll add a smooth function for both Brazil and Costa Rica. We’ll accept all default values. Using the linetype of linewidth options in the geom_smooth() geometry, you could change the linetype of e.g. the smoothed line and make it a bit thinner than the line showing the actual data (or vice versa).\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, color = country)) +\n  geom_line() +\n  geom_smooth() +\n  theme_minimal()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n#####geom_path()\nThe previous geometries show the data as they appear in the dataset. This is not always the most useful representation of the data. If you want to show co-movement for instance, geom_line() or geom_step() are not the most suitable options. To illustrate, consider the dataset in data_beveridge.csv. The dataset includes quarterly data for the unemployment rate (the number of unemployed to the number of employed + the number of unemployed) and the vacancy rate (the number of vacancies to the number of jobs in a country). The Beveridge curve shows how both move together: when the economy is solid and growth the good, the unemployment rate should be low and the vacancy rate high. The opposite should be the case in times of recession. The dataset is includes in data &gt; raw &gt; data_beveridge.csv. To import the data, you can run\n\ndata_beveridge &lt;- read_csv(here::here(\"data\", \"raw\", \"data_beveridge.csv\")) \n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): TIME PERIOD, FREQUENCY\ndbl  (2): vacancy_rate, unemployment_rate\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_beveridge &lt;- data_beveridge |&gt; rename(date = DATE, quarter = `TIME PERIOD`)\ndata_beveridge |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 5\n  date       quarter vacancy_rate unemployment_rate FREQUENCY\n  &lt;date&gt;     &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;    \n1 2023-06-30 Q2 2023          3.1              6.49 Q        \n2 2006-03-31 Q1 2006          1.1              8.90 Q        \n3 2010-12-31 Q4 2010          1.4             10.2  Q        \n4 2009-03-31 Q1 2009          1.1              9.05 Q        \n5 2012-09-30 Q3 2012          1.4             11.6  Q        \n\n\nThe data include a variable date showing the last day of the quarter, quarter, showing the quarter and the vacancy_rate and unemployment_rate. One way to show co-movement is to add both to the same line graph. Do do so, we add two line geometries: one for the unemployment rate and one for the vacancy rate.\n\ndata_beveridge |&gt; ggplot(aes(x = date)) +\n  geom_line(aes(y = unemployment_rate),color = \"red\") +\n  geom_line(aes(y = vacancy_rate), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, as you can see from the graph, this is not very helpful. Even if both series are measured in percent and both are usually less than 10%-points removed from one another (i.e. the are close enough to compare), it is difficult to spot co-movement. The vacancy rate’s volatility is much higher than the volatility of the unemployment rate.\ngeom_path allows you to map one series on the horizontal axis and another on the vertical axis. Doing so, you create a point geometry with one dot for every (x-value, y-value) pair. A line now connects all points in the order in which they appear in the for the variable mapped on the horizontal axis. To see how the graph builds, let’s start form the point geometry with the unemployment rate on the horizontal axis and the vacancy rate on the vertical axis:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is a scatterplot with one dot for every observation in the dataset. We don’t know however how both variables moved together. Starting from any dot in the scatter plot, there is no way to tell which dot came in the quarter after that dot. As a matter of fact, there is not way to identify the first dot in the data. This is where geom_path() makes the difference: it connects dots as they appear in the data. In other words, it shows which “dot” came first, which dot came after the first, which one came after the second, … . In short is shows the path from one point to the other: if you start from the start of the line, you can follow the path until is reaches the end of the line. Here, both the geom_path() and the point geometry are shown. However, this does not need to be the case and you can use the line path geometry as a stand alone layer.\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you add a geom_text or geom_label() layer, you can add additional information to the graph. To do so, you need to map a variable on the aesthetic label. In this case, we could use the variable quarter as it shows in which quarter the unemployment rate and vacancy rate were measured. Adding this label adds additional information: you see the movements per quarter. Fitting the label using geom_text() or geom_label() often requires some experimenting with the settings e.g. the size of the font and the nudge left, right, up or down (using the argument nudge_x or nudge_y) or the color of the font:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path() +\n  geom_text(aes(label = quarter), size = 3, color = \"grey\", nudge_x = 0.2, check_overlap = TRUE) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe label is shown in grey, with the check_overlap = TRUE and a nudge to the right to avoid too much overlap with the line. In addition to the options that are similar to those for the other line geometries, geom_path() includes arguments that allows you to determine how the line ends (lineend =), how various parts connect (line_join =) and e.g. the use of an arrow (arrow =).\n\ngeom_path(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  lineend = \"butt\",\n  linejoin = \"round\",\n  linemitre = 10,\n  arrow = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nFor the arrow, changing arrow = NULL in TRUE, add an arrow. You can change the default options if you add arrow = arrow() which allow you to set the angle of the arrow head (the width of the angle), length of the arrow head measured in “inches” or “mm”, if arrows are needed at the end of the line (“last”), at the beginning (“first”) or at both ends (“both”) and if you would like to arrow head to be open or closed.\n\narrow(angle = 30, length = unit(0.25, \"inches\"),\n      ends = \"last\", type = \"open\")\n\nYou can now add an arrow at the end. Here, we draw an arrow with a closed arrow head with length 6 mm and angle 22.5°:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate)) +\n  geom_path(\n    arrow = arrow(angle = 22.5, length = unit(6, \"mm\"), ends = \"last\", type = \"closed\")) +\n  geom_text(aes(label = quarter), size = 3, color = \"grey\", nudge_x = 0.2, check_overlap = TRUE) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the {ggrepel} packages includes a geometry greom_text_repel() that allows you to add, e.g. line segments connecting the text and the dot. If you create a graph using geom_path() and you want to add labels, it is worthwhile to look at the options in this package. Using geom_text_repel you can fine tune most of the label or text parts (in addition to Slowikowski (2024) you can also use “?ggrepel::geom_text_repel” in the console to check all the options). To show only a limited set of possibilities with this package, here is a graph using the geom_text_repel() geometry:\n\ndata_beveridge |&gt; ggplot(aes(x = unemployment_rate, y = vacancy_rate, label = `quarter`)) +\n  geom_path() +\n  geom_point(color = \"red\") +\n  geom_text_repel(\n    size = 3, \n    color = \"#003049\", \n    min.segment.length = 0, \n    seed = 42, \n    box.padding = 0.5, \n    max.overlaps = getOption(\"ggrepel.max.overlaps\", default = 20)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.4.0.2.2 geom_hline(), geom_vline() and geom_abline()\nThese geometries allow you to add horizontal lines, vertical lines and line segments to a plot. Doing so, you can identify e.g. moments in time or or show average values for the y-axis over a longer period of time. The arguments for these three functions are very similar. As an example for geom_hline(), these arguments are\n\ngeom_hline(\n  mapping = NULL,\n  data = NULL,\n  ...,\n  yintercept,\n  na.rm = FALSE,\n  show.legend = NA\n)\n\nIn addition to the usual arguments, this function include the argument yintercept: the value where the horizontal line crosses the vertical axis. The argument is xintercept in geom_vline() equals the value where the vertical line with intersect the horizontal axis. For geom_abline(), you need the intercept, where the line will intersect the vertical axis if the value on the horizontal axis is 0 and the slope, the increase per unit on the horizontal axis. To illustrate the use of these geometries, let’s use the unemployment rate from the data_Beveridge dataset and show this variable using a line geometry:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to add the average for the entire period. The average value equals\n\nave_unemployment &lt;- mean(data_beveridge$unemployment_rate)\n\nWe can add this to the plot using geom_hline().\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  geom_hline(yintercept = ave_unemployment, color = \"lightblue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_vline() you can also highlight the quarter where the major investment bank Lehman Brothers collapsed (third quarter 2008):\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"grey\", linewidth = 0.75) +\n  geom_vline(xintercept = as.Date(\"2008-09-30\"), color = \"orange\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_abline() you can draw any straight line. Suppose that you have a scatter plot looking like Figure 10.7\n\n\n\n\n\nFigure 10.7: Using geom_abline() in a scatter plot\n\n\n\n\n\n\n\n\nand would like top draw a red line with intercept 0 and slope equal to 10. Using geom_abline() as a additional layer, you can add this line to the scatter plot:\n\ndf |&gt; ggplot() +\n  geom_point(aes(x = x, y = z1)) +\n  geom_abline(intercept = 0, slope = 10, color = \"red\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAll lines where drawn crossing the entire plot panel. Using geom_segment() and geom_curve() you can draw straight lines and curves that are shorter.\n\n\n10.4.0.2.3 geom_segment() and geom_curve()\nWith geom_segment() and geom_curve you can connect two points in a graph. To illustrate both, we’ll use the life expectancy dataset and filter observations for 1980 and 2020. Using both geom_segment() and geom_curve() we will connect two points. For every country, the first point is the (per capita gdp, life expectancy) pair in 1980. The second point for each country are the values for the same variables in 2020. The required aesthetic mappings for both geometries are x, the first observations for the variable mapped on the horizontal axis, y, the first observation for the variable mapped on the vertical axis and xend and yend, the last observations of the variables mapped on the horizontal and vertical axis. For the life expectancy dataset, x = gpd per capita in 1980, y = life expectancy in 1980, xend = gdp per capita in 2020 and yend = life expectancy in 2020. Other aesthetic mappings include the color of the line, its width and type and, for geom_curve(), also the curvature.\nLet’s first prepare the dataset: filter the observations of 1980 and 2020 the life_df. In addition, we need a variable for gdp in 1980 and one for gdp in 2020. The same holds for life expectancy. To create these variables, we need to pivot the dataset wider (Chapter 8):\n\nlife_seg &lt;- life_df |&gt; \n  filter(date == 1980 | date == 2020) |&gt; \n  pivot_wider(\n    names_from = c(date), \n    values_from = c(gdp_capita, life_exp, pop),\n    names_glue = \"{date}_{.value}\")\n\nWe can now use life_seg to illustrate geom_segment(). To show the data, we kept the log-transformation on the horizontal axis and added the aesthetic color to show lines in different colors per region:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region)) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nUsually, to show direction, you would add arrows. You can do so using the arrow = argument in both geom_segment() and geom_curve(). Here, the arrow head is closed and is 2.5 mm long, with an angle equal to 11 degrees:\n\nlife_seg |&gt;\n  ggplot() +\n  geom_segment(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\") )) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\nFor every country, can now spot the evolution of both variables between 1980 and 2020. An line and arrow which points to the top right, suggests that both per capita gdp and life expectancy improved over that time period, a line pointing top left suggests that per capita gdp fell (left) but life expectancy rose (top). If the line and arrow suggest a moment down, life expectancy fell while per capita gdp rose (line facing right) or fell (life facing left).\nAnother way to use geom_segment() is to combine it with a point geometry and connect two points. Using the life expectancy data for 1980 and 2020, we can use a point geometry to show both values. We will do so for a sample of 40 countries and first remove observations with missing data for life expectancy in 1980. We map the countries on the vertical axis. We sort them in descending order of life expectancy at birth in 1980. To do so, we use {forcats} fct_reorder() function (Chapter 4). In that way, the countries will be ordered in descending order of life expectancy at birth in 1980. We use geom_point() to show the observations. To do so, we map life expectancy at birth on the horizontal axis and also on the size and color aesthetics. Doing so, higher values for life expectancy in will show up through a larger dot as well as the color of the dot. We drop the legend. We need two point geometries: one for 1980 and one for 2020.\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE)\n\n\n\n\n\n\n\n\nWe now have two dots. One with life expectancy at birth in 1980 and one in 2020. Using geom_segment() we can now connect these dots. In the aesthetics, we map life expectancy in 1980 on the x aesthetic, live expectancy in 2020 on the xend aesthetic. We also map the latter on the color aesthetic. Doing so, the segment will have the same color as the dots for life expectancy in 2020. In the last line, we change the color:\n\nlife_seg |&gt; filter(!is.na(`1980_life_exp`)) |&gt; slice_sample(n = 40) |&gt;\n  ggplot(aes(y = fct_reorder(country, `1980_life_exp`, .desc = TRUE, na.rm = TRUE))) +\n  geom_point(\n    aes(x = `1980_life_exp`, size = `1980_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_point(\n    aes(x = `2020_life_exp`, size = `2020_life_exp`, color = `1980_life_exp`), \n    show.legend = FALSE) +\n  geom_segment(\n    aes(x = `1980_life_exp`, xend = `2020_life_exp`, color = `2020_life_exp`), \n    arrow = arrow(angle = 12, type = \"closed\", length = unit(2, \"mm\")),\n    show.legend = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is some further layout work, e.g. the choice of colors for the dots and lines, the labels of the axis, a caption, … . We could add the flag of the country next to its name, … . But here, you have the basic “dumbell chart”: a combination of two point geometries and one segment.\ngeom_curve() shows a similar plot, but adds a curved line. You can control the level of the curvature using the curvature = argument. A positive value will produce a right hand curve, a negative value a left and curvature and 0 shows a straight line. In the example, the curvature is -0.5. The plot also includes arrows.\n\nlife_seg |&gt;\n  ggplot() +\n  geom_curve(\n    aes(x = `1980_gdp_capita`, xend = `2020_gdp_capita`, y = `1980_life_exp`, yend = `2020_life_exp`, color = region), \n    arrow = arrow(angle = 11, type = \"closed\", length = unit(2.5, \"mm\")), \n    curvature = -0.50) +\n  scale_x_continuous(\n    transform = \"log\",\n    breaks = c(100, 1000, 10000, 100000),\n    labels = scales::label_currency(prefix = \"$\")) +\n  theme_minimal()\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_curve()`).\n\n\n\n\n\n\n\n\n\nGetting the curvature right usually requires some experimenting with various numbers, both positive and negative, usually starting from the default value and moving closer to 0 to reduce curvature or futher from 1 to add to curvature.\nYou can use both geometries also to add a line or curve segment on a plot. To do so, you can use the aes() mapping. For any segment or curve you would like to add as a layer to the graph, you add a value for x, xend, y and yend in the aesthetic mapping in geom_segment() or geoom_curve() and these geometries will add a line or curve between the points (x, y) and (xend, yend). Using the unemployment data in the Beverdidge dataset, the next graph adds a curve line stressing the rise in the unemployment rate and a segment to stress the fall in the unemployment rate. The x- values x and xend are dates. For both the curve and the segment the end is shown with an arrow:\n\ndata_beveridge |&gt;\n  ggplot(aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  geom_curve(\n    aes(x = as.Date(\"2008-03-31\"),\n        y = 7.5, \n        xend = as.Date(\"2013-03-31\"), \n        yend = 12.5), \n    curvature = -0.25, \n    color = \"red\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  geom_segment(\n    aes(x = as.Date(\"2013-06-30\"),\n        y = 12.5, \n        xend = as.Date(\"2025-03-30\"), \n        yend = 6.5), \n    color = \"#2ECC71\", \n    arrow = arrow(angle = 22.5, type = \"closed\", length = unit(5, \"mm\"))) +\n  theme_minimal()\n\nWarning in geom_curve(aes(x = as.Date(\"2008-03-31\"), y = 7.5, xend = as.Date(\"2013-03-31\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = as.Date(\"2013-06-30\"), y = 12.5, xend = as.Date(\"2025-03-30\"), : All aesthetics have length 1, but the data has 76 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nAs you can see, R suggests to use another function annotate() to draw these lines. We cover annotate() Chapter 11.\nNote that you can use geom_hline(), geom_vline(), geom_abline(), geom_segment() and geom_curve() to draw graphs such as a supply and demand diagram.\n\n\n\n\n\n\nSupply and demand\n\n\n\n\n\nTo keep it as simple as possible and use as many line geoms as possible, let’s start from the y-axis, which we will call “Price”. We generete this series as a simple sequence from 0 to 100:\n\nsup_dem &lt;- data.frame(\n  price = seq(0, 100, by = 1))\n\nWe can now add demand\n\\[\nQ_D = 800 - 8 * P\n\\]\nand, using the inverse supply curve, supply\n\\[\nP = \\frac{1}{8} * Q_S =&gt; Q_S = 8 * P\n\\]\n\nsup_dem$demand = 800 - 8*sup_dem$price\nsup_dem$supply = 8 * sup_dem$price\n\nUsing the data in sum_dem we can now build the supply and demand plot:\n\nsup_dem |&gt; ggplot(aes(y = price)) + # Price on the vertical axis\n  # Show quanity on the horizontal axis, add some color and linetype and width (as a setting)\n  geom_line(aes(x = demand), color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  geom_line(aes(x = supply),  color = \"grey70\", linewidth = 0.75, linetype = \"solid\") +\n  \n  # Dotted lines for the equilibruim (note that you can calculate the equilibrium \n  # using the demand and supply parameters and add them here)\n  geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", color = \"black\") +\n  \n  # Create x and y-axis \n  geom_hline(yintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  geom_vline(xintercept = 0, linewidth = 1, color = \"darkgrey\") +\n  \n  # Set the labels for the axis\n  labs(\n    x = \"Quanity\", \n    y = \"Price\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 95\n  # here the text is \"Supply\"\n  annotate(\"text\", x = 700, y = 95, label = \"Supply\") +\n  \n  # Add some \"text\" annotations at position x = 700 and y = 5\n  # here the text is \"Demand\"\n  annotate(\"text\", x = 700, y = 5, label = \"Demand\") +\n  \n  # Remove all ticks and labels for the x and y axis\n  theme(axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      \n  # Show on a white background\n      panel.background = element_rect(fill = \"white\"))\n\nWarning in geom_segment(aes(x = 0, y = 50, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 400, y = 0, xend = 400, yend = 50), linetype = \"dotted\", : All aesthetics have length 1, but the data has 101 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.4.0.3 Area geometries\nThere are two area geometries that are often used: geom_area() and geom_ribbon().\n\n10.4.0.3.1 geom_area()\ngeom_area() allows you to show the composition of a variables changes over time, e.g. the share of various components in GDP, the share of items in household expenditures, the share of one product or region in total sales … . To illustrate this geometry, we’ll use the life_expectancy at birth dataset for Brazil and Costa Rica and the AMECO database with data on age structure of the population in France: the population ages 0-14, aged 15-64 and 65 and over.\nFor the first dataset, if you haven’t used it yet, you can run the following code\n\nlife_df &lt;- read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRows: 13671 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): iso2c, iso3c, country, region\ndbl (4): date, gdp_capita, life_exp, pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_life_cribra &lt;- filter(life_df, iso3c == \"CRI\" | iso3c == \"BRA\") |&gt; rename(year = date)\n\nIn Chapter 7, you used the AMECO dataset to learn some tidying skills. In the next box, I’ll create the dataset from the data in data &gt; raw &gt; ameco_xlsx folder. We’ll need AMEC01.XLSX.\n\n\n\n\n\n\nFrench population data\n\n\n\n\n\nFirst import the dataset\n\nlibrary(readxl)\n\nameco1 &lt;- read_excel(\n  path = here::here(\"data\", \"raw\", \"ameco_xlsx\", \"AMECO1.XLSX\"), \n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  na = c(\" \", \"NA\")\n)\n\nNew names:\n• `UNIT` -&gt; `UNIT...5`\n• `UNIT` -&gt; `UNIT...11`\n\n\nWe’ll store two series: one with data (ameco1) and one with data labels (ameco2):\n\nameco2 &lt;- ameco1 |&gt; select(CODE, TITLE, UNIT...11)\nameco1 &lt;- ameco1 |&gt; select(CODE, COUNTRY, matches(\"\\\\d{4}\"))\n\nTo tidy the dataset, we need to pivot, first longer followed by a pivto wider\n\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = matches(\"[0-9]{4}\"), names_to = \"year\", values_to = \"value\")\nameco1 &lt;- ameco1 |&gt; pivot_wider(names_from = \"CODE\", values_from = \"value\")\n\nAfter selecting the series we need, we need to pivot the data back to longer format: the population structure will be used as one variable with three level: one for each age category:\n\n# Select the variables we need\nameco1 &lt;- ameco1 |&gt; select(COUNTRY, year, NPTN, NPCN, NPAN, NPON)\n\n# year is a character variable: need to change in numeric\nameco1$year &lt;- as.numeric(ameco1$year)\n\n# Pivot longer to crate one variable, \"age_group\" with 3 values, one for each age category\nameco1 &lt;- ameco1 |&gt; pivot_longer(cols = starts_with(\"NP\"), names_to = \"age_group\", values_to = \"pop\")\n\nTo get nice labels, we first identify the unique labels and merge those with the data. This will allows us to generate e.g. nice legends:\n\nameco2 &lt;- ameco2 |&gt; filter(CODE == \"NPTN\" | CODE == \"NPCN\" | CODE == \"NPAN\" | CODE == \"NPON\")\nameco2 &lt;- unique(ameco2)\nameco1 &lt;- ameco1 |&gt; left_join(ameco2, by = join_by(age_group == CODE))\n\nTidy the names and select France for all years to 2022:\n\nameco1 &lt;- ameco1 |&gt; rename(unit = UNIT...11, country = COUNTRY, population = TITLE)\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s check the data:\n\npop_fra |&gt; slice_sample(n = 5)\n\n# A tibble: 5 × 6\n  country  year age_group    pop population                    unit        \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                         &lt;chr&gt;       \n1 France   1989 NPCN      11823. Population: 0 to 14 years     1000 persons\n2 France   1996 NPCN      11649. Population: 0 to 14 years     1000 persons\n3 France   2007 NPON      10434. Population: 65 years and over 1000 persons\n4 France   1968 NPCN      13004. Population: 0 to 14 years     1000 persons\n5 France   1963 NPCN      12875. Population: 0 to 14 years     1000 persons\n\n\n\n\n\nLet’s first use an area graph with one variable. Using geom_area() will map the variable year on the horizontal axis and life expectancy at birth for Brazil on the vertical one. These two aesthetics are required. We will then use geom_area() and accept all default values:\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWith one variable, geom_area() essentially creates a line geometry but fills the area between the line, the horizontal axis and the vertical axis. Using the settings, you can change the color or transparency and using geom_line() you can show the line as an additional layer. For instance,\n\ndf_life_cribra |&gt; filter(iso3c == \"BRA\") |&gt;\n  ggplot(aes(x = year, y = life_exp)) + \n  geom_area(fill = \"blue\", alpha = 0.20) +\n  geom_line(color = \"blue\", linewidth = 0.75) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the dataset, we have two countries, Brazil and Costa Rica. If you map the variable country on the fill aesthetic, ggplot() will stack the life expectancy data. In other words, it add life expectancy in Brazil to life expectancy in Costa Rica and shows the stacked outcome.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis outcome, which doesn’t make sense, is due to the default values in the geom_area() function:\n\ngeom_area(\n  mapping = NULL,\n  data = NULL,\n  stat = \"align\",\n  position = \"stack\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  ...,\n  outline.type = \"upper\"\n)\n\nThe mapping, data, na.rm, orientation, show.legend, inherit.aes and ... should be familiar. Although we already met stat and position, for the first we always kept identity and for the latter, we kept identity or jitter. The first, stat = \"align\", shows what will happen is two series are stacked, but have no common x-coordinates. By default, R will interpolate these values and align the values for each series. If there are two series, R stacks the values: in other words, it adds the values for the first series to those of the second to show the second series, the sum of the values of the first two are added to the values of the third to show the third series, … . This is the default: position = stack. Using stack  = \"identiy\" changes this default. To illustrate, in the graph with life expectancy for Brazil and Costa Rica, geom_area() first plotted life expectancy for Costa Rica. The second series was the sum of life expectancy in Costa Rica and Brazil. If there would have been a third country, the third line would have shown the sum of life expectancy in all three these countries. Changing these defaults, R will plot two curves. However, as life expectancy in Costa Rica was always higher than in Brazil, the area for Costa Rico will overlap and mask the area for Brazil.\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo deal with these issues, you can add transparency using the alpha setting and add a line per country, change the order in which they appear, … .\n\ndf_life_cribra |&gt;\n  ggplot(aes(x = year, y = life_exp, fill = country)) + \n  geom_area(stat = \"identity\", position = \"identity\", alpha = 0.10) +\n  geom_line(aes(color = country)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing geom_area() with multiple variables, stacking them should make sense. Else there is little reason why you wouldn’t use geom_line. This is the case for the French population dataset. In pop_fra we have a dataset which includes the number of people aged 0-14, 15-64 and 65 and older. The sum of these three population groups equals the total population in France. geom_area() plots these three series and stacks their values. Doing so, it creates three new series: one with the number of people ages 0-14, a second with the number of people ages 0-65 (the sum of those ages 0-14 and those ages 15-64) and a third series with the sum across age categories. To show these series, it fills the area below the first series and the x-axis, between the first and second series and between the second and last series with a color. Doing so, the size of the area shows the size of each subgroup.\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the graph makes sense: it shows the evolution of the total population in France as the sum of three components: those ages 0-14, 15-64 and 65 and over. The size of each cohort is shown on the vertical axis. However, there is one issue. R shows the order of the categories starting with the largest: NPAN (15-64), then the young (or children) NPCN and then the “old” or NPON. Dhis is standard as R orders to values to stack first from low to high. This order doesn’t make a lot of sense as these categories imply an order and the graph should preferably show the young at the bottom, those aged 15-64 in the middle and those aged 65 and over at the top. There are a couple of ways to deal with this. The first is to order the values in age_group (an ordered factor). Here, we will first order the ameco1 dataset and then filter the data for France. Doing so, you can create an area graph for other countries by changing the filter:\n\nameco1$age_group &lt;- factor(ameco1$age_group, \n                           levels = c(\"NPTN\", \"NPON\", \"NPAN\", \"NPCN\"), \n                           ordered = TRUE)\n\npop_fra &lt;- ameco1 |&gt; filter(\n  country == \"France\" & (age_group == \"NPCN\" | age_group == \"NPAN\" | age_group == \"NPON\") & year &lt;= 2022)\n\nLet’s now see the result of the plot:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = age_group)) +\n  geom_area()+ \n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that R ordered the area’s in line with the age categories. It also shows a different color. This is due to the fact that R chooses another color scale for ordered factors as apposed to unordered factors. The yellow at the bottom is lighter than the dark blue at the top: in other words, the hue implies an order: lighter colors for smaller values (in this cases ordered per age group).\nThere are other ways to change the order. For see how they work let’s create a geom_area() but now map the variable population on the fill aesthetic:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the legend, you can see that the plot shows to order from young to old. This is due to the fact that the order of the categories in population in alphabetical order happens to put the category with the youngest first and with the oldest last. If you want to reverse the order, you can add the option position = position_stack(reverse = TRUE):\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe problem is not fully solved: the plot is fine, but the legend isn’t. Here, you need to add scales_fill_manual and set the legend labels manually (see Chapter 11):\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, fill = population)) +\n  geom_area(position = position_stack(reverse = TRUE)) +\n  scale_fill_discrete(breaks=c(\"Population: 65 years and over\", \"Population: 15 to 64 years\", \"Population: 0 to 14 years\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen we used geom_line() we kept the default position = \"identity\". Given how close geom_area() and geom_line() are connected, it shouldn’t come as a complete surprise that you can add position = \"stack\" as an argument in geom_line(). Doing so, shows the stacked lines:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop, color = age_group)) +\n  geom_line(position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that this graph is a bit misleading: a lot of people will interpret this graph to mean that the largest population group in France are the oldest and that there are hardly any young people, as they will not immediately notice that this is a stacked line chart, not one with position = identity. This the main reason why you should use an area chart: this because it fills the areas, it also suggests that categories are stacked. Note that you can also combine geom_area() and geom_line() with stacked lines; Doing so, allows you to add a line between the areas in the area chart. If you add some transparency to the areas, these lines will stand out:\n\npop_fra |&gt; ggplot(aes(x = year, y = pop)) +\n  geom_area(aes(fill = age_group), alpha = 0.20) +\n  geom_line(aes(color = age_group), position = \"stack\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.4.0.3.2 geom_ribbon()\ngeom_ribbon() is a special case of geom_area() It allows you to plot a “ribbon” defined by minimum and maximum values for the variable mapped on the vertical axis. The required aesthetics for this geometry are the variables mapped on the x-axis and those on the ymin and ymax aesthetics. The latter two define the boundaries of the ribbon. To illustrate, we’ll use the diamonds dataset, filter diamonds with carat &lt;= 2.5, calculate the standard deviation and mean price per level of carat and show the result using a ribbon where the maximum value is defined as the mean price plus 1.96 times the standard deviation and the minimum value is defined as the mean price minus 1.96 times the standard deviation. Let’s first create the summary data frame:\n\ndiamonds_sum &lt;- diamonds |&gt;\n  filter(carat &lt;= 2.5) |&gt;\n  group_by(carat) |&gt; \n  summarize(\n    min_price = min(price), \n    max_price = max(price), \n    mean_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE))\n\nUsing geom_ribbon() and accepting all default values for the settings:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis ribbon shows the carat on the horizontal axis. The vertical axis shows two variables: the mean price + 1.96 the standard deviation of the price (per carat group) and the mean prices - 1.96 times the standard deviation (per carat group). Using e.g. fill you can change the color of the ribbon, using alpha the transparency of the ribbon and using the setting color you can change the color of the lines that show the minimum and maximum values. In addition, you can set the line width and line type, For instance:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightblue\", color = \"blue\", linewidth = 0.50, linetype = \"solid\", alpha = 0.20) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you want to also the mean values, you add a line geometry:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = mean_price - 1.96 * sd_price, \n    ymax = mean_price + 1.96 * sd_price)) +\n  geom_ribbon(fill = \"lightgrey\", color = \"darkgrey\", alpha = 0.50) +\n  geom_line(aes(y = mean_price), color = \"blue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_ribbon() is a special case of geom_area(). To see this, let’s change the minimum value in the previous graph to ymin = 0 and the maximum value, ymax = mean_price:\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    ymin = 0, \n    ymax = mean_price)) +\n  geom_ribbon(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe result is equal the the one you would have gotten with geom_area():\n\ndiamonds_sum |&gt; \n  ggplot(aes(\n    x = carat, \n    y = mean_price)) +\n  geom_area(fill = \"lightblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause of these similarities, it shouldn’t come as a surprise that the arguments for geom_ribbon() and geom_area() are very similar:\n\ngeom_ribbon(\n  mapping = NULL,\n  data = NULL,\n  stat = \"identity\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"both\"\n)\n\nWith the exception of the default that are specific to geom_area() - stat and position - all others are very similar.\n\n\n\n10.4.0.4 Bar or column geometries\nThere are two bar geometries: geom_bar() and geom_col(). By default, first shows the number of observations in each category of the variable that is mapped on the x-axis. The second allows you to show values for a variable mapped on the vertical axis, for every value of the variable mapped on the x-axis. geom_bar() has one required aesthetic: the x-axis. This variable is a discrete variable (nominal or ordinal)\n\n10.4.0.4.1 geom_bar()\nFor geom_bar() the arguments are\n\ngeom_bar(\n  mapping = NULL,\n  data = NULL,\n  stat = \"count\",\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nHere the stat argument has the value \"count\". In other words geom_bar() by default shows the total number of observations on the vertical axis. This also means that there is only one required aesthetic: the variable for whose values the count will happen. The alternative, \"identity\" shows the values of the variable mapped on the y-axis. Before moving on to the position and stat argument, a short word about some of the others. The argument just positions the bar over the major grid line for the value on the x-axis. The default value centers the bars over that grid line. Alternative values are 0 or 1 to align left or right. The width argument - which is be default 0.90 - measures how much space the bars will take up in the plot. By default this is 90% of the total area. If you reduce this to e.g. 0.75, the sum of the area or the bars will take up 75% of the total panel area.\nThe position argument with default \"stack\" matters in case the aesthetics include a mapping on e.g. fill or color. Mapping a variable on the fill aesthetic will show the number of observations for each variable on the horizontal axis, but will differentiate between various values of the variable mapped on the fill aesthetic using different colors. The color aesthetic shows similar values, but does so using a different color for the line between various subgroups. The position argument as two alternatives: \"fill\" and \"dodge\" or \"dodge2\". The first rescales the vertical axis from 0 to 100% and shows the various subcategories for each category on the x-axis as a percent of total values for the x-category. The second, \"dodge\" or \"dodge2\" shows various subgroups next to each other and not stacked.\nTo illustrate, we’ll use the diamonds dataset. To show the number of observations per level of cut, we map the cut variable on the x-axis and accept all defaults. Recall that geom_bar() will plot the count on the vertical axis. In other words, one aesthetic mapping, x = cut, is sufficient:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see from the plot, geom_bar() adds the label “count” to the vertical axis. As cut is an ordered factor, the values on the x-axis are shown in that order. As usual, you can change the setting (i.e. layout options) using e.g. the fill (fill or the bar), color (color of the line around a bar), … by specifying those in geom_bar():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar(fill = \"lightblue\", color = \"blue\", linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuppose that you want to now the number of observations for each level cut-clarity combination. To do so, you have to map the variable clarity on an aesthetic. You can use the fill aesthetic. Doing so, geom_bar() will show the number of observations per level of and will do so using different colors to fill the bars, where each color show one value of clarity:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are other aesthetics that you can use to map other discrete variable, e.g. color or line width. The color shows the various subcategories using a different color for a line:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, color = clarity)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, except when you set the color of the bars to white, the different lines reveal little on their own. The same holds for the other aesthetic mappings. In other words, the preferred aesthetic to other variables is fill.\nBy default, geom_bar() stacks (position = stack) the various values of the variable mapped on the fill aesthetic. To show them next to one another, you add position = \"dodge\" or position = \"dodge2\":\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every level of cut, geom_bar() now shows 8 bars: one per level of clarity. If you use position = \"dodge2\", you can add further details using\n\nposition_dodge2(\n  width = NULL,\n  preserve = \"total\",\n  padding = 0.1,\n  reverse = FALSE\n)\n\nIf you use position = \"dodge\", you can specify the width and preserve. The first, width is relevant is you have different geometries with different width, for instance, point geometry and a bar geometry. The second preserve is relevant in case not all values of the variable mapped on the x-axis have the same number of subcategories for the variable mapped on the fill aesthetic. By default, R preserves the total width of the of all bars for each value on the horizontal axis. The alternative, \"single\" preserves the width of the subcategories. To illustrate, here are two examples taken from Chang (2025):\n\n\n\n\n\nFigure 10.8: the preserve argument\n\n\n\n\n\n\n\n\nThe padding argument in position_dodge2() allows to to specify the distance between two bars. The default value is 0.1. If you increase that value, the distance between two bars at the same x position widens:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = 0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nA negative value causes overlap:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe argument reverse = FALSE keeps the order of the subcategories. Changing this into TRUE, reverses the order of the subcategories:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing position = fill, geom_bar() will show proportions. For every value of the variable mapped on the horizontal axis, the total is set equal to 100. The subcategories are then shown in a percentage on the number of observations for each of the subcategories in relation to the total number of observations for their category shown on the x-axis:\n\ndiamonds |&gt; \n  ggplot(aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#####geom_count()\ngeom_count() allows you to show values for the variable mapped on the vertical axis for every value of the variable mapped on the horizontal axis. This geometry needs at least two aesthetics: a discrete variable to map on the x-axis and the variable to map on the y-axis. This geometry allows you to show, e.g. the average for a variable (e.g. price), for every value of a discrete variable (e.g. cut). Here, we’ll illustrate this geometry to visualize summary data.\nThe argument of this function are very similar to the arguments for the geom_bar() geometry:\n\ngeom_col(\n  mapping = NULL,\n  data = NULL,\n  position = \"stack\",\n  ...,\n  just = 0.5,\n  width = NULL,\n  na.rm = FALSE,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nSuppose you want to visualize the average price per cut. Using {dplyr}’s group_by() and summarize() functions, you can calculate these averages using diamonds |&gt; group_by(cut) |&gt; summarize(ave_price = mean(price, na.rm = TRUE)) (see Chapter 8). Because these function return a data frame, we can connect their output in a pipe with ggplot(). In the next code, the first two lines calculate the average price per cut. We map the cut, the discrete variable, on the horizontal axis and the average price on the vertical axis. Note that the names of these variables are found in the tibble returned by the summarize() function. This data frame is use by ggplot() to find the variables. Here, we accept all default values.\n\ndiamonds |&gt; group_by(cut) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFor every value of cut, the variable mapped on the horizontal axis, geom_col() shows the variable ave_price, the mean of the diamond’s prices for each category of cut on the vertical axis. Note that here ggplot(data, aes() ...) uses the data frame returned by {dplyr}’s functions as the data.\nUsing the other aesthetics, you can add other categories. As was the case with geom_bar(), usually only the fill aesthetic is used. Other aesthetics are not visually appealing to differentiate across categories. Here, we’ll show the average price for every value of clarity and cut. Doing so, we first need to calculate these values. Again, we do so using group_by() and summarize(). We map cut on the horizontal axis van use the fill aesthetic to map the variable clarity\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nNote that the output doesn’t make sense. The previous graph showed that the average price for fair cut diamonds was a little over 4000. Here, the average price is over 30000. This result is due to the fact that geom_col() by default stacks the values. In this case, this is not how it should be done. Stacking values per category is only relevant is the sum of these values is relevant. Here, this is not the case. Changing the position to dodge2 shows the various averages next to each other, grouped by level of cut. The position dodge or dodge2 can be modified in the same way as in geom_bar().\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n10.4.0.4.2 Using geom_bar() as geom_col() and vice versa\nBoth geometries are closely related. In the arguments for geom_bar(), there was stat = \"count\". Changing this into stat = \"identity\" tells this geometry to plot values, not counts. Using this value, you can create a bar chart using geom_bar() that is identical to the one created with geom_col(). To see this, let’s use geom_bar() in stead of geom_col() in the previous plot and use the stat = \"identity\" argument:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_bar(stat = \"identity\", position = \"dodge2\") +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\nBecause we used stat = \"identity\", geom_bar() now plots the values for the variable mapped on the vertical axis as values that that axis. In other words, using this argument, you can often use geom_bar() as geom_col().\ngeom_bar() shows counts. However, you can also use geom_col() to show counts. Let’s use this graph created with geom_bar()\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo create the same chart in geom_col() we need a data frame that includes the number of observations but value for cut. Using summarize(n = n()), this is what we can do. Using the result of these function in ggplot() with geom_col() shows a geom_bar() type of output:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n10.4.0.4.3 Coordinate flip\nWe already touched upon coordinates. There, I illustrated coordinate flip or coord_flip(). For bar and column charts, the effect is that variable mapped on the horizontal axis is shown on the vertical axis and the values shown on the vertical axis are measured along the horizontal axis. Note that you don’t have to change the aesthetics mapping. If is sufficient to add coord_flip():\n\ndiamonds |&gt;\n  ggplot(aes(x = cut)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou would have had the same result if you mapped the discrete variable on the vertical axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = cut)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHowever, the advantage of coord_flip() is that is doesn’t change e.g. values for scales. As the discrete variable is mapped on the x-axis and is only shown on the y-axis, scale_x_discrete() … refer to the right variable. You can also flip coordinates with geom_col() and with more than one category where you reverse the order of the subcategories:\n\ndiamonds |&gt; group_by(cut, clarity) |&gt;\n  summarize(ave_price = mean(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = cut, y = ave_price, fill = clarity)) +\n  geom_col(position = position_dodge2(padding = -0.3, reverse = TRUE)) +\n  coord_flip() +\n  theme_minimal()\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n\n\n\n\n10.4.0.4.4 Adding text\nSometimes it is useful to add values as text to a bar or column chart. Recall that you can add text using, e.g. geom_text() or geom_label(). Adding this text layer to a bar or column plot allows you to add e.g. the values they represent. Here, the values are added in white to a bar chart in blue. To do so, add geom_text() and map the sum n to the label aesthetic. This geometry inherits the x- and y- mappings. In other words, geom_tex() knows the values on the x-asis and the height along the y-axis. The values are nudged down 200 units. As the units on the vertical axis are measured from 0 to over 20000, a 200 nudge down brings these values within the bars. This might require a bit of experimenting to get right. Adding a positive nudge would add them to the top.\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_col(fill = \"lightsteelblue\") +\n  geom_text(aes(label = n), nudge_y = -200, color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI refer to the section on point geometries for further possibilities with geom_text() and similar geometries.\n\n\n\n10.4.0.5 Geometries for distributions\nThere are multiple ways to show the distribution of a variable. geom_histrogram() and geom_density() visualize the full distribution. Using geom_boxplot() or geom_violin() you can show the distribution, using summary statistics such as the mean, various quantiles and percentiles, minimum and maximum. For two continuous variables, you can use geom_bin2d(). geom_density_2d() or geom_density_2d_filled(). Here we will not go into detail on all of these geometries. We’ll focus on those for one variable and show, as an illustration, how you can extend them to two variables.\n\n10.4.0.5.1 geom_histrogram() and geom_density()\ngeom_histrogram() divides the full range of possible values for a variable into bins. For every bin, geom_histrogram() shows the number of observation. Recall that is is also what geom_bar() did. In other words, you would be able to use geom_bar() or geom_col() to generate this plot. The arguments of geom_histrogram() are:\n\ngeom_histogram(\n  mapping = NULL,\n  data = NULL,\n  stat = \"bin\",\n  position = \"stack\",\n  ...,\n  binwidth = NULL,\n  bins = NULL,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nMost should be familiar. The binwidth = allows you to specify the width of a bin, e.g. 50 or 25. bins allows you to set the number of bins. By default this value of 30. There is one other way to change the bins and that is to define them using breaks. Using this argument, you can include a vector with bins or use, e.g. seq() to generate the bins. There is only one required aesthetic: the variable to map on the x-axis. To illustrate this function, we’ll visualize the distribution for price in the diamonds dataset. Accepting all defaults:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou can change the settings of the plot in the usual way: change the fill, change the color of the lines, … . Experimenting with the number of bins is often a good idea. Doing so, you can see what number is best for the data. Let’s change the number of bins in three ways:\n\nincrease the number of bins from 30 to 50:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nset the bin width to 250:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 250) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nuse seq() to set bins every 500:\n\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(breaks = seq(from = 0, to = 20000, by = 500)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIf you map another variable on e.g. the aesthetic fill, geom_historgram() will show the count for each level of cut in every bin using different colors. As with the bar geometries, other aesthetics are much less suitable to use as aesthetic mapping. Using the fill aesthetic to map cut, the histogram shows all observations for each level of cut within each bin:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_histogram() +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFor two variables, geom_hex() extends geom_histogram(). Both variables are divided into bins (default 30) and the graph shows the number of observations per crossed bin. The function requires a mapping on the x-axis and one on the y-axis. To illustrate, I’ll use a data frame with random draws from a bivariate normal distribution\n\nmat1 &lt;- matrix(rnorm(2000, 0, 1), nrow = 1000, ncol = 2)\ncov &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2, ncol = 2)\nmat1 &lt;- mat1 %*% cov\ndf_mat1 &lt;- as.data.frame(mat1)\ndf_mat1 &lt;- df_mat1 |&gt; rename(var1 = V1, var2 = V2)\n\nWith 50 bins, this is how the histograms of these two variables, var1 and var2 look like:\n\nplothist1 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var1)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist2 &lt;- df_mat1 |&gt; \n  ggplot(aes(x = var2)) +\n  geom_histogram(bins = 50) +\n  theme_minimal()\n\nplothist1 + plothist2\n\n\n\n\n\n\n\n\nUsing geom_hex() you plot both in one plot. Here, for every of 2500 bins (50 for var1 and 50 for var1) this plot shows the number observations in each and every one of these bins:\n\ndf_mat1 |&gt; \n  ggplot(aes(x = var1, y = var2)) +\n  geom_hex(bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere color scale here, shows a higher number of observations with a lighter blue color. Consistent with the histograms for the individual series var1 and var2, geom_hex() show higher counts in bins closer to 0.\ngeom_histrogram() shows the count by default. You can change that in a density (i.e. probability) using an after_stat() value for the stat argument. Using after_state(\"density\"), geom_histrogram() maps these estimated probabilities on the y-axis. With 50 bins, this generates this plot:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo show a density, you can also use geom_density(). The arguments of this geometry are:\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\nThe stats = \"density\" by default calculates densities. Changing this into “count” generate a geom_histromgram(). geom_density() uses a by default a Gaussian kernel density estimator to smooth the estimates of the probabilities. For instance, the density estimate for the price of diamonds:\n\ndiamonds |&gt;\n  ggplot(aes(x = price)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing the full aesthetic, you can show density plots for every value of the variable mapped on that aesthetic. For instance, to plot these price density for various values of cut, you map the latter on the fill aesthetic:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHer problem here is that the density plot for e.g. “Ideal” with the one for e.g. “Fair”. To show all density plots, you can add some transparency using the alpha setting. If, in addition, you map cut also on the color aesthetic, geom_density() will add lines at the top of each density:\n\ndiamonds |&gt;\n  ggplot(aes(x = price, fill = cut, color = cut)) +\n  geom_density(alpha = 1/10, linewidth = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo plot a density with 2 variables, you can either use geom_density2d() or geom_density2d_filled(). The first shows the bivariate density using contour plots, the second using filled contour plots. To illustrate, we’ll use the random data in df_mat1. The individual densities look like this:\n\nplotmat1 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var1)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat2 &lt;- df_mat1 |&gt;\n  ggplot(aes(x = var2)) +\n  geom_density() +\n  theme_minimal()\n\nplotmat1 + plotmat2 \n\n\n\n\n\n\n\n\nUsing geom_density2d(), the result is\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe contour lines reveal higher probabilities the more they are circled with other countour lines. Here, consistent with the univariate densities, the probability that you’ll see a pair with values for var1 and var2 increases as the values for both these variables close in on 0. With geom_density2d_filled() the plot is filled:\n\ndf_mat1 |&gt;\n  ggplot(aes(x = var1, y = var2)) +\n  geom_density2d_filled() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, the color reveals information on the probabilities. For this plot, value pairs with a high probability are drawn in yellow. Ast he color scale moves from yellow to blue, the probability of a value pair in that range falls.\n\n\n10.4.0.5.2 geom_boxplot() and geom_violin()\nA boxplot shows the distribution of a variable using a box, which shows the 1st and 3rd quartile as its outer ranges, a whisker up and down, both with a length of 1.50 times the interquartile range (or the value of the 3rd minus the value of the 1st quartile). In addition, the box shows the median. You can adapt the way in which geom_boxplot() shows the outliers. Outliers are values outside of the range of the boxplot. To do so, you need to change the arguments of the function:\n\ngeom_boxplot(\n  mapping = NULL,\n  data = NULL,\n  stat = \"boxplot\",\n  position = \"dodge2\",\n  ...,\n  outliers = TRUE,\n  outlier.colour = NULL,\n  outlier.color = NULL,\n  outlier.fill = NULL,\n  outlier.shape = 19,\n  outlier.size = 1.5,\n  outlier.stroke = 0.5,\n  outlier.alpha = NULL,\n  notch = FALSE,\n  notchwidth = 0.5,\n  staplewidth = 0,\n  varwidth = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nThe stat = \"boxplot\" defines the default values. If you want to change these, you need to add, e.g. coef = 1.75 to extend the whiskers to 1.75 the IQR. The treatment of outliers is governed by the arguments starting with outlier. The first, outliers = TRUE by default shows the outliers. Changing this into FALSE hides the outliers. The axis of the plot will be adjusted accordingly, unless you add oulier.shape = NA. If outliers are shown, you can change their color, fill, shape, size, stroke and alpha setting. Here, I refer to the point geometries to see how these affect the plot. notch = FALSE makes a traditional boxplot. Setting this to TRUE, R will show a notched boxplot, where the notches allow you to compare the medians across groups. You can set the width of the notch using notchwidth, by default this value equals 0.50, i.e. the notch covers half of the box width. The staples mark the end of the whiskers. The staple width, which is by default 0, allows you to specify the width of these staples. The last argument, varwidth = FALSE allows you to change the width of the box. By default, this is not the case. If changed to TRUE, the width of the box will be proportional to the square root of observations.\nLet’s first draw a boxplot for one variable, price. We’ll map that variable on the y-axis:\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote that the x-asis in these case has no real meaning, so you can actually leave it blank. Let’s add a notch with width 0.25, increase the length of the whiskers to 2 times the IQR, change the way outliers are shown (color = red) and add a staple at the end of the whisker. Note that you use additional settings to change, e.g. the color of the lines, the fill of the box, …\n\ndiamonds |&gt;\n  ggplot(aes(y = price)) +\n  geom_boxplot(\n    coef = 2, \n    notch = TRUE, \n    notchwidth = 0.25,\n    outlier.color = \"red\",\n    staplewidth = 0.10) +\n  theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nTo compare distributions across values of another variable, we need to map this variable on the x-aesthetic. If you map the same variable on the fill aesthetic, the color of the boxes will change with these values the plot will also show a legend. As such, it is not necessary as the box plot will show the values on of the variables mapped on the x-asis as label. However, doing so, you can show a legend and remove the x-axis. To start, let’s map cut on the x-axis and price on the y-axis. With the exception of notch, which we’ll set to TRUE, all default valules apply:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\ngeom_boxplot() now shows one boxplot per value of cut. The notches suggest that here is a significant difference in the median value between the price for “Ideal” cut diamonds and the price for “Premium” cut diamonds. Whether this is also the case for the values “Fair” and “Good” is not immediately visible as the notches seem to overlap. Using varwidth = TRUE we can adjust the width of the box using the square root of the number of observations as criterium. To do this, you set varwidth = TRUE:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = TRUE, varwidth = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe width of each box is now proportional to the square root of the number of observations. With 1610 observations in “Fair” and 21551 in “Ideal”:\n\ndiamonds |&gt; group_by(cut) |&gt; summarize(n = n()) |&gt; mutate(sqrtn = round(sqrt(n), 2))\n\n# A tibble: 5 × 3\n  cut           n sqrtn\n  &lt;ord&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Fair       1610  40.1\n2 Good       4906  70.0\n3 Very Good 12082 110. \n4 Premium   13791 117. \n5 Ideal     21551 147. \n\n\nthe width of the “Ideal” box is\n\\[\n\\frac{\\sqrt{21551}}{\\sqrt{1610}} = 3.65\n\\] 3.65 times wider then the width of the “Fair” box.\nTo illustrate these we use of the aesthetic fill to map cut and some other setting for the outliers, let’s use the fill aesthetic to map cut and add staples with width equal to 25% of the box width:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_boxplot(\n    notch = TRUE, \n    varwidth = TRUE, \n    staplewidth = 0.25, \n    outlier.colour = \"red\", \n    outlier.alpha = 1/5) +\n   theme(\n    axis.line.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nViolin plots are closely related to boxplots, but reveal fore information about the distribution of the variable. They are meant to show the distribution of one continuous variable per value of a discrete variable. The arguments in geom_violin() should be largely familiar. The argument draw_quantiles = FALSE allows you to add horizontal lines for every quantile. The scale argument determines the area of the violin. By default, this area is equal across violins. Changing this default value into “count” will creates violins where areas are scaled proportionally to the number of observations while “width” keeps the width of the violins equal across groups:\n\ngeom_violin(\n  mapping = NULL,\n  data = NULL,\n  stat = \"ydensity\",\n  position = \"dodge\",\n  ...,\n  draw_quantiles = NULL,\n  trim = TRUE,\n  bounds = c(-Inf, Inf),\n  scale = \"area\",\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\nLet’s use a violin plot to show the distribution of price for every value of cut. To do so, we map the variable price on the vertical axis and cut on the horizontal axis:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe violins for every value of cut show you where the observations are. The very thin upper part shows that there are almost no observations for high price levels. At the lower part of the price levels, there seems to be more spread for “Fair” cut diamonds that for e.g. “Ideal” cut diamonds. For the latter, the distribution is very wide at the lowest part of the price range, suggesting that here are a lot of observations for these price cut pairs. To see this more in detail, let’s add the a sample of the observations of the diamonds dataset using a point geometry with a bit of jitter and transparency:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price)) +\n  geom_violin() +\n  geom_point(\n    data = diamonds |&gt; slice_sample(prop = 0.10),\n    aes(x = cut, y = price), \n    position = position_jitter(width = 0.20, height = 0.20), alpha = 1/5, \n    color = \"red\", \n    size = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere you see that indeed, there are a lot of observations in the lower price range for “Ideal” cut diamonds.\nAs is the case with geom_histogram() you can map the same variable on both the x-axis and e.g. fill aesthetic. Doing so, geom_violin() will fill the violins and the color to the legend. If you want to add quantiles, e.g. the 10th and 90th percentile and the first and third quartile, you can specify those using draw_quartiles = c(0.10, 0.25, 0.75, 0.90). Here, we use the color white to draw these lines. By default R uses black and this color wouln’t show up in the “Fair” cut category:\n\ndiamonds |&gt;\n  ggplot(aes(x = cut, y = price, fill = cut)) +\n  geom_violin(draw_quantiles = c(0.10, 0.25, 0.75, 0.90), color = \"white\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n10.4.0.6 Other geometries\n{ggplot2} includes many other geometries. However, you can create most of these graphs with the geometries that we have covered here. To illustrate, geom_linerange(), geom_pointrange() and geom_errorbar() allow you to draw a line in a specific range, a line with a range including a dot in the middle, or a line with a bar at both ends. These geometries allow you to e.g. show the range of values in a dataset. Here, you can define the range using the mean and standard deviation, median and interquartile distance, … . However, you can recreate all three geometries using geom_segment() and geom_point(). To illustrate, we’ll show the price range for various categories of clarity in the diamonds using each of the two range geometries and the errorbar geometry was well as the segment and point geometry. The range if always defined as one standard deviation below and above the mean.\n\ngeom_linerange()\n\n\nplotlinerange1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_linerange(aes(ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n  labs(\n    title = \"geom_linerange()\"\n  ) +\n  theme_minimal()\n\nplotlinerange2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n  labs(\n    title = \"geom_segment()\"\n  ) +\n  theme_minimal()\n\nplotlinerange1 + plotlinerange2\n\n\n\n\n\n\n\n\n\ngeom_pointrange()\n\n\nplotpointrange1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity)) +\n  geom_pointrange(aes(y = ave_price, ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 1, size = 2) +\n  coord_flip() +\n    labs(\n    title = \"geom_pointrange()\"\n  ) +\n  theme_minimal()\n\nplotpointrange2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 1, show.legend = FALSE) +\n  geom_point(aes(y = ave_price, color = clarity), size = 8, shape = 16) +\n  coord_flip() +\n    labs(\n    title = \"geom_segment and geom_point()\"\n  ) +\n  theme_minimal()\n\nplotpointrange1 + plotpointrange2\n\n\n\n\n\n\n\n\n\ngeom_errorbar()\n\n\nploterror1 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity)) +\n  geom_errorbar(aes(ymin = ave_price - sd_price, ymax = ave_price + sd_price, color = clarity), linewidth = 2) +\n  coord_flip() +\n    labs(\n    title = \"geom_errorbar\"\n  ) +\n  theme_minimal()\n\nploterror2 &lt;- diamonds |&gt; group_by(clarity) |&gt;\n  summarize(\n    ave_price = mean(price, na.rm = TRUE), \n    sd_price = sd(price, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = clarity, y = ave_price, fill = clarity)) +\n  geom_segment(aes(y = ave_price - sd_price, yend = ave_price + sd_price, color = clarity), linewidth = 2) +\n  geom_point(aes(y = ave_price - sd_price, color = clarity), shape = \"\\uFE31\", size = 10, stroke = 2, show.legend = FALSE) +\n  geom_point(aes(y = ave_price + sd_price, color = clarity), shape = \"\\uFE31\", size = 10, stroke = 2, show.legend = FALSE) +\n  coord_flip() +\n    labs(\n    title = \"geom_segment and geom_point()\"\n  ) +\n  theme_minimal()\n\nploterror1 + ploterror2\n\n\n\n\n\n\n\n\nHere, I used a unicode character “uFE31” to end each line segment.\nIn addition there are many other packages that were designed with specific application in mind. For instance using {[ggwordloud] (https://lepennec.github.io/ggwordcloud/index.html)} (Le Pennec and Slowikowski (2024)) you can build word clouds showing which words occurred most in a text by increase the size of the font, {[treemapify] (https://wilkox.org/treemapify/index.html)} makes it easier to create treemaps and show, e.g. the relative importance product or market in total sales or exports, {[ggradar] (https://github.com/ricardo-bion/ggradar)} is very useful to design radar charts to compare how various products, countries for firms score on a fixed set of characteristics or{[ggbump] (https://github.com/davidsjoberg/ggbump)} can be used to create bump charts to visualize the change in ranking over time.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "10_Plot_types.html#annotations",
    "href": "10_Plot_types.html#annotations",
    "title": "10  ggplot 1: plot types (geometries)",
    "section": "10.5 Annotations",
    "text": "10.5 Annotations\nUsing geom_text(), geom_label(), geom_hline(), geom_vline(), geom_abline(), geom_segment() or geom_curve() you can add visual annotations to a graph: a label with the values for a bar chart, a horizontal or vertical line showing a specific data or an average, … . Using annotations() there is another way to add additional information to a plot. The main arguments of the annotation() function are:\n\nannotate(\n  geom,\n  x = NULL,\n  y = NULL,\n  xmin = NULL,\n  xmax = NULL,\n  ymin = NULL,\n  ymax = NULL,\n  xend = NULL,\n  yend = NULL,\n  ...,\n  na.rm = FALSE\n)\n\nThe first argument geom refers to the geometry to use e.g. text, rectangle, line segment, point range:\n\n“point for a point: x, y\n“text” for a text annotation: x, y, label\n“rect” for a rectangle: xmin, xmax, ymin, ymax\n“segment” to add a line segment: x, y, xend, yend\n“pointrange” to add a point range: x, y, ymin, ymax\n\nEvery geometry needs a minimum of values: a point needs a position (x, y), a text needs a position, defined by the (xmin, ymin) coordinate and a text to add (label), a rectangle needs to four corners, a line segment needs a start and end position, both defined by (xmin, ymin) and (xmax, ymax) and a pointrange - a vertical line with a midpoint - needs an x value, a value where the line starts and ends (ymin and ymax) as well the position where to add a point. For all these geometries, you can add further settings in line with that geometry. For instance, for a text, you can add a font family, fontface, size or the horizontal and vertical alignment (0 = left/top, 1 = right/bottom, 0.5 = center), annotations that include lines, you can add setting for line types or width and a rectangle can be filled. To illustrate these annotations, we’ll use the vacancy rate in the Beverdige dataset. If you haven’t imported it yet, you can do so here:\n\ndata_beveridge &lt;- read_csv(here::here(\"data\", \"raw\", \"data_beveridge.csv\")) \ndata_beveridge &lt;- data_beveridge |&gt; rename(date = DATE, quarter = `TIME PERIOD`)\n\nHere, we’ll map the date on the horizontal axis and the vacancy rate on the vertical axis and use a simple line geometry to show the data. In addition, we use annotate() to add 3 rectangles: one to highlight the financial crisis, one to highlight the Euro area debt crisis and one to highlight the pandemic. We’ll fill these reactangles with a different color. In addition, we’ll add a text annotation to include the reference to the crisis, aligh this text left of the rectangle and use the font family “serif”. To set the limits for the rectangle, we’ll first set the maximum value for the vacancy rate. The full code for this plot is:\n\nym &lt;- max(data_beveridge$vacancy_rate)\n\ndata_beveridge |&gt; select(date, quarter, vacancy_rate) |&gt;\n  ggplot(aes(x = date, y = vacancy_rate)) +\n  geom_line() +\n  annotate(\"rect\", \n           xmin = as.Date(\"2008-03-31\"), \n           xmax = as.Date(\"2009-08-31\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"lightgrey\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2008-03-31\"),\n           y = ym - 0.5, \n           label = \"Financial crisis\", \n           color = \"darkgrey\", \n           hjust = 0, \n           family = \"serif\") +\n  annotate(\"rect\", \n           xmin = as.Date(\"2011-06-30\"), \n           xmax = as.Date(\"2013-03-31\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"lightsteelblue1\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2011-06-30\"),\n           y = ym - 0.5, \n           label = \"Euro area debt crisis\", \n           color = \"steelblue\", \n           hjust = 0, \n           family = \"serif\") +\n  annotate(\"rect\", \n           xmin = as.Date(\"2020-01-31\"), \n           xmax = as.Date(\"2021-09-30\"), \n           ymin = 0, \n           ymax = ym, \n           fill = \"sienna2\", \n           alpha = 1/5) +\n  annotate(\"text\", \n           x = as.Date(\"2020-03-31\"),\n           y = ym - 0.5, \n           label = \"Covid pandemic\", \n           color = \"sienna4\", \n           hjust = 0,\n           family = \"serif\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUsing annotations can add value to a plot. Recall for instance that we use such as annotation to highlight “Premium” cut diamonds in a point geometry.\n\n\n\n\n\n\nChang, Winston. 2025. R Graphics Cookbook, 2nd Edition. O’Reilly Media, Sebastopol, CA. https://r-graphics.org/.\n\n\nLe Pennec, Erwan, and Kamil Slowikowski. 2024. Ggwordcloud: A Word Cloud Geom for ’Ggplot2’. https://github.com/lepennec/ggwordcloud.\n\n\nSlowikowski, Kamil. 2024. Ggrepel: Automatically Position Non-Overlapping Text Labels with ’Ggplot2’.\n\n\nYutani, Hiroaki. 2024. Gghighlight: Highlight Lines and Points in ’Ggplot2’. https://yutannihilation.github.io/gghighlight/.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ggplot 1: plot types (geometries)</span>"
    ]
  },
  {
    "objectID": "12_Tables.html",
    "href": "12_Tables.html",
    "title": "12  Tables",
    "section": "",
    "text": "12.1 Components of a table\nA table has rows and columns, at their intersectin, each table is divided in various cells. A table has various parts (Figure 12.1). Moving from the inside to the table to the outside, the table body refers to the cells that include the data. These cells can include numbers, characters, icons or images, charts, … . In Figure 12.1, you see that the table body also includes summary cells. The values in these summary cells can be included in your dataset or you can use {gt} to calculate these statistics. Moving up, the column labels show which variables are included in the cells of a column. These column labels may include the units of measurement (e.g. “usd”, “miles”, “hours”) if these would not be clear. Spanner column labels include labels that “span” multiple columns. For instance, if your dataset includes variables such as “sales (in units)”, “unit price” and “revenue” as well as “costs of goods sold” and “selling, general and administrative expenses” and “depreciation and amortization” you can include a spanner “revenues” above the first 3 columns and add a spanner “Operating costs” above the last three columns. You can have spanners at multiple levels: for instance: for each region in a country, you show the population and population density per city. Here, population and population density could be column labels. The level 1 spanner would include the city and the level 2 spanner - grouping cities across regions - would be the region. The stubs include the row labels and the group row labels. The allow you to “group” rows. For instance, if you have a dataset with income data for jobs (accountant, marketing manager, …) in various industries (banking, pharmaceuticals, automotive, retail, fashion, …) the group rows could show the industry and the rows the job. The cells to the right of the row labels include values; those to the right of the group row labels usually don’t. If the table includes summary statistics, their label (summary label) shows the statistics (e.g. average, median, …). The stub head shows which units are in the rows.\nFigure 12.1: Components of a table\nAt the top of the table, you find the table header. This header includes the title and the subtitle. These should be short and explain what the type of data the table presents. If you need to add further explanatory notes, you can do so at the bottom of each table using one or more footnotes. These footnotes can be attached to a column or row label, a spanner column label, a row group label or an individual cell in the table body. The source notes allow you to add the source of the data. The footnotes and source notes are in the table footer. In addition, you can add a table caption, e.g. “table 1”.\nUsing {gt} you refer to these components using a function that looks like tab_\"component\", e.g. tab_header(), tab_spanner(), tab_row_group(), tab_stubhead(), tab_footnote(), tab_source_note() and tab_caption(). The arguments of function follow from the component they refer to, e.g. tab_header() includes title = and subtitle = as it main argument; tab_spanner() includes the label = for the spanner, the columns who are part of the spanner in columns = and the spanners = for the spanners that should be spanned over in case you have more than one level of column spanners; tab_footnote() requires a text in footnote =, a location in location = and a placement where you want to add the footnote (to the right or the left of the data, label, …) placement = c(\"auto\", \"right\", \"left\"). {gt} uses a similar strategy to identify the cells, e.g. cells_title(groups = c(title\", \"subtitle\")) allows you to identify the title or subtitle; cells_body(column = everything(), rows = everything()) allows you to identify one or more cells at the intersection of one or more rows and one or more columns and cell_row_groups(groups = everyting()) will be used to identify one or more row group labels. Using these, you can target individual cells. For instance, to add a footnote to a table body cell on the 4th row in the 3rd column you can set the exact location using tab_footnote(location = cells_body(column = 3, row = 4)).\nTo identify rows and column you can also use &lt;tidy-select&gt; syntax. Recall from Chapter 8 that this syntax includes the following functions that allows you to select one or more columns (Henry and Wickham (2024)):",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "12_Tables.html#gt-workflow",
    "href": "12_Tables.html#gt-workflow",
    "title": "12  Tables",
    "section": "12.2 {gt}: workflow",
    "text": "12.2 {gt}: workflow\nAs was the case with {ggplot2}, {gt} uses a data frame or tibble as its input. As you enter the data frame into the gt() function, R creates a gt table object: gt_tbl. Using the pipe operator |&gt; you can start to add components, e.g. a title and subtitle, a footnote, column labels, … change the columns (e.g. merging or moving), format the columns using one of the fmt_ functions fmt_number(), fmt_currency(), fmt_percent(), … change styling using tab_style(), tab_style_body() or one of the opt_ functions. Using these you can change the font used to show the value or label in a cell using cell_text(), the fill color of the cell using cell_fill() or, using cell_line(), the lines at the top, bottom, left and right of each cell. If none of the styling options offer what you need, you can always use tab_options() to change any element of the table you would like. This last function is equivalent to the themes() function {ggplot2}. Note that there is one major difference with {ggplot2}: {gt} using the pipe operator while {ggplot2} used the +. The reason for this difference is that the latter adds layers following the grammar of graphics while the former creates a gt_tbl object. Using the pipe operator, you pipe this object in the next function. In addition, the workflow in {ggplot2} was more or less governed by the grammar of graphics: you first identity the data and aesthetics, select the geometry, change the scales and guides, add facets and adjust themes. As a matter of fact, without aesthetics, you can not adjust the scales or introduce facets. For a table, the workflow allows for more degrees of freedom. For instance, you could start with the header then move on to the column labels, spanner labels, row and row group labels then adjust the formatting of the numbers in the table body but you can also reverse the order and start with the formatting.\nIf you are satisfied with the output, you can save the table using gtsave(). This function allows you to save a table as a docx, html, tex, rtf, … file). In addition, you can use as_word() to save a table in an Open Office XML format that you can import in e.g. Word.\n{gt} allows you to use html or markdown options for e.g. column labels or table headings. To do so, you include the html code in html(text) where text refers to the text that R needs to preserve as html output. Using md(text), {gt} will interpret the text part as markdown formatting. Using this function it is easy to add quick styling. For instance adding you can add bold “text” using md(\"**text**\"), italics “text” using md(\"*text*\") and {gt} will in bold/italics “text” using md(\"***text***\").\nTo illustrate the workflow, we will use life_table as the main dataset. You can import the dataset from your raw data folder:\n\nlife_table &lt;- readr::read_csv(here::here(\"data\", \"raw\", \"life_table.csv\"))\n\nRows: 9 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): iso2c, iso3c, country, region\ndbl (4): 1980_gdp_capita, 1980_life_exp, 2020_gdp_capita, 2020_life_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe data frame includes a random sample of 3 countries and 3 regions. It includes iso2C, iso3c, country, region, 1980_gdp_capita, 2020_gdp_capita, 1980_life_exp and 2020_life_exp. With 3 regions and 3 countries per region, we have sufficient data to illustrate most of {gt}’s functions.\n\n12.2.1 Getting started\nAs a first step, you introduce the data frame in the gt() function. This function has a couple of arguments:\n\ngt(\n  data,\n  rowname_col = \"rowname\",\n  groupname_col = dplyr::group_vars(data),\n  process_md = FALSE,\n  caption = NULL,\n  rownames_to_stub = FALSE,\n  row_group_as_column = FALSE,\n  auto_align = TRUE,\n  id = NULL,\n  locale = getOption(\"gt.locale\"),\n  row_group.sep = getOption(\"gt.row_group.sep\", \" - \")\n)\n\nThe data refer to the data frame or tibble. As this is the first argument of the function, you can use the pipe operator |&gt; to introduce your dataset in this function. The next two arguments deal with the table stub. Using the first, you can include a column whose values will be used for the row labels. The second, groupname_col allows you to identify the column whose values will be used to add row group labels. Recall that a data frame can include row names. If this is the case, there is an alternative to add row labels: using rownames_to_stub = TRUE will add these row names as label to the plot. Using row_group_as_column you can determine if the row group labels will be put in a separate column. By default, this is not the case and they will be added in a dedicated row in the same column as the one that includes the row labels. The locale arguments includes the possibility to change, e.g. the language. Using this function with the life_table dataset and accepting all default values shows the gt_tbl object:\n\nlife_table |&gt; gt()\n\n\n\n\n\n\n\niso2c\niso3c\ncountry\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nCN\nCHN\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJP\nJPN\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHK\nHKG\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPT\nPRT\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBG\nBGR\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nDE\nDEU\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNI\nNIC\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGT\nGTM\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTT\nTTO\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nThe table includes the variable names as column labels. The variable names are shown as they appear in the data. By default, {gt} also adds a number of horizontal lines: at the top and bottom of the column label cells and at the bottom of the table. The table doesn’t include any vertical lines. For now, we will remove the iso2c and iso3c columns. As we have the country name in the variable country, these variables don’t add value. To do so, we use the select function from {dplyr} and assign this data set to life_table_gt.\n\nlife_table_gt &lt;- life_table |&gt; \n  dplyr::select(country, region, starts_with(c(\"1980\", \"2020\")))\n\nThe table now looks like\n\nlife_table_gt |&gt; gt()\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nWe can add row labels - labels that identify the observations in the rows - using the option rowname_col. Here, we can add the values in the variable country to use as row labels:\n\nlife_table_gt |&gt; gt(rowname_col = \"country\") \n\n\n\n\n\n\n\n\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nAlthough the table didn’t change, much, there are some notable differences with the case where we didn’t add row names: the table now includes the country names as row labels but doesn’t add a stub head (the reference to “country” disappeared) and the table now includes a vertical line separating the row labels from the table body.\nIf the data include related observations, you can collect those related observations under a row group. In the examplme, the data frame includes a variable region which we can use to add row group labels. Doig so, the countries will be grouped according to their region and {gt} will add a row group label with the value for each region. To do so, we have multiple options:\n\nadd the row group label in a dedicated row in the same column as the row labels:\n\n\nlife_table_gt |&gt; gt(groupname_col = \"region\")\n\n\n\n\n\n\n\ncountry\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nEast Asia & Pacific\n\n\nChina\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nEurope & Central Asia\n\n\nPortugal\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nLatin America & Caribbean\n\n\nNicaragua\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\n\nadd the row group label in a new column:\n\n\nlife_table_gt |&gt; gt(groupname_col = \"region\", row_group_as_column = TRUE)\n\n\n\n\n\n\n\n\ncountry\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nEast Asia & Pacific\nChina\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nEurope & Central Asia\nPortugal\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nLatin America & Caribbean\nNicaragua\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nFor both options, including rowname_col = \"country\" would add the country names as row labels. Doing so, R would add a vertical lines between these labels and the table body. With one dedicated column per row group, the table would include two vertical lines: one separating the row group labels from the row labels and one separating the row labels from the table body.\nHere, we used an existing variable in the table to add row groups. You can also create these groups without such a variable. To do so, you use the tab_row_group() function: tab_row_group(data, label, rows, id = label). In addition to the data, this function needs a label for the row group (label = \"\") and well as the rows to add to group (rows =). You can further specify an id for the row group. By default {gt} uses the label as id. If you add an id, e.g. id = rg1, you can use that id in subsequent operation, e.g. to style that row group. To identify rows, you can use e.g. &lt;tidy-select&gt; syntax or filter rows using boolean operators. To illustrate, we”ll add two groups: “high income” and “low income” and filter rows for the first group as those where 2020 per capita GDP is higher than 22000 and group countries whose 2020 per capita GDP is lower than or equal to 22000 under the “low income label). Note that the level (22000) is chosen arbitrarily. To add these labels we identify rows using rows = '2020_gdp_capita &gt; 22000 for the first label and rows = '2020_gdp_capita &lt;= 22000 for the second.\n\nlife_table_gt |&gt; gt() |&gt;\n  tab_row_group(\n    group = \"high income\", \n    rows = `2020_gdp_capita` &gt; 22000) |&gt;\n  tab_row_group(\n    group = \"low income\", \n    rows = `2020_gdp_capita` &lt;= 22000)\n\nWarning: Since gt v0.3.0 the `group` argument has been deprecated.\n• Use the `label` argument to specify the group label.\nThis warning is displayed once every 8 hours.\n\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nlow income\n\n\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\nhigh income\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\n\n\n\n\n\nUsing boolean operators, you can add more refined row groups, e.g. rows = var1 &lt; x & (var2 &gt; y | var3 &lt; z) would collect rows where the first variable is smaller than x and the second variable is larger than y or the third variable is smaller than z.\nLet’s return to the table without row groups or row labels. Recall that this table includes the variables 1980_gdp_capita and 2020_gdp_capita. However, these are now shown next to each other: the data frame shows the “1980” variable for both per capita GDP and life expectancy at birth first while it shows their “2020” values in the last two columns. Here, {gt} uses the order as they appear in the dataset. Suppose you would like to show per capita GDP and life expectancy next to each other. In other words, you would like to table to show the “1980” and “2020” values for per capita gdp first and those for life expectancy in the last two columns. There are two ways to do this. First there is the cols_move() function in {gt}:\n\ncols_move(data, columns, after)\n\nHere you can identify the columns you like to move as well as a location using after. You can identify the columns referring to their name, include them in a vector or use &lt;tidy-select&gt; syntax. For instance, to move the column “2020_gdp_capita” after “1980_gdp_capita” and using &lt;tidy-select&gt; syntax (starts_with()):\n\nlife_table_gt |&gt; gt() |&gt; cols_move(starts_with(\"2020_gdp\"), after = `1980_gdp_capita`)\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n2020_gdp_capita\n1980_life_exp\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n10358.170\n64.42000\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n34650.797\n75.98902\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n41451.326\n74.65341\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n19778.714\n71.21463\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n7990.693\n71.15756\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n42362.647\n72.80032\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n1959.788\n57.91400\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n4005.839\n56.02400\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n15789.614\n67.57200\n74.40600\n\n\n\n\n\n\n\nThe second option would be to use {dplyr}’s relocate() function and change the data frame used to pipe into gt():\n\nlife_table_gt &lt;- life_table_gt |&gt; \n  dplyr::relocate(`2020_gdp_capita`, .after = `1980_gdp_capita`)\n\n\n\n12.2.2 Formatting\nThe dataset includes two variables measured in USD (per capita gdp for 1980 and 2020) and two numeric variables. As you could in {ggplot2}, e.g. using {scales} (see Chapter 11), you can add formatting of various “standard” formats in {gt} as well. For instance, using fmt_currency() and fmt_number() you can change the formatting of values in the columns that include currency or numeric values. Before we do so to format the table, let’s review the arguments of the latter function:\n\nfmt_number(\n  data,\n  columns = everything(),\n  rows = everything(),\n  decimals = 2,\n  n_sigfig = NULL,\n  drop_trailing_zeros = FALSE,\n  drop_trailing_dec_mark = TRUE,\n  use_seps = TRUE,\n  accounting = FALSE,\n  scale_by = 1,\n  suffixing = FALSE,\n  pattern = \"{x}\",\n  sep_mark = \",\",\n  dec_mark = \".\",\n  force_sign = FALSE,\n  system = c(\"intl\", \"ind\"),\n  locale = NULL\n)\n\nThe first three arguments should be familiar: they allow to identify the data as well as the columns and rows. By default, {gt} using everything(). In other words fmt_number() formats all rows of every column. If you add specific columns, e.g. using one of the &lt;tidy-select&gt; helper functions, the column name, a vector with column names of their column number, {gt} will format all rows for these columns. Likewise, if you identify a specific rows, R will format all columns for each of these rows. Setting both, R will format the cells that are at the intersection of the identified columns and rows. The next 6 arguments allow you to specify the number of decimals, the number of significant figures, to determine if the table needs to drop trailing zero’s (redundant zeros after the decimal mark), if the the table needs to drop trailing decimal marks (if FALSE, the table will show 15. and not 15), if there is a thousand separator identified in sep_mark = (different from the one in the locale) and if the table needs to show negative values within brackets (accounting = TRUE). The scale argument works as it did in {ggplot2}. As an alternative to scale, you can use suffixing. Here you can supply {gt} with T (trillion), B (billion) M (million) andK (thousand). R will scale the values accordingly and add a suffix. For instance, using suffixing = \"M\", the table will show 7.5M if the data includes 7500000. As an alternative, you can add suffixing = TRUE and R will determine the appropriate scale factor. If you specify a pattern using {x} for the value, the table will show that pattern e.g. the pattern “approx {x}” would add “approx” to every value. The arguments sep_markand dec_mark allow you to specify the decimal mark (by default a dot) and the thousand separator (by default a comma). If you change the locale, these default values will change. However, adding a specif mark here overrules the locale. If you want to add a plus sign to positive values, you can change the value force_sign from FALSE into TRUE. For the system, by default it used the international system where thousands are separated (i.e. three digits). The alternative is the Indian system.\nThe arguments of fmt_currency() are very similar\n\nfmt_currency(\n  data,\n  columns = everything(),\n  rows = everything(),\n  currency = NULL,\n  use_subunits = TRUE,\n  decimals = NULL,\n  drop_trailing_dec_mark = TRUE,\n  use_seps = TRUE,\n  accounting = FALSE,\n  scale_by = 1,\n  suffixing = FALSE,\n  pattern = \"{x}\",\n  sep_mark = \",\",\n  dec_mark = \".\",\n  force_sign = FALSE,\n  placement = \"left\",\n  incl_space = FALSE,\n  system = c(\"intl\", \"ind\"),\n  locale = NULL\n)\n\nHere, the argument use_subunits = TRUE shows sub-units, e.g. cent for the Euro. Changing this into FALSE removes these sub-units e.g. €15.12. becomes €15. {gt} includes a wide range of currencies and symbols. To add the Euro symbol for instance, you can use currency = \"EUR\". Doing so, R will add a “€” sign to the values in the columns and rows. By default, it will add these symbols left. To change that into right you can change placement = \"left\" into \"right\". R supports the following symbols:\n\ninfo_currencies(type = \"symbol\")\n\n\n\n\n\n\n\nCurrencies Supported in gt\n\n\nCurrency symbols are used in the fmt_currency() function.\n\n\nCurrency Symbol Keyword\nFormatted Currency\n\n\n\n\ndollar\n$49.95\n\n\neuro\n€49.95\n\n\npound\n£49.95\n\n\nyen\n¥49.95\n\n\nfranc\n₣49.95\n\n\nlira\n₤49.95\n\n\npeseta\n₧49.95\n\n\nwon\n₩49.95\n\n\nsheqel\n₪49.95\n\n\ndong\n₫49.95\n\n\nkip\n₭49.95\n\n\ntugrik\n₮49.95\n\n\ndrachma\n₯49.95\n\n\npeso\n₱49.95\n\n\nguarani\n₲49.95\n\n\naustral\n₳49.95\n\n\nhryvnia\n₴49.95\n\n\ncedi\n₵49.95\n\n\nrupee\n₹49.95\n\n\ngeneric\n¤49.95\n\n\n\n\n\n\n\nTo see which currencies {gt} supports, you can use info_currencies(begins_with = ). For instance, to show all currencies that start with “a”:\n\ninfo_currencies(begins_with = \"a\")\n\n\n\n\n\n\n\nCurrencies Supported in gt\n\n\nCurrency codes are used in the fmt_currency() function\n\n\nCurrencyName\nAlphaCode\nNumericCode\nNo. ofSubunits\nFormattedCurrency\n\n\n\n\nUnited Arab Emirates dirham\nAED\n784\n2\nDH49.95\n\n\nAfghan afghani\nAFN\n971\n2\n؋49.95\n\n\nAlbanian lek\nALL\n8\n2\nLek49.95\n\n\nArmenian dram\nAMD\n51\n2\nAMD49.95\n\n\nNetherlands Antillean guilder\nANG\n532\n2\nNAƒ49.95\n\n\nAngolan kwanza\nAOA\n973\n2\nKz49.95\n\n\nArgentine peso\nARS\n32\n2\n$49.95\n\n\nAustralian dollar\nAUD\n36\n2\n$49.95\n\n\nAruban florin\nAWG\n533\n2\nƒ49.95\n\n\nAzerbaijani manat\nAZN\n944\n2\n₼49.95\n\n\n\n\n\n\n\nUsing the three digit alpha code, R will show the currencies with their symbol.\nThere are many other fmt_ function including e.g. fmt_percent, fmt_date, fmt_time, fmt_datetime for the date and time (for the style for date/time options, see the guide in the {gt}’s online material), fmt_fraction, fmt_roman (to show Roman number, e.g. IV, XII) and a general fmt() function. Here, you can add a function e.g. paste (e.g. fns = \\(x) paste(\"[\", x/2, \"]\") to divide the values by 2 and add square brackets. For these function, most of the arguments follow from their format and you can use Iannone et al. (2025) to see them.\nTo illustrate using live_table_gt let’s add a currency format to the gdp variables and a number format to the life expectancy format. In both cases, we will show two decimals. For the currency, we will use the dollar and add a white space as separator mark:\n\nlife_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\", \n    sep_mark = \" \") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2)\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n2020_gdp_capita\n1980_life_exp\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n$430.86\n$10 358.17\n64.42\n78.08\n\n\nJapan\nEast Asia & Pacific\n$19 334.37\n$34 650.80\n75.99\n84.56\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12 553.22\n$41 451.33\n74.65\n85.50\n\n\nPortugal\nEurope & Central Asia\n$10 832.52\n$19 778.71\n71.21\n80.98\n\n\nBulgaria\nEurope & Central Asia\n$3 431.52\n$7 990.69\n71.16\n73.66\n\n\nGermany\nEurope & Central Asia\n$23 977.27\n$42 362.65\n72.80\n81.04\n\n\nNicaragua\nLatin America & Caribbean\n$1 850.01\n$1 959.79\n57.91\n71.80\n\n\nGuatemala\nLatin America & Caribbean\n$3 287.02\n$4 005.84\n56.02\n71.80\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n$8 612.16\n$15 789.61\n67.57\n74.41\n\n\n\n\n\n\n\nTo change the currency values using suffixing and show life expectancy with 1 significant digit after the decimal we can use suffixing = TRUE in the currency formatting function and n_sigfig = 3 in the number formatting function.\n\nlife_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\", \n    suffixing = TRUE) |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    n_sigfig = 3)\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n2020_gdp_capita\n1980_life_exp\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n$430.86\n$10.36K\n64.4\n78.1\n\n\nJapan\nEast Asia & Pacific\n$19.33K\n$34.65K\n76.0\n84.6\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12.55K\n$41.45K\n74.7\n85.5\n\n\nPortugal\nEurope & Central Asia\n$10.83K\n$19.78K\n71.2\n81.0\n\n\nBulgaria\nEurope & Central Asia\n$3.43K\n$7.99K\n71.2\n73.7\n\n\nGermany\nEurope & Central Asia\n$23.98K\n$42.36K\n72.8\n81.0\n\n\nNicaragua\nLatin America & Caribbean\n$1.85K\n$1.96K\n57.9\n71.8\n\n\nGuatemala\nLatin America & Caribbean\n$3.29K\n$4.01K\n56.0\n71.8\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n$8.61K\n$15.79K\n67.6\n74.4\n\n\n\n\n\n\n\nHere, you can see that {gt} scales the values with 1/1000 and adds a “K”. For the number of years, the table now shows 1 digit after the decimal sign and R rounded using the usual rounding rules (Chapter 3).\nThere are a couple of useful functions to deal with missing values, zeros or very large and very small values. The first sub_missing() allows you to substitute missing values with a different character. By default the table will show “—”, however, you can change that if you add a specific character or white space. As with all other functions, you can use this substitution for specific columns/rows. A similar function is sub_zero(). This function allows you to replace “0”-s in the table. By default the table will add “nil”. Using sub_small_vals() you can substitute small values, i.e. values below a threshold which you can set as one of this function’s arguments with a alternative pattern, e.g. “&lt;0.01”. By default, R uses 0.01 as a threshold and changes all values smaller than this threshold to “&lt;0.01”. In other words, 0.0025, 0.00025 will all be set to “&lt;0.01”. In a table that includes significance levels for e.g. a t-statistics or regression estimates, this is a useful way to change values such as 0.0000 into “&lt;0.01”. sub_large_vals() performs a similar calculations for very large values.\nIn addition to the format, you can also align the columns left, right or center. To do so, you can use cols_align(align = , columns = ) where you add “left”, “center” or “right” after align and identify the columns after the columns = argument e.g. using the &lt;tidy-select&gt; syntax, a vector with column names or a vector with column positions. For numeric values, you can further use cols_align_decimal(columns = , dec_mark = \".\", locale = ). Using this function {gt} will align the columns using the decimal mark specified in dec_mark = or the decimal mark that follows from the locale argument. This last function is usually only relevant if you didn’t specify the number of decimals and the number of decimals is different for the elements in the column’s cells. To illustrate, we’ll align the numeric columns using cols_align_decimal and the character columns using cols_align. Using the latter, we’ll center the character columns:\n\nlife_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    scale_by = 1,\n    columns = 3:4,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_align(\n    columns =  where(is.character), \n    align = \"center\") |&gt;\n  cols_align_decimal(\n    columns = where(is.numeric))\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n2020_gdp_capita\n1980_life_exp\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n   $430.86\n$10,358.17\n64.42\n78.08\n\n\nJapan\nEast Asia & Pacific\n$19,334.37\n$34,650.80\n75.99\n84.56\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12,553.22\n$41,451.33\n74.65\n85.50\n\n\nPortugal\nEurope & Central Asia\n$10,832.52\n$19,778.71\n71.21\n80.98\n\n\nBulgaria\nEurope & Central Asia\n $3,431.52\n $7,990.69\n71.16\n73.66\n\n\nGermany\nEurope & Central Asia\n$23,977.27\n$42,362.65\n72.80\n81.04\n\n\nNicaragua\nLatin America & Caribbean\n $1,850.01\n $1,959.79\n57.91\n71.80\n\n\nGuatemala\nLatin America & Caribbean\n $3,287.02\n $4,005.84\n56.02\n71.80\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n $8,612.16\n$15,789.61\n67.57\n74.41\n\n\n\n\n\n\n\n\n\n12.2.3 Labels, headers, footnotes and sources\nBefore we continue, let’s first assign the formatted table to an object life_table_gt_form. Doing so, we can use this table object with formatted columns as the starting point in our code. Here, we will use 2 decimals for both the number of currency format:\n\nlife_table_gt_form &lt;- life_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2)\n\nThe table includes the variable names as they appear in the data set. To change the column labels, you can use cols_label(). Here, you have two options. The first is to add labels of each column where you would like to change the label. To illustrate Markdown, we’ll add bold to the label for the variables country and region and change the other ones referring only to their years and written in italics. To add these bold and italic fonts, using md() you add the part you would like in bold between double stars and the parts you would like in italics between single stars. For every old name you include a new name using old name = new name:\n\nlife_table_gt_form |&gt;\n  cols_label(\n    country = md(\"**Country**\"), \n    region = md(\"**Region**\"), \n    `1980_gdp_capita` = md(\"*1980*\"),\n    `2020_gdp_capita` = md(\"*2020*\"),\n    `1980_life_exp` = md(\"*1980*\"),\n    `2020_life_exp` = md(\"*2020*\"))\n\n\n\n\n\n\n\nCountry\nRegion\n1980\n2020\n1980\n2020\n\n\n\n\nChina\nEast Asia & Pacific\n$430.86\n$10,358.17\n64.42\n78.08\n\n\nJapan\nEast Asia & Pacific\n$19,334.37\n$34,650.80\n75.99\n84.56\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12,553.22\n$41,451.33\n74.65\n85.50\n\n\nPortugal\nEurope & Central Asia\n$10,832.52\n$19,778.71\n71.21\n80.98\n\n\nBulgaria\nEurope & Central Asia\n$3,431.52\n$7,990.69\n71.16\n73.66\n\n\nGermany\nEurope & Central Asia\n$23,977.27\n$42,362.65\n72.80\n81.04\n\n\nNicaragua\nLatin America & Caribbean\n$1,850.01\n$1,959.79\n57.91\n71.80\n\n\nGuatemala\nLatin America & Caribbean\n$3,287.02\n$4,005.84\n56.02\n71.80\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n$8,612.16\n$15,789.61\n67.57\n74.41\n\n\n\n\n\n\n\nThe table now shows new column labels: “1980”, “2020”, “1980” and “2020”. There is a second way to change column labels. Most variable names in a dataset include a part which you can use for your labels. In 1980_life_exp for instance, the variable name includes “1980” but also “life”, “exp” or “life_exp”. We can use these parts to build column labels. To do so, we first extract the column names from the dataset using colnames(). The result is a vector that includes the variable names as they appear in the data. We can now extract the parts of these names that we need. For instance, in this dataset, we need the 1980 and 2020. To extract these parts, we can use one of the string function (Chapter 3 or Chapter 4). Here we can use e.g. stringr::str_remove() to remove the part “_gdp_capita” and “life_exp”. As an alternative, you can use str_extract_all() or grep(value = TRUE) to extract the strings you want to include in the labels of the variables. We now have a vector with names we like to include in the table. Using this vector, we can add formatting. Here, we do so using str_to_title, alternatives are toupper, tolower … or you can replace e.g. life by “life expectancy” using a string replace function. The result is a vector with variables names as we would like them in the table. In the last step, we create a named vector where the names of the variables as they appear in the data are used to name the labels we want in the table. Do do so, we use names() to add names to the names_for_table vector. The result is a vector where the column names are the names as they appear in the dataset and the values are the names as they should be used in the table.\n\nnames_in_data &lt;- colnames(life_table_gt)\nnames_for_table &lt;- names_in_data |&gt; stringr::str_remove(\"_[a-z]+_[a-z]+\")\nnames_for_table &lt;- names_for_table |&gt; stringr::str_to_title()\nnames(names_for_table) &lt;- names_in_data\n\nWe can now use this vector, names_for_table in the cols_label() argument. To do so, we add .list = names_for_table. Using this named vector, R will use the names of every column to identify the names as they appear in the dataset. If there is a match with the names in the dataset, R will replace these names with the values in the vector. In other words, it will replace the names as they appear in the data with the names as they should show in the table. If there is no match, R will not changese these labels.\n\nlife_table_gt_form |&gt;\n  cols_label(.list = names_for_table)\n\n\n\n\n\n\n\nCountry\nRegion\n1980\n2020\n1980\n2020\n\n\n\n\nChina\nEast Asia & Pacific\n$430.86\n$10,358.17\n64.42\n78.08\n\n\nJapan\nEast Asia & Pacific\n$19,334.37\n$34,650.80\n75.99\n84.56\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12,553.22\n$41,451.33\n74.65\n85.50\n\n\nPortugal\nEurope & Central Asia\n$10,832.52\n$19,778.71\n71.21\n80.98\n\n\nBulgaria\nEurope & Central Asia\n$3,431.52\n$7,990.69\n71.16\n73.66\n\n\nGermany\nEurope & Central Asia\n$23,977.27\n$42,362.65\n72.80\n81.04\n\n\nNicaragua\nLatin America & Caribbean\n$1,850.01\n$1,959.79\n57.91\n71.80\n\n\nGuatemala\nLatin America & Caribbean\n$3,287.02\n$4,005.84\n56.02\n71.80\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n$8,612.16\n$15,789.61\n67.57\n74.41\n\n\n\n\n\n\n\nThe column labels are set. We can now add spanner columns. To do so, we use tab_spanner(). To add these spanners, we have to identify to columns for each spanner. To do so, we use the &lt;tidy-select&gt; helpers. For instance, using contains(\"gdp\") to identify all columns for the first spanner, R will “year_per capita gdp”. For the second spanner using we use contains(\"life\") to collect all the “year_life expectancy” columns. There are many alternatives: you can refer to the columns by their number (e.g. c(3, 4), 3:4) or in a vector by their (old) name. The spanner label is added after the label argument in tab_spanner(). Here,we use Markdown (md()) to show them in bold by including the label between **. As an alternative, you can use a traditional “character label” and add styling later.\n\nlife_table_gt_form |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nRegion\n\nPer capita GDP\n\n\nLife expectancy\n\n\n\n1980\n2020\n1980\n2020\n\n\n\n\nChina\nEast Asia & Pacific\n$430.86\n$10,358.17\n64.42\n78.08\n\n\nJapan\nEast Asia & Pacific\n$19,334.37\n$34,650.80\n75.99\n84.56\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n$12,553.22\n$41,451.33\n74.65\n85.50\n\n\nPortugal\nEurope & Central Asia\n$10,832.52\n$19,778.71\n71.21\n80.98\n\n\nBulgaria\nEurope & Central Asia\n$3,431.52\n$7,990.69\n71.16\n73.66\n\n\nGermany\nEurope & Central Asia\n$23,977.27\n$42,362.65\n72.80\n81.04\n\n\nNicaragua\nLatin America & Caribbean\n$1,850.01\n$1,959.79\n57.91\n71.80\n\n\nGuatemala\nLatin America & Caribbean\n$3,287.02\n$4,005.84\n56.02\n71.80\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n$8,612.16\n$15,789.61\n67.57\n74.41\n\n\n\n\n\n\n\nThe table includes column and spanner labels. Now we can add a header using tab_header() and a source using tab_source_note(). The first allows you to set a title and a subtitle after the title = and subtitle = arguments. For the second, you add the text of the note after the source_note argument. To add a footnote, {gt} includes the tab_footnote() function. Here, you have to add the text of the footnote after the footnote argument as well as the location and the placement of the footnote. For the first, you have to identify where you want to show the footnote. Do so so, you can use e.g cells_title(groups = \"title\" or \"subtitle\") to add a note to the title or subtitle, cells_stubhead() to add the footnote to the stub head, cells_column_labels(columns = ) to add a note to one or more column labels identified in columns, cells_column_spanner(spanners = ) to add a note to one or more column spanners, … . Here we will add a note to the data for Germany for 1980 to explain that these data refer to Western Germany. To do so we use cells_body() and use the columns and rows argument to identify the exact table body cells. The note should be added to the columns including (“1980”) and the rows that show data for Germany. Here, we will add the names of the columns in a vector. Note that starts_with(\"1980\"), contains(\"1980\"), … could have been used as an alternative. For the rows, we identify them using rows = country == \"Germany\". Note that for the last argument, you start with rows = and then add the condition column == value. The condition can include multiple boolean operations. With respect to the placement, you can add the footnote reference “right” or “left” of the content of the cell. By default, R uses numbers. You can change these marks using tab_options(footnotes.marks = ). Here, you can add e.g. letters, LETTERS, extended or standard. The first two use the letters vectors and add lower or upper case letters while the last two use symbols: an Asterisk, a dagger, a double dagger, a section sign, a double vertical line and a paragraph sign. The first, extended uses all 6, the second uses only the first four. To add the headers, source note and footnote to the table and using numbers for footnote marks, we add these to the previous table:\n\nlife_table_gt_form |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      Region\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    China\nEast Asia & Pacific\n$430.86\n$10,358.17\n64.42\n78.08\n    Japan\nEast Asia & Pacific\n$19,334.37\n$34,650.80\n75.99\n84.56\n    Hong Kong SAR, China\nEast Asia & Pacific\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Portugal\nEurope & Central Asia\n$10,832.52\n$19,778.71\n71.21\n80.98\n    Bulgaria\nEurope & Central Asia\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Germany\nEurope & Central Asia\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Nicaragua\nLatin America & Caribbean\n$1,850.01\n$1,959.79\n57.91\n71.80\n    Guatemala\nLatin America & Caribbean\n$3,287.02\n$4,005.84\n56.02\n71.80\n    Trinidad and Tobago\nLatin America & Caribbean\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nLet’s now add the group rows back to the table. We can add them as a separate row in the same column the row labels:\n\nlife_table_gt |&gt; gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China\n$430.86\n$10,358.17\n64.42\n78.08\n    Japan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    Hong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    Portugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    Bulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Germany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    Nicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    Guatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    Trinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nor use a separate column to show these row group labels:\n\nlife_table_gt |&gt; gt(\n  groupname_col = \"region\",\n  row_group_as_column = TRUE) |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    East Asia & Pacific\nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    Japan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    Hong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Europe & Central Asia\nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    Bulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Germany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Latin America & Caribbean\nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    Guatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    Trinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nUsing a spanner, we added a label on top of two or more columns. You can also merge to columns. Merging columns is useful when your dataset includes e.g. two variables that show the boundaries of a confidence interval (e.g. 1.96 standard deviation below and above the mean). You can leave these values as they are and add a spanner label “confidence interval” but you could also merge these two columns and show the values as e.g. [lower, higher] or (lower - higher) To do so, {gt} includes the cols_merge() function:\n\ncols_merge(\n  data,\n  columns,\n  hide_columns = columns[-1],\n  rows = everything(),\n  pattern = NULL)\n\nAs always, the first argument is the data. The second argument identifies the columns to merge. The third argument, hide_columns = columns[-1] shows the column label to hide. By default hides all but the first columns. To see this, recall from Chapter 4 that if you subset a vector with a minus sign, R shows all but the values on the index position with the minus sign. Here, as R hides all columns expect those not in the hide_columns argument, using columns[-1] hides all, but the first. In addition to the columsn, you can also target rows. The last argument shows the pattern for the merged column values. Here, you identify columns using {1} for the first column in the columns argument, {2} for the second, … . Adding a pattern, e.g. “{1} - {2}” will show the merged columns as “value_col1 - value_col2”. Although the life expectancy dataset doesn’t include variable where you would often use merged column, to illustrate, we’ll merge the gdp columns and use a pattern [1980gdp - 2020gdp]. Because there is not need for a spanner label, we will remove the spanner label for gdp:\n\nlife_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    scale_by = 1,\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\") |&gt;\n  cols_merge(\n    columns = 3:4,\n    pattern = \"[{1} - {2}]\") \n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      Region\n      1980\n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n    \n  \n  \n    China\nEast Asia & Pacific\n[$430.86 - $10,358.17]\n64.42\n78.08\n    Japan\nEast Asia & Pacific\n[$19,334.37 - $34,650.80]\n75.99\n84.56\n    Hong Kong SAR, China\nEast Asia & Pacific\n[$12,553.22 - $41,451.33]\n74.65\n85.50\n    Portugal\nEurope & Central Asia\n[$10,832.52 - $19,778.71]\n71.21\n80.98\n    Bulgaria\nEurope & Central Asia\n[$3,431.52 - $7,990.69]\n71.16\n73.66\n    Germany\nEurope & Central Asia\n[$23,977.27 - $42,362.65]1\n72.801\n81.04\n    Nicaragua\nLatin America & Caribbean\n[$1,850.01 - $1,959.79]\n57.91\n71.80\n    Guatemala\nLatin America & Caribbean\n[$3,287.02 - $4,005.84]\n56.02\n71.80\n    Trinidad and Tobago\nLatin America & Caribbean\n[$8,612.16 - $15,789.61]\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nNote in the output that {gt} also reformats the footnote: it add the the number after (recall the footnote was placed on the right) the pattern used to show the merged cells. R uses the label of the first column for the merged column label. We can change that label and also format other elements of this column, e.g. re-align. To do the first, we’ll use cols_label and set a new label of this column. To do the second, we use cols_align and use that function to center this merged column\n\nlife_table_gt |&gt; gt() |&gt;\n  fmt_currency(\n    scale_by = 1,\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\") |&gt;\n  cols_merge(\n    columns = 3:4,\n    pattern = \"[{1} - {2}]\") |&gt;\n  cols_label(\n    `1980_gdp_capita` = \"Per capita gdp\") |&gt;\n  cols_align(\n    columns = contains(\"gdp\"), \n    align = \"center\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      Region\n      Per capita gdp\n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n    \n  \n  \n    China\nEast Asia & Pacific\n[$430.86 - $10,358.17]\n64.42\n78.08\n    Japan\nEast Asia & Pacific\n[$19,334.37 - $34,650.80]\n75.99\n84.56\n    Hong Kong SAR, China\nEast Asia & Pacific\n[$12,553.22 - $41,451.33]\n74.65\n85.50\n    Portugal\nEurope & Central Asia\n[$10,832.52 - $19,778.71]\n71.21\n80.98\n    Bulgaria\nEurope & Central Asia\n[$3,431.52 - $7,990.69]\n71.16\n73.66\n    Germany\nEurope & Central Asia\n[$23,977.27 - $42,362.65]1\n72.801\n81.04\n    Nicaragua\nLatin America & Caribbean\n[$1,850.01 - $1,959.79]\n57.91\n71.80\n    Guatemala\nLatin America & Caribbean\n[$3,287.02 - $4,005.84]\n56.02\n71.80\n    Trinidad and Tobago\nLatin America & Caribbean\n[$8,612.16 - $15,789.61]\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nLet’s return to the table with un-merged columns. Here, you can see that the columns for “Per capita GDP” and “Life expectancy at birth” have a different width. As R tries to show the table using as little space as possible and life expectancy data only need 5 positions (2 decimals, a dot and 2 decimals) while the gdp columns all require more; R limits the width of the former. Sometimes you don’t want that and you columns with the same width. Using cols_width() you can specify the columns and add their width using ~px(value) where px refers to pixels. Here, we will set the width of the gdp and life columns equals to 100 pixels:\n\nlife_table_gt |&gt; gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    scale_by = 1,\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n   tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\") |&gt;\n  cols_width(\n    contains(\"gdp\") ~px(100), \n    contains(\"life\") ~px(100))\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China\n$430.86\n$10,358.17\n64.42\n78.08\n    Japan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    Hong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    Portugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    Bulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Germany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    Nicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    Guatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    Trinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nAs an alternative to the pixel size, you can also use pct(x) which changes the size of a component (e.g. text or width) with x%.\n\n\n12.2.4 Summary data\nTables often show summary data. {gt} allows you to add summary statistics to the table where it used the values in the table body to calculate these statistics. Before we show how you can do this, we’ll first save the table as we have it now and add the summary to this table:\n\nlife_table_gt_lab &lt;- life_table_gt |&gt; gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = c(`1980_life_exp`, `2020_life_exp`)) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\")\n\nThere are two ways to add summary data: using grand_summary_rows() you add one block of summary statistics whose values will be calculated from all rows in every column you identify:\n\ngrand_summary_rows(\n  data,\n  columns = everything(),\n  fns = NULL,\n  fmt = NULL,\n  side = c(\"bottom\", \"top\"),\n  missing_text = \"---\")\n\nThe arguments of this function, in addition to the data, identify the columns for which you want to calculate summary statistics. By default, {gt} calculates summary statistics for all column in the data (columns = everything()). You can change this using a vector with column names, column positions or using the &lt;tidy-select&gt; syntax. Using the second argument, fns = you add which summary statistics you would like to show. To do so, you add these in a list. This list includes the label that you will see in the table as well as the function to use. For the latter, you use the ‘tilde’ approach and add a dot to show R where it needs to add the data for each column. For instance, to add the mean variable for a column with label “Average”, you can use Average = ~mean(., na.rm = TRUE ). In the fmt argument, you specify the formatting of the values in the summary columns and rows. Here, you use one of the fmt_() functions to format e.g. currencies, numbers, percentages. To tell R it needs to use the data from the summary function, you start these function with a tilde and add a dot inside the formatting function. In addition, you can add the side(\"bottom\" or \"top\"). The first is the default and adds the summary rows at the bottom of the table. Using missing_text = \" \" you can specify how {gt} should show the values in the columns where there are no summary statistics.\nTo illustrate, let’s add the average, minimum and maximum values for each numeric column and use the same format as the one for the values in the body of the table. To to so, we add the grand_summary_rows(), specify the columns and functions and use fmt_ functions to format. Without the latter, {gt} would show the summary rows using default formatting.\n\nlife_table_gt_lab |&gt;\n  grand_summary_rows(\n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\")) |&gt;\n  grand_summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2))\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n—\n$9,367.66\n$19,816.40\n67.97\n77.98\n    Minimum\n—\n$430.86\n$1,959.79\n56.02\n71.80\n    Maximum\n—\n$23,977.27\n$42,362.65\n75.99\n85.50\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nTo add statistics per group (or a number of groups) {gt} includes summary_rows(). Here the arguments are identical to those for grand_summary_rows() with one difference: in the former you can specify the groups, while in the latter this is not the case. In other words, with summary_rows you can add summary data for one specific group in the table. Here, we’ll show summary statistics for all groups (groups = everything()):\n\nlife_table_gt_lab |&gt;\n  summary_rows(\n    groups = everything(), \n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\")) |&gt;\n  summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2))\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n—\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n—\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n—\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n—\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n—\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n—\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n—\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n—\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n—\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nTo illustrate some of the options: let’s add summary statistics for per capita GDP for East Asia and Pacific and Europe and Central Asia and for life expectancy for Latin America & Caribbean and remove the default lines for missing values and use empty cells to show these missing values:\n\nlife_table_gt_lab |&gt;\n  summary_rows(\n    groups = contains(\"Asia\"), \n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\"), \n    missing_text = \"\") |&gt;\n  summary_rows(\n    groups = contains(\"Latin\"), \n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2),\n    missing_text = \"\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n\n\n    Minimum\n\n$430.86\n$10,358.17\n\n\n    Maximum\n\n$19,334.37\n$41,451.33\n\n\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n\n\n    Minimum\n\n$3,431.52\n$7,990.69\n\n\n    Maximum\n\n$23,977.27\n$42,362.65\n\n\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n\n\n60.50\n72.67\n    Minimum\n\n\n\n56.02\n71.80\n    Maximum\n\n\n\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nThe table now includes a limited set of summary statistics for both variables in both years. In addition, missing values - e.g. cells without a summary statistic - are now shown as empty.\n\n\n12.2.5 Highlighting cells\nYou can highlight specific cells in the body of the table by fill the background of the cell with a color. Before we illustrate how you can do this, we”ll first save the table where we showed grand summary rows to an new table object life_table_gt_sum and add these visual cues to that table:\n\nlife_table_gt_sum &lt;- life_table_gt_lab |&gt;\n  grand_summary_rows(\n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\")) |&gt;\n  grand_summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2), \n    missing_text = \"\")\n\nThe first way to color cells is to use the data_color function from the {gt} package:\n\ndata_color(\n  data,\n  columns = everything(),\n  rows = everything(),\n  direction = c(\"column\", \"row\"),\n  target_columns = NULL,\n  method = c(\"auto\", \"numeric\", \"bin\", \"quantile\", \"factor\"),\n  palette = NULL,\n  domain = NULL,\n  bins = 8,\n  quantiles = 4,\n  levels = NULL,\n  ordered = FALSE,\n  na_color = NULL,\n  alpha = NULL,\n  reverse = FALSE,\n  fn = NULL,\n  apply_to = c(\"fill\", \"text\"),\n  autocolor_text = TRUE,\n  contrast_algo = c(\"apca\", \"wcag\")\n\nMost of the arguments are straightforward: in addition to the data, you need to add in which columns and rows {gt} will add a color. In addition, with direction you determine if coloring applies to the rows or columns. By default, this is the column. The method refers to how {gt} needs to set the colors for different values. By default (auto) {gt} will use numeric for numeric data and factor if data are factors. With bin R will determine the number of colors from the bins argument. By default, the number of bins is 8. The methods quantile sets the number of colors equal to the number of quantiles set in the quantiles argument. By default, this value is 4. Changing this to two, R would use 2 colors: one to show values below the median and one for values larger than the median. For the method factor, you can specify the levels in levels. If these factors are ordered, you can change ordered = FALSE in TRUE. In that case {gt} will order the colors in line with the factors. If there are missing values, you can set the color is these cells using na_color. With the palette function you set the color palette. Here, you can use e.g. a viridis, ColorBrewer or paletter color palette. In addition you can add your own colors using a vector. If you add a domain, values outside of that domain will be be shown using the palette colors. With alpha you change the transparency and using reverse you change the order. You can set the background color of the cell or the text. By default, {gt} fills the background color. Setting apply_to to “text”, applies the color to the text. With fn you can use e.g. {scales} functions to map colors to ranges.\nTo illustrate these arguments, we’ll use life_table_gt_sum and highlight the columns for including gdp data using 4 quantiles and the colorBrewer palette “YlOrRd” (Yellow - Orange - Red). To do so, we use &lt;tidy-select&gt; syntax contains(\"gdp\") to select the columns, set method = quantile and use quantiles = 4. The palette is palette = \"YlOrRd\". We’ll use the viridis color palette to fill the columns with life expectancy data. Here, we include a domain which sets the values to color between 50 and 85. The dataset includes one value for life expectancy that falls outside of this range. As R treats those as “NA”, we can set a different color to highlight these values. Here we use “yellow” but you could any other color.\n\nlife_table_gt_sum |&gt;\n  data_color(\n    columns = contains(\"gdp\"), \n    method = \"quantile\", \n    palette = \"YlOrRd\",\n    quantiles = 4) |&gt;\n  data_color(\n    columns = contains(\"life\"),\n    method = \"numeric\",\n    palette = \"viridis\",\n    reverse = TRUE,\n    domain = c(50, 85), \n    na_color = \"yellow\")\n\nWarning: Some values were outside the color scale and will be treated as NA\n\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$9,367.66\n$19,816.40\n67.97\n77.98\n    Minimum\n\n$430.86\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$23,977.27\n$42,362.65\n75.99\n85.50\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nThe result now shows the table where cells are filled with a color. Note that {gt} sets the color of the text to keep it as visible as possible. For instance, all values in the gdp columns filled with yellow or orange are shown in black text, while those in red are shown with a while colored text. The value “85.50” for Hong Kong is shown in yellow. To change the color of the text and not the fill color, we need to add the apply_to = \"text\" argument. To illustrate, we’ll use the ggthemr::solarized palette (included in {paletteer} to color the text for life expectancy data and use red and green to color gdp data. We’ll show gdp using 2 bins:\n\nlife_table_gt_sum |&gt;\n  data_color(\n    columns = contains(\"life\"),\n    method = \"numeric\",\n    palette = \"ggthemr::solarized\",\n    reverse = TRUE, \n    apply_to = \"text\") |&gt;\n  data_color(\n    columns = contains(\"gdp\"), \n    method = \"bin\", \n    palette = c(\"red\", \"green\"),\n    bins = 2, \n    apply_to = \"text\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$9,367.66\n$19,816.40\n67.97\n77.98\n    Minimum\n\n$430.86\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$23,977.27\n$42,362.65\n75.99\n85.50\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nRecall that the data_color() function includes the target_column = argument. Here, you can fill a column’s cells using values in another column. In other words and in our example you could fill the column cells for life expectancy using e.g. data for gdp. To do you, you include the values to determine the color in columns and the columns to fill in target_columns. For instance, using the data for 1980 per capita gdp to color 1980 life expectancy and 2020 per capita gdp to color 2020 life expectancy using colorBrewer “Blues” for the first and “Reds” for the second, the table shows:\n\nlife_table_gt_sum |&gt;\n  data_color(\n    columns = contains(\"1980_gdp\"),\n    target_columns = contains(\"1980_life\"),\n    method = \"numeric\",\n    palette = \"Blues\") |&gt;\n  data_color(\n    columns = contains(\"2020_gdp\"),\n    target_columns = contains(\"2020_life\"),\n    method = \"numeric\",\n    palette = \"Reds\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$9,367.66\n$19,816.40\n67.97\n77.98\n    Minimum\n\n$430.86\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$23,977.27\n$42,362.65\n75.99\n85.50\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nA useful application of this approach is to color an “empty” column. To do so, we first create an empty column in the table. You can do this using the mutate(empty_col = \"\") and introduce a new data frame to the gt() function or you can use cols_add() function. Using the latter, you set the set the name of the column and its values as well as its position using e.g. .after or .before. Here, we will illustrate the latter approach. We’ll add two columns: col_gdp and col_life, keep them empty (e.g. set their values using \"\") and position them after the 2020 gdp and life columns. To include these columns in the spanner label, we have to adjust the code for life_table_gt_sum: we add the two empty columns after the formatting functions fmt_ using cols_add(col_gdp = \"\", .after = contains(\"2020_gdp)) and cols_add(col_life = \"\", .after = contains = (\"2020_life\")) and include these columns in the spanner. We keep all other components of the table as in life_table_gt_sum and assign this new table to live_table_gt_sum2:\n\nlife_table_gt_sum2 &lt;- life_table_gt |&gt; gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = contains(\"life\"), \n    decimals = 2) |&gt;\n  cols_add(\n    col_gdp = \"\", .after = contains(\"2020_gdp\")) |&gt;\n  cols_add(\n    col_life = \"\", .after = contains(\"2020_life\")) |&gt;\n  cols_label(.list = names_for_table) |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = contains(\"life\")) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"Source: World Bank Development indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\", \n    cells_body(columns = c(`1980_gdp_capita`, `1980_life_exp`), rows = country == \"Germany\"), \n    placement = \"right\") |&gt;\n  grand_summary_rows(\n    columns = contains(\"gdp_capita\"),\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\"),\n    missing_text = \"\") |&gt;\n  grand_summary_rows(\n    columns = contains(\"life_exp\"),\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2), \n    missing_text = \"\")\n\nlife_table_gt_sum2\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      col_gdp\n      1980\n      2020\n      col_life\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n\n64.42\n78.08\n\n    \nJapan\n$19,334.37\n$34,650.80\n\n75.99\n84.56\n\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n\n74.65\n85.50\n\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n\n71.21\n80.98\n\n    \nBulgaria\n$3,431.52\n$7,990.69\n\n71.16\n73.66\n\n    \nGermany\n$23,977.271\n$42,362.65\n\n72.801\n81.04\n\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n\n57.91\n71.80\n\n    \nGuatemala\n$3,287.02\n$4,005.84\n\n56.02\n71.80\n\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n\n67.57\n74.41\n\n    Average\n\n$9,367.66\n$19,816.40\n\n67.97\n77.98\n\n    Minimum\n\n$430.86\n$1,959.79\n\n56.02\n71.80\n\n    Maximum\n\n$23,977.27\n$42,362.65\n\n75.99\n85.50\n\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nAs you can see the table now includes two empty columns: col_gdp and col_life. We can now fill these columns using 2020 per capita gdp and 2020 life expectancy using data_color. To do so, we identify the columns to use using the fact that they both include 2020 and the target columns using the fact that they both include “col”. On the last lines, we remove the label and change the width of both these colored columns:\n\nlife_table_gt_sum2 |&gt;\n    data_color(\n    columns = contains(\"2020\"),\n    target_columns = contains(\"col\"),\n    method = \"numeric\",\n    palette = \"viridis\") |&gt;\n  cols_label(\n    col_gdp = \"\", \n    col_life = \"\") |&gt;\n  cols_width(\n    contains(\"col\") ~ px(10))\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      \n      1980\n      2020\n      \n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n\n64.42\n78.08\n\n    \nJapan\n$19,334.37\n$34,650.80\n\n75.99\n84.56\n\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n\n74.65\n85.50\n\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n\n71.21\n80.98\n\n    \nBulgaria\n$3,431.52\n$7,990.69\n\n71.16\n73.66\n\n    \nGermany\n$23,977.271\n$42,362.65\n\n72.801\n81.04\n\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n\n57.91\n71.80\n\n    \nGuatemala\n$3,287.02\n$4,005.84\n\n56.02\n71.80\n\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n\n67.57\n74.41\n\n    Average\n\n$9,367.66\n$19,816.40\n\n67.97\n77.98\n\n    Minimum\n\n$430.86\n$1,959.79\n\n56.02\n71.80\n\n    Maximum\n\n$23,977.27\n$42,362.65\n\n75.99\n85.50\n\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nThe table now includes two columns whose color reflects the values in the column immediately to the left. Using these, you can now visualize where countries differ: for instance if a country shows a “good” color for per capita gdp, but doesn’t show the same color for life expectancy, it suggests that it does good in gdp, but less so on life expectancy.\n{gtExtras} includes two functions that you can use to highlight rows (gt_highlight_rows) or columns (gt_highlight_cols). Both functions allows you to set the fill color, transparency, font weight (“normal”, “lighter”, “bolder” or a numeric variable between 1 and 1000) as well as the font color. In addition, these functions also include the option to set a target column. To illustrate, we’ll use gt_highlight_rows() to highlight all countries whose life expectancy in 2020 was higher than 80 using a fill color “steelblue” and setting the font color to “white” and the font weight to “bolder”. To select the rows, we use rows = 2020_life_exp &gt;= 80:\n\nlife_table_gt_sum2 |&gt;\n  gt_highlight_rows(\n    rows = `2020_life_exp` &gt;= 80,\n    fill = \"steelblue\", \n    font_weight = \"bolder\",\n    font_color = \"white\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      col_gdp\n      1980\n      2020\n      col_life\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n\n64.42\n78.08\n\n    \nJapan\n$19,334.37\n$34,650.80\n\n75.99\n84.56\n\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n\n74.65\n85.50\n\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n\n71.21\n80.98\n\n    \nBulgaria\n$3,431.52\n$7,990.69\n\n71.16\n73.66\n\n    \nGermany\n$23,977.271\n$42,362.65\n\n72.801\n81.04\n\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n\n57.91\n71.80\n\n    \nGuatemala\n$3,287.02\n$4,005.84\n\n56.02\n71.80\n\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n\n67.57\n74.41\n\n    Average\n\n$9,367.66\n$19,816.40\n\n67.97\n77.98\n\n    Minimum\n\n$430.86\n$1,959.79\n\n56.02\n71.80\n\n    Maximum\n\n$23,977.27\n$42,362.65\n\n75.99\n85.50\n\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nUsing gt_highlight_cols() you can perform similar tasks for the columns.\n\n\n12.2.6 Finetuning: adding images and charts\n{gt} and {gtExtras} both include functions that allow you to add images or graphs to a table. To illustrate we’ll use the same countries and variables as in the previous examples, but now use the country iso2 and iso3 codes included in iso2c and iso3c:\n\nlife_table_gt_2 &lt;- life_table |&gt; dplyr::select(iso2c, iso3c, region, starts_with(c(\"1980\", \"2020\"))) |&gt; dplyr::relocate(`2020_gdp_capita`, .after = `1980_gdp_capita`)\n\nUsing this data frame in gt() with the region as row group variable, the labels for the iso-variables set to an empty value and otherwise identical to the the previous example, the table now looks as:\n\nlife_table_gt_3 &lt;- life_table_gt_2 |&gt;\n  gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  fmt_number(\n    columns = 5:6, \n    decimals = 2) |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"World Bank Development Indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\",\n    cells_body(columns = 1, rows = 6), \n    placement = \"right\") |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = contains(\"life\")) |&gt;\n  cols_label(\n    iso2c = \"\",\n    iso3c = \"\",\n    `1980_gdp_capita` = md(\"*1980*\"),\n    `2020_gdp_capita` = md(\"*2020*\"),\n    `1980_life_exp` = md(\"*1980*\"),\n    `2020_life_exp` = md(\"*2020*\"))\n\nlife_table_gt_3\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    CN\nCHN\n$430.86\n10,358.17\n64.42\n78.07700\n    JP\nJPN\n$19,334.37\n34,650.80\n75.99\n84.56000\n    HK\nHKG\n$12,553.22\n41,451.33\n74.65\n85.49634\n    \n      Europe & Central Asia\n    \n    PT\nPRT\n$10,832.52\n19,778.71\n71.21\n80.97561\n    BG\nBGR\n$3,431.52\n7,990.69\n71.16\n73.65854\n    DE1\nDEU\n$23,977.27\n42,362.65\n72.80\n81.04146\n    \n      Latin America & Caribbean\n    \n    NI\nNIC\n$1,850.01\n1,959.79\n57.91\n71.79500\n    GT\nGTM\n$3,287.02\n4,005.84\n56.02\n71.79700\n    TT\nTTO\n$8,612.16\n15,789.61\n67.57\n74.40600\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nThe first function we will use is fmt_country()\n\n  data,\n  columns = everything(),\n  rows = everything(),\n  pattern = \"{x}\",\n  sep = \" \",\n  locale = NULL)\n\nThis function uses the iso-codes in the column included in the columns and rows identified in columns = and rows = to add full country names to the table. Doing so, it replaces the iso codes. This function supports 242 regions. Using the locale to set a locale, these names will be shown in the language of the locale. You can add a pattern to the name in pattern. Here you use {x} to identify the country. By default, {gt} shows the country name. Using this function with life_table_gt_3 with the iso-codes in iso3c:\n\nlife_table_gt_3 |&gt;\n  fmt_country(columns = \"iso3c\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    CN\nChina\n$430.86\n10,358.17\n64.42\n78.07700\n    JP\nJapan\n$19,334.37\n34,650.80\n75.99\n84.56000\n    HK\nHong Kong\n$12,553.22\n41,451.33\n74.65\n85.49634\n    \n      Europe & Central Asia\n    \n    PT\nPortugal\n$10,832.52\n19,778.71\n71.21\n80.97561\n    BG\nBulgaria\n$3,431.52\n7,990.69\n71.16\n73.65854\n    DE1\nGermany\n$23,977.27\n42,362.65\n72.80\n81.04146\n    \n      Latin America & Caribbean\n    \n    NI\nNicaragua\n$1,850.01\n1,959.79\n57.91\n71.79500\n    GT\nGuatemala\n$3,287.02\n4,005.84\n56.02\n71.79700\n    TT\nTrinidad & Tobago\n$8,612.16\n15,789.61\n67.57\n74.40600\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nAs you can see, the table now includes the country names and not the country iso3 codes.\nWith fmt_flag() you can add the national flags of a country. This function supports the same regions and includes the same arguments. In addition, you can set the height of the flag using the argument height. To illustrate using the first column with iso2 codes for the flags and the second column for the country names:\n\nlife_table_gt_3 |&gt;\n  fmt_country(columns = 2) |&gt;\n  fmt_flag(columns = 1)\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China\nChina\n$430.86\n10,358.17\n64.42\n78.07700\n    Japan\nJapan\n$19,334.37\n34,650.80\n75.99\n84.56000\n    Hong Kong\nHong Kong\n$12,553.22\n41,451.33\n74.65\n85.49634\n    \n      Europe & Central Asia\n    \n    Portugal\nPortugal\n$10,832.52\n19,778.71\n71.21\n80.97561\n    Bulgaria\nBulgaria\n$3,431.52\n7,990.69\n71.16\n73.65854\n    Germany1\nGermany\n$23,977.27\n42,362.65\n72.80\n81.04146\n    \n      Latin America & Caribbean\n    \n    Nicaragua\nNicaragua\n$1,850.01\n1,959.79\n57.91\n71.79500\n    Guatemala\nGuatemala\n$3,287.02\n4,005.84\n56.02\n71.79700\n    Trinidad & Tobago\nTrinidad & Tobago\n$8,612.16\n15,789.61\n67.57\n74.40600\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nTo show the flags closer to the country name, we can merge the first two columns:\n\nlife_table_gt_3 |&gt;\n  fmt_country(columns = 2) |&gt;\n  fmt_flag(columns = 1) |&gt;\n  cols_merge(columns = 1:2)\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China China\n$430.86\n10,358.17\n64.42\n78.07700\n    Japan Japan\n$19,334.37\n34,650.80\n75.99\n84.56000\n    Hong Kong Hong Kong\n$12,553.22\n41,451.33\n74.65\n85.49634\n    \n      Europe & Central Asia\n    \n    Portugal Portugal\n$10,832.52\n19,778.71\n71.21\n80.97561\n    Bulgaria Bulgaria\n$3,431.52\n7,990.69\n71.16\n73.65854\n    Germany Germany1\n$23,977.27\n42,362.65\n72.80\n81.04146\n    \n      Latin America & Caribbean\n    \n    Nicaragua Nicaragua\n$1,850.01\n1,959.79\n57.91\n71.79500\n    Guatemala Guatemala\n$3,287.02\n4,005.84\n56.02\n71.79700\n    Trinidad & Tobago Trinidad & Tobago\n$8,612.16\n15,789.61\n67.57\n74.40600\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nIn addition to flags, {gt} and {gtExtras} include functions to add images () to a table. For instance, using {gtExtra}’s gt_img_rows(gt_object, columns, img_source = \"web\", height = 30) you can add images to the table in gt_object. gt_img_rows will add the images to the columns in columns. The images can be stored on the “web” or “local” and by default, their height is 30. To illustrate, we’ll update an example shown in Albert Rapp’s Creating beautiful tables in R with {gt} and create a table which includes pictures of the recent prime ministers of the United Kingdom. The dataset includes the name of the prime minister in the first column as well as a web location where you can find a picture in the second:\n\npm_data &lt;- tibble::tribble(\n  ~Name, ~Image,\n  \"Keir Starmer\", \"https://upload.wikimedia.org/wikipedia/commons/9/91/Prime_Minister_Keir_Starmer_Portrait_%28cropped%29.jpg\",\n  \"Rishi Sunak\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Rishi_Sunak%27s_first_speech_as_Prime_Minister_Front_%28cropped%29.jpg/1024px-Rishi_Sunak%27s_first_speech_as_Prime_Minister_Front_%28cropped%29.jpg\",\n  \"Liz Truss\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Liz_Truss_official_portrait_%28cropped%292.jpg/292px-Liz_Truss_official_portrait_%28cropped%292.jpg\",\n  \"Boris Johnson\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Boris_Johnson_official_portrait_%28cropped%29.jpg/288px-Boris_Johnson_official_portrait_%28cropped%29.jpg\", \n  \"Theresa May\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Official_portrait_of_Baroness_May_of_Maidenhead_crop_2.jpg/1024px-Official_portrait_of_Baroness_May_of_Maidenhead_crop_2.jpg\")\n\nTo create a table, we first change the data frame into an gt_tbl object and add this table to the gt_img_rows function:\n\npm_data |&gt;\n  gt() |&gt;\n  gt_img_rows(columns = 'Image', height = 100)\n\n\n\n\n\n\n\nName\nImage\n\n\n\n\nKeir Starmer\n\n\n\nRishi Sunak\n\n\n\nLiz Truss\n\n\n\nBoris Johnson\n\n\n\nTheresa May\n\n\n\n\n\n\n\n\nIn addition to images or emoji’s, {gtExtras} includes a number of functiosn to add a chart to a table. Using gt_plt_dumbbell() it is possible to change the values in two columns in a dumbbell chart. To do you, you add the first and last value of the dumbbell in the arguments col1 and col2. You can further specify the label and the color palette. The first color is used to fill the first point of the dumbbell chart, the second to fill the second point and the third color to set the color of the horizontal line. The chart will show, the values. you can set the size and other formats of these values using text_args and text_size:\n\ngt_plt_dumbbell(\n  gt_object,\n  col1 = NULL,\n  col2 = NULL,\n  label = NULL,\n  palette = c(\"#378E38\", \"#A926B6\", \"#D3D3D3\"),\n  width = 70,\n  text_args = list(accuracy = 1),\n  text_size = 2.5\n)\n\nTo see what these default values show, let’s add a dumbbell charts for life expectancy:\n\nlife_table_gt_3 |&gt;\n  gt_plt_dumbbell(col1 = `1980_life_exp`, col2 = `2020_life_exp`)\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    CN\nCHN\n$430.86\n10,358.17\n    6478\n    JP\nJPN\n$19,334.37\n34,650.80\n    7685\n    HK\nHKG\n$12,553.22\n41,451.33\n    7585\n    \n      Europe & Central Asia\n    \n    PT\nPRT\n$10,832.52\n19,778.71\n    7181\n    BG\nBGR\n$3,431.52\n7,990.69\n    7174\n    DE1\nDEU\n$23,977.27\n42,362.65\n    7381\n    \n      Latin America & Caribbean\n    \n    NI\nNIC\n$1,850.01\n1,959.79\n    5872\n    GT\nGTM\n$3,287.02\n4,005.84\n    5672\n    TT\nTTO\n$8,612.16\n15,789.61\n    6874\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nThe table needs some additional layout: e.g. adding country names and flags; removing the 1980 life expectancy labels, … but in general, this table illustrates the use of a chart as part of a table. This is an example of a chart which uses values in the table. In the Try this out “Adding other plots” we include two additional examples to add plots to a table\n\n\n\n\n\n\nTry this out: Adding other plots\n\n\n\n\n\nFor a dumbbell chart, you could use the data in the table. For other charts, you need more data. In other words, you have to use a data frame which includes, in addition to the data you would like to show in the table, the data you want to add to e.g. a sparkline, a density plot or a bullet plot. To so so, we will use the fact that a data frame is essentially a list (Chapter 4). In other words, we can add a column which includes list. In the example, to add a sparkline showing all life expectancy data for all years for each of the 9 countries, we have to add a column which includes these values. We will do this in various steps.\nFirst we identify the unique countries in our data. To do so, we use unique(live_table$iso3c). This function returns the unique iso3 codes for the dataset we use. In the second step, we import the full life expectancy at birth dataset. This dataset includes, for all countries, all data for each year. As we don’t need all data for all countries but only the data for the 9 countries in the table, we use filter to filter from the full dataset only those countries we want to show in the table. This is the first inline of code after importing the full dataset. In the third step, we add a column which includes all values for life expectancy at birth for each of the 9 countries in the dataset. To do so, we first group_by country and then add a column using mutate(). This column includes a list will all data in the variable life_exp. Because we used group_by this list will include data per country in the group_by variable. We now have a dataset which includes a new variable: data_spark. For each country, the values of this variable are a list with all life expectancy at birth values per country.\nIn the fifth step, we will merge this dataset with the dataset we used to create the tables. As we only want to add the values for life expectancy, we can select the two columns to keep: one column that we can use to join and the data_spark column with the values to use for the sparkline. This is the last step in this code.\n\ncountries &lt;- unique(life_table$iso3c)\nlife_df &lt;- readr::read_csv(here::here(\"data\", \"raw\", \"life_df.csv\"))\n\nRows: 13671 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): iso2c, iso3c, country, region\ndbl (4): date, gdp_capita, life_exp, pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlife_table_dataspark &lt;- life_df |&gt; \n  dplyr::filter(iso3c %in% countries) |&gt; \n  dplyr::group_by(iso3c) |&gt;\n  dplyr::mutate(data_spark = list(life_exp)) |&gt;\n  dplyr::select(iso3c, data_spark)\n\nUsing this data frame we can now add the data for the sparkline to the life_table dataset that we used to create the tables. To do so, we use inner_join:\n\nlife_table_spark &lt;- life_table |&gt; dplyr::inner_join(life_table_dataspark, by = \"iso3c\", multiple = \"first\")\n\nlife_table_spark now includes all variables we need:\n\nstr(life_table_spark)\n\nspc_tbl_ [9 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ iso2c          : chr [1:9] \"CN\" \"JP\" \"HK\" \"PT\" ...\n $ iso3c          : chr [1:9] \"CHN\" \"JPN\" \"HKG\" \"PRT\" ...\n $ country        : chr [1:9] \"China\" \"Japan\" \"Hong Kong SAR, China\" \"Portugal\" ...\n $ region         : chr [1:9] \"East Asia & Pacific\" \"East Asia & Pacific\" \"East Asia & Pacific\" \"Europe & Central Asia\" ...\n $ 1980_gdp_capita: num [1:9] 431 19334 12553 10833 3432 ...\n $ 1980_life_exp  : num [1:9] 64.4 76 74.7 71.2 71.2 ...\n $ 2020_gdp_capita: num [1:9] 10358 34651 41451 19779 7991 ...\n $ 2020_life_exp  : num [1:9] 78.1 84.6 85.5 81 73.7 ...\n $ data_spark     :List of 9\n  ..$ : num [1:63] 33.3 40.5 50.8 51.4 52.2 ...\n  ..$ : num [1:63] 67.7 68.4 68.6 69.7 70.2 ...\n  ..$ : num [1:63] 65.9 66.6 67 67.7 68.4 ...\n  ..$ : num [1:63] 64 62.7 64.2 64.8 65 ...\n  ..$ : num [1:63] 69.2 70.2 69.5 70.3 71.1 ...\n  ..$ : num [1:63] 69.1 69.6 69.8 69.9 70.4 ...\n  ..$ : num [1:63] 46.1 46.9 47.6 48.3 49.2 ...\n  ..$ : num [1:63] 45.8 46.5 47 47.4 47.7 ...\n  ..$ : num [1:63] 63 62.9 63.4 63.8 63.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   iso2c = col_character(),\n  ..   iso3c = col_character(),\n  ..   country = col_character(),\n  ..   region = col_character(),\n  ..   `1980_gdp_capita` = col_double(),\n  ..   `1980_life_exp` = col_double(),\n  ..   `2020_gdp_capita` = col_double(),\n  ..   `2020_life_exp` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nLet’s now use this new dataset to create a table. Because we will show life expectancy using a sparkline, we don’t need to select the 1980 and 2020 values for this variable In the code below, you’ll recognize that we build the table as we already did. However, as we add life expectancy as a sparkline, we omit the fmt_number and set a label for the sparkline and remove the spanner. At the end, we add {gtExtras}’ function gt_plt_sparkline(). The function includes a number of option, e.g. to add reference lines showing median or mean values, …, set the color palette. To see these options, you can check the function. Here, we only include the column which holds the sparkline data:\n\nlife_table_spark |&gt; select(iso2c, iso3c, region, `1980_gdp_capita`, `2020_gdp_capita`, data_spark) |&gt;\n  gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"World Bank Development Indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\",\n    cells_body(columns = 1, rows = 6), \n    placement = \"right\") |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  cols_label(\n    iso2c = \" \",\n    `1980_gdp_capita` = md(\"*1980*\"),\n    `2020_gdp_capita` = md(\"*2020*\"),\n    data_spark = md(\"*Life expectancy*\")) |&gt;\n  fmt_country(columns = 2) |&gt;\n  fmt_flag(columns = 1) |&gt;\n  cols_merge(columns = 1:2) |&gt;\n  gt_plt_sparkline(data_spark)\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n       \n      \n        Per capita GDP\n      \n      Life expectancy\n    \n    \n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China China\n$430.86\n10358.170\n          78.6\n    Japan Japan\n$19,334.37\n34650.797\n          84.0\n    Hong Kong Hong Kong\n$12,553.22\n41451.326\n          83.7\n    \n      Europe & Central Asia\n    \n    Portugal Portugal\n$10,832.52\n19778.714\n          81.6\n    Bulgaria Bulgaria\n$3,431.52\n7990.693\n          74.4\n    Germany Germany1\n$23,977.27\n42362.647\n          80.7\n    \n      Latin America & Caribbean\n    \n    Nicaragua Nicaragua\n$1,850.01\n1959.788\n          74.6\n    Guatemala Guatemala\n$3,287.02\n4005.839\n          68.7\n    Trinidad & Tobago Trinidad & Tobago\n$8,612.16\n15789.614\n          74.7\n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nAs an alternative using gl_plt_bullet you can add a bullet chart. These charts show the value of life expectancy and add a reference line which shows the a summary statistic using the data in the table. In the example below, we add the mean value for each group as the target. Do so do, we first need to add this mean to the dataset. We do so in two columns, one for each year, and label these columns target_col1 and target_col2. At the end of the code, we use gl_plt_bullet() to create a bullet chart: a bar to show the value for each country and a vertical line to show the target. You can find all options for this function in the reference guide. Here, we include three: we set the width of the bullet plot and add a color palette to fill the bar and show the target:\n\nlife_table_spark |&gt; \n  dplyr::group_by(region) |&gt; \n  dplyr::mutate(\n    target_col1 = mean(`1980_life_exp`),\n    target_col2 = mean(`2020_life_exp`)) |&gt; \n  dplyr::ungroup() |&gt; \n  dplyr::select(iso2c, iso3c, region, `1980_gdp_capita`, `2020_gdp_capita`, `1980_life_exp`, `2020_life_exp`, target_col1, target_col2) |&gt;\n  gt(groupname_col = \"region\") |&gt;\n  fmt_currency(\n    columns = 3:4,\n    decimals = 2,\n    currency = \"USD\") |&gt;\n  tab_header(\n    title = \"GDP per capita and life expectancy at birth\", \n    subtitle = \"Selected countries and years\") |&gt;\n  tab_source_note(\n    source_note = \"World Bank Development Indicators\") |&gt;\n  tab_footnote(\n    footnote = \"Data prior to 1991 refer to Western Germany (GDR)\",\n    cells_body(columns = 1, rows = 6), \n    placement = \"right\") |&gt;\n  tab_spanner(\n    label = md(\"**Per capita GDP**\"), \n    columns = contains(\"gdp\")) |&gt;\n  tab_spanner(\n    label = md(\"**Life expectancy**\"), \n    columns = contains(\"life\")) |&gt;\n  cols_label(\n    iso2c = \" \",\n    `1980_gdp_capita` = md(\"*1980*\"),\n    `2020_gdp_capita` = md(\"*2020*\"), \n    `1980_life_exp` = md(\"*1980*\"),\n    `2020_life_exp` = md(\"*2020*\")) |&gt;\n  fmt_country(columns = 2) |&gt;\n  fmt_flag(columns = 1) |&gt;\n  cols_merge(columns = 1:2) |&gt;\n  gt_plt_bullet(column = `1980_life_exp`, target = target_col1, width = 30,\n                palette = c(\"lightsteelblue1\", \"lightsteelblue4\")) |&gt;\n  gt_plt_bullet(column = `2020_life_exp`, target = target_col2, width = 30,\n                palette = c(\"steelblue4\", \"steelblue1\"))\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n       \n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China China\n$430.86\n10358.170\n          \n          \n    Japan Japan\n$19,334.37\n34650.797\n          \n          \n    Hong Kong Hong Kong\n$12,553.22\n41451.326\n          \n          \n    \n      Europe & Central Asia\n    \n    Portugal Portugal\n$10,832.52\n19778.714\n          \n          \n    Bulgaria Bulgaria\n$3,431.52\n7990.693\n          \n          \n    Germany Germany1\n$23,977.27\n42362.647\n          \n          \n    \n      Latin America & Caribbean\n    \n    Nicaragua Nicaragua\n$1,850.01\n1959.788\n          \n          \n    Guatemala Guatemala\n$3,287.02\n4005.839\n          \n          \n    Trinidad & Tobago Trinidad & Tobago\n$8,612.16\n15789.614\n          \n          \n  \n  \n    \n      World Bank Development Indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\n\n\n\n\n\n12.2.7 Styling\nWe can now add styling to the table. Styling refers to e.g. colors used to fill cells or print lines, font used in the text, the width of the table as well as the columns, … . {gt} includes a number of quick styling functions who generally start with opt_. These functions offer a quick way to set, e.g. the font for the entire table opt_table_font(), to set line opt_table_lines(\"all\", \"none\", \"default\") where all includes all possible lines, none shows no lines and default with the default (horizontal) lines, to align the table header opt_align_table_header(align = \"left\", \"center\", \"right\"), … . The function opt_style() allows you to quickly set 6 styles using 6 colors: “blue”, “cyan”, “pink”, “green”, “red” and “gray”. For instance using style = 6 and color = \"blue\", opt_style() shows this table:\n\nlife_table_gt_lab |&gt; \n  opt_stylize(style = 6, color = \"blue\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    China\n$430.86\n$10,358.17\n64.42\n78.08\n    Japan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    Hong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    \n      Europe & Central Asia\n    \n    Portugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    Bulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Germany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    \n      Latin America & Caribbean\n    \n    Nicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    Guatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    Trinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nChanging the style to 1 and the color to pink and adding summary rows to see how these are styled:\n\nlife_table_gt_lab |&gt;\n  summary_rows(\n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\"),\n    missing_text = \"\") |&gt;\n  summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2), \n    missing_text = \"\") |&gt;\n  opt_stylize(style = 1, color = \"pink\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\n{gtExtras} includes a number of themes, e.g. a theme base on the English newspaper the Guardian (gt_theme_guardian(gt_object, ...)). If you want to use this theme, you can add at at the end of the table.\n\nlife_table_gt_lab |&gt;\n  summary_rows(\n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\"),\n    missing_text = \"\") |&gt;\n  summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2), \n    missing_text = \"\") |&gt;\n  gt_theme_guardian()\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nLet’s now try to add some tailored styling. Setting a style is usually something you don’t need to do for every table. For instance, the font family, background colors or table headings or lines are often similar across tables. In other words, if you set these values once, you will be not to repeat these steps for subsequent tables. Before we do so, we first assign the table to life_table_gt_sum:\n\nlife_table_gt_sum &lt;- life_table_gt_lab |&gt;\n  summary_rows(\n    columns = 3:4,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_currency(\n      ., \n      decimals = 2, \n      currency = \"USD\"),\n    missing_text = \"\") |&gt;\n  summary_rows(\n    columns = 5:6,\n    fns = list(\n      Average = ~mean(., na.rm = TRUE),\n      Minimum = ~min(., na.rm = TRUE), \n      Maximum = ~max(., na.rm = TRUE)),\n    fmt = ~fmt_number(\n      ., \n      decimals = 2), \n    missing_text = \"\")\n\nTo add styling, you can use tab_options(). This function is similar to the theme() function in {ggplot2}. To see all options, I refer to the online reference for this function. In these arguments, you’ll recognize the arguments from the setup of the plot, e.g. column_labels_font_size, row_group_border_bottom_width, table_body_hlines.color … . As you can see, these arguments usually follow the same logic: first the part of the table (Figure 12.1), second the element to style and third the style option. The element to style and style options follow from the component of the table, e.g. for a label, you can add options to change the text e.g. font, size, alignment; for a border, you can set the color, the width or style, … . I’ll illustrate some of these options here:\n\nAdd some styling to the table header:\n\n\nlife_table_gt_sum |&gt;\n  tab_options(\n  heading.title.font.size = px(20),\n  heading.title.font.weight = \"bold\",\n  heading.subtitle.font.size = px(15),\n  heading.subtitle.font.weight = \"lighter\",\n  heading.border.bottom.style = \"double\",\n  heading.border.bottom.color = \"lavenderblush4\",\n  heading.background.color = \"lavenderblush2\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\n\nAdd some styling to the row groups, spanners and column labels:\n\n\nlife_table_gt_sum |&gt;\n  tab_options(\n  heading.title.font.size = px(20),\n  heading.subtitle.font.size = px(15),\n  heading.subtitle.font.weight = \"lighter\",\n  heading.border.bottom.style = \"double\",\n  heading.border.bottom.color = \"lavenderblush4\",\n  heading.background.color = \"lavenderblush2\", \n  row_group.background.color = \"lightsteelblue1\", \n  row_group.font.weight = \"bold\", \n  column_labels.background.color = \"lightgrey\")\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\n\nAdd some styling to the footnote and source note:\n\n\nlife_table_gt_sum |&gt;\n  tab_options(\n  heading.title.font.size = px(20),\n  heading.subtitle.font.size = px(15),\n  heading.subtitle.font.weight = \"lighter\",\n  heading.border.bottom.style = \"double\",\n  heading.border.bottom.color = \"lavenderblush4\",\n  heading.background.color = \"lavenderblush2\", \n  row_group.background.color = \"lightsteelblue1\", \n  row_group.font.weight = \"bold\",\n  column_labels.background.color = \"lightgrey\", \n  footnotes.font.size = px(8), \n  source_notes.font.size = px(8))\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\nIn addition to the tab_options function, {gt} includes two tab_style function allowing you to target specific cells. Using both, you first include the style and second the location. For the style, you can add cell_text, cell_fill or cell_border. Each of these include arguments relevant for the style (text size, fill color, border width, …). In addition you add a location to identify the location of a cell. To do so, you add e.g. cells_row_groups, cells_summary, cells_title, … . Combining style and location, you can target individual cells to style. If you want to change more than one style component, you add them in a list (e.g. list(cell_fill(), cell_text()). Doing so, you can change multiple style elements in a cell. If the same styling applies to multiple location, you add collect these too in a list, e.g. list(cells_summary(), cells_stub_summary()). As an example, here we will align the column labels “center”, fill the cell_row_groups in lightsteeblue1, set the text to “bold” and the borders at the top and the bottom to a width of 3 pixels and using #B0C4DE as the color. In addition, for the summary cells, we set the size of the text to 14 pixels and add color and width to their borders. We add the color steelblue2 to the header and set the size of the text using 25 pixels and add a color to the 1, 3, 4, 6, 7 and 9: lightblue.\n\nlife_table_gt_sum |&gt;\n    tab_style(\n    style = cell_text(align = \"center\"), \n    locations = list(cells_column_labels(), cells_column_spanners())) |&gt;\n  tab_style(\n    style = \n      list(\n        cell_fill(color = \"lightsteelblue1\"), \n        cell_text(weight = \"bold\"), \n        cell_borders(\n          sides = c(\"top\", \"bottom\"), \n          color = \"#B0C4DE\", \n          weight = px(3))),\n    locations = cells_row_groups()) |&gt;\n  tab_style(\n    style = list(cell_text(weight = \"lighter\", size = px(14)), cell_borders(sides = \"top\", color = \"#B0C4DE\", weight = px(2))), \n    locations = list(cells_summary(), cells_stub_summary())) |&gt;\n  tab_style(\n    style = list(cell_fill(color = \"steelblue2\"), cell_text(weight = \"bolder\"), size = px(25)), \n    locations = cells_title()) |&gt;\n    tab_style(\n    style = list(cell_fill(color = \"lightblue\")),\n    locations = cells_body(rows = c(1, 3, 4, 6, 7, 9)))\n\n\n\n\n  \n    \n      GDP per capita and life expectancy at birth\n    \n    \n      Selected countries and years\n    \n    \n      \n      Country\n      \n        Per capita GDP\n      \n      \n        Life expectancy\n      \n    \n    \n      1980\n      2020\n      1980\n      2020\n    \n  \n  \n    \n      East Asia & Pacific\n    \n    \nChina\n$430.86\n$10,358.17\n64.42\n78.08\n    \nJapan\n$19,334.37\n$34,650.80\n75.99\n84.56\n    \nHong Kong SAR, China\n$12,553.22\n$41,451.33\n74.65\n85.50\n    Average\n\n$10,772.82\n$28,820.10\n71.69\n82.71\n    Minimum\n\n$430.86\n$10,358.17\n64.42\n78.08\n    Maximum\n\n$19,334.37\n$41,451.33\n75.99\n85.50\n    \n      Europe & Central Asia\n    \n    \nPortugal\n$10,832.52\n$19,778.71\n71.21\n80.98\n    \nBulgaria\n$3,431.52\n$7,990.69\n71.16\n73.66\n    \nGermany\n$23,977.271\n$42,362.65\n72.801\n81.04\n    Average\n\n$12,747.10\n$23,377.35\n71.72\n78.56\n    Minimum\n\n$3,431.52\n$7,990.69\n71.16\n73.66\n    Maximum\n\n$23,977.27\n$42,362.65\n72.80\n81.04\n    \n      Latin America & Caribbean\n    \n    \nNicaragua\n$1,850.01\n$1,959.79\n57.91\n71.80\n    \nGuatemala\n$3,287.02\n$4,005.84\n56.02\n71.80\n    \nTrinidad and Tobago\n$8,612.16\n$15,789.61\n67.57\n74.41\n    Average\n\n$4,583.06\n$7,251.75\n60.50\n72.67\n    Minimum\n\n$1,850.01\n$1,959.79\n56.02\n71.80\n    Maximum\n\n$8,612.16\n$15,789.61\n67.57\n74.41\n  \n  \n    \n      Source: World Bank Development indicators\n    \n  \n  \n    \n      1 Data prior to 1991 refer to Western Germany (GDR)\n    \n  \n\n\n\n\ntab_style_body() allows you to fill individual cells.\nNote that adding color to cells has to be consistent with earlier formatting using e.g. data_color(): if some cells are filled using data_color() there would be an inconsistency if you fill the cells of the body of the table using tab_style. In addition, adding color with a plot, a flag or another image risks reducing the visual attraction of that plot, flag or image. Here again, adding color is something that you need to do carefully. As a last remark: it is not because {gt} and {gtExtras} allow you to change almost all components of a table that you actually have to do that. Often trying to limit the number of fills, border styles of widths will improve your table. Here, I refer to Tom Mock’s 10 Guidelines with gt for more information on how you can build good (and bad) tables.\n\n\n\n\n\n\nGohel, David, and Panagiotis Skintzos. 2024. Flextable: Functions for Tabular Reporting. https://ardata-fr.github.io/flextable-book/.\n\n\nHenry, Lionel, and Hadley Wickham. 2024. Tidyselect: Select from a Set of Strings. https://tidyselect.r-lib.org.\n\n\nHlavac, Marek. 2022. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Social Policy Institute. https://CRAN.R-project.org/package=stargazer.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. Gt: Easily Create Presentation-Ready Display Tables. https://gt.rstudio.com.\n\n\nMock, Thomas. 2024. gtExtras: Extending ’Gt’ for Beautiful HTML Tables. https://github.com/jthomasmock/gtExtras.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "12_Tables.html#components-of-a-table",
    "href": "12_Tables.html#components-of-a-table",
    "title": "12  Tables",
    "section": "",
    "text": "starts_with(match, ignore.case = FALSE, vars = NULL)\nends_with(match, ignore.case = FALSE, vars = NULL)\ncontains(match, ignore.case = FALSE, vars = NULL)\nmatches(match, ignore.case = FALSE, vars = NULL)\nIn all these functions match refers to an expression. If more than one expression is included, they should by included in c(). For the first three functions, the expression is exact (e.g. “var”, “new”, …). Using matchyou can also include regular expressions. The second argument, ignore.case = FALSE is familiar from other character functions. The last argument, vars allows you to include a vector of variable names. By default, the variable names are taken from the dataset.\nnum_range(prefix, range, suffix = \"\", width = NULL, vars = NULL)\nUsing this function you can select variables such as var1x, var2x or x001, x002. To do so, you can include a prefix, e.g. “var”, a suffix (e.g. “x” in var1x) and a numerical range, e.g. 1:2. Adding a width allows you to select columns such as x002 where width in this case is 3 (x002 includes three numbers: 0, 0 and 2).\neveryting(vars = NULL)\nSelect all columns or those in the vector vars.\nlast_col(offset = 0L, vars = NULL)\nSelect the last column by default. If offset is different from zero (e.g. 10), this function selects the 10nd variable from the end. By default, this function selects the last column from the dataset.\ngroup_cols()\nSelect the columns that are used to group a dataset.\nall_of(), any_of(x, vars = NULL)\nHere, x is a vector with variable names. R will select all columns that are included in this vector. The first is strict: if one of the names in the vector doesn’t appear in the data, R will return a error. The second doesn’t. If you add not, !any_of can be used to remove variables from a dataset.\nwhere(fn)\nThe argument fn refers to a function that returns TRUE or FALSE. For instance, to select all numeric columns, where(is.numeric) can be used. Likewise, to select all variables whose minimum is negative: where(\\(x) min(x) &lt; 0) would select these columns.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "12_Tables.html#getting-started",
    "href": "12_Tables.html#getting-started",
    "title": "12  Tables",
    "section": "12.3 Getting started",
    "text": "12.3 Getting started\nAs a first step, you introduce the data frame in the gt() function. This function has a couple of arguments:\n\ngt(\n  data,\n  rowname_col = \"rowname\",\n  groupname_col = dplyr::group_vars(data),\n  process_md = FALSE,\n  caption = NULL,\n  rownames_to_stub = FALSE,\n  row_group_as_column = FALSE,\n  auto_align = TRUE,\n  id = NULL,\n  locale = getOption(\"gt.locale\"),\n  row_group.sep = getOption(\"gt.row_group.sep\", \" - \")\n)\n\nThe data refer to the data frame or tibble. As this is the first argument of the function, you can use the pipe operator |&gt; to introduce your dataset in this function. The next two arguments deal with the table stub. Using the first, you can include a column whose values will be used for the row labels. The second, groupname_col allows you to identify the column whose values will be used to add row group labels. Recall that a data frame can include row names. If this is the case, there is an alternative to add row labels: using rownames_to_stub = TRUE will add these row names as label to the plot. Using row_group_as_column you can determine if the row group labels will be put in a separate column. By default, this is not the case and they will be added in a dedicated row in the same column as the one that includes the row labels. The locale arguments includes the possibility to change, e.g. the language. Using this function with the life_table dataset and accepting all default values shows the gt_tbl object:\n\nlife_table |&gt; gt()\n\n\n\n\n\n\n\niso2c\niso3c\ncountry\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nCN\nCHN\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJP\nJPN\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHK\nHKG\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPT\nPRT\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBG\nBGR\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nDE\nDEU\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNI\nNIC\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGT\nGTM\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTT\nTTO\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nThe table includes the variable names of the column labels. By default, {gt} also adds a number of horizontal lines: at the top and bottom of the column label cells and at the bottom of the table. The table doesn’t include any vertical lines. For now, we will remove the iso2c and iso3c columns. As we have the country name in the variable country, these variables don’t add value. To do so, we use the select function from {dplyr} and assign this data set to life_table_gt.\n\nlife_table_gt &lt;- life_table |&gt; dplyr::select(country, region, starts_with(c(\"1980\", \"2020\")))\n\nThe table now looks like\n\nlife_table_gt |&gt; gt()\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nWe can add row labels we use the option rowname_col and include the variable country to use for the row labels:\n\nlife_table_gt |&gt; gt(rowname_col = \"country\") \n\n\n\n\n\n\n\n\nregion\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nNote two differences with the case where we didn’t add row names: the table now includes the country names of row labels and doesn’t add a stubhead (the reference to “country” disappeared) and the table now includes a vertical line separating the row labels from the table body.\nThe data frame includes a variable region which we can use to add row group labels. Doing so, we have multiple options:\n\nadd the row group label in a dedicated row in the same column as the row labels:\n\n\nlife_table_gt |&gt; gt(groupname_col = \"region\")\n\n\n\n\n\n\n\ncountry\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nEast Asia & Pacific\n\n\nChina\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nEurope & Central Asia\n\n\nPortugal\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nLatin America & Caribbean\n\n\nNicaragua\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\n\nadd the row group label in a new column:\n\n\nlife_table_gt |&gt; gt(groupname_col = \"region\", row_group_as_column = TRUE)\n\n\n\n\n\n\n\n\ncountry\n1980_gdp_capita\n1980_life_exp\n2020_gdp_capita\n2020_life_exp\n\n\n\n\nEast Asia & Pacific\nChina\n430.8554\n64.42000\n10358.170\n78.07700\n\n\nJapan\n19334.3747\n75.98902\n34650.797\n84.56000\n\n\nHong Kong SAR, China\n12553.2179\n74.65341\n41451.326\n85.49634\n\n\nEurope & Central Asia\nPortugal\n10832.5150\n71.21463\n19778.714\n80.97561\n\n\nBulgaria\n3431.5213\n71.15756\n7990.693\n73.65854\n\n\nGermany\n23977.2699\n72.80032\n42362.647\n81.04146\n\n\nLatin America & Caribbean\nNicaragua\n1850.0056\n57.91400\n1959.788\n71.79500\n\n\nGuatemala\n3287.0240\n56.02400\n4005.839\n71.79700\n\n\nTrinidad and Tobago\n8612.1558\n67.57200\n15789.614\n74.40600\n\n\n\n\n\n\n\nFor both options, including rowname_col = \"country\" would add the country names as row labels. Doing so, R would add a vertical lines between these labels and the table body. With one dedicated column per row group, the table would include two vertical lines: one separating the row group labels from the row labels and one separating the row labels from the table body.\nLet’s return to the table without row groups of row labels. Recall that this table includes the variables 1980_gdp_capita and 2020_gdp_capita. However, these are now shown next to each other: the data frame shows the “1980” variable for both per capita GDP and life expectancy at birth first while it shows their “2020” values in the last two columns. Suppose you would like to show per capita GDP and life expectancy next to each other. In other words, you would like to table to show the “1980” and “2020” values for per capita gdp first and those for life expectancy in the last two columns. There are two ways to do this. First there is the cols_move() function in {gt}:\n\nols_move(data, columns, after)\n\n\nlife_table_gt |&gt; gt() |&gt; cols_move(starts_with(\"2020_gdp\"), after = `1980_gdp_capita`)\n\n\n\n\n\n\n\ncountry\nregion\n1980_gdp_capita\n2020_gdp_capita\n1980_life_exp\n2020_life_exp\n\n\n\n\nChina\nEast Asia & Pacific\n430.8554\n10358.170\n64.42000\n78.07700\n\n\nJapan\nEast Asia & Pacific\n19334.3747\n34650.797\n75.98902\n84.56000\n\n\nHong Kong SAR, China\nEast Asia & Pacific\n12553.2179\n41451.326\n74.65341\n85.49634\n\n\nPortugal\nEurope & Central Asia\n10832.5150\n19778.714\n71.21463\n80.97561\n\n\nBulgaria\nEurope & Central Asia\n3431.5213\n7990.693\n71.15756\n73.65854\n\n\nGermany\nEurope & Central Asia\n23977.2699\n42362.647\n72.80032\n81.04146\n\n\nNicaragua\nLatin America & Caribbean\n1850.0056\n1959.788\n57.91400\n71.79500\n\n\nGuatemala\nLatin America & Caribbean\n3287.0240\n4005.839\n56.02400\n71.79700\n\n\nTrinidad and Tobago\nLatin America & Caribbean\n8612.1558\n15789.614\n67.57200\n74.40600\n\n\n\n\n\n\n\n\n\n\n\n\n\nGohel, David, and Panagiotis Skintzos. 2024. Flextable: Functions for Tabular Reporting. https://ardata-fr.github.io/flextable-book/.\n\n\nHenry, Lionel, and Hadley Wickham. 2024. Tidyselect: Select from a Set of Strings. https://tidyselect.r-lib.org.\n\n\nHlavac, Marek. 2022. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Social Policy Institute. https://CRAN.R-project.org/package=stargazer.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. Gt: Easily Create Presentation-Ready Display Tables. https://gt.rstudio.com.\n\n\nMock, Thomas. 2024. gtExtras: Extending ’Gt’ for Beautiful HTML Tables. https://github.com/jthomasmock/gtExtras.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.",
    "crumbs": [
      "Data visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Tables</span>"
    ]
  },
  {
    "objectID": "13_Essential_programming_structures.html#conditional-execution",
    "href": "13_Essential_programming_structures.html#conditional-execution",
    "title": "13  Programming: control structures",
    "section": "",
    "text": "13.1.1 if … then … else\nThe simplest if statement includes only a condition\n\nif (condition) {expression}\n\nHere the condition is included between braces. The condition evaluates to TRUE or FALSE. In other words, the part between braces includes conditions such as x &gt; 10, x &lt; 12, is.character(x), x &lt; 10 & y &gt; 25, x %% 2 == 0 (see Chapter 3 for boolean operators). If this condition evaluates to TRUE, then the expression or code block in expression is executed. If this is not the case, R will not execute the expression in expression and continue with the rest of the code. The expression of code black is usually enclosed in curly braces {}. Although this is not necessary if these expressions are short and fit on one line, it is good practice to add them anyway.\nNote that R doesn’t use a then statement. In other programming languages that statement if often used and reads as IF condition THEN expression. Using R, this then follows from the structure of the code: if a condition in (condition) is met R executes all code between curly braces ´{}`.\nTo illustrate the basic if statement, say you have two variables a and b. Say that b equals 1. If a equals 10, the value of b should be change to 100. In terms of an if statement: if the condition a = 10 is TRUE, the value of b must change to 100 or if (a == 10) {b &lt;- 100} where the condition is (a == 10) and the expression is {b &lt;- 100}:\n\na &lt;- 10\nb &lt;- 1\n\nif (a == 10) {b &lt;- 100}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nAs the condition (a == 10) evaluates to TRUE, R executes the expression {b &lt;- 100}. After it completes this code block, R continues with the rest of the code. There the rest of the code is the print() statement that shows the value of b.\nLet’s now change the value of a to 11 and rerun the same lines of code as in the previous block:\n\na &lt;- 11\nb &lt;- 1\n\nif (a == 10) {b &lt;- 100}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 1\"\n\n\nAs the condition (a == 11) evaluates to FALSE R doesn’t execute the expression (b &lt;- 100) and continues with the rest of the code: it prints the value of b.\nTo add a code block to execute in case the condition evaluates to FALSE, you need an else statement:\n\nif (condition) {\n  expression if TRUE\n} else {\n  expression if FALSE\n}\n\nR will check the condition in (condition). If this condition evaluates to TRUE, R will execute the code in expression if TRUE. If the condition evaluates to FALSE, R will execute the code in expression if FALSE\nNote the use of curly braces. These curly braces define a hierarchy in R and the style guide includes a number of rules:\n\nthe first opening { follows after the (condition) part and is the last character on that line.\nthe code to execute if the condition is TRUE is intended with 2 spaces.\nthe closing brace } should be on the next line and should the the first and only character on that line unless it is followed by else.\n\nThis style guide is relevant for all braced expressions. Using braces, you can group multiple lines of code in one expression. If the condition is TRUE, R will execute all code between the first opening brace and the first closing brace. Likewise, if the condition if FALSE, R will execute all code between the opening and closing brace after the else expression. In other words, the part within a braced expression forms a hierarchy in R: the statements will be executed as a group: starting on the first line after the first curly brace and running through all lines until R meets the closing curly brace.\nLet’s now add an else statement to the previous code: if a equals 10, R should reassign the value of b to 100 and if a is not equal to 10, R should reassign the value of b to 50. Here, the condition is (condition == 10), the expression if TRUE is b &lt;- 100 and the expression if FALSE is b &lt;- 50. With a &lt;- 10, the code returns:\n\na &lt;- 10\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {\n  b &lt;- 50\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nIn this case, R evaluates the condition (a == 10) to TRUE and executes the code b &lt;- 100. As this line is the only line in the braced expression, R will continue to execute the code that follows after the if statement. In this case, this is the print function. Let’s change the value of a in 11:\n\na &lt;- 11\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {\n  b &lt;- 50\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 50\"\n\n\nIn this code, the condition (a == 10) is FALSE and R executes the code after the else statement: b &lt;- 50. As this is the only line in the braced expression after the else statement R sets the value of b to 50 and returns to the code after the if else statement: it prints the value of 50.\nWith an if (condition) {} else {} structure, if the condition evaluates to FALSE, R executes the code included in the block after the else statement. In the example, R will execute the that code and set b equal to 50 for all value of a where a is not equal to 10. In other words, there is only one occasion where b we be set equal to 100 (if a equals 10). For all other cases, b we be set equal to 50. However, sometimes this is not what you want. Say that you would want to set the value of bequal to 100 if a equals 10 and set b equal to 500 if a equals 11 but keep bequal to 1 if a is not equal to 10 or 11.\nThe else statement allows you to add these additional conditions. Within that statement, you can include additional if (condition) {} statements or an additional if (condition) {} else {} statements. This first creates an “if else if ladder”. It is used to create code where you want to add multiple options: if “condition 1” is met, then do something, if this is not the case verify if “condition 2” is met. If that is the case do something else. If that second condition is not met, do yet another thing. This statement differs from the if (condition) {} else {} because it specifies additional conditions if the first condition is not met. If the first condition is not met in if (condition) {} else {}, R executes executes the code after the else statement. Here you add an additional condition, e.g. if (condition2) {expression if 2 is TRUE} else {expression if 2 is FALSE}. Doing so, if the first condition is not met, R will check if condition 2 is met. If that is the case, R will execute the code in the expression after the second if statement. If that second conditions is FALSE and you added an else statement, R will execute the code after that else statement. If you didn’t include an else statement R will continue with the code after the if block.\nAn “if else if” ladder including an if (condition) {} statement looks like\n\nif (condition1) {\n  expression if condition1 is TRUE\n} else {if (condition2) {\n    expression if condition 2 is TRUE\n  } \n}\n\nor, with an additional else statement:\n\nif (condition1) {\n  expression if condition1 is TRUE\n} else {if (condition2) {\n    expression if condition 2 is TRUE\n  } else {\n    expression if condition 2 is FALSE\n  }\n}\n\nBuilding on the example, say you want to set the value of b equal to 100 if a equals 10 and to 500 if a equals 11. If a is not equal to 10 or 11, you keep the value of b equal to 1. Here, you can start from the first condition (a == 10). If that condition is TRUE, R needs to excecute {b &lt;- 100}. If that first condition if false, R needs to check if the condition (a == 11) returns TRUE. If that is the case, R needs to set the value of b equal to 500 ({b &lt;- 500}). If that second condition is also FALSE, R can continue with the code after the “if else if” ladder (here this is the print statement). Let’s start from a equal to 10\n\na &lt;- 10\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {if (a == 11) {\n  b &lt;- 500\n  }\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nAs the first condition (a == 10) is met, R sets the value of b equal to 100 and continues with the code after the “if else if” ladder. Now let’s set the value of a equal to 11:\n\na &lt;- 11\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {if (a == 11) {\n  b &lt;- 500\n  }\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 500\"\n\n\nAs the condition (a == 10) returns a FALSE, R continues to the else statement. Here, R first checks the if statement and evaluates the condition (a == 11) as this condition is TRUE, R executes the code b &lt;- 500.\nWith a equal to 15\n\na &lt;- 15\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {if (a == 11) {\n  b &lt;- 500\n  }\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 1\"\n\n\nboth conditions are FALSE and R proceeds without making changes to b.\nIf you add an if (condition) {} else {} in the ladder, R will execute the code in the last else statement if non of the conditions are met. To illustrate, say we include in that expression b &lt;- 1000, then R would change the value of b from 1 into 1000 if a is not equal to 10 or 11. Using the previous example and adding this line:\n\na &lt;- 15\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {if (a == 11) {\n  b &lt;- 500\n  } else {\n    b &lt;- 1000\n  }\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 1000\"\n\n\nAs none of the conditions are met, R changes the value of b to 1000.\nBuilding on the last example, you can add further conditions. For instance, set the value of b equal to 1000 if a equals 12. To do so, you would continue to add this condition after the second else statement (if (a == 12) {b &lt;- 1000}):\n\na &lt;- 12\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n  } else {if (a == 11) {\n  b &lt;- 500\n    } else {if (a == 12) {\n    b &lt;- 1000\n    }\n  }\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 1000\"\n\n\nUsing additional else statements, you can continue to add to the ladder.\nNested if else statements differ from if else if ladders in that they include in both the code if the condition if met as well as the code block to execute if the condition is not met additional if ... else statements:\n\nif (parent_condition) {\n  if (child_condition1) {\n    expression if parent and child1 are TRUE\n  } else {\n    expression if parent is true but child1 is FALSE\n} else if (child_condition2) {\n      expression if parent is FALSE ahd child2 is TRUE\n    } else {\n      expression if parent if FALSE and child2 if FALSE\n    }\n}\n\nLet’s see how this code is evaluated. R starts from the first condition parent_condition, if this condition is TRUE, R executes the code block:\n\nif (child_condition1) {\n  expression if parent is TRUE and child1 is TRUE\n} else {\n  expression if parent is TRUE but child1 is FALSE\n}\n\nHere, it first evaluates the “child condition”. If this condition is TRUE, is executes the expression if both parent and child condition are TRUE. If this child condition if FALSE, it executes the code after the else statement in this nested if else statement. If the parent_condition is FALSE, R moves to the code block\n\nif (child_condition2) {\n  expression if parent is FALSE but child2 is TRUE\n} else {\n  expression if parent is FALSE and child2 is FALSE\n}\n\nIn this second nested if else statement, R now evaluates if the second child condition is TRUE. If that is the case, R executes the code expression if parent if FALSE but child 2 is TRUE. If that second child condition if FALSE, R executes the code expression if parent if FALSE and child2 is FALSE. To illustrate, let’s further expand the example. If a equals 10 and b equals 1, R should set the value of c to 1; if a equals 10 but b not is not equal to 1, the value of c should be set to 2. If a doesn’t equal 10, then c should be equal to 3 if b is smaller than 10 and to 4 if b is not smaller than 10:\n\na &lt;- 10\nb &lt;- 1\nc &lt;- 0\n\nif (a == 10) {\n  if (b == 1) {\n    c &lt;- 1\n  } else {\n    c &lt;- 2\n  }\n} else if (b &lt; 10) {\n    c &lt;- 3\n  } else {\n  c &lt;- 4\n}\n\nprint(paste(\"c equals\", c))\n\n[1] \"c equals 1\"\n\n\nAs a equals 10, the R checks if b equals 1. As this is the case, R sets the value of c equal to 1. If we change the value of a to 15 and the value of b to 25, the first condition will be FALSE and R will move evaluate the condition b &lt; 10. As this condition is also FALSE, R will set c equal to 4:\n\na &lt;- 15\nb &lt;- 25\nc &lt;- 0\n\nif (a == 10) {\n  if (b == 1) {\n    c &lt;- 1\n  } else {\n    c &lt;- 2\n  }\n} else if (b &lt; 10) {\n    c &lt;- 3\n  } else {\n  c &lt;- 4\n}\n\nprint(paste(\"c equals\", c))\n\n[1] \"c equals 4\"\n\n\nNote that you can include further if ... else statements, add ladders, … . Doing so, you add further condition in case e.g. the parent condition is FALSE but the child condition is TRUE.\nYou can write the previous code block in a different way and assign the outcome of the code as a new value of b. In the examples, we assign the value of b as part of the if ... else structure. However, if you assign the outcome of the if ... else structure to b, that would produce the same output. For instance, these two statements, one which repeats the example with assignment in the if ... else structure\n\na &lt;- 10\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {\n  b &lt;- 50\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nand one with assignment of the if ... else structure’s result\n\na &lt;- 10\nb &lt;- 1\n\nb &lt;- if (a == 10) {\n  100\n} else {\n  50\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nare equivalent. However, although both results are correct, it is not good practice to assign the result of if ... else statement as in the second example. The main reason is that the code becomes harder to read, especially if you have multiple conditions and complex code blocks.\n\n\n13.1.2 swicth()\nThe swicth statement is closely related to the if ... else statement and is useful in case you want to set the value of an object based on the value of another. For instance, if the value of a variable, e.g. x should be equal to a/2 if bequals 1, a^2 if b equals 2 and log(a, base = 10) if b equals 10 an if ... else statement looks like\n\na &lt;- 10\nb &lt;- 2\nx = 0\n\nif (b == 1) {\n  x &lt;- a/2\n} else {\n  if (b == 2) {\n    x &lt;- a^2\n  } else {\n    x &lt;- log(a, base = 10)\n  }\n}\n\nprint(paste(\"x equals\", x))\n\n[1] \"x equals 100\"\n\n\nYou the switch statement you can shorten this code. The statement needs an expression and cases. The expression determines which case will be used. For instance, if the expression is 2, then the second case will be used; if the expression equals 3, then the third case will be used. As an alternative, you add “names to the cases. Doing so, the expression needs to refer to a name. The switch statement in general looks like:\n\nswitch(expression, case1, case2, case3....)\n\nApplied to the example, if b equals 1, the value of x should be set as a/2; if b equals 2, x should equal a^2 and if b equals three x should be equal to the log(a, base = 10). Using these three cases in the switch statement:\n\na &lt;- 10\nb &lt;- 2\nc &lt;- 0\n\nx &lt;- switch(\n  b,                 # Expression\n  a/2,               # case 1\n  a^2 ,              # case 2\n  log(a, base = 10)  # case 3\n)\n\nprint(paste(\"x equals\", x))\n\n[1] \"x equals 100\"\n\n\nAs b equals 2, R moves to the second case and sets the value of x equal to 10^2 = 100. Note that the use of numeric variables is only possible if they are integers. Second, the statement needs to include all cases, else R returns a NULL. For instance, if we set b equal to 4 with only 3 cases, x will be empty:\n\na &lt;- 10\nb &lt;- 4\nc &lt;- 0\n\nx &lt;- switch(\n  b,                 # Expression\n  a/2,               # case 1\n  a^2 ,              # case 2\n  log(a, base = 10)  # case 3\n)\n\nprint(paste(\"x equals\", x))\n\n[1] \"x equals \"\n\n\nOften the switch statement is used with character variables. In this example, the value of b is a character \"b2\". We can now add a name to all options, e.g. \"b1\" = a/2, … . R now selects the case using that name:\n\na &lt;- 10\nb &lt;- \"b2\"\nc &lt;- 0\n\nx &lt;- switch(\n  b,                        # Expression\n  \"b1\" = a/2,               # case 1\n  \"b2\" = a^2 ,              # case 2\n  \"b3\" = log(a, base = 10)  # case 3\n)\n\nprint(paste(\"x equals\", x))\n\n[1] \"x equals 100\"\n\n\n\n\n13.1.3 ifelse(), dplyr::if_else, dplyr::case_when() and dplyr::case_match()\nRecall that a lot of function in R are vectorized. The if ... else structure is not a function, but a structure. If you want to use it with a vector, you have to include the statement in a loop. Here, base R’s ifelse() function is useful: this function applies an if ... else statement to all individual elements of a vector. The function has three arguments:\n\nifelse(test, yes, no)\n\nThe argument test includes the condition. R tests this condition for every element in a data structure such as a vector, matrix or data frame. If the condition if TRUE, then the yes argument shows the return value while the no argument shows the return value in case test evaluates to FALSE. These return values are assigned to the elements in the same position of vector with the same length as the vector used test condition. In other words, if the vector used in the test condition has length 10, then the output vector will also have length 10. Extending the example from one value to a vector vec_a with 3 values, 10, 11 and 12 and a vector vec_b whose values will be set, we will use ifelse to set the value of vec_b to 100 if the corresponding value in vec_a &lt; 11 and 200 otherwise:\n\nvec_a &lt;- c(10, 11, 12)\nvec_b &lt;- c(0, 0, 0)\n\nvec_b &lt;- ifelse(vec_a &lt; 11, 100, 200)\nvec_b\n\n[1] 100 200 200\n\n\nFor every element of vec_a the function ifelse() evaluates the condition vec_a[i] &lt; 11. If that condition is TRUE, it sets the value in vec_b[i] equal to 100. This is the code included in the yes part of the ifelse() function. If the condition evaluates to FALSE, R executes the code in the no argument and sets the value in vec_b[i] equal to 200. In both the yes and no argument, you can add additional conditions. For instance, suppose that vec_a is (10, 11, 12) and vec_b is (100, 200, 300) and you want to fill a vector vec_c using both the values in vec_a and vec_b where the values are: if vec_a[i] is less then vec_c[i] should equal 25 if vec_b[i] is equal to 100 and 50 otherwise. If vec_a[i] is not smaller than 12 then vec_c[i] should equal 100 if vec_b[i] is larger than 100 and 75 otherwise. Using ifelse():\n\nvec_a &lt;- c(10, 11, 12)\nvec_b &lt;- c(100, 200, 300)\nvec_c &lt;- c(0, 0, 0)\n\nvec_c &lt;- ifelse(vec_a &lt; 12, ifelse(vec_b == 100, 25, 50), ifelse(vec_b &gt; 100, 75, 100))\nvec_c\n\n[1] 25 50 75\n\n\nAs a single value is also a vector, you can use this ifelse() function also as a substitute for if ... else statements. For instance the statement\n\na &lt;- 10\nb &lt;- 1\n\nif (a == 10) {\n  b &lt;- 100\n} else {\n  b &lt;- 50\n}\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\nand\n\na &lt;- 10\nb &lt;- 1\n\nb &lt;- ifelse(a == 10, 100, 50)\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\ngive the same result. For short statements, using ifelse() requires a lot less code. However, if you have multiple lines of code in the statements after if of else, an if ... else structure is easier to read.\nIn Chapter 8, we looked at {dplyr}’s is_else() function. This function performs the same operation as base R’s ifelse() function but allows you to include explicit values in case there are missing values in the condition. Base R’s ifelse() function doesn’t include an argument to determine what happens in case of a missing value. Here, base R’s ifelse() will set the value to NA. For instance, with a vector vec_a (10, 15, NA), the function ifelse(vec_a &gt; 10, 100, 200) returns 200, 100, NA:\n\nvec_a &lt;- c(10, 15, NA)\nifelse(vec_a &gt; 10, 100, 200)\n\n[1] 200 100  NA\n\n\nIf you want to set an explicit value in case of a missing value, {dplyr}’s if_else() allows you to set that value using the argument missing =. For instance, to set the missing values equal to 99 for the previous vector:\n\nvec_a &lt;- c(10, 15, NA)\ndplyr::if_else(vec_a &gt; 10, 100, 200, missing = 999)\n\n[1] 200 100 999\n\n\nThe second noteworthy difference between base R’s ifelse() and {dplyr}’s if_else() is that the former strips attributes while the latter preserves the attributes. To illustrate, let’s use a sequence of three data/time values, starting on 2025-03-25 where the sequence increments by day. Suppose now that you want to add 182 days in case the date is earlier than 2025-03-26 and add 365 days if that is not the case. In other words, if the condition vec_a_time &lt; \"2025-03-26\" is TRUE, vec_time should be pushed forward with 182 days while it should be pushed forward with 365 days if the condition if FALSE. Within this sequence, the first values (2022-03-25) meets this condition while the other two will return FALSE: strips attributes\n\nvec_a_time &lt;- seq.Date(from = as.Date(\"2025-03-25\"), by = \"day\", length.out = 3)\n\nvec_b_time &lt;- ifelse(vec_a_time &lt; \"2025-03-26\", vec_a_time + 182, vec_a_time + 365)\nvec_b_time\n\n[1] 20354 20538 20539\n\n\nHere, you see that R changes data/time vector into a numeric vector. To restore the attribute, you can set the class of the return vector, vec_b_time equal to the class of vec_a_time using class() or you use as.Date() to restore the attributes:\n\nclass(vec_b_time) &lt;- class(vec_a_time)\nvec_b_time\n\n[1] \"2025-09-23\" \"2026-03-26\" \"2026-03-27\"\n\n\nUsing {dplyr}’s if_else() this last step is not necessary as this function preserves the class of a vector:\n\nvec_a_time &lt;- seq.Date(from = as.Date(\"2025-03-25\"), by = \"day\", length.out = 3)\n\nvec_b_time &lt;- dplyr::if_else(vec_a_time &lt; \"2025-03-26\", vec_a_time + 182, vec_a_time + 365)\nvec_b_time\n\n[1] \"2025-09-23\" \"2026-03-26\" \"2026-03-27\"\n\n\nNote that dplyr::if_else() can also be used with single values:\n\na &lt;- 10\nb &lt;- 1\n\nb &lt;- dplyr::if_else(a == 10, 100, 50)\n\nprint(paste(\"b equals\", b))\n\n[1] \"b equals 100\"\n\n\n{dplyr}’s case_when() function allows you to vectorize multiple if_else() statements. We covered this function in Chapter 8. To illustrate how it its in these conditional statements, let’s use vec_a and set the value of vec_b: if vec_a[i] equals 10, vec_b[i] will be set to 100, if vec_a[i] equals 15, vec_b[i] will be set to 300 and if vec_a[i] is NA, vec_b[i] is set to 99. In addition - although we know we covered all cases for vec_a - we add the argument .default = NA to set values in vec_b to NA in case the statement in the case_when() function did not cover all cases in vec_a:\n\nvec_a &lt;- c(10, 15, NA)\nvec_b &lt;- c(0, 0, 0)\n\nvec_b &lt;- dplyr::case_when(\n  vec_a == 10 ~ 100, \n  vec_a == 15 ~ 300,\n  is.na(vec_a) ~ 99,\n  .default = NA\n)\n\nvec_b\n\n[1] 100 300  99\n\n\ndplyr::case_match() extends the switch statement to the case of a vector. Recall from Chapter 8 that case_match(.x, match) finds a match in the vector x and uses the statements in the match conditions to set new values. For instance, suppose that you have a vector vec_a = c(\"a\", \"b\", \"a\", \"b\", \"c\", \"e\") and you want to set the values of a second vector, vec_b equal to 10 if vec_a is “a”, equal to 20 if vec_a is “b” and equal to 30 if vec_a is “c”. For all values in vec_a without a match in the condition, vec_b should be set to 100. The code for this operation is:\n\nvec_a &lt;- c(\"a\", \"b\", \"a\", \"b\", \"c\", \"e\")\n\nvec_b &lt;- dplyr::case_match(vec_a,\n  \"a\" ~ 10, \n  \"b\" ~ 20,\n  \"c\" ~ 30,\n  .default = 100\n)\n\nvec_b\n\n[1]  10  20  10  20  30 100",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Programming: control structures</span>"
    ]
  },
  {
    "objectID": "13_Essential_programming_structures.html#iteration",
    "href": "13_Essential_programming_structures.html#iteration",
    "title": "13  Programming: control structures",
    "section": "13.2 Iteration",
    "text": "13.2 Iteration\nIterations are used every time an operation has to be repeated, e.g. to something for every element in a data structure, for every file in a folder, … . There are three types of iterations: for loops, while loops and repeat loops. A for loop iterates over a numeric range, elements in a vector, columns in a matrix, variables in a data frame, … . Because we know the number of values in the numeric range or vector, the number of columns in a matrix of the number of variables in a data frame, we also know how many iterations will be performed. A while loop runs as long as a condition holds. Here we don’t know the exact number of iterations. A repeat loop runs until it is stopped.\n\n13.2.1 for loops\n\n13.2.1.1 The for loop syntax\nA for loop iterates over a numeric range or e.g. elements in a vector, components of a list, columns/rows in a matrix or variables in a data frame. The basic structure of a for loop is\n\nfor (i in sequence) {\n  expression\n}\n\nIn this code (i in sequence) the loop variable i takes on each value in the sequence included in sequence. For instance, with (i in 1:10) i will take on the value 1, 2, 3, 4, … 10. There is nothing “magic” about the use if i and you can use var, x, abcdefg, … in stead of i to use as loop variables. For instance, if you would use (var in 1:10) then var would take the value 1, 2, 3, 4, … 10. In other words, i or var are temporary used to store the value of an element in the sequence. The sequence lists all the values to iterate over. In (i in 1:10) the sequence is 1:10 which is shorthand for seq(from = 1, to = 10, by = 1) or c(1, 2, 3, 4, ... 10). If a matrix mat1 has ncol columns, you can set the sequence as (col in 1:ncol(mat1)). With a vector vec_char &lt;- c(\"a\", \"q\", \"z\") as a sequence, (k in vec_char) would set k equal to “a”, then “q” and then “z”. The expression is included in curly braces and includes the code you want to run for every value if i. As the for loop starts, i takes the first value in the sequence and executes the code in curly braces using that value of i. If the code is executed, R returns to (i in sequence) and sets the value of i equal to the second value in the sequence and runs through the code again. R repeats until i reaches the final value in the sequence. If that is the case, R executes the expression a lst time. After that, R exits the for loop and continues with the rest of the code. Note again the position of the curly braces: the opening brace is the last character after (i in sequence). The closing brace if the first character on a new line. Usually, this is also the only character on that line. The expression is intended with two spaces.\nTo illustrate how R runs through a for loop, let’s use a simple example. We’ll use (var in c(1, 2, 3, 4, 5)) and include a single line of code in the expression: print(paste(\"the value of var is\", var)):\n\nfor (var in c(1, 2, 3, 4, 5)) {\n  print(paste(\"the value of var is\", var))\n}\n\n[1] \"the value of var is 1\"\n[1] \"the value of var is 2\"\n[1] \"the value of var is 3\"\n[1] \"the value of var is 4\"\n[1] \"the value of var is 5\"\n\ncat(\"\\n Rest of the code\")\n\n\n Rest of the code\n\n\nThis code illustrates how R runs through the for loop. The sequence is c(1, 2, 3, 4, 5). As R starts the loop, var takes the value equal to the first value in the sequence. In the example, this is 1. R then executes the expression. In this example, R prints “the value of var is 1”. After completing this expression, R returns to the sequence and sets the value of var equal to the second value in the sequence c(1, 2, 3, 4, 5): 2. Using that value, R executes the code in expression and prints: “the value of var is 2”. After finishing the code, R returns to the sequence and var takes on the third value in the sequence. If var reaches the final value in the sequence, R executes the expression one more time. After finishing that last loop, R exits the loop and returns to the rest of the code. In the code in the example, the statement cat(\"\\n Rest of the code) prints a empty line and shows “Rest of the code”.\nBecause 1:5 returns the same vector as c(1, 2, 3, 4, 5), including the former or the latter returns the same output:\n\nfor (var in 1:5) {\n  print(paste(\"the value of var is\", var))\n}\n\n[1] \"the value of var is 1\"\n[1] \"the value of var is 2\"\n[1] \"the value of var is 3\"\n[1] \"the value of var is 4\"\n[1] \"the value of var is 5\"\n\ncat(\"\\n Rest of the code\")\n\n\n Rest of the code\n\n\nUsing sec() you can use a sequence which doesn’t start at 1 and/or whose increments differ from 1. For instance, using seq(from = 10, to = 20, by = 2) to start the sequence at 10, end at 20 with increments of 2, the for loop returns:\n\nfor (var in seq(from = 10, to = 20, by = 2)) {\n  print(paste(\"the value of var is\", var))\n}\n\n[1] \"the value of var is 10\"\n[1] \"the value of var is 12\"\n[1] \"the value of var is 14\"\n[1] \"the value of var is 16\"\n[1] \"the value of var is 18\"\n[1] \"the value of var is 20\"\n\ncat(\"\\n Rest of the code\")\n\n\n Rest of the code\n\n\nAs the sequence starts at 10, R sets the value of var equal to 10 as it initiates the loop. Using that value, it executes the expression. As in the previous examples, the code in the expression prints “the value of var is” followed by the value of var. As var equals 10, the expression prints “the value of var is 10”. After executing the expression, R returns to the sequence and set the value of var equal to the second element in the sequence. In this case, this second element is 12. R executes the code, prints “the value of var if 12” and returns to the sequence to change the value of var. Now var takes the value of the third element in the sequence (14). R continues with the loop until all elements of the sequence have been used as values for var. If that is the case, R exists the loop.\nIn the previous examples, the sequences were c(1, 2, 3, 4, 5), 1:5 or seq(from = 10, to = 20, by  = 2) and their values where used to set the value of var in the for loop. You can set the sequence outside of the for loop. To illustrate, in the next code block, we’ll first create a vector vec_char with three random strings. We then introduce that vector in the for loop (abs in vec_char). In addition, we also introduce a variable counter which we set equal to 1. As part of that loop, we change the value of that variable counter by adding one in each iteration. Here the expression has two lines of code: on the first line, we print the value of both counter and abc and on the second, we add 1 to the value of counter:\n\nvec_char &lt;- c(\"September\", \"European Union\", \"James Bond\")\ncounter &lt;- 1\n\nfor (abc in vec_char) {\n  print(paste(\"Element\", counter, \"in vec_char is\", abc))\n  counter &lt;- counter + 1\n}\n\n[1] \"Element 1 in vec_char is September\"\n[1] \"Element 2 in vec_char is European Union\"\n[1] \"Element 3 in vec_char is James Bond\"\n\n\nIn other words, you sequence can be defined as part of the for loop (e.g. as in (i in 1:5)) or outside of the loop. Second, R loops over the values of the sequence and these values do not have to be a numeric sequence, but can include any vector. Here, we have a character vector. In the code, the vector includes unrelated items. In other words, the sequence doesn’t have to be a sequence such as 1, 2, 3 or “a”, “b”, “c” but can include a (subset) of variable names in a data frame, a numeric sequence where the various positions change by more than 1, … .\n\n\n13.2.1.2 Using length(), seq_len() and seq_along()\nIn the previous examples (var in  (abc in vec_char) or (abc in vec_char), the variables var or abc took the value of the elements in the vectors c(1, 2, 3, 4, 5), 1:5, seq(from = 10, to = 20, by = 2) or vec_char &lt;- c(\"September\", \"European Union\", \"James Bond\"). You can also iterate over the various values by subsetting the vector by position (see Chapter 4). To do so the (i in sequence) part includes a numeric range with index positions starting. This numeric range starts at 1. If you want to iterate over all values of the vector, this numeric range ends at the number of elements in the vector. This numeric range changes with 1 in each iteration. There are a number of ways to define this range. First, recall that the function length() returns the number of elements in a vector (or list). If a vector has n (e.g. 5) elements, then the length of that vector is n (e.g. 5). Using this function, the sequence (1:length(vector)) equals 1, 2, … n (with n number of elements in the vector e.g. 5). To see how this works, the sequence 1:length(vec_char) is\n\n1:length(vec_char)\n\n[1] 1 2 3\n\n\nUsing (i in 1:length(vec_char)), you can replicate the previous code by subsetting the vector vec_char by position:\n\nvec_char &lt;- c(\"September\", \"European Union\", \"James Bond\")\n\nfor (abc in 1:length(vec_char)) {\n  print(paste(\"Element\", abc, \"in vec_char is\", vec_char[abc]))\n}\n\n[1] \"Element 1 in vec_char is September\"\n[1] \"Element 2 in vec_char is European Union\"\n[1] \"Element 3 in vec_char is James Bond\"\n\n\nR starts the for loop and sets the value of abc equal to 1, the first element in the sequence 1:length(vec_char). As abc equals 1, extract the first element of vec_char using vec_char[abc]. In the expression, R prints “Element abc in ver_char is vec_char[abc]”. As abc is 1, vec_char[abc] returns the first element of vec_char. This element is “September”. The print statement statement returns “Element 1 in vec_char is September”. As R completes the expression, it returns to the sequence and sets the value of abc equal to 2. With vec_char[abc] equal to “European Union” it prints the expression “Element 2 in vec_char is European Union”. The next value in the sequence is 3. This is also the final value. As R completes the expression it exists the loop.\nRecall that seq_len(n) returns a sequence starting at 1, ending at n with increments of 1. In other words, seq_len(n) returns a sequence 1, 2, 3, … n. You can use this as an alternative for 1:length(vec_char). Using sec_len(length(vec_char)) you generate the same sequence as using 1:length(vec_char):\n\nvec_char &lt;- c(\"September\", \"European Union\", \"James Bond\")\n\nfor (i in seq_len(length(vec_char))) {\n  print(paste(\"Element\", i, \"in vec_char is\", vec_char[i]))\n}\n\n[1] \"Element 1 in vec_char is September\"\n[1] \"Element 2 in vec_char is European Union\"\n[1] \"Element 3 in vec_char is James Bond\"\n\n\nHere used i and not abc as this is more common.\nThe function seq_along(x) is an often used alternative. This function returns a sequence of integers along the length of its argument x. For instance, seq_along(vec_char) returns 1, 2, 3:\n\nvec_char &lt;- c(\"September\", \"European Union\", \"James Bond\")\n\nseq_along(vec_char)\n\n[1] 1 2 3\n\n\nUsing this in a for loop:\n\nvec_char &lt;- c(\"September\", \"European Union\", \"James Bond\")\n\nfor (i in seq_along(vec_char)) {\n  print(paste(\"Element\", i, \"in vec_char is\", vec_char[i]))\n}\n\n[1] \"Element 1 in vec_char is September\"\n[1] \"Element 2 in vec_char is European Union\"\n[1] \"Element 3 in vec_char is James Bond\"\n\n\nThe advantage of seq_along() is that it returns an integer vector of length 0 if the input vector is of length 0. This is not the case for length() as this function returns an integer with the value 0. To see the difference, let’s create a numeric vector of length 0 using numeric(length = 0L). The function length returns 0 and 1:length() returns 1, 0.\n\nvec_length0 &lt;- numeric(length = 0L)\n1:length(vec_length0)\n\n[1] 1 0\n\n\nSuppose that you write a for loop using (i in 1:length(vec_length0)) and use the value of i in your expression. The sequence is not “empty” as it as two values: 1 and 0. As R starts the loop, i will be set equal to 1, the first value in the sequence 1:length(vec_length0) and R will execute the expression using that value for i. Suppose that you add 5 to i in your expression to calculate another value, j. R will add 5 to i and set j equal to 6. R now returns to the sequence and sets i equal to the second element in the sequence. This element is 0. Using the that value, R sets the value of j equal to 5.\n\nvec_length0 &lt;- numeric(length = 0L)\nj &lt;- 0\n\nfor (i in 1:length(vec_length0)) {\n  j = i + 5\n  print(j)\n}\n\n[1] 6\n[1] 5\n\n\nIn other words, using 1:length(x) with x an empty vector executes the loop twice: one for the value of 1 and once for the value of 0.\nThe function sec_along() behaves differently: it returns an integer vector of length 0. In other words, it returns a sequence with 0 elements:\n\nvec_length0 &lt;- numeric(length = 0L)\nseq_along(vec_length0)\n\ninteger(0)\n\n\nBecause there is no sequence, R doesn’t initiate the loop, even if your for loop includes a statement such as j = i + 5 or print(j).\n\nvec_length0 &lt;- numeric(length = 0L)\nj &lt;- 0\n\nfor (i in seq_along(vec_length0)) {\n  j = i + 5\n  print(j)\n}\n\nBecause this is probably the outcome you want - if a vector is of length 0, you don’t want to initiate the for loop - using seq_along(x) is preferred over 1:length(x).\n\n\n13.2.1.3 Iterating through columns or rows a matrix\nIf you now the exact number of rows or columns of a matrix, you can use these values to subset these rows or columns. However, if this is not the case, the function nrow() and ncol() (see Chapter 4) allow you to define a sequence that allows you to iterate over all row or column of a matrix. Recall from Chapter 4 that you subset a matrix using the row and column indices, e.g. mat[6, 1] extracts the element on the sixth row and first column, mat[8, ] extract the eigth row and mat[, 3] extract the third column. To ilustrate, we wil use the matrix mat1 which includes 3 rows and 2 columns. The elements in the first column are 1, 2 and 3 and the elements in the second column are 4, 5 and 6:\n\nmat1 &lt;- matrix(1:6, nrow = 3, ncol = 2)\nmat1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nTo extract all rows of this matrix using a for you can use (i in 1:nrow(mat1)) to determine the number of iterations and and mat1[i, ] to extract the ith row. Using this to print the rows - note that you could perform many alternative calculations on the data on each row - the for loop to use is:\n\nfor (i in 1:nrow(mat1)) {\n  print(paste(\"row\", i, \"of mat1 is\"))\n  print(mat1[i, ])\n}\n\n[1] \"row 1 of mat1 is\"\n[1] 1 4\n[1] \"row 2 of mat1 is\"\n[1] 2 5\n[1] \"row 3 of mat1 is\"\n[1] 3 6\n\n\nIn this code, the sequence 1:nrow(mat1) equals 1, 2, … n with n the number of rows. As R initiates the loop, it sets the value of i equal to 1. In the expression, mat1[1, ] extracts the first row. The expression prints that row. At the end of the expression, R returns to the sequence and changes the value of i from 1 into 2. In the expression mat1[i, ] is now equal to mat1[2, ] and R extracts the second rows from mat1. This proces continues until the looping variable i equals nrow(mat1). If that is the case, R executes the expression for a last time before it exits the loop.\nSimilarly, to extract the columns you can use mat1[, i]:\n\nfor (i in 1:ncol(mat1)) {\n  print(paste(\"column\", i, \"of mat1 is\"))\n  print(mat1[, i])\n}\n\n[1] \"column 1 of mat1 is\"\n[1] 1 2 3\n[1] \"column 2 of mat1 is\"\n[1] 4 5 6\n\n\nRecall that mat1[, i] simplifies the result. To keep the matrix structure, you need to add drop = FALSE. Doing so, R will preserve the matrix structure and extract columns as matrices with 1 column each:\n\nfor (i in 1:ncol(mat1)) {\n  print(paste(\"column\", i, \"of mat1 is\"))\n  print(mat1[, i, drop  = FALSE])\n}\n\n[1] \"column 1 of mat1 is\"\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n[1] \"column 2 of mat1 is\"\n     [,1]\n[1,]    4\n[2,]    5\n[3,]    6\n\n\n\n\n13.2.1.4 Iteraling through a list\nRecall from Chapter 4 that lists are a very flexible data structure. You can loop over the components of a list in multiple ways. Doing so, you use the subsetting appraoches we introduced in Chapter 4. To illustrate, let’s use the list, list1. This list includes 3 short vectors, each including 10 random draws from a normal distribution:\n\nlist1 &lt;- list(vec1 = rnorm(10, 5, 1), \n              vec2 = rnorm(10, 10, 10), \n              vec3 = rnorm(10, 15, 20))\n\nRecall that you can extract the names of a named list using names():\n\nnames(list1)\n\n[1] \"vec1\" \"vec2\" \"vec3\"\n\n\nSecond, you can subset the list using the subsetting operators [] and [[]]. Here we will use the latter as is simplifies the output. We can now use two approaches to subset this list. The first uses the names of its components. The second uses the index positions. To illustrate, we’ll used both the calculate the mean of each component of list1. Using the names of the list, we can define the sequence using (i in names(list1)). Doing so, i will first take the value vec1. This value will be used to extract component vec1 from the list. In the code, we’ill calculate the mean of the values of this component. As R finishes the expression, i takes the second value in names(list1) - vec2 - and extracts this component. This continues untill all elements in the names vector have been used. The code:\n\nfor (i in names(list1)) {\n  print(mean(list1[[i]]))\n}\n\n[1] 5.395587\n[1] 13.8307\n[1] 25.36171\n\n\nThe second approach uses index positions. Here, we have multiple options to determine the sequence: (i in 1:length(list1)) or (i in seq_along(list1)). Using the latter approach:\n\nfor (i in seq_along(list1)) {\n  print(mean(list1[[i]]))\n}\n\n[1] 5.395587\n[1] 13.8307\n[1] 25.36171\n\n\n\n\n13.2.1.5 Iteratig through a data frame\nRecall that data frames are lists(Chapter 4). In other words, you can use a similar approach to loop over the columns of a data frame as you would use to loop over the components of a list. To illustrate, we’ll use the same components of list1 but now uses them as variables in a data frame:\n\ndf &lt;- data.frame(\n  vec1 = rnorm(10, 5, 1), \n  vec2 = rnorm(10, 10, 10), \n  vec3 = rnorm(10, 15, 20))\n\nUsing colnames() we can create a vector with the variable names in the data frame:\n\ncolnames(df)\n\n[1] \"vec1\" \"vec2\" \"vec3\"\n\n\nWe now have multiple ways to use a iterate over the columns of a data frame. To illustrate, we’ll calculate the mean of each variable in three different ways. The first approach uses the sequence (i in 1:ncol(df)) and subsets the data frame using df[, i]:\n\nfor (i in 1:ncol(df)) {\n  print(mean(df[, i]))\n}\n\n[1] 5.207655\n[1] 11.50349\n[1] 20.43848\n\n\nThe second approach uses colnames(df) with sec_along() and subsets the data frame using df[[i]]. Note that you also can use df[, i].\n\nfor (i in seq_along(colnames(df))) {\n  print(mean(df[[i]]))\n}\n\n[1] 5.207655\n[1] 11.50349\n[1] 20.43848\n\n\nThe last approach uses the variable names in colnames(df). Here, the loop variable i takes the value of a column in the data frame and is used in df[[i]] to subset the data frame.\n\nfor (i in colnames(df)) {\n  print(mean(df[[i]]))\n}\n\n[1] 5.207655\n[1] 11.50349\n[1] 20.43848\n\n\n\n\n\n13.2.2 Nested loops\nThe expression in the previous examples was kept simple: a print() statement of a mean() function. In the expression, we can include further loops. Doing so, we create a nested loop:\n\nfor (i in sequence1) {\n  for (j in sequence2) {\n    expression\n  }\n}\n\nExecuting a nested loop, R starts from the outer loop: for (i in sequence1). Suppose that the sequence is a range 1:n. R sets the value of the outer looping variable i equal to 1. I then continues to execute the expression for the outer loop. In a nested loop, this outer loop expression is the inner loop: for (j in sequence2). Here, R sets the value of the inner looping variable j equal to the first value of the sequence. As R starts with the expression in expression. As R completes the expression, it returns to the inner loop sequence and sets the value of j equal to the second value in the sequence. Note that the value of the looping variable in the outer loop is not changed that this moment. Using the second value for j, R executes the expression in expression. It continues to do so until j equals the last value in the second sequence sequence2. After completing the expression for that last value, R exits the inner loop. It now returns to the outer looping variable and sets the value of this variable equal to the second value in the first sequence (2 if the sequence is 1:n). Using that value for i R starts the expression for the outer loop. In a nested loop, this expression is the inner loop: R sets the value of j equal to the first value in sequence2 and executes the code in expression2. After finishing that expression, the value if j is set equal to the second value in sequence2 and R executes expression2 using that second value for j. Here, too, the value of i is not altered. R will continue to run through the inner loop until it finishes expression with the last value for j in sequence2. If that code is completed, R exits the inner loop and returns to the outer loop. There, the value of i is changed to the third value in the sequence sequence1 and R restarts the inner loop.\nIn other words, for every value of i in sequence1 R excecutes the entire inner loop. The following code illustrates this process:\n\nfor (i in 1:3) {\n  for (j in LETTERS[1:3]) {\n    print(paste(\"the value of i is\", i, \"and the value of j is\", j))\n  }\n}\n\n[1] \"the value of i is 1 and the value of j is A\"\n[1] \"the value of i is 1 and the value of j is B\"\n[1] \"the value of i is 1 and the value of j is C\"\n[1] \"the value of i is 2 and the value of j is A\"\n[1] \"the value of i is 2 and the value of j is B\"\n[1] \"the value of i is 2 and the value of j is C\"\n[1] \"the value of i is 3 and the value of j is A\"\n[1] \"the value of i is 3 and the value of j is B\"\n[1] \"the value of i is 3 and the value of j is C\"\n\n\nHere you see that for each value of the outer loop, R executes the full inner loop. Using a nested loop, you can subsets the individual element of a matrix. Using the outer loop for the row and the inner loops for the columns to subset the matrix we used earlier (mat1):\n\nmat1 &lt;- matrix(1:6, nrow = 3, ncol = 2)\n\nfor (i in 1:nrow(mat1)) {\n  for (j in 1:ncol(mat1)) {\n    print(paste(\"the element in row\", i, \"and column\", j, \"equals\", mat1[i,j])) \n  }\n}\n\n[1] \"the element in row 1 and column 1 equals 1\"\n[1] \"the element in row 1 and column 2 equals 4\"\n[1] \"the element in row 2 and column 1 equals 2\"\n[1] \"the element in row 2 and column 2 equals 5\"\n[1] \"the element in row 3 and column 1 equals 3\"\n[1] \"the element in row 3 and column 2 equals 6\"\n\n\nIn the outer loop, R loops over the rows. The inner loop iterates over the columns. Doing so, R first extracts the values in both column on the first row. It then moves to the second row and extracts both values in the two column on that row. Note that the order of the inner and outer loops matters. For instance, changing the inner and outer loop in the previous code, R would iterate over the rows in the inner loop and the columns in the outer loop. Doing so, R would first extract the value on the rows in the first column before continuing with the values in the second column:\n\nmat1 &lt;- matrix(1:6, nrow = 3, ncol = 2)\n\nfor (i in 1:ncol(mat1)) {\n  for (j in 1:nrow(mat1)) {\n    print(paste(\"the element in row\", j, \"and column\", i, \"equals\", mat1[j,i])) \n  }\n}\n\n[1] \"the element in row 1 and column 1 equals 1\"\n[1] \"the element in row 2 and column 1 equals 2\"\n[1] \"the element in row 3 and column 1 equals 3\"\n[1] \"the element in row 1 and column 2 equals 4\"\n[1] \"the element in row 2 and column 2 equals 5\"\n[1] \"the element in row 3 and column 2 equals 6\"\n\n\nYou can add further inner loops. Doing so, you can e.g. subset arrays. For instance, using the array arr1\n\narr1 &lt;- array(c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2))\n\nto loop over the rows, then over the columns and then over the matrices you need one outer loop and two inner loops.\n\nfor (i in 1:2) {\n  for (j in 1:2) {\n    for (k in 1:2) {\n      print(paste(\"element on row\", i, \"column\", j, \"of matrix\", k, \"is\", arr1[i, j, k]))\n    }\n  }\n}\n\n[1] \"element on row 1 column 1 of matrix 1 is 1\"\n[1] \"element on row 1 column 1 of matrix 2 is 5\"\n[1] \"element on row 1 column 2 of matrix 1 is 3\"\n[1] \"element on row 1 column 2 of matrix 2 is 7\"\n[1] \"element on row 2 column 1 of matrix 1 is 2\"\n[1] \"element on row 2 column 1 of matrix 2 is 6\"\n[1] \"element on row 2 column 2 of matrix 1 is 4\"\n[1] \"element on row 2 column 2 of matrix 2 is 8\"\n\n\n\n\n13.2.3 Conditional execution in a loop\nAdding if ... else statements in a for loop allows to perform the operations in the expression using conditions. Note that this is what ifelse() of dplyr::if_else() try to avoid. However, not all operations can be vectorized. If this is not the case, or if the code you want to execute as part of the if ... else statement is complex, you can use a for loop to iterate over all elements in one or multiple vectors. In general, the code looks like:\n\nfor (i in sequence) {\n  if (condition) {\n    expression if condition is TRUE\n  } else {\n    expression if condition is FALSE\n  }\n}\n\nHere, R will start the loop and set i equal to the first value in the sequence. Using that value, it evaluaties the condition. If the condition return TRUE, R executes the expression in “expression if condition is TRUE”, if the condition returns FALSE, R executes the expression in “expression of condition is FALSE”. Here, completes the conditional statement and returns to the loop. It changes the value of i to the second value in sequence. Using that value, R executes the conditional statement. The process continues untill the i is set to the last value of sequence. If that is the case, R completes the conditional statement one additional time and exits the loop.\n\n\n13.2.4 Preallocating the output container\nAs part of a loop, you will often store values. For instance, in a simulation, you run a decision model multiple times and store the mean, min or maximum values for each model run in a separate vectors such as average, min and max. Here, these three vectors are output containers. If the output requires a list, then the output container is a list. Using a for loop, you will store results of your loop in every iteration. In other words, in every loop, you will add one or more elements to a vector, a matrix or a list. In general, there are two ways to create these containers: you set their dimensions outside of the loop or you allow these containers to grow inside the loop. In the first case, you create e.g. an empty numeric vector using vector(mode = \"numeric\", length = n) (see Chapter 4) where n is the number of results you want to store. To create an empty matrix with m rows and n columns you can use matrix(, nrow = m, ncol = n) and to create an empty list with n components you can use vector(mode = \"list\", length = n). If you do so outside of the loop, you preallocate that container. Doing so, the output container will not “grow” as you add elements to the vector, matrix or list. If you start from an empty vector without adding the length of the vector, e.g. using vec &lt;- numeric(length = 0L) you can add elements to the vector in each iteration of your loop. To do so, you use append(vec, value) or - using shorthand - c(vec, value) where in both cases value refers to the value you store in each iteration (see Chapter 4). Here, the container “grows” in length as your run through the loop.\nTo illustrate the difference, let’s start from the case where you allow your container to grow. We first define a container container to store numeric values. The length of the container is 0. The loop includes 10 iterations. In each, we set the value of container[i] &lt;- i. In other words, the first value in the vector container will be 1, the second 2, the third 3, … . We also print the length of the container. In other words, we show how many values are stored in each iteration:\n\ncontainer &lt;- numeric(length = 0)\npaste(\"the length of container is\", length(container))\n\n[1] \"the length of container is 0\"\n\nfor (i in 1:10) {\n  container &lt;- c(container, i)\n  print(paste(\"in loop\", i, \"the length of container is\", length(container)))\n}\n\n[1] \"in loop 1 the length of container is 1\"\n[1] \"in loop 2 the length of container is 2\"\n[1] \"in loop 3 the length of container is 3\"\n[1] \"in loop 4 the length of container is 4\"\n[1] \"in loop 5 the length of container is 5\"\n[1] \"in loop 6 the length of container is 6\"\n[1] \"in loop 7 the length of container is 7\"\n[1] \"in loop 8 the length of container is 8\"\n[1] \"in loop 9 the length of container is 9\"\n[1] \"in loop 10 the length of container is 10\"\n\n\nHere, you see that the vector container “grows”: if starts from a length 0 and “grows” in each iteration with one unit.\nThe alternative is to set the length of the vector outside of the loop. To do so, we define container using vector(mode = \"numeric, length = 10). Doing so, R creates an empty vector of length 10, i.e. a vector that can store 10 elements. We now use the same for loop and print the length of the container in each iteration.\n\ncontainer &lt;- vector(mode = \"numeric\", length = 10)\npaste(\"the length of container is\", length(container))\n\n[1] \"the length of container is 10\"\n\nfor (i in 1:10) {\n  container[i] &lt;- i\n  print(paste(\"in loop\", i, \"the length of container is\", length(container)))\n}\n\n[1] \"in loop 1 the length of container is 10\"\n[1] \"in loop 2 the length of container is 10\"\n[1] \"in loop 3 the length of container is 10\"\n[1] \"in loop 4 the length of container is 10\"\n[1] \"in loop 5 the length of container is 10\"\n[1] \"in loop 6 the length of container is 10\"\n[1] \"in loop 7 the length of container is 10\"\n[1] \"in loop 8 the length of container is 10\"\n[1] \"in loop 9 the length of container is 10\"\n[1] \"in loop 10 the length of container is 10\"\n\n\nIn this code, the vector container doesn’t grow.\nThe difference between both is relevant in terms of efficiency of the for loop. Using the first approach, you slow down the loop. To illustrate this, let’s run the loop and determine the number of seconds it takes to complete 10000 iterations. To do so, we use Sys.time() (see Chapter 3) to set the current time before and after the loop. The difference between both is the number of seconds R took to complete the loop.\nFirst, we allow the container to grow:\n\nstart_time &lt;- Sys.time()\n\nn &lt;- 10000\ncontainer &lt;- numeric(length = 0)\n\nfor (i in 1:n) {\n  container &lt;- c(container, i)\n}\n\nend_time &lt;- Sys.time()\n\ntime_diff1 &lt;- end_time - start_time\ntime_diff1\n\nTime difference of 0.2582259 secs\n\n\nNow we preallocate the container:\n\nstart_time &lt;- Sys.time()\n\nn &lt;- 10000\ncontainer &lt;- vector(mode = \"numeric\", length = n)\n\nfor (i in 1:10000) {\n  container[i] &lt;- i\n}\n\nend_time &lt;- Sys.time()\n\ntime_diff2 &lt;- end_time - start_time\ntime_diff2\n\nTime difference of 0.00445199 secs\n\n\nAs you can see, the first approach takes longer then the second. This difference will grow with the number of iterations. If you code includes nested loops, this number increases quite fast. For instance in a nested loop with 100 iteration in the outer loop and 100 iterations in the inner loop, the code will complete 10000 iterations. With 1000 iterations in both, you run 1000000 iterations.\n\n\n13.2.5 next and break\nUsing next you can skip an iteration. With break you can exit the loop. To illustrate the first, let’s create a for loop which prints the value of i, the looping variable, and i runs from 1 to 5. Suppose now we want to skip the iteration is i == 2. To do so, we add an if statement: if (i == 2) {next}. Doing so, R will skip the iteration if i equals to and continue the loop for the next value of the looping variable:\n\nfor (i in 1:5) {\n  if (i == 2) {\n    next\n  }\n  print(i)\n}\n\n[1] 1\n[1] 3\n[1] 4\n[1] 5\n\n\nAs you can see, R prints all value of i if i equals 1, 3, 4 or 5 but didn’t print the result if i is 2. In other words, R skipped the iteration for that value. Here, we used the looping value to determine which iterations R had to skip. However, you can use any other variable to determine which iterations to complete and which onces to skip. To illustrate, in the net code, R will skip the iteration of the value of the character vector vec is “B” and print the results of that is not the case:\n\nvec &lt;- c(\"A\", \"B\", \"C\")\n\nfor (i in 1:3) {\n  if (vec[i] == \"B\") {\n    next\n  }\n  print(vec[i])\n}\n\n[1] \"A\"\n[1] \"C\"\n\n\nUsing break allows you to exit the loop even if the looping variable didn’t take on all values in the sequence. The code is similar to the one for the next statement: you add if (condition) {break}. To illustrate, we’ll use the same code as the one for the next statement but now you break:\n\nfor (i in 1:5) {\n  if (i == 2) {\n    break\n  }\n  print(i)\n}\n\n[1] 1\n\n\nAs you can see, R exits the loop if i reaches the value of 2. With next R continued the loop, but skipped the iteration. Using a break can be useful if there is not need to process anything further after a result is reached. Suppose that your run a simulation until an “event” happens. After the event, there is not need to continue with the code e.g. because the outcome after that event is certain and doesn’t require any further simulation. However, if the event doesn’t happen, the simulation should continue to run. To illustrate, suppose that you have a vector c(1, 2, 3) and you sample 2 observations from that vector. In the loop, you print the samples as long as the condition that the sum of both sample values was not equal to 5. If that happens, you want to exit the loop. To so do, you include the condition if (sum(x) == 5) {break} in the for loop. The loop will continue as long as this condition isn’t met or as long as the sequence isn’t completed:\n\nfor (i in 1:100) {\n  x &lt;- sample(c(1, 2, 3), 2, replace = TRUE)\n  if (sum(x) == 5) {\n    break\n  }\n  print(x)\n}\n\n[1] 1 3\n[1] 2 1\n\n\n\n\n13.2.6 While loops\nA while loop iterates as long as a condition holds. The basic syntax is\n\nwhile (condition) {\n  expression\n}\n\nIn every iteration, the while loop checks the condition in condition. If this condition holds, it starts a new iteration and executes the code in expression. This code is includes in curly braces. If this is not the case, R exits with loop. Here, your condition must be evaluated from the results of the code in expression. If this is not the case, the while loop would run indefinitely. To illustrate, let’s write a while loop that prints the value of i as long as i &lt; 5:\n\ni &lt;- 0\nwhile(i &lt; 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n\n\nHere, the value of i is first set to 0. R checks if the condition i &lt; 5 is met. As 0 &lt; 5, R prints the value of i. In the second line, it changes the value of i to 1. As the code in expression is completed, R checks if the condition i &lt; 5 holds. Because this is the case, R executes the code: its prints i and changes the value of i. R will continue as long as the condition holds: here, it will exit the loop if i = 5 or i &gt; 5. Note two parts in this code where a while loop differs from a for loop. First, the expression must alter a value relevant for the condition. In the example, this is the line i &lt;- i + 1. Suppose that you wouldn’t include that line, then R would never exit the loop. In the example, R would continue to print 0. Second, the first line of the code set the value if i equal to 0. To see why this is relevant, let’s rerun the loop and print the final value of i as R exits the loop:\n\ni &lt;- 0\nwhile(i &lt; 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n\nprint(paste(\"the final value of i is\", i))\n\n[1] \"the final value of i is 5\"\n\n\nLet’s rerun this loop again, but now without that first line:\n\nwhile(i &lt; 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\nprint(paste(\"the expression ran\", 5 - i, \"times\"))\n\n[1] \"the expression ran 0 times\"\n\n\nBecause you change the value of i as part of the expression and the R exits the loop if the value of i is 5, i will be equal to 5 as R exits the loop. You can see this e.g. in your Environment pane. If you now rerun the code a second time, but you don’t reset the value of i to 0. R will not execute the code as the condition i &lt; 5 is not met.\nNot that you can write a for loop as a while loop. Using the first, you fix the maximum value in the sequence (i in sequence). To print i as it loops from 1 to 5 using a for loop:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nTo do the same using a while loop, you set the condition (i &lt;= 5) and increase the looping variable i at the end of the code. Doing so, R will print the first 5 values and exit the loop as i changes from 5 to 6:\n\ni &lt;- 1\nwhile (i &lt;= 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nYou can add a next or break statement in the while loop. Using the former, R will skip the value; using the latter, R exits the loop. However, as a while loop defines an exit condition, you can usually add the break condition in the while condition e.g. using while(i &lt;= 5 & condition2 == FALSE) R would exit the loop in if i &gt; 5 or if a second condition changes from FALSE into TRUE.\n\n\n13.2.7 Repeat loops\nrepeat() loops execute an expression until a condition is met. If the condition is met, R exits the loop. The general syntax of this loop is\n\nrepeat { \n   expression\n \n   if(condition) {\n      break\n   }\n}\n\nAs R enters the loop, it executes the code in expression. That code usually includes the looping variable that will be checked in the condition. After executing the expression, R enter into the if statement and checks if the condition in condition is met. If that is the case, R exits the loop: if the condition is met, the if structure executes break. If the condition is not met, R repeats the code in expression. To illustrate, we’ll perform the same operation as in the last while loop:\n\ni &lt;- 1\n\nrepeat{\n  print(i)\n  i &lt;- i + 1\n  \n  if (i &gt; 5) {\n    break\n  }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nWith the looping variable equal to 1 as R enter the repeat loop, it first executes the code: R prints the value of i (which is 1) and increases that value from 1 to 2. On the third, line, R enter the if statement: it checks if the condition, i &gt; 5 holds. Because this is not the case and we don’t have an else structure, R exits the if statement and repeats the expressions print(i) and i &lt;- i + 1 for a second time. With the value of i now equal to 3; it checks in the if statement is the condition i &gt; 5 is satisfied. Because this is not the case, R continues with the loop. This will continue until i reaches 5. As R executes the expression, it will print “5” and increase the value of i to 6. As the condition in the if statement now returns TRUE, R continues in that statement and executes break. In other words, R exits the repeat loop.\nAs in the while loop, you have to make sure that the looping variable, i in the example, is reset. If you rerun the repeat loop without that line, it wouldn’t show “6”.\n\nrepeat{\n  print(i)\n  i &lt;- i + 1\n  \n  if (i &gt; 5) {\n    break\n  }\n}\n\n[1] 6\n\n\nYou would avoid this by adding the condition first in the repeat loop. However, in that case, R wouldn’t print anything as the looping variable meets the condition i &gt; 5. However, this value was not set as part of your code but because you ran already executed the loop once. Doing so, i was set to such as level that R exited the loop. In other words, i by construction meets the exit condition.\n\nrepeat{\n\n  if (i &gt; 5) {\n    break\n  }\n  \n  print(i)\n  i &lt;- i + 1\n}\n\nNote also that, as with the while loop you need to change the looping variable as part of your code. If you don’t do so, R will either never execute the loop (the looping variable meets to condition to break at the start of the loop) or would never exit the loop.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Programming: control structures</span>"
    ]
  },
  {
    "objectID": "13_Essential_programming_structures.html#apply-family",
    "href": "13_Essential_programming_structures.html#apply-family",
    "title": "13  Programming: control structures",
    "section": "13.3 Apply family",
    "text": "13.3 Apply family\nThe “apply” family includes the “apply” functions: apply(), lapply(), sapply(), tapply(), vapply() and mapply(). In Chapter 4, we used the first three. Recall that these function allow you to iterate over rows or columns in a matrix (apply()) or components in a list (lapply() and sapply()). In other words, you can use them often as a substitute for for loops: if you want to apply a function to all columns in a data frame (variable) or rows or columns in a matrix, you can do so using both a for loop as well as one of the apply functions. In the next section Chapter 14 we will cover how you can write your own functions. Using these, you can expand the usage of the apply family considerably.\nThese functions take another function as one of their arguments. This is why they are referred to as functionals. The other functions that we used, e.g. mean(), max(), sd(), ncol() or ggplot() and gt() don’t do so. Here, you include a vector, matrix or data frame as well as a number of arguments that determine how R will use these functions: mean(x, na.rm = TRUE) tells R to skip missing values and calculate an average using only non missing data, geom_point(aes(x = data1, y = data2)) sets the aesthetics of the point geometry, … . The apply family is different: one of their arguments is always a function FUN. This function can a base R function, an anonymous function or a function you wrote to perform a specific set of tasks. In addition to this function, these functions always need to know the data structure which will be used as input for that function and often require further arguments to set e.g. options or output formats.\napply(X, MARGIN, FUN, ..., simplify = TRUE) applies a function including in FUN to the rows (if MARGIN = 2) or the columns (if MARGIN = 2) of the matrix or array in X. By default, the function tries the simplify the result. In other words, if will return a vector is that is possible. If you want to return a matrix, you need to overrule this default and add simplify = FALSE). If X is not a matrix or array, R will try to coerce that data structure using as.matrix() or as.array(). To refer to the function, you refer to the function name without (), i.e. FUN = mean and not FUN = mean(), FUN = sum and not FUN = sum(). To add other arguments to that function, you can use the ..., e.g. FUN = mean, na.rm = TRUE, FUN = quantile, c(0.25, 0.50, 0.75). Using the function in FUN and the additional arguments in ..., R creates the function call. For instance, with FUN = quantile, c(0.25, 0.50, 0.75), R creates the function quantile(X, probs = c(0.25, 0.50, 0.75)) where is will be a column or row in the matrix X, a component of the list X or a variable in a data frame X.\nTo illustrate these function, consider the matrix mat2\n\nmat2 &lt;- matrix(runif(30, 5, 10), nrow = 6, ncol = 5)\n\nSuppose now that you want to calculate the average for column. Do so so, you can use a for loop:\n\nave_cols_for &lt;- vector(mode = \"numeric\", length = ncol(mat2))\n\nfor (i in 1:ncol(mat2)) {\n  ave_cols_for[i] &lt;- mean(mat2[, i])\n}\n\nUsing the apply function: the matrix is mat2, we want to apply the function mean to the columns (MARGIN = 2) and include the optional na.rm = TRUE argument to add to the mean function and we accept the default simplify = TRUE. Although we don’t have to include the latter, here we do so to show all arguments of the function:\n\nave_cols_apply &lt;- apply(mat2, MARGIN = 2, FUN = mean, na.rm = TRUE, simplify = TRUE)\n\nYou can see that both include the same values\n\nifelse(ave_cols_apply == ave_cols_for, \"equal\", \"not equal\")\n\n[1] \"equal\" \"equal\" \"equal\" \"equal\" \"equal\"\n\n\nIn addition the a known R function, you can add anonymous functions. Anonymous functions are, as, there name suggests, anonymous in the sense that they don’t have a name such as mean or sd. In other programming languages, e.g. Python or in Excel they are referred to as lambda functions. Using these functions, you can include the your own function in a the FUN argument of apply(). An anonymous functions start with the function call function(x) or, using short hand \\(x). The function body - what the function does - is included in curly braces { }. If the function body is short, you can leave these curly braces.\nSuppose you want to add 1 to every column of mat2. An anonymous function would look like function(x) {x + 1} or \\(x) {x + 1}. The rows (MARGIN = 1) or the columns (MARGIN = 2) of the matrix in X will now be used as arguments x in the anonymous function.\n\napply(mat2, MARGIN = 2, FUN = \\(x)  {x + 1})\n\n         [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 8.510592  9.724245  9.438551  6.417457 10.140879\n[2,] 6.350806  8.188205  7.257179 10.520844 10.571784\n[3,] 6.258652  8.041628 10.336023  8.475817  6.546518\n[4,] 9.143760 10.997597  9.283681  6.106027  7.689959\n[5,] 6.142131 10.569054  8.978952 10.673483  8.429682\n[6,] 9.141299  6.251721  9.838995  6.986158  7.497547\n\n\nThe result shows that R added 1 to each column of the matrix mat2. A for loop would have required\n\nadd_1_cols &lt;- matrix(0, nrow = 6, ncol = 5)\n\nfor (i in 1:ncol(mat2)) {\n  add_1_cols[, i] &lt;- mat2[, i] + 1\n  }\nadd_1_cols\n\n         [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 8.510592  9.724245  9.438551  6.417457 10.140879\n[2,] 6.350806  8.188205  7.257179 10.520844 10.571784\n[3,] 6.258652  8.041628 10.336023  8.475817  6.546518\n[4,] 9.143760 10.997597  9.283681  6.106027  7.689959\n[5,] 6.142131 10.569054  8.978952 10.673483  8.429682\n[6,] 9.141299  6.251721  9.838995  6.986158  7.497547\n\n\nNote that in this case, adding one to all elements of the matrix could have been done using mat2 + 1.\nTo show another example, let’s calculate the standardized values for each column in mat2. To standardize a value, you subtract the mean of the value and you divide by its standard deviation. The anonymous function to do so is \\(x) {(x - mean(x))/sd(x)}:\n\napply(mat2, MARGIN = 2, FUN = \\(x) {(x - mean(x))/sd(x)})\n\n           [,1]       [,2]        [,3]       [,4]        [,5]\n[1,]  0.6178141  0.4247574  0.23620793 -0.8761951  1.05231718\n[2,] -0.8335321 -0.4312776 -1.82767789  1.1446122  1.32523457\n[3,] -0.8954585 -0.5129650  1.08534355  0.1374914 -1.22420600\n[4,]  1.0432944  1.1343962  0.08967895 -1.0295659 -0.49999699\n[5,] -0.9737586  0.8955689 -0.19863753  1.2197827 -0.03148607\n[6,]  1.0416406 -1.5104799  0.61508499 -0.5961253 -0.62186268\n\n\nHere, a for loop would have been longer to write. For every column, you first calculate the mean and standard deviation. Using these values, you can standardize the column:\n\nstand_mat_cols = matrix(0, nrow = 6, ncol = 5)\n\nfor (i in 1:ncol(mat2)) {\n  ave &lt;- mean(mat2[, i])\n  sd &lt;- sd(mat2[, i])\n  stand_mat_cols[, i] &lt;- (mat2[, i] - ave)/sd\n}\n\nstand_mat_cols\n\n           [,1]       [,2]        [,3]       [,4]        [,5]\n[1,]  0.6178141  0.4247574  0.23620793 -0.8761951  1.05231718\n[2,] -0.8335321 -0.4312776 -1.82767789  1.1446122  1.32523457\n[3,] -0.8954585 -0.5129650  1.08534355  0.1374914 -1.22420600\n[4,]  1.0432944  1.1343962  0.08967895 -1.0295659 -0.49999699\n[5,] -0.9737586  0.8955689 -0.19863753  1.2197827 -0.03148607\n[6,]  1.0416406 -1.5104799  0.61508499 -0.5961253 -0.62186268\n\n\nIf the data structure in X is not a matrix, apply() uses as.matrix or as.array() and tries to coerce a data structure into a matrix or array. If you have a data frame that includes only numeric values, as.matrix(df) will coerce the data frame df in a matrix. As a result, you can also use apply() with data frames as long as all there elements are of the same type. To illustrate, let’s use this data frame df with two variables, var1 and var2:\n\ndf &lt;- data.frame(\n  var1 = runif(10, 10, 100), \n  var2 = runif(10, 10, 100)\n)\n\nTo standardize these variables, you can use apply():\n\ndf_mat &lt;- apply(df, 2, \\(x) (x - mean(x))/sd(x))\ndf_mat\n\n            var1       var2\n [1,]  0.5774816  1.0729378\n [2,] -0.7644541  0.8230842\n [3,] -0.6165743 -0.3429094\n [4,] -1.1527721 -0.3248031\n [5,]  1.5108234 -1.1457859\n [6,]  0.0477781 -1.0970574\n [7,]  0.0872722  0.7521211\n [8,] -1.1700595  0.2784886\n [9,] -0.1585258  1.3682836\n[10,]  1.6390306 -1.3843593\n\n\nAs you can see, R returns a matrix, and adds the variables names in df as column names. You can coerce this matrix into a data frame:\n\ndf_df &lt;- as.data.frame(df_mat)\nhead(df_df)\n\n        var1       var2\n1  0.5774816  1.0729378\n2 -0.7644541  0.8230842\n3 -0.6165743 -0.3429094\n4 -1.1527721 -0.3248031\n5  1.5108234 -1.1457859\n6  0.0477781 -1.0970574\n\n\nNote that in these case, and in addition to a for loop, you could have used {dplyr}, e.g. \n\ndf |&gt; dplyr::mutate(across(var1:var2, \\(x) (x - mean(x))/sd(x)))\n\n         var1       var2\n1   0.5774816  1.0729378\n2  -0.7644541  0.8230842\n3  -0.6165743 -0.3429094\n4  -1.1527721 -0.3248031\n5   1.5108234 -1.1457859\n6   0.0477781 -1.0970574\n7   0.0872722  0.7521211\n8  -1.1700595  0.2784886\n9  -0.1585258  1.3682836\n10  1.6390306 -1.3843593\n\n\nRecall that lapply() (lapply(X, FUN, ...)) and sapply() (sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)) use lists as their input data structure. If that structure is not a list both will (try to) coerce the structure to a list using as.list. The difference between both is that lapply() also returns a list (the “l” in “l”apply) while sapply() tries to simplify the result (the “s” in sapply: “s”implifies the result). In addition, both functions need a function (FUN) to apply to the components of a list. With sapply(), you can further specify if the result needs to be simplified (by default TRUE) and if the output needs to use the names of the components in the list. For both, the ... allow you to add additional arguments to the function. Here, too, you can add known R function in FUN or use anonymous functions. For both, the components of the list are always used as the first arguments in the function in FUN\nConsider for instance the data frame that we already used:\n\ndf &lt;- data.frame(\n  vec1 = rnorm(10, 5, 1), \n  vec2 = rnorm(10, 10, 10), \n  vec3 = rnorm(10, 15, 20))\n\nTo calculate the mean for each column, you can use a for loop:\n\nave_df_for &lt;- vector(mode = \"numeric\", length = length(df))\n\nfor (i in seq_along(colnames(df))) {\n  ave_df_for[i] &lt;- mean(df[[i]])\n}\n\nBecause a data frame is also a list, we can use to apply the function mean to the data frame df. In addition, we add the argument na.rm = TRUE: lapply(df, FUN = mean, na.rm = TRUE)\n\nave_df_list &lt;- lapply(df, FUN = mean, na.rm = TRUE)\n\nUsing sapply(), we can now extract the components of the ave_df_list list and return a vector. To do so, we apply the function \"[\" to the elements of the components of that list. This function extracts (subsets) to values in the list. As an alternative, we could have used unlist(). Using sapply():\n\nave_df_mat &lt;- sapply(ave_df_list, FUN = \"[\")\n\nWe can use sapply() also to calculate the mean of each column and return a vector. Here, you apply the function mean to the data frame (list) df, simplify the result to a vector or matrix and copy the names of the components (defaults, not shown in the code):\n\nave_df_vector &lt;- sapply(df, FUN = mean, na.rm = TRUE)\n\nTo verify that they all return the same values:\n\nifelse(ave_df_for == ave_df_mat, ifelse(ave_df_mat == ave_df_vector, \"equal\", \"not equal\"), \"not equal\")\n\n   vec1    vec2    vec3 \n\"equal\" \"equal\" \"equal\" \n\n\nvapply() is very similar to sapply(). The arguments of this function are\n\nvapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\n\nAs you can see, there is only one difference with sapply(): the FUN.VALUE argument. The sapply() function “tries” to simplify the result to a vector. However, if sapply() can’t to that, it will return a list but will do so without any further warning or message. This makes sapply() somewhat unpredictable. To illustrate, let’s ask sapply() to create 4 random draws from a uniform distribution. The input list in this case is as.list(1:4): a list with 4 components, the first equal to the numeric value 1, the second component equal to the numeric value 2, … .\n\nas.list(1:4)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n\nIf we include this list in the sapply() function with rnuif, sapply() will “apply” the function runif to every component of the list and use the component as the first argument in runif. This first argument is the number of random draws. In other words, it will run rnuif(1), runif(2), runif(3) and runif(4). The output of the first is one number: one random draw from a uniform distribution with minimum 0 and maximum 1; the second result will be a vector with two draws from the uniform distribution with the minimum and maximum values equal to 0 and 1; the third result will be a vector with three random draws from that uniform distribution and the last result will be a vector with four random draws. Recall that R’s only data structure that can handle components of different length is a list. All other structures can’t store components if their length is not equal: a matrix needs n row values for every column, a data frame needs the same number of observations for every variable, … . In other words, R can’t store the sapply() output in a data structure unless it uses a list. As sapply() can not simplify the results, it returns a list whose first component is a number, the second component a vector with 2 elements, the third component a vector with 3 elements, … .\n\nsapply(as.list(1:4), runif)\n\n[[1]]\n[1] 0.3200079\n\n[[2]]\n[1] 0.9600407 0.6207772\n\n[[3]]\n[1] 0.9133408 0.7558969 0.8980294\n\n[[4]]\n[1] 0.9079993 0.8881058 0.6078219 0.1434507\n\n\nIf this is what you had in mind, that is ok. However, if this isn’t the case, note that R doesn’t warn that it couldn’t simplify the result: sapply() returns a list without any further message or error. This is where vapply() enters. In the FUN.VALUE argument, you can include the type of output that you expect. For instance, if you expect one numeric value, you can enter numeric(1), if you expect one logical values, you can use logical(1), if you expect 3 integers, you add integer(3). If the function doesn’t return this output, vapply() will trigger a message. To illustrate, let’s run the same code as.list(1:4), runif but use vapply() and include the predicted output numeric(1):\n\nvapply(as.list(1:4), runif, numeric(1))\n\nError in vapply(as.list(1:4), runif, numeric(1)): values must be length 1,\n but FUN(X[[2]]) result is length 2\n\n\nThe first result of the function fits with the expected output: runif(1) returns one numeric value. However, the second doesn’t: as vapply() expects one numeric value and running runif(2) returns two, vapply() produces an error: it shows where the error occurred: in “Error in vapply(as.list(1:4), runif, numeric(1)) : values must be length 1, but FUN(X[[2]]) result is length 2”. In other words, it tells you where the code didn’t return the expected result: FUN(X[[2]]) or, in other words, as it was using the second component of the list (X[[2]]) to the runif function. Doing so, vapply() is predictable: it will either manage to simplify the result to the desired outcome (numeric(1), numeric(4), logical(1), …) or it stops the code and returns a error.\ntapply() applies a function (FUN) to each group defined in INDEX in the matrix, vector or data frame X: (tapply(X, INDEX, FUN, ..., simplify = TRUE)). If you think about this function in terms of {dplyr}, the vector in INDEX would the variable used in group_by(), FUN would be e.g. summarize() and X would be the data frame. To illustrate, suppose that you have one vector with numeric values (here draws from a random distribution, the first 10 with mean 5 and the second 10 with mean 50) and a second vector, with the same number of elements as the first, but that you can use to identify groups, e.g. a character vector whose first 10 values are A and whose second group of 10 values is B:\n\nvec1 &lt;- c(rnorm(10, 5, 10),  rnorm(10, 50, 10))\nvec2 &lt;- c(rep(\"A\", 10), rep(\"B\", 10))\n\nSuppose now that you want to calculate the mean of the values in vec1 but you want to do so for two groups: you want the mean for the first 10 values and the mean for the second group of 10 values. The values in vec2 allow you to identify these two groups. Using tapply(X = vec1, INDEX = vec2, FUN = mean), this function will use vec2 to identify two groups in vec1: the first groups includes all elements in the same position in vec1 where vec2 has “A”; the second group includes all elements in the same position in vec1 where vec2 has “B”. In other words, R splits vec1: if vec2[i] has value “A”, then vec1[i] is part of the first group; if vec2[j] has value “B”, then vec1[j] is part of the second. For each group, tapply() applies the function mean:\n\ntapply(vec1, vec2, mean)\n\n       A        B \n11.05302 49.93080 \n\n\nIf X is a data frame, then you you can identify one column in the data frame whose values can be used to group and another column whose values will be used as input in the function FUN, e.g. mean, sd or an anonymous function using function(x) ....\nThe use of tapply() is closely related to base R’s split(x, f, drop = FALSE, ...) function. Using that function R “splits” a vector or data frame in groups using a factor f. If f is not defined as a factor, R will use as.factor() to coerce to a factor. f can also include a function, e.g logical function allowing to split the vector or data frame in x. The function returns a list of vectors: where each component in that list is a vector of values for each group and the names of the list refer to the group:\n\nsplit(vec1, vec2)\n\n$A\n [1]  5.994230  1.239080 31.009353 13.720597  6.579992  2.948110  6.400865\n [8] 25.170761 11.688247  5.778989\n\n$B\n [1] 46.91231 59.23124 47.77651 40.52357 39.86541 59.78492 43.88287 72.36882\n [9] 44.58143 44.38088\n\n\nsplit() is often used with lapply() to apply a function per group. For instance, calculate the sum in each group using lapply() uses the list returned by split() as an the input list in the lapply() function. As this function returns a list where each components includes the values for a group, including the function sum in lapply() allows you to calculate the sum of the values for each group in vec1:\n\nlapply(split(vec1, vec2), sum)\n\n$A\n[1] 110.5302\n\n$B\n[1] 499.308\n\n\ntapply() and split() allow you to apply function to values in a specific group. Without these function, doing so would requires an if else statement in a for loop: for every value in a vector or column in a data frame evaluate to which group this value belongs and store the result in a matrix, list of data frame which includes only the values for that group. Using these structures, apply the function you need to apply. In other words, the code would look like:\n\ngroupA &lt;- vector(mode = \"numeric\", length = 0)\ngroupB &lt;- vector(mode = \"numeric\", length = 0)\n\nfor (i in seq_along(vec1)) {\n  if (vec2[i] == \"A\") {\n    groupA = c(groupA, vec1[i])\n  } else {\n    groupB = c(groupB, vec1[i])\n  }\n}\n\nsumA &lt;- sum(groupA)\nsumB &lt;- sum(groupB)\n\nmapply() is the multivariate version in sapply(). If allows you to include more than one argument in call to a function. This function is the first argument in the mapply() function:\n\nmapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)\n\nIn other words, the function can look like function(x, y, z) and include multiple arguments in stead of function (x) or mean, sd which includes only one argument as in the sapply() case. The arguments, are includes in the .... If you want to pass more arguments to the function, include them in a list and add them in MoreArgs, here you can e.g. add list(na.rm = TRUE).\nLet’s see how we can create 2 vectors with normal random variable, each of length 10 but one with mean 5 and standard deviation 10 and the other with mean 50 and standard deviation 100. Recall that rnorm(n, mean = , sd = ) has 3 arguments. Including those in ...: n = 10, mean = c(5, 50) and sd = c(10, 100) the function mapply() will first run rnorm(n = 10, mean = 5, sd = 10) and then run rnorm(n = 10, mean = 50, sd = 100). Doing so, it creates two vector. As mapply() by default tries to simplify the output, it will store these vectors as columns in a matrix.\n\nmapply(rnorm, n = 10, mean = c(5, 50), sd = c(10, 100))\n\n             [,1]        [,2]\n [1,]   3.8908220  236.211816\n [2,]   9.8786180 -280.408603\n [3,]   8.2921461   45.273120\n [4,]   1.2006210  115.731192\n [5,]   0.7991230    4.137462\n [6,]   3.9846388  -22.274116\n [7,]   0.7629664  -35.409765\n [8,]   6.6472929   98.048659\n [9,] -11.3754524   -1.962527\n[10,]  12.4735423  141.460400\n\n\nAlthough creating two vectors with random normal draws wouldn’t require less code then the case where you use mapply(), this would become a issue if you need a large number of these vectors: without mapply() you would need to add 1 line of code for each random vector and add values for the means and standard deviation in each line. Doing so, you are likely to make mistakes. Here, mapply() offers a way to creates these vector without copy paste and change values processes.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Programming: control structures</span>"
    ]
  },
  {
    "objectID": "13_Essential_programming_structures.html#using-control-structures-examples",
    "href": "13_Essential_programming_structures.html#using-control-structures-examples",
    "title": "13  Programming: control structures",
    "section": "13.4 Using control structures: examples",
    "text": "13.4 Using control structures: examples\n\n13.4.1 Central limit theorem:\nsee lecture code\n\n\n13.4.2 Lottery\nsee lecture code\n\n\n13.4.3 ggplot\nsee lecture code\n\n\n13.4.4 Project uncertainty\nsee lecture code\n\n\n13.4.5 Using apply for the central limit theorem\nsee lecture code\n\n\n13.4.6 Using apply famility for the lottery\nsee lecture code",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Programming: control structures</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#the-function-syntax",
    "href": "14_Functions.html#the-function-syntax",
    "title": "14  Functions",
    "section": "",
    "text": "14.1.1 Illustration 1: the function add_one\nLet’s illustrate this template with a simple function, add_one:\n\nadd_one &lt;- function(x) {\n  \n  x + 1\n  \n}\n\nThe function’s name if add_one. There is one formal argument, x. The body of the function includes the code that R will execute if you call this function. Here, this code is short and includes only one line: add one to x or x + 1. As this is the only expression in the body, this will also be the output of this function. The function add_one() is now defined in R. If you type add in the console, the function now appears in the suggestions. If you select that function, R also shows the arguments. In this case, there is only one: x. You can see the function add_one the environment pane.\nLet’s assign the value of 5 to the formal argument x. There are two ways of doing so. First, via name matching:\n\nadd_one(x = 5)\n\n[1] 6\n\n\nor positional matching:\n\nadd_one(5)\n\n[1] 6\n\n\nThe function returns 6. To use this value in subsequent parts of your code, you need to assign the return value of the function to a variable. Here, R will returns the output of x + 1. With respect to the arguments of add_one(), this means that you can include all values and data structures that allow R to add one to x, e.g. \n\na value\n\n\nadd_one(x = 5)\n\n[1] 6\n\nclass(add_one(x = 5))\n\n[1] \"numeric\"\n\n\n\na vector\n\n\nadd_one(1:10)\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\nclass(add_one(1:10))\n\n[1] \"numeric\"\n\n\n\na matrix\n\n\nmat1 &lt;- matrix(c(1:10, 101:110), nrow = 10, ncol = 2)\n\nadd_one(mat1)\n\n      [,1] [,2]\n [1,]    2  102\n [2,]    3  103\n [3,]    4  104\n [4,]    5  105\n [5,]    6  106\n [6,]    7  107\n [7,]    8  108\n [8,]    9  109\n [9,]   10  110\n[10,]   11  111\n\nclass(add_one(mat1))\n\n[1] \"matrix\" \"array\" \n\n\n\na data frame\n\n\ndf1 &lt;- data.frame(\n  A = 1:10, \n  B = 21:30\n)\n\nadd_one(df1)\n\n    A  B\n1   2 22\n2   3 23\n3   4 24\n4   5 25\n5   6 26\n6   7 27\n7   8 28\n8   9 29\n9  10 30\n10 11 31\n\nclass(add_one(df1))\n\n[1] \"data.frame\"\n\n\n\nor a function\n\n\nadd_one(add_one(x = 5))\n\n[1] 7\n\n\nNote that in the latter case, the function add_one() is nested in the function add_one(). The inner function returns add_one(n = 5) or 6. The is the input of the outer add_one() function. This function now returns add_one(6) or 7.\nYou can use this function on other functions, e.g. \n\napply(mat1, 2, add_one)\n\n      [,1] [,2]\n [1,]    2  102\n [2,]    3  103\n [3,]    4  104\n [4,]    5  105\n [5,]    6  106\n [6,]    7  107\n [7,]    8  108\n [8,]    9  109\n [9,]   10  110\n[10,]   11  111\n\n\nor\n\nlist1 &lt;- list(\n  C = 1:10, \n  D = 21:30\n)\n\nlapply(list1, add_one)\n\n$C\n [1]  2  3  4  5  6  7  8  9 10 11\n\n$D\n [1] 22 23 24 25 26 27 28 29 30 31\n\n\nLast, recall that the pipe operator |&gt; puts the value on its left hand side as the first argument of the function on its right hand side. Here, with only one formal argument, this means that you can leave the argument open:\n\n100 |&gt; add_one()\n\n[1] 101\n\n\nThese last two examples and the function definition of add_one() allow to highlight the similarities with the anonymous functions. Recall that we used these in e.g. Chapter 13. As a matter of fact, we used the anonymous function function(x) {x + 1} or, using shorthand, \\(x) {x + 1} to illustrate apply(). As you can see, with the exception of the name, add_one the anonymous function shares a lot of similarities with the named function. The body in both the is same and, if you are not using the shorthand version, so is the function() directive. However, as the anonymous function is not an object in R, you can not reuse it in subsequent parts of your code. Using a named function, reuse if possible.\n\n\n14.1.2 Illustration 2: the function final_value\nAs a second example, let’s define a function that calculate the value of a value of a bond after one year with an interest rate equal to 5% (0.05). For every 100 in that bond, the bond returns 100 + 5% of 100 = 100 + 0.05 * 100 = 100 * (1 + 0.05), or, more in general, for every 100 invested in that bond and with an interest rate equal to i, the bond returns after one year:\n\\[\n100 * (1 + i)^1\n\\]\nIf the bond runs for two years the bond’s value after two years equals the value after one year plus the interest rate on that value earned in the second year: (100 + 5% of 100) + 5% of 105 = (100 + 5% of 100) + 5% of (100 + 5% of 100) = 100 (1 + 0.05)^2. More in general, with an interest rate equal to i, the bond’s value after two years is\n\\[\n100 * (1 + i)^2\n\\]\nAfter three years, the value of the bond equals\n\\[\n100 * (1 + i)^3\n\\] and after n years, the value is\n\\[\n100 * (1 + i)^n\n\\]\nLet’s build a function that allows to calculate the value for any investment (i.e. not necessarily equal to 100), for any interest rate (i.e. with i not necessarily equal to 5%) and for varying investment periods (i.e. for varying n). Here, this function as three arguments: x, the initial value of the investment, i, the interest rate on the bond and n the number of years in the investment horizon. The body of the functions calculates the value of the bond after n years: x * (1 + i)^n. Let’s use the name final_value() for this function. We now have all the elements we need to create the function final_value():\n\nfinal_value &lt;- function(x, i, n) {\n  \n  x * (1 + i)^n\n  \n}\n\nIn the environment pane, you can see that this function is no defined. If you type fin in the console, R will include final_value as one of the suggestions. We can no call this function to calculate the value of a bond. Matching the arguments by name to calculate the value of 100 after one year and with an interest rate of 5%:\n\nfinal_value(x = 100, i = 0.05, n = 1)\n\n[1] 105\n\n\nAs we matched the arguments by name, we don’t have to follow their order:\n\nfinal_value(i = 0.05, n = 1, x = 100)\n\n[1] 105\n\n\nMatching the arguments by position need to respect the order:\n\nfinal_value(100, 0.05, 1)\n\n[1] 105\n\n\nIf you don’t do so, the function will not return the correct final value\n\nfinal_value(1, 0.05, 100)\n\n[1] 131.5013\n\n\nHere, you calculate the value of an investment equal to 1 at an interest rate of 5% and for a period of 100 years.\nAs with add_one, final_value allows you to include a vector of investments as values for the formal arguments, e.g. \n\nfinal_value(x = 100:109, i = 0.05, n = 2)\n\n [1] 110.2500 111.3525 112.4550 113.5575 114.6600 115.7625 116.8650 117.9675\n [9] 119.0700 120.1725\n\n\ncalculates the value of the bond after two years using an interest rate equal to 5% for 10 values of x: 100, 101, …, 109;\n\nfinal_value(x = 100, i = seq(from = 0.01, to = 0.10, by = 0.01), n = 2)\n\n [1] 102.01 104.04 106.09 108.16 110.25 112.36 114.49 116.64 118.81 121.00\n\n\ncalculates the value of the bond with an intitial investment of 100, for 2 years but using interest rates starting at 1% and ending at 10% in steps of 1%-point and\n\nfinal_value(x = 100, i = 0.05, n = 1:10)\n\n [1] 105.0000 110.2500 115.7625 121.5506 127.6282 134.0096 140.7100 147.7455\n [9] 155.1328 162.8895\n\n\ncalculates the value of the bond with an initial investment of 100, an interest rate of 5% but for periods starting at 1 year and ending at 10 years in steps of 1 year. However, you can not add various values for different arguments at the same time\n\nfinal_value(x = 100:109, seq(from = 0.01, to = 0.10, by = 0.01), n = 1)\n\n [1] 101.00 103.02 105.06 107.12 109.20 111.30 113.42 115.56 117.72 119.90\n\n\nAs you can see, R returns the final value for every investment but does so only for an interest rate equal to 1%. The reason is that R can apply a function, e.g. * (1 + 0.01), to a vector (function are vectorized) but can not apply a whose arguments change to a vector. For instance, R will only use the first element of the second vector in this multiplication:\n\nc(100, 200) * c(2, 5)\n\n[1]  200 1000\n\n\nUsing the |&gt; operator, you can “pipe” values for x in the function:\n\n1000 |&gt; final_value(0.05, 2)\n\n[1] 1102.5\n\n\nTo pipe an interest rate of investment horizon into the function, you have to use a named argument with the place holder _ (Chapter 1)\n\n0.05 |&gt; final_value(100, i = _, 1)\n\n[1] 105",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#the-function-environment-and-scoping-rules",
    "href": "14_Functions.html#the-function-environment-and-scoping-rules",
    "title": "14  Functions",
    "section": "14.2 The function environment and scoping rules",
    "text": "14.2 The function environment and scoping rules\nIf you run a function, R creates a special environment for that function. A variables that your create inside a function only exist within that function. To illustrate, let’s create an variant of the function final_value() and define final_value2() as:\n\nfinal_value2 &lt;- function(x, i, n) {\n  \n  y &lt;- x * (1 + i)^n\n  y &lt;- y^2\n  \n}\n\nNote that final_value2() only differs from final_value() because it assigns the outcome of the calculation to a variable y and then calculates the square of y? If you now run the function, it doesn’t return anything:\n\nfinal_value2(x = 100, i = 0.05, n = 1)\n\nThis shouldn’t come as a surprise as you assign the value of the calculation to a variable y. If you assign a value to a name, R never returns the value. For instance, here\n\nz &lt;- 10 + 20\n\nR creates the variable z, assigns the value of 30 but doesn’t return the value of 30 in the console. In the environment pane, you can see that variable z as well as its value 30. However, if you run\n\nfinal_value2(x = 100, i = 0.05, n = 1)\n\nYou don’t see the variable y in the environment pane. If you use the value of y outside of the function,\n\nfinal_value2(x = 100, i = 0.05, n = 1)\ny\n\nError: object 'y' not found\n\n\nR shows an error as it can not find the value of y even if you just called the function final_value2() where y was assigned the value of the bond. The reason for this is that R creates a new environment every time the function is called. The objects that are created when the function is called only exist as within that function and do not have an impact on values in other environments, e.g. the global environment (the environment pane). Applied to final_value2(): R creates the object y within that function, but forgets about it as the function is executed. As a result, R shows the error y not found.\nWhat happens if you already had a value for y defined outside of the function? For instance, suppose that you have the following two lines of code:\n\ny &lt;- 50\nfinal_value2(x = 100, i = 0.05, n = 1)\n\nHere, y is first assigned the value of 50. We know that the function final_value2() will first assign the value 100 * (1 + 0.05)^1 or 105 to y and second, will assign the value of y^2 to y. If we now ask for the value of y, R returns\n\ny &lt;- 50\nfinal_value2(x = 100, i = 0.05, n = 1)\ny\n\n[1] 50\n\n\nthe value of 50, i.e. the value that was assigned to y in the first line. The reason lies in the fact that R creates a separate environment as the function executes. Within that environment, the value of y stands on its own, or, it “masks” the the object with the same name outside of the function. This is due to the fact that R uses lexical scoping rules: R first looks for the value of an object in the function itself, then in any enclosing environment (e.g. the global environment), then in a package, … . Here, as R finds the value of y it needs to compute the square of y inside the function final_value2(), it doesn’t look for the value of y in the global environment. As R “forgets” about the values its creates in a function, the value of y in the global environment (50) was never changed as final_value2() was executed.\nThis also means that every time you call the function, the objects that you define within that function don not have a value. In other words, R forgets the value of y permanently.\nIf you define a variable outside of the function (in the global environment), R will first search in body of the function to see if it can find the associated value. If that is not the case, R will search for a value in the global environment. In other words, using final_value() and assigning the value of 100 to x outside of the function, R will use this value for x as it executes the code.\n\nx &lt;- 100\n\nfinal_value(x, i = 0.05, n = 1)\n\n[1] 105\n\nx\n\n[1] 100",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#the-formal-arguments-of-a-function",
    "href": "14_Functions.html#the-formal-arguments-of-a-function",
    "title": "14  Functions",
    "section": "14.3 The formal arguments of a function",
    "text": "14.3 The formal arguments of a function\n\n14.3.1 Default values\nThe function final_value() includes 3 formal arguments: x, i and n. If one of these arguments is missing, the function will not return a value:\n\nfinal_value(x = 100, i = 0.05)\n\nError in final_value(x = 100, i = 0.05): argument \"n\" is missing, with no default\n\n\nOne way to avoid this is to include default values. With default values, R will use these values unless the function call includes others. To do so, you add the default values for all formal arguments where you want to define a default value in the definition of the function. For instance, suppose you want to include a default value for the interest rate iequal to 5% and a default value for the investment period n of 1 year without definining a default value for the initial investment x, you add these values in the definition:\n\nfinal_value &lt;- function(x, i = 0.05, n = 1) {\n  \n  x * (1 + i)^n\n  \n}\n\nThe environment pane shows these default values. In addition if you type fin in the console and select the function final_value() R now shows the default values. With these values, you can now call the function final_value() with one argument: the value for x:\n\nfinal_value(1000)\n\n[1] 1050\n\n\nTo calculate the final value using another interest rate, you can specify its value:\n\nfinal_value(1000, 0.075)\n\n[1] 1075\n\n\nAs you can see, R now uses the value of 7.5% for the interest rate while it keeps the value for the investment horizon n. Changing the investment horizon n while keeping the default value for the interest rate requires a name match as\n\nfinal_value(1000, 10)\n\n[1] 11000\n\n\nreturns the value of the bond after one year with with an interest rate equal to 1000% and multiplies the investment with (1 + 10) ^1. In other words, if you want to calculate the final value for an investment horizon of 10 years using the interest rate’s default value, you have to name the argument:\n\nfinal_value(1000, n = 10)\n\n[1] 1628.895\n\n\n\n\n14.3.2 Lazy evaluation\nArguments in the function are evaluated as they are needed in the body of the function. This is also referred to as lazy evaluation. One of the consequences is that R will execute a function even if one of its arguments is never used. To illustrate, let’s consider the following example.\n\nf_lazy &lt;- function(x, y) {\n  \n  2 * x\n  \n}\n\nThe function f_lazy() has two formal arguments: x and y. However, the function never uses the second argument, y. Neither of these arguments has a default value. R will execute the function without a warning or error as long as you include a value for x. Leaving out a value for y will show the expected result:\n\nf_lazy(x = 2)\n\n[1] 4\n\n\nThis also holds in case you don’t refer to the name of the argument as the value of 2 will be matched with x by position:\n\nf_lazy(2)\n\n[1] 4\n\n\nYou will also get the expected result if you add a value for y:\n\nf_lazy(x = 2, y = 10)\n\n[1] 4\n\n\n\n\n14.3.3 Free variables\nTo illustrate a free variable, consider this function f_free():\n\nf_free &lt;- function(x, y) {\n  \n  (x + y) / z\n}\n\nThis function includes two formal arguments, x and y but uses three values in the body of the function: x, y and z. We already know that R will search for the value of z first within the function and then in the global environment. If the value of z is defined in that environment, R will use that value. If that isn’t the case, R will report an error:\n\nf_free(x = 10, y = 20)\n\n[1] 1\n\n\nIf there is a value assigned to z, R will use that value:\n\nz &lt;- 5\nf_free(x = 10, y = 20)\n\n[1] 6\n\n\n\n\n14.3.4 Are the arguments correct?\nIn the body of the function, you can add a check on the values. As an example, for the interest rate, you can use 0.05 as well as 5.00. The first is assumed in the code, the second would return a value, but that final value wouldn’t be correct. To avoid these mistakes, you can add a check: if the interest rate is higher than or equal to e.g. 1 (or 100%), you assume that the interest rate is included as 2% and not as 0.02. Using that assumption, use divide the value if the argument i by 100 is the condition i &gt;= 1 is met. In addition, you print a warning that this changes was made. You can add this in the body of the function:\n\nfinal_value &lt;- function(x, i = 0.05, n = 1) {\n  \n  if (i &gt;= 1) {\n    print(paste(\"Warning: Interest rate\", i, \"divided by 100. Interest rate used is\", i/100))\n    i &lt;- i/100\n    }\n  x * (1 + i)^n\n  \n}\n\nIf you include e.g. 2 as a value for i, the function will now use 0.02 to calculate the final value but will also warn that it didn’t use 2 but divided the interest rate by 100.\n\nfinal_value(100, i = 2, n = 1)\n\n[1] \"Warning: Interest rate 2 divided by 100. Interest rate used is 0.02\"\n\n\n[1] 102",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#the-return-value",
    "href": "14_Functions.html#the-return-value",
    "title": "14  Functions",
    "section": "14.4 The return value",
    "text": "14.4 The return value\nA function returns the last line expression that was executed. In final_value() - assuming that the interest rate is correct - this is x * (1 + i)^n. In addition, a function returns only one data structure. What if you want to show more than the final value but would like to include the initial value as will as the interest rate and investment horizon? To do so, you have to add all values that the function needs to return in a single object and include that object in the last line of your code, e.g\n\na (named) vector,\n\n\nfinal_value &lt;- function(x, i, n) {\n  y &lt;- x * (1 + i)^n\n  c(\"init_value\" = x, \"final_value\" = y)\n}\n\nwhich returns\n\nfinal_value(x = 100, i = 0.05, n = 1)\n\n init_value final_value \n        100         105 \n\n\n\nor, e.g. a list\n\n\nfinal_value &lt;- function(x, i, n) {\n  y &lt;- x * (1 + i)^n\n  list(init_value = x, final_value = y)\n}\n\nwhich allows to store heterogeneous results\n\nfinal_value(x = 100, i = 0.05, n = 1)\n\n$init_value\n[1] 100\n\n$final_value\n[1] 105\n\n\nAs you define the type of the result, your functions become more predictable: if you set the return value to be a list, the function will return a list.\nA function returns the last expression. The return() statement allows you to be explicit on what the function will return. In the parentheses of this statement, you add the expression that the function will return. Adding this statement in the previous definition:\n\nfinal_value &lt;- function(x, i, n) {\n  y &lt;- x * (1 + i)^n\n  return( c(\"init_value\" = x, \"final_value\" = y))\n}\n\nshows that the function will return a named vector with two values: x and y.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#functions-data-frames-and-dplyr-verbs",
    "href": "14_Functions.html#functions-data-frames-and-dplyr-verbs",
    "title": "14  Functions",
    "section": "14.5 Functions, data frames and {dplyr} verbs",
    "text": "14.5 Functions, data frames and {dplyr} verbs\nUsing functions in conjunction with {dplyr}’s summarize() or mutate() allows you write multiple lines of code in one function. This is especially useful if you have to repeat the same steps multiple times, e.g. calculate the same summary statistics or have to use mutate to add variables to a data frame. Recall that these function allow you to refer to variables without an explicit reference to the data frame. For instance, you can write summarize(df, mean = mean(varname)) and not summarize(mean(df$varname)) or mutate(df, squared_var = var^2) and not df$squared_var &lt;- df$var^2. In addition, with function such as across() or filter() you can use  syntax (starts_with(), where(), …). Here too, you write the name of the variable without explicit reference to the data frame. If you write a function, where the arguments include a variable in a data frame, {dplyr}’s functions need to\nTo illustrate, we’ll use an example taken from Wickham, Cetinkaya-Rundel, and Grolemund (2023). Suppose you want to write a function to calculate the means of a variable mean_var across groups defined by group_var. Using {dplyr}’s group_by() and summarize(), you could write this function as:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(ave_price = mean(mean_var))\n}\n\nLet’s now use this function to calculate the mean price of diamonds for every level of cut. Here, the group_var is cut and the variable mean_var is price. Using the grouped_mean() fucntion:\n\ndiamonds |&gt; grouped_mean(cut, price)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nHere, R returns an error: it can’t find the column group_var. The problem is that R looks for the variable group_var and that it can not find as it is not included in the diamonds dataset. In other words, you need a way to tell R that it had to look in that data frame to find the variables cut and price. To do so, you need to embrace the variables in the body of the function between two curly braces: { var } (note the space around var). Doing so, R will not use the variable name, but the variable stored in the variable name. Rewriting the grouped_mean() function where the group_var and mean_var are embraced in the body of the function:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(ave_price = mean({{ mean_var }}))\n}\n\nWe can now use this function with the diamonds dataset:\n\ndiamonds |&gt; grouped_mean(cut, price)\n\n# A tibble: 5 × 2\n  cut       ave_price\n  &lt;ord&gt;         &lt;dbl&gt;\n1 Fair          4359.\n2 Good          3929.\n3 Very Good     3982.\n4 Premium       4584.\n5 Ideal         3458.\n\n\nAs you can see from the result of this function, the function now shows the mean prices per level of cut.\nIf you write a function to produce a graph using {ggplot2}, you also need embracing. For instance, to write a function that returns a point geometry with a mapping of var1 on the horizontal axis, var2 on the vertical axis and var3 on the color aesthetic:\n\nfun_geompoint &lt;- function(df, var1, var2, var3) {\n  df |&gt; ggplot(aes(x = {{ var1 }}, y = {{ var2 }}, color = {{ var3 }})) +\n    geom_point() +\n    theme_minimal()\n}\n\nPlotting (10%) of the diamonds dataset with carat on the horizontal axis, price on the vertical axis can cut mapped on the color aesthetic:\n\ndf_sample &lt;- slice_sample(diamonds, prop = 0.10)\n\nfun_geompoint(df_sample, carat, price, cut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Cetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (2nd Edition). Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#functions-data-frames-and-the-tidyverse",
    "href": "14_Functions.html#functions-data-frames-and-the-tidyverse",
    "title": "14  Functions",
    "section": "14.5 Functions, data frames and the tidyverse",
    "text": "14.5 Functions, data frames and the tidyverse\nUsing functions in conjunction with {dplyr}’s summarize() or mutate() of ggplot()allows you write multiple lines of code in one function. This is especially useful if you have to repeat the same steps multiple times, e.g. calculate the same summary statistics or have to use mutate to add variables to a data frame. Recall that these function allow you to refer to variables without an explicit reference to the data frame. For instance, you can write summarize(df, mean = mean(varname)) and not summarize(mean(df$varname)) or mutate(df, squared_var = var^2) and not df$squared_var &lt;- df$var^2. In addition, with function such as across() or filter() you can use  syntax (starts_with(), where(), …). Here too, you write the name of the variable without explicit reference to the data frame. If you write a function, where the arguments include a variable in a data frame, {dplyr}’s functions need to\nTo illustrate, we’ll use an example taken from Wickham, Cetinkaya-Rundel, and Grolemund (2023). Suppose you want to write a function to calculate the means of a variable mean_var across groups defined by group_var. Using {dplyr}’s group_by() and summarize(), you could write this function as:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(ave_price = mean(mean_var))\n}\n\nLet’s now use this function to calculate the mean price of diamonds for every level of cut. Here, the group_var is cut and the variable mean_var is price. Using the grouped_mean() function:\n\ndiamonds |&gt; grouped_mean(cut, price)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nHere, R returns an error: it can’t find the column group_var. The problem is that R looks for the variable group_var and that it can not find as it is not included in the diamonds dataset. In other words, you need a way to tell R that it had to look in that data frame to find the variables cut and price. To do so, you need to embrace the variables in the body of the function between two curly braces: { var } (note the space around var). Doing so, R will not use the variable name, but the variable stored in the variable name. Rewriting the grouped_mean() function where the group_var and mean_var are embraced in the body of the function:\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(ave_price = mean({{ mean_var }}))\n}\n\nWe can now use this function with the diamonds dataset:\n\ndiamonds |&gt; grouped_mean(cut, price)\n\n# A tibble: 5 × 2\n  cut       ave_price\n  &lt;ord&gt;         &lt;dbl&gt;\n1 Fair          4359.\n2 Good          3929.\n3 Very Good     3982.\n4 Premium       4584.\n5 Ideal         3458.\n\n\nAs you can see from the result of this function, the function now shows the mean prices per level of cut. As this function returns a data frame, you can use it as part of a flow e.g. as input in a function that create a plot.\nIf you write a function to produce a graph using {ggplot2}, you also need embracing. For instance, to write a function that returns a point geometry with a mapping of var1 on the horizontal axis, var2 on the vertical axis and var3 on the color aesthetic:\n\nfun_geompoint &lt;- function(df, var1, var2, var3) {\n  df |&gt; ggplot(aes(x = {{ var1 }}, y = {{ var2 }}, color = {{ var3 }})) +\n    geom_point() +\n    theme_minimal()\n}\n\nPlotting (10%) of the diamonds dataset with carat on the horizontal axis, price on the vertical axis can cut mapped on the color aesthetic:\n\ndf_sample &lt;- slice_sample(diamonds, prop = 0.10)\n\nfun_geompoint(df_sample, carat, price, cut)\n\n\n\n\n\n\n\n\nThe englue() function of the R package {rlang} works similar to glue::glue()(see Chapter 4) but allows for the use of the double curly braces. Using this function you can add labels to you plot, e.g.:\n\nfun_geompoint &lt;- function(df, var1, var2, var3) {\n  \n  title &lt;- rlang::englue(\"The relation between {{ var1 }} and {{ var2 }} by {{ var3 }}\")\n  xaxis &lt;- rlang::englue(\"{{ var1 }}\")\n  yaxis &lt;- rlang::englue(\"{{ var2 }}\")\n  color &lt;- rlang::englue(\"{{ var3 }}\")\n\n  \n  df |&gt; ggplot(aes(x = {{ var1 }}, y = {{ var2 }}, color = {{ var3 }})) +\n    geom_point() +\n      labs(\n      title = title,\n      x = stringr::str_to_title(xaxis),\n      y = stringr::str_to_title(yaxis),\n      color = stringr::str_to_title(color)) +\n    theme_minimal()\n}\n\n\nfun_geompoint(df_sample, var1 = carat, var2 = price, var3 = cut)\n\n\n\n\n\n\n\n\nNote that you can add other variables that to include in e.g. the breaks or labels for the axis, … . To do so, you include e.g. the number of breaks as a separate formal argument in the function. This also holds for the theme. You can set the value of a theme using the function directive\n\ntheme_yourtheme &lt;- function(){\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    axis.line = element.line(color = \"grey\")\n  ... )\n}\n\nIn the arguments of the function you can set, e.g. the font family or font size, a base color, … . After creating your theme function, you can use that function to add lay-out options to your ggplot.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "14_Functions.html#functions-in-your-code",
    "href": "14_Functions.html#functions-in-your-code",
    "title": "14  Functions",
    "section": "14.6 Functions in your code",
    "text": "14.6 Functions in your code\nFrom these examples, you can see that functions are often a very good way to deal with repetitive tasks: using a function you can load libraries,\n\nfun_load &lt;- function() {\n  library(tidyverse)\n  library(gt)\n  library(gtExtrax)\n  library(readxl)\n}\n\nimport data, clean and tidy data using, produce various summary statistics and present them in tables or plots using minimum code. If you save your function in an R file, you can re-use this function other code using the source() function. If you save your script with your function in e.g. your scripts folder, you can access these function using source(here::here(\"scripts\", \"functions.R\")). If you add this line in your code, all function in that script are available to use.\n\n\n\n\n\n\nWickham, Hadley, Cetinkaya-Rundel Mine, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data (2nd Edition). Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "15_The_apply_familily.html",
    "href": "15_The_apply_familily.html",
    "title": "15  The apply familiy",
    "section": "",
    "text": "see Chapter 13",
    "crumbs": [
      "Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The `apply` familiy</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#data-types",
    "href": "02_Examples.html#data-types",
    "title": "2  A tour of R",
    "section": "",
    "text": "2.1.1 Numeric values: real numbers (double) and integers\nReal numbers and integers are two numeric values. Real numbers are also called double values of doubles. They can represent fractions, both positive and negative (1.25, -52.25) as well as whole numbers (10, 25, 456). Let’s create two variables, a and b and assign the value of 12.25 to the first and -4.15 to the second:\n\na &lt;- 12.25\nb &lt;- -4.15\n\nFrom Chapter 1, you know that R uses a dot . as its decimal separator and a - for negative numbers. In the environment pane, you can see these variables as well as the values that were assigned to them. You can check the type of a variable using R’s typeof() function. For instance, to verify the type of a, you would use\n\ntypeof(a)\n\n[1] \"double\"\n\n\nRecall that R is case sensitive. In other words, if you would ask R to show the value of A, R will show an error: Error: object 'A' not found.\nThe values 12.25 and -4.15 are now assigned to the variables (or more general objects) a and b. You can now use these variables in your workflow, e.g. to do math\n\na + b\n\n[1] 8.1\n\na - b\n\n[1] 16.4\n\na * b\n\n[1] -50.8375\n\na / b\n\n[1] -2.951807\n\n\nor use them in one of the many base R of package functions, e.g. such as the natural logarithm (log), square root (sqrt) or calculate the absolute value (abs):\n\nlog(a)\n\n[1] 2.505526\n\nsqrt(a)\n\n[1] 3.5\n\nabs(b)\n\n[1] 4.15\n\n\nIn these examples, we didn’t assign the outcome of these calculations to another variable. If you need to use the result of the functions in subsequent calculations, you could assign the value of the calculation to a new variable. As an example, let’s assign the value of the sum of a and b to a variable `a_plus_b:\n\na_plus_b &lt;- a + b\n\nRecall that a_plus_b does not remember where it came from. In other words, the value of a_plus_b will not change if you would change the value of a or b. To illustrate, let’s change the value of b from -4.15 to -40.15 and ask R to print the value of a_plus_b. To do so, we’ll use the print function and print “a_plus_b equals” with the value of a_plus_b:\n\nb &lt;- -40.15\nprint(paste(\"a_plus_b equals \", a_plus_b))\n\n[1] \"a_plus_b equals  8.1\"\n\n\nIntegers or integer values can only represent whole numbers, e.g. 1, 25 or 50. If you assign a the value of 10 to the variable c and want to be explicit that c is an integer, you can do so by adding L after the integer value:\n\nc &lt;- 10L\n\nUsing typeof() you can verify that c is an integer:\n\ntypeof(c)\n\n[1] \"integer\"\n\n\nThe environment pane shows the L to indicate that the variable is an integer. If you don’t add the L, R will treat the number as a double. For instance, if you ask the type of the value 10 (without the L)\n\ntypeof(10)\n\n[1] \"double\"\n\n\nR shows double. The same holds if you mix integer and double values in mathematical operations: the result will be double unless you explicitly indicate the values are integers and the outcome is also an integer. For instance, is you add 4 to c the type will be double\n\ntypeof(c + 4)\n\n[1] \"double\"\n\n\nUsing c + 4L will create an integer:\n\ntypeof(c + 4L)\n\n[1] \"integer\"\n\n\nThe same holds for multiplication\n\ntypeof(4L * c)\n\n[1] \"integer\"\n\n\nbut not for division, even if the outcome would be a whole number:\n\ntypeof(10L/2L)\n\n[1] \"double\"\n\n\nIf you add an L to a fraction, e.g. 12.25L\n\n12.25L\n\n[1] 12.25\n\n\nR will treat the value as a double but will show a warning message: integer literal 12.25L contains decimal; using numeric value. You can verify that 12.25L is a double using typeof()\n\ntypeof(12.25L)\n\n[1] \"double\"\n\n\n\n\n2.1.2 String or character variables\nA string (of character variable) is a sequence of characters: letters, numbers, whitespaces, symbols, … . A string variable starts and ends with a double quotation mark \" \". Anything within these quotation marks is treated as a string of character variable. As an example, let’s create two character variables, char1 and char2 and assign the values “Bachelor of buisiness administration” to the first and “KU Leuven” to the second:\n\nchar1 &lt;- \"Bachelor of business administration\"\nchar2 &lt;- \"KU Leuven\"\n\nUsing typeof() you can verify that char1 is a character variable:\n\ntypeof(char1)\n\n[1] \"character\"\n\n\nchar1 is one character variable, even if it includes multiple letters and 3 whitespaces. To count the number of characters in char1, you can use the nchar function:\n\nnchar(char1)\n\n[1] 35\n\n\nYou can verify that char1 includes 35 characters: 32 letters and 3 whitespaces. As you can see, R counts a whitespace as a character. To show this, let’s ask the type of a whitespace enclosed in quotation marks:\n\ntypeof(\" \")\n\n[1] \"character\"\n\n\nR treats anything in quotation marks as a string. This holds for whitespaces but also for numeric values. For instance\n\nchar3 &lt;- \"12.25\"\n\nassigns the character “12.25” to the variable char3 and the type of char3 is a character variable:\n\ntypeof(char3)\n\n[1] \"character\"\n\n\nUsing char3 with mathematical operations, e.g. char3 * 2 causes an error Error in char3 * 2 : non-numeric argument to binary operator\n\nchar3 * 2\n\nError in `char3 * 2`:\n! non-numeric argument to binary operator\n\n\nYou can (try to) change a “number as a character” in a “number”. To do so, you need a function: as.double(). This function will (try to) change whatever is within the round braces into a double:\n\nas.double(char3)\n\n[1] 12.25\n\n\nFrom the output - 12.25 doesn’t have quotation marks - you can see that R changed the string “12.25” in a double 12.25.\nR includes functions that allow you to work with strings. The {stringr} or {glue} packages extend these base R functions to analyse and work with character variables and allow you to analyse texts (e.g. word counts, emotions, …). To illustrate some of these base R functions, we’ll use char1 and char2 and\n\nadd both into one string:\n\n\npaste(char1, char2)\n\n[1] \"Bachelor of business administration KU Leuven\"\n\n\n\nsplit the string char1 in multiple strings using the whitespace to delineate the substrings:\n\n\nstrsplit(x = char1, split = \" \")\n\n[[1]]\n[1] \"Bachelor\"       \"of\"             \"business\"       \"administration\"\n\n\n\nchange the case of char2 to lowercase:\n\n\ntolower(char2)\n\n[1] \"ku leuven\"\n\n\n\nor to uppercase\n\n\ntoupper(char2)\n\n[1] \"KU LEUVEN\"\n\n\n\nreplace a pattern in a string (e.g. change “Bachelor” into “Master”)\n\n\nsub(pattern = \"Bachelor\", replacement = \"Master\", x = char1)\n\n[1] \"Master of business administration\"\n\n\n\n\n2.1.3 Boolean or logical values\nBoolean or logical variables take one of two values: TRUE or FALSE:\n\nbool1 &lt;- TRUE\nbool2 &lt;- FALSE\n\nAs with the other data types, typeof() will show if a variable is boolean or not:\n\ntypeof(bool1)\n\n[1] \"logical\"\n\n\nYou can use T as shorthand for TRUE and F as shorthand for FALSE.\nR stores these values as 1 (TRUE) or 0 (FALSE). This allows you to use them in calculations, e.g.\n\n2 * bool1 + bool2\n\n[1] 2\n\nbool2 * a\n\n[1] 0\n\nbool1 + b\n\n[1] -39.15\n\nTRUE + TRUE + FALSE + TRUE\n\n[1] 3\n\n\nIf you evaluate a statement such as “a equals b”, “a is larger than or equal to b”, “a is not equal to b”, “a is smaller than b”, this statement is either false or true. In R, these statements are written as a == b (equal to), a &gt;= b (larger than or equal to), a != b (not equal to) and a &lt; b (smaller than). Using these statements, you create boolean values (recall that a was assigned the value of 12.25 and b the value of -4.15):\n\na == b\n\n[1] FALSE\n\na &lt;= b\n\n[1] FALSE\n\na != b\n\n[1] TRUE\n\na &lt; b\n\n[1] FALSE\n\n\nYou can also assign the outcome of the evaluation to a new variable:\n\nbool3 &lt;- a == b\n\nIn the environment pane, you can verify that bool3 was assigned the value of FALSE.\nA lot of function create a boolean value. For instance, to check if a value is double, we used the typeof() function. Base R’s is.double() function equals TRUE if a variable is a double and FALSE otherwise. The same holds for, e.g. is.numeric(), is.charachter(), is.logical():\n\nis.double(a)\n\n[1] TRUE\n\nis.integer(c)\n\n[1] TRUE\n\nis.character(char1)\n\n[1] TRUE\n\nis.logical(bool2)\n\n[1] TRUE\n\n\n\n\n2.1.4 Date and time\nData often includes a date or time stamp: a year, month, day, an hour or minute. Dates are usually written in the YYYY-MM-DD format: first the year with 4 digits, the month with 2 digits and the day with 2 digits. For instance, August 31st 2026 is written as 2026-08-31 and September 14th 2035 as 2035-09-14. Let’s create a date. To do so, we’ll use the Sys.Date() function. This functions shows the date today. Note that this date will change each time you run that code on a different day.\n\ndate1 &lt;- Sys.Date()\ndate1\n\n[1] \"2026-02-09\"\n\n\nAs you can see, R shows the date in the format YYYY-MM-DD. In the environment pane, R adds the time zone: UTC. We’ll cover time zones more in depth in Chapter 3.\nIf you ask for the type of date1, R shows that it is a double:\n\ntypeof(date1)\n\n[1] \"double\"\n\n\nAs we’ll see in Chapter 3, R stores a date as the number of days since January 1st, 1970. To show that number, we’ll use the as.double() function to change the way in which R shows the date:\n\nas.double(date1)\n\n[1] 20493\n\n\nBecause R stores a date as a double, you can add of subtract days:\n\ndate1 + 300\n\n[1] \"2026-12-06\"\n\ndate1 - 750\n\n[1] \"2024-01-21\"\n\n\nBase R and the {lubridate} package include a wide range of functions to work with dates. To illustrate we’ll use base R’s quarters(), months() and weekdays() functions to extract information from a date: the quarter of the year, the month of the year and the day of the week. R will show the month and day of the week in the system language of your computer:\n\nquarters(date1)\n\n[1] \"Q1\"\n\nmonths(date1)\n\n[1] \"februari\"\n\nweekdays(date1)\n\n[1] \"maandag\"\n\n\nTimes are very similar to dates. Here, we’ll use Sys.time() to create a time variable. This functions shows the current time:\n\ntime1 &lt;- Sys.time()\ntime1\n\n[1] \"2026-02-09 20:46:35 CET\"\n\n\nThe time notation shows the date in YYYY-MM-DD a white space and adds the hour, minutes and seconds separated by a colon (:) as well as the timezone (Central European Time). As with dates, R stores a time as a numeric value: the number of seconds since January 1st 1970:\n\nas.double(time1)\n\n[1] 1770666395\n\n\nAs a result, you can do math with times:\n\ntime1 + 60 * 60 * 24 * 31\n\n[1] \"2026-03-12 20:46:35 CET\"\n\ntime1 - 60 * 60 * 24 * 365\n\n[1] \"2025-02-09 20:46:35 CET\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#data-structures",
    "href": "02_Examples.html#data-structures",
    "title": "2  A tour of R",
    "section": "2.2 Data structures",
    "text": "2.2 Data structures\nVectors, matrices and arrays, lists or data frames are data structures: they are used to hold at least one (and potentially a very large number of) values. Usually, these values are related: they hold the grades for your exams, the names of your pets, a dataset with sales data for various products covering multiple markets and years, interest rates across countries or results of functions. These data structures differ in two ways: the homogeneity of the values and the number of dimensions. A vector is a one dimensional data structure (one row with at least one column) where all values share the same type (homogeneous); matrices can have more than one dimension (a matrix can have two: rows and columns and an array three: rows, columns and matrices) but only allow one type of value. In addition, each row or each column of a matrix must have the same number of values. Lists are heterogeneous (you can use them to store values with various types), multidimensional and each component of a list can have a different number of values. Data frames are a special type of lists: they can hold different types of data, have two dimensions (columns and rows) but each column or row must hold the same number of values. In Chapter 4 will will cover these data structures more in depth. In this quick tour of R, we’ll introduce them but leave a lot of detail for later.\n\n2.2.1 Vectors\nA vector is a one dimensional homogeneous data structure. A vector contains at least one element. If it contains more than one element, all elements must have the same type.\nThe first way to create a vector is to include all its elements within the c() function. Doing so combines these elements into one data structure. In the next example, we will create a vector with numeric values and assign it to the object grades:\n\ngrades &lt;- c(15, 11, 14, 18, 10, 13)\n\nIn the environment pane, you can now see the object grades. The information also includes the type of the vector (num: numeric) and shows that it has six elements. The environment pane also shows the elements of the vector. If the vector is too long, the environment pane shows the first elements. Here we have 6 elements.\nA vector is homogeneous and all elements have the same type. What happens if we mix e.g. numeric values and a character? Suppose that we have the grades2 vector where all elements are numeric, except one, which is a character variable \"F\":\n\ngrades2 &lt;- c(15, 11, \"F\", 18, 10, 13)\n\nIn the environment pane, you can see that this vector is a character vector (chr) with 6 elements. All the elements are shown within double quotation marks. You an also verify this using typeof():\n\ntypeof(grades2)\n\n[1] \"character\"\n\n\nWhy did R return a character vector? R implicitly coerces the type of a vector to one that matches all elements. In this example: R couldn’t change the \"F\" into a numeric variable while is could change the numeric values in characters. As a result R changes all values in characters.\nLet’s take a different example. Here, we include the F without quotation marks. Recall that F is shorthand for FALSE. The vector grades3 is\n\ngrades3 &lt;- c(15, 11, F, 18, 10, 13)\n\nThe environment pane shows a numeric vector. Here, R could change the (boolean) F into a numeric value (0). As a result, R implicitly coerced grades3 into a numeric vector. In the environment pane, you can see that the third element of grades3 is set to 0.\nIn the previous examples, we used a numeric vector. A vector doesn’t have to be numeric. I can contain all data types. If a vector contains a mixture of data types, R will coerce the vector to a common type. As an example, let’s create a character vector pets with elements \"dog\", \"cat\", \"bird\", \"fish\":\n\npets &lt;- c(\"dog\", \"cat\", \"bird\", \"fish\")\n\nThe environment pane shows that this vector has 4 elements and that the elements are character values.\nThe c() function doesn’t require that you list all elements of a vector. You can also use this function to e.g. create a vector filled with elements from two or more other vectors. In the next example, we’ll create a vector vec3 using the elements of two other vectors: vec1 and vec2. The c() function combines the both vectors in the order in which they appear in the function:\n\nvec1 &lt;- c(1, 2, 3)\nvec2 &lt;- c(4, 5, 6)\nvec3 &lt;- c(vec1, vec2)\nvec3\n\n[1] 1 2 3 4 5 6\n\n\nAccessing individual elements or parts of a vector is called subsetting. In R, the subsetting operator is a pair of square brackets [ ]. R stores the first elements of a vector in position 1, the second element in position 2, … . Other programming languages (e.g. Python) use position 0 to store the first element, position 1 to store the second, … . In R, you refer to the elements you want to retrieve by their position within []. For instance, to access the first element of the grades vector:\n\ngrades[1]\n\n[1] 15\n\n\nAs you can see, R returns 15, which is the first element of grades. To access the third element of the pets vector,\n\npets[3]\n\n[1] \"bird\"\n\n\nR now returns \"bird\".\nIf you use a position that does not exist (e.g. position 7 in grades). R will now show an error but NA or not available.\n\ngrades[7]\n\n[1] NA\n\n\nTo retrieve more than one element, you can refer to the first and last position of the elements that you want to retrieve with a colon between these two positions. For instance, to retrieve the first, second and third element (but not the fourth) of pets you can write\n\npets[1:3]\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\nHere, we subsetted multiple adjacent columns. Using the c() with the column numbers as elements, you can subset non-adjacent columns. For instance, to retrieve the first and third column of grades, you add c(1, 3) in the subsetting operator:\n\ngrades[c(1, 3)]\n\n[1] 15 14\n\n\nR returns a vector with 2 values: 15 and 14: the first and third element of grades.\nTo change the value of an element, you can assign that new value to the element where you use the subsetting operator to identify the element. For instance, to change the last value of grades from 13 to 18, you first identify the element (grades[5]) and re-assign its value:\n\ngrades[5] &lt;- 18\n\nYou can create an empty vector of a given type (or mode) and a given number of elements using the vector(mode = , length = ) function. Here, the mode refers to e.g. “numeric”, “character”, “logical”, “integer”, “double”, … . The length shows the number of elements. This is an integer value. An empty vector is filled with 0 for a numeric vector or whitespaces for a character vector. For instance, to create an empty numeric vector vecnum5 with 5 values,\n\nvecnum5 &lt;- vector(mode = \"numeric\", length = 5L)\n\nFrom the environment pane, you can verify that all the elements of vecnum5 are 0. For a character vector with 6 values vecchar6\n\nvecchar6 &lt;- vector(mode = \"character\", length = 6L)\n\nshows a vector with 6 whitespaces in the environment pane.\nYou can now fill these vectors by assigning values to their positions. For instance, to change first and second value of vecnum5 in 10 and 20:\n\nvecnum5[1] &lt;- 10\nvecnum5[2] &lt;- 20\n\nThe vector() function can be useful is you already know that you will fill an e.g. numeric vector with x values as part of your workflow. For large vectors, defining both the type and the length before you set its elements can save quite some computing time relative to the case where you start with vector with one element that allow to grow.\nR is vectorized. This means that calculations work on an element by element basis. For instance, if you mutiply grades with 2, subtract 5, raise grades to the power 2 or calculate the natural logarithm of grades, R will multiply each element with 2, subtract 5 from each element, raise each element to the power 2 or calculate the natural logarithm of each element:\n\ngrades * 2\n\n[1] 30 22 28 36 36 26\n\ngrades - 5\n\n[1] 10  6  9 13 13  8\n\ngrades^2\n\n[1] 225 121 196 324 324 169\n\nlog(grades)\n\n[1] 2.708050 2.397895 2.639057 2.890372 2.890372 2.564949\n\n\nThe same holds for calculations with two vectors. If you calculate the sum or product of vec1 and vec2 or divide vec1 by vec2, R will perform these calculates element wise: R adds the first element, multiplies the first element or divides the first element of vec1 to or by the first element of vec2, adds the second element, multiplies the second element or divides the second element of vec1to or by the second element of vec2, … :\n\nvec1 + vec2\n\n[1] 5 7 9\n\nvec1 * vec2\n\n[1]  4 10 18\n\nvec1 / vec2\n\n[1] 0.25 0.40 0.50\n\n\nR will also perform these calculations if the number of elements in both vectors differ. As long as the number of elements in one vector is a multiple of the number of elements in the other, R will reuse the shortest vector if necessary. For instance, recall that vec3 had 6 elements and vec2 3. As 6 is a multiple of 3 in the following calculation, R will use vec2 two times:\n\nvec3 * vec2\n\n[1]  4 10 18 16 25 36\n\n\nIn other words, R will perform this calculation using vec3 and c(vec2, vec2). Note that R doesn’t issue a warning in this case. It only does if the number of elements in the longer vector is not a multiple of the number of elements in the shorter vector. To show this, let’s create a vector with 7 elements by adding one element to vec3. Using that vector, we can now try to calculate the product of this new vector with vec2:\n\nvec4 &lt;- c(vec3, 25)\nvec4 * vec2\n\nWarning in vec4 * vec2: longer object length is not a multiple of shorter\nobject length\n\n\n[1]   4  10  18  16  25  36 100\n\n\nAs you can see, R now shows the error longer object length is not a multiple of shorter object length.\nBase R comes with a lot of functions that use a vector as their input. We’ll explore those more in dept in Chapter 4, but here are three examples:\n\ndetermine the number of elements in a vector:\n\n\nlength(pets)\n\n[1] 4\n\n\n\ncalculate the mean or sum of the elements in a vector:\n\n\nmean(grades)\n\n[1] 14.83333\n\nsum(grades)\n\n[1] 89\n\n\nUp until now, we created a vector using the c() function. Some functions return a vector as their output. The seq(from =, to = , by = ) function for instance returns a sequence that starts from from, ends at to and adds by in each step. As an example, the next line of code creates a sequence that starts at 1, ends at 10 and moves in steps of 2 from start to end:\n\nvec_seq &lt;- seq(from = 1, to = 10, by = 2)\n\nNote that this sequence ends at 9. If R would add another step, the sequence would end at 11, i.e. a value higher than the value in to. In other words, the sequence will always stop either at the value in to (here 10) or before that value (here 9). In case you need a sequence that moves in steps of 1, you can use the shorthand notation: start:end. Using this notation, you can generate a sequence from 1 to 10 in steps of 1 using\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nor from 11 to 20 in steps of 1 using\n\n11:20\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nIf you need a specified number of values in your sequence, seq(from =, to = , length.out = ) generates a sequence with length.out values starting from from, ending at to and in equal steps determined that R calculates from the value in from, to and length.out. For instance, a sequence with 5 values starting from 1 and ending at 37:\n\nseq(from = 1, to = 30, length.out = 5)\n\n[1]  1.00  8.25 15.50 22.75 30.00\n\n\nOther examples of functions that return a vector are, for instance:\n\nrnorm(n = , mean = , sd = ): to generate a sequence of nnormally distributed variables with mean mean and standard deviation sd. As an example, this code generates 10 normally distributed values with mean 5 and standard deviation 10\n\n\nnorm_dist &lt;- rnorm(n = 10, mean = 5, sd = 10)\n\n\nsample(x = , size =, replace = ): to generate a random sample of size size from the vector x with or without replacement. To generate a random sample of 3 values from the pets vector without replacement, you use the code:\n\n\nsample_pet &lt;- sample(x = pets, size = 3, replace = FALSE)\n\nR also comes with build-in vectors. Two examples are letters and LETTERS. The first includes all letters of the alphabet in lowercase, while the second includes letters in uppercase. The first 5 elements of these vectors are the first 5 letters of the alphabet\n\nletters[1:5]\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\nLETTERS[1:5]\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nUsing the following vectors\n\nyour_turn1 &lt;- c(1, 2, 3, T, 5, 6)\nyour_turn2 &lt;- c(as.Date(2028-01-12), 123456, 25)\nyour_turn3 &lt;- c(\"25\", \"36\", F, T)\n\nWhat is the type of each of these three vectors? Before your check the code below, try to determine their type from the elements of each vector.\n\n\nCode\ntypeof(your_turn1)\n\n\n[1] \"double\"\n\n\nCode\ntypeof(your_turn2)\n\n\n[1] \"double\"\n\n\nCode\ntypeof(your_turn3)\n\n\n[1] \"character\"\n\n\nGenerate a sequence, starting at 5, ending at 15 in steps of 3 and assign the output to my_seq\n\n\nCode\nmy_seq &lt;- seq(from = 5, to = 15, by = 3)\n\n\nWould that function work in steps of 0.25?\n\n\nCode\nmy_seq &lt;- seq(from = 5, to = 15, by = 0.25)\n\n\nWhat happens in case your value in from is higher than your value in to, e.g. from = 10, to = 1, by = 3?\n\n\nCode\nmy_seq &lt;- seq(from = 10, to = 1, by = 3)\n\n\nError in `seq.default()`:\n! wrong sign in 'by' argument\n\n\nWhat happens if you adjust the by to by = -3?\n\n\nCode\nmy_seq &lt;- seq(from = 10, to = 1, by = -3)\n\n\nWhat is the 16th letter of the alphabet (in lowercase)?\n\n\nCode\nletters[16]\n\n\n[1] \"p\"\n\n\nRetreive the 10th to 20th (inclusive) letter of the alphabet in uppercase and assign the output to a vector my_letters\n\n\nCode\nmy_letters &lt;- LETTERS[10:20]\n\n\nSample, without replacement, 5 values from my_letters and assign to sam_let1\n\n\nCode\nsam_let1 &lt;- sample(x = my_letters, size = 5, replace = FALSE)\n\n\nRepeat this sampling procedure a second time and assign the outcome to sam_let2:\n\n\nCode\nsam_let2 &lt;- sample(x = my_letters, size = 5, replace = FALSE)\n\n\nCompare each element of sam_let1 with the element in the same position of sam_let2: are there positions with the same letter?\n\n\nCode\nsam_let1 == sam_let2\n\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nHow many positions share the same letter? Hint: use the fact that you can sum across all elements in a vector and that TRUE equals 1 and FALSE equals 0.\n\n\nCode\nsum(sam_let1 == sam_let2)\n\n\n[1] 1\n\n\nIf you repeat both sampling procedures, do you expect to have an equal number of positions with the same letter?\nGenerate a sequence of 25 random normal numbers with mean 10 and standard deviation 5 and assign the result to my_rnorm\n\n\nCode\nmy_rnorm &lt;- rnorm(n = 25, mean = 10, sd = 5)\n\n\nCalculate the mean of my_rorm. Do you expect that this mean would equal exact 10?\n\n\nCode\nmean(my_rnorm)\n\n\n[1] 11.6986\n\n\nCreate an empty numeric vector my_num_vec with 10 values:\n\n\nCode\nmy_num_vec &lt;- vector(mode = \"numeric\", length = 10L)\n\n\n\nchange the value of the first element to 10:\n\n\n\nCode\nmy_num_vec[1] &lt;- 10\n\n\n\nchange the value of the second and fifth element to 20 and 50. Do so in one line of code.\n\n\n\nCode\nmy_num_vec[c(2, 5)] &lt;- c(20, 50)\n\n\nSuppse that you don’t know how many elements there are in my_num_vec. Fill the last position with 100 (hint: you can calculate the total number of elements in a vector using length()):\n\n\nCode\nmax_value &lt;- length(my_num_vec)\n\nmy_num_vec[max_value] &lt;- 100\n\n\nNow assign the character value of “60” to the sixth element of my_vec_num. What do you expect will happen?\n\n\nCode\nmy_num_vec[6] &lt;- \"60\"\n\n# Implicit coercion: R changes the type of the vector from numeric into character\n# the environment pane shows the values between \" \". Suppose that you assigned\n# the sixth position using \"60\" by accident. How would you change the type of the\n# vector back to numeric? \n\n# my_num_vec &lt;- as.double(my_num_vec)\n\n\n\n\n\n\n\n2.2.2 Matrices\nMatrices are two-dimensional: they include rows and columns. To create a matrix, R includes the function matrix(data =, rnow = , ncol = ). Using this function, the data in data fills a matrix with nrow rows and ncol columns. For instance, to fill a matrix with the numbers 1 to 6 across 2 rows and 3 colums,\n\nmat1 &lt;- matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\n\nNote two things: first, by default matrices are filled by column (or are filled down: first all rows of the first column, second all rows in the second column, …). Second, in the environment pane, a matrix is not shown among the values, but as part of the Data. Note that this was not the case for a vector. Although you might have referred to a vector as a matrix with one row and multiple columns in your mathematics lectures, this is not how R stores both. If you want to create a vector as a matrix, you need to use the matrix function with one row and many columns.\nIn the console, you can see the matrix by typing its name or running the code\n\nmat1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nin the editor. R adds row and column index numbers to the left and at the top of the matrix. You can determine the number of elements of a matrix using length(), the number of rows using nrow() and the number of columns using ncol():\n\nlength(mat1)\n\n[1] 6\n\nnrow(mat1)\n\n[1] 2\n\nncol(mat1)\n\n[1] 3\n\n\nMatrices are homogeneous and can store any type of data, as long as all elements have a common type. For instance, a matrix with the first 12 lower case letters of the alphabet:\n\nmat2 &lt;- matrix(data = letters[1:12], nrow = 6, ncol = 2)\n\nTo retrieve a part of a matrix, you use the subsetting operator [] and add the row and column numbers, separated by a comma. The row number comes first, the column number second. The element in the second row and third column of mat1 equals:\n\nmat1[2, 3]\n\n[1] 6\n\n\nTop subset multiple rows (or columns) your refer to the first and last column to subset separated by a colon. For instance, to retrieve all elements on the first two rows and second and third column of mat1:\n\nmat1[1:2, 2:3]\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6\n\n\nTo retrieve an entire column or row, you refer to the column (or range of columns) or row (or range of rows) and leave the row or column indicator empty. The second column of mat1and the first row of mat1 are selected using:\n\nmat2[, 2]\n\n[1] \"g\" \"h\" \"i\" \"j\" \"k\" \"l\"\n\nmat2[1, ]\n\n[1] \"a\" \"g\"\n\n\nTo select the first two columns of mat1 or the second to fourth row of mat2, you would use\n\nmat1[, 1:2]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nmat2[2:4, ]\n\n     [,1] [,2]\n[1,] \"b\"  \"h\" \n[2,] \"c\"  \"i\" \n[3,] \"d\"  \"j\" \n\n\nFor non-adjacent columns (or rows), you can include the column (row) positions in a vector. For instance, to subset the first and third column of mat1:\n\nmat1[, c(1, 3)]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nCreate a matrix mat10 with using a sequence starting at 10, ending at 200 in steps of 10. The matrix should have 5 rows. Determine the number of columns you need. Can you predict the elements in the first column of this matrix? Copy the following code in your editor or console to see the outcome.\n\n\nCode\nmat10 &lt;- matrix(seq(from = 10, to = 200, by = 10), nrow = 5, ncol = 4)\n\n# You have 5 rows and 20 elements. This matrix requires 4 columns. \n\n\nWhat happens if you don’t add the number of rows nrow = 5 but only the number of columns? Copy the following code in your editor or console to see the outcome.\n\n\nCode\nmatrix(seq(from = 10, to = 200, by = 10), ncol = 4)\n\n# R adds the number of rows using the number of elements in the matrix and the\n# number of columns\n\n\nTry to predict what happens in case you create this matrix with 5 rows and 8 columns. Copy the following code in your editor or console to see the outcome.\n\n\nCode\nmatrix(seq(from = 10, to = 200, by = 10), nrow = 5, ncol = 8)\n\n# R uses the sequence two times to fill the matrix.\n# columns 5, 6, 7 and 8 are equal to columns 1, 2, 3 and 4\n\n\nTry to predict what happens in case you create this matrix with 5 rows and 6 columns. Copy the following code in your editor or console to see the outcome.\n\n\nCode\nmatrix(seq(from = 10, to = 200, by = 10), nrow = 5, ncol = 6)\n\n# 5 x 6 = 30. This is not a multiple of 20. R returns an error. \n\n\nUsing mat10\n\nretreive the columns 2 to 4 from the matrix:\n\n\n\nCode\nmat10[, 2:4]\n\n\n     [,1] [,2] [,3]\n[1,]   60  110  160\n[2,]   70  120  170\n[3,]   80  130  180\n[4,]   90  140  190\n[5,]  100  150  200\n\n\n\nsubset the element on the third row and fourth column\n\n\n\nCode\nmat10[3, 4]\n\n\n[1] 180\n\n\n\nsubset the first, third and fifth row\n\n\n\nCode\nmat10[c(1, 3, 5), ]\n\n\n     [,1] [,2] [,3] [,4]\n[1,]   10   60  110  160\n[2,]   30   80  130  180\n[3,]   50  100  150  200\n\n\n\ncalculate the mean of the second column (hint: a column is a vector)\n\n\n\nCode\nmean(mat10[, 2])\n\n\n[1] 80\n\n\n\ncalculate the sum of all elements on the third row\n\n\n\nCode\nsum(mat10[3, ])\n\n\n[1] 420\n\n\n\ncalculate the mean and sum of all element in mat10:\n\n\n\nCode\nmean(mat10)\n\n\n[1] 105\n\n\nCode\nsum(mat10)\n\n\n[1] 2100\n\n\n\n\n\n\n\n2.2.3 Lists\nLists allow to store heterogeneous data and data structures. For instance, you can use a list to store a vector, matrix and a character variable in one data structure. We will not use lists often to store datasets. However, as they are very flexible in terms of the data types and structures that they can store, list are often used to return the output of functions in R.\nIn previous sections, we created a character variable (char1), a vector with pets (pets) and grades (grades), a boolean variable (bool1), a date variable (date1) and matrix mat1. To add these into one data structure, you need a list. To create a list, you use the list() function. Although it is not necessary to do so, we’ll add names to each of the elements of the list. To do so, we first add the name of the element, an equality sign and the element. The name of char1 is my_char, of pets is your_pets, … :\n\nlist1 &lt;- list(\n  my_char = char1, \n  your_pets = pets, \n  his_grades = grades, \n  her_boolean = bool1, \n  today = date1, \n  a_matrix = mat1)\n\nThe list list1 is shown in the Data section of the environment pane. Note that there is a small triangle before the namelist1. If you click on that triangle, you can see the structure oflist1\\: the name and type of each component of the list and in case an element of the list is a data structure (e.g. vector or matrix, ) its dimensions as well as the first values.\nTo see the structure of the list, R includes the function str():\n\nstr(list1)\n\nList of 6\n $ my_char    : chr \"Bachelor of business administration\"\n $ your_pets  : chr [1:4] \"dog\" \"cat\" \"bird\" \"fish\"\n $ his_grades : num [1:6] 15 11 14 18 18 13\n $ her_boolean: logi TRUE\n $ today      : Date[1:1], format: \"2026-02-09\"\n $ a_matrix   : num [1:2, 1:3] 1 2 3 4 5 6\n\n\nThe function returns the same information as the one shown in the environment pane. Using length(), you can count the number of components in a list:\n\nlength(list1)\n\n[1] 6\n\n\nThere are three ways to subset a list. The first uses the subsetting operator [ ] (single square brackets) with the position of the component you want to retrieve in the square brackets. For instance, to extract the second element of list1:\n\nlist1[2]\n\n$your_pets\n[1] \"dog\"  \"cat\"  \"bird\" \"fish\"\n\n\nThe output shows the name of the element after the dollar sign with the element below the name. In this case, the name of the second element is your_pets and the content is \"dog\", \"cat\", \"bird\", \"fish\". However, the type of the output is not a vector (recall that pets was a character vector), but a list:\n\ntypeof(list1[2])\n\n[1] \"list\"\n\n\nThis is because the single square brackets subsetting operator [ ] preserves the structure of the data. To extract the pets vector as a vector, you need to use double square brackets [[ ]] (with the position of the element in the square brackets):\n\nlist1[[2]]\n\n[1] \"dog\"  \"cat\"  \"bird\" \"fish\"\n\n\nWe now have a vector\n\ntypeof(list1[[2]])\n\n[1] \"character\"\n\n\nIn other words, the structure list is not preserved if you use the [[ ]] subsettting operator and R returns the data structure of the component.\nAs list1[[2]] is a vector, you can use the subsetting rules for a vector to extract individual elements. For instance,\n\nlist1[[2]][3]\n\n[1] \"bird\"\n\n\nreturns \"bird\". Note how R interprets this line: list1[[2]] returns a vector. This vector is used by R to extract the third element. In other words, this line is equivalent to\n\nvec_with_pets &lt;- list1[[2]]\nvec_with_pets[3]\n\n[1] \"bird\"\n\n\nAs the components of the list are named, there is a second way to extract them: the $ notation. In the environment pane, you saw that the name of each component was preceded by a $ sign. If you put this dollar sign between the name of the list list1 and the name of the component (e.g. your_pets) R extracts that component:\n\nlist1$your_pets\n\n[1] \"dog\"  \"cat\"  \"bird\" \"fish\"\n\n\nThe structure of the data is not preserved: using the $ operator is similar to [[ ]].\nNote that as your typed the $ sign, RStudio listed all components in list1 and you can select the component from RStudio.\nIf the components are named, there is a third way to subset a list: referring to the name within the [ ] or [[ ]] operators:\n\nlist1[\"my_char\"]\n\n$my_char\n[1] \"Bachelor of business administration\"\n\nlist1[[\"a_matrix\"]]\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nFrom the output, you can see that the first preserved the structure of the list while the second returns a matrix.\nTo illustrate why lists are very useful, let’s test if the mean of two normally distributed random variable is equal or not. First, we’ll create these random variables. The first var1 is drawn from a normal distribution with mean 5 and standard deviation 10 while the distribution used for the second, var2, has a mean equal to 10 and a standard deviation equal to 20.\n\nvar1 &lt;- rnorm(n = 100, mean = 5, sd = 10)\nvar2 &lt;- rnorm(n = 100, mean = 10, sd = 20)\n\nYou may recall from your Statistics class that you can use a t-test to check if the mean of two variables is equal or not. In R, you can perform a t-test using the t.test() function. Here, we use a two sided test (the mean is different) and test if the difference between the mean of var1 and var2 is equal to 0. The data are not paired and the variances of var1 and var2 are not equal. The t-test will apply a 95% confidence level. Don’t worry if you don’t recall what a t-test does. The illustration here is used to show how lists are often used. The code for the t-test is:\n\nres_test &lt;- t.test(x = var1, \n                   y = var2, \n                   alternative = \"two.sided\", \n                   mu = 0, \n                   paired = FALSE,\n                   var.equal = FALSE, \n                   conf.level = 0.95)\n\nThe output of the function is assigned to res_test. You can see the structure of this list using str():\n\nstr(res_test)\n\nList of 10\n $ statistic  : Named num -3.44\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 147\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.000745\n $ conf.int   : num [1:2] -11.34 -3.07\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] 3.49 10.7\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 2.09\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"var1 and var2\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nFrom the structure, you can see why this function returns a list: the output includes e.g. the test statistic or the degrees of freedom both as numeric variables the confidence interval and the estimate of the mean of var1 and var2 as a numeric vector or the alternative hypothesis or the method as character variables. Here, the elements of the components are also named. You can now access the various elements of res_test by subsetting the list. Using the [ ] operator preserves the structure of the data and returns a list. To return the test statistic as a list:\n\nres_test[1]\n\n$statistic\n        t \n-3.444641 \n\n\nUsing the [[ ]] or $ operators return the values of vectors:\n\nres_test[[2]]\n\n      df \n146.9853 \n\nres_test$conf.int\n\n[1] -11.340256  -3.071847\nattr(,\"conf.level\")\n[1] 0.95\n\n\nAs a second example, suppose you want to find the location of the words “fair”, “fog” or “filthy” (all in lowercase) in the sentences “Fair if foul and foul is fair.” and “Hover through the fog and filthy air.” from Shakespeare’s play Macbeth. The package {stringr} includes the function str_locate_all. This function returns the first and last position of a word in a sentence. Let’s use this function to find the location of the words. As we’all cover that function more in depth, you can focus on the output:\n\nmacbeth &lt;- c(\"Fair if foul and foul is fair.\",  \"Hover through the fog and filthy air.\")\nstringr::str_locate_all(macbeth, pattern = \"fair|fog|filthy\")\n\n[[1]]\n     start end\n[1,]    26  29\n\n[[2]]\n     start end\n[1,]    19  21\n[2,]    27  32\n\n\nHere too, the output requires a list. In the first sentence there is only one word that occurs (“fair” without uppercase). In the second sentence both the word “fog” and “filthy” occur. In other words, to identify the location of the match in the first sentence, R needs a vector to show the first and last position. To do so for the second sentence, R needs a matrix with one row for each match. Note that you can subset the components from the output. For instance: to extract the first row from the matrix showing the locations in the second sentence:\n\nstringr::str_locate_all(macbeth, pattern = \"fair|fog|filthy\")[[2]][1, ]\n\nstart   end \n   19    21 \n\n\nThere are multiple ways to add components to a list. Here, we’ll show one. To add a (named) component to a list, you add the name after the $ operator in the list’s name and add the component. For instance, to add a sequence starting at 1, ending at 20 in steps of 2 named seq1to list1:\n\nlist1$seq1 &lt;- seq(from = 1, to = 20, by = 2)\n\nIn the environment pane, you can see that list1 has grown in length.\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nCreate a list list1 with 3 elements: - char: a charactervector with letters 14 to 20 in uppercase from the alfabet - num: a numeric vector with 10 draws from a normal distribution with mean = 1 and standard deviation = 2 (rnorm(n = 10, mean = 1, sd = 2) - bool: a boolean value TRUE\n\n\nCode\nlist1 &lt;- list(char = LETTERS[14:20], num = rnorm(n = 10, mean = 1, sd = 2), bool = TRUE)\n\n\nCan you identify the structure of list1?\n\n\nCode\nstr(list1)\n\n\nList of 3\n $ char: chr [1:7] \"N\" \"O\" \"P\" \"Q\" ...\n $ num : num [1:10] -0.703 -1.948 1.745 -2.347 2.872 ...\n $ bool: logi TRUE\n\n\nAdd a matrix with 3 rows and 3 columns mat as one of the elements to the list. This matrix mat includes numbers starting at 1, ending at 9 in steps of 1.\n\n\nCode\nlist1$mat &lt;- matrix(1:9, nrow = 3, ncol = 3)\n\n\nRetrieve the numeric vector num from list1 and assign the value to a vector num_vec (the output should be a vector). How many ways can you identify to do this?\n\n\nCode\nnum_vec &lt;- list1$num\n\n# alternative\n# num_vec &lt;- list1[[2]]\n# num_vec &lt;- list1[[\"num\"]]\n\n\nWhich letter is on the 3th position of the char-element in list1. Try to use only 1 line of code to extract that value. How many ways can you identify to do this?\n\n\nCode\nlist1[[\"char\"]][3]\n\n# alternative\n# list1$char[1]\n# list1[[1]][1]\n\n\nCan you change char from uppercase into lowercase (list1$char should be changed in lowercase)\n\n\nCode\nlist1$char &lt;- tolower(list1$char)\n\n# Note: R first evaluates 'tolower(list1$char)` i.e. it changes the case from\n# upper to lower and then assign the outcome. This allows you to change the \n# content of a variable using the value of that variable (in this case, change\n# the case from upper to lower and store the result using the same variable name\n# as the one that was used as an argument to the tolower function).\n\n\nCan you calculate the mean of matrix mat? User one line of code to do so.\n\n\nCode\nmean(list1$mat)\n\n\n[1] 5\n\n\n\n\n\n\n\n2.2.4 Data frames and tibbles\nA data frame is a list whose components are vectors with the same length. As data frames are a special class of lists you they can hold heterogeneous components (numeric, date/time, string, boolean values). Unlike like lists (but like matrices), the components of a data frame have the same length. As a result, they are very useful to store variables that measure a property for various observations: with one observation per row and one variable per column, a data frame looks like a matrix with n observation and m variables. However, unlike a matrix, these variables do not have to share a common type: one variable can be a character variable, the other a date/time variable while yet other variables are double of integer variables.\nMost data frames that you will use will be imported from e.g. an excel spreadsheet, a database or through an API. The functions that we will cover to show you how you can do so return a data frame. However, for some applications you’ll also need to create your own data frame.\nTo create a data frame, R includes the function data.frame(). Within the round brackets, you add the name of the variable and its values as a vector. A small data frame including the first 5 months of the year (variable month) as well as the number of days in that month (variable days) is created using\n\nmy_df &lt;- data.frame(month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"), days = c(31, 28, 31, 30, 31))\n\nIn the environment pane, you can see that the data frame my_df has 5 observations with 2 variables for each observation. As with lists, the arrow before the name can be used to show additional information: the name and type of the variables as well as the first observations. Because a data frame is a special class of type list, you can use all subsetting operations that where shown for lists, e.g.\n\nreturn the first variable as a list:\n\n\nmy_df[1]\n\n  month\n1   Jan\n2   Feb\n3   Mar\n4   Apr\n5   May\n\n\n\nreturn the second variable as a vector (of numeric variables):\n\n\nmy_df[[2]]\n\n[1] 31 28 31 30 31\n\nmy_df$days\n\n[1] 31 28 31 30 31\n\n\n\nto select all values for all variables for the second to fourth observation:\n\n\nmy_df[2:4, ]\n\n  month days\n2   Feb   28\n3   Mar   31\n4   Apr   30\n\n\n\nto select all values for the first and fourth observations:\n\n\nmy_df[c(1, 4), ]\n\n  month days\n1   Jan   31\n4   Apr   30\n\n\nTo add a variable to a data frame which is calculated from the values of other variables, we will use {dplyr}’s mutate() function in Chapter 8. An alternative is to add a variable as we did with lists. For instance, to calculate the number of hours per month, the latter approach uses the following code:\n\nmy_df$hours &lt;- my_df$days * 24\n\nAs an illustration, using {dplyr}’s mutate() function to calculate the number of seconds per month\n\nmy_df &lt;- dplyr::mutate(my_df, seconds = hours * 60)\n\nIn this code, we use dplyr:: as this package isn’t loaded into the memory. To use the function, we have to add a reference to the package where the function can be found. If the package would have been loaded, the part dplyr:: would not have been necessary.\nYou can order a data frame using base R’s order() function. Here too, {dplyr} includes a function that does the same operation. To illustrate the former, let’s sort my_df in ascending order for the number of days per year:\n\nmy_df[order(my_df$days), ]\n\n  month days hours seconds\n2   Feb   28   672   40320\n4   Apr   30   720   43200\n1   Jan   31   744   44640\n3   Mar   31   744   44640\n5   May   31   744   44640\n\n\nUsing {dplyr}:\n\ndplyr::arrange(my_df, days)\n\n  month days hours seconds\n1   Feb   28   672   40320\n2   Apr   30   720   43200\n3   Jan   31   744   44640\n4   Mar   31   744   44640\n5   May   31   744   44640\n\n\nR includes sample data frames. To illustrate the use of a larger data frame than my_df, we’ll use mtcars, a data frame that includes fuel efficiency and other characteristics for 32 cars. You can see the full data frame using Vieuw(mtcars). This function openens a new tab in the source pane where you view the full data frame. Using the arrows on the right of the variable names, you can sort that data frame ascending or descending. Often, you’ll want to look at a subset of the data. To do so, you can use the head() or tail() function. The first shows the first 6 observations, the second the last 6 observations. You can add more variable, but here we’ll keep the default value and ask R to show 6 observations. Let’s look at the first six observations:\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThe last six observations are shown using\n\ntail(mtcars)\n\n                mpg cyl  disp  hp drat    wt qsec vs am gear carb\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.7  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.9  1  1    5    2\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.5  0  1    5    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.5  0  1    5    6\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.6  0  1    5    8\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.6  1  1    4    2\n\n\nYou can look at other observations using by e.g. subsetting the data frame. For instance, the 20th to 25th observations are\n\nmtcars[20:25, ]\n\n                  mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla   33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona    21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin      15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28       13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n\n\nTo view one variable, e.g. mpg:\n\nmtcars$mpg\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nTo see the observations with the smallest or largest values on one variable, you can use head() or tail() on the ordered data frame. For instance, the observations with the smallest values for disp are\n\nhead(mtcars[order(mtcars$dis), ])\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n\n\nwhile these with the largest values can be seen from\n\ntail(mtcars[order(mtcars$dis), ])\n\n                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout   18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nDuster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4\nPontiac Firebird    19.2   8  400 175 3.08 3.845 17.05  0  0    3    2\nChrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4\nLincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4\nCadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4\n\n\nLet’s now add a variable kilometers per liter kml as the product of miles per gallon mpg and 0.4251. To do so, we create a new variable to add to the data frame and assign it the value mpg * 0.4251:\n\nmtcars$kml &lt;- mtcars$mpg * 0.4251\n\nIn the environment pane, you can see that the data frame mtcars includes this new variable kml.\nTibbles and data frames are close siblings. Tibbles are data structures introduced with the tidyverse collection of packages ({dplyr}, {stringr}, …). Working with tibbles requires the {tibble} package. Tibbles are more consistent relative to base R’s data frames. To illustrate a couple of differences, let’s use mtcars_df and mtcars_tib the tibble version of the data frame:\n\nmtcars_df &lt;- mtcars\nmtcars_tib &lt;- tibble::tibble(mtcars)\n\nIf you print a data frame, R shows all observations and tries to show all columns. Printing a tibble only shows the first 10 observations and only the columns that fit on your screen. You can see this if you type mtcars_df and mtcars_tib in the console. If you look at the output in the console, you’ll also see that a tibble includes the column type below the name of the column. This is not the case for the data frame. Here, you have to use str() to see the column types.\nData frames allow for partial matching of variable names. The dataset includes a variable mpg but no variable m. Yet, mtcars_df$m will show mtcars_df$mpg as output. A tibble does not allow partial matching:\n\nmtcars_df$m\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nshows prints output, while\n\nmtcars_tib$m\n\nWarning: Unknown or uninitialised column: `m`.\n\n\nNULL\n\n\nshows an error.\nAs a result, tibbles are more predictable as you can’t use variables in a dataset that you didn’t meant to use. There are some other differences and we”ll cover them in Chapter 3 but for now, we’ll mostly work with data frames.\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nCreate a data frame df1 with 3 variables: - units: a sample with size 100 taken with replacement from the sequence 1:12 (sample(x = 1:12, size = 100, replace = TRUE)) - revenue: 100 draws from a normal distribution with mean 100 and standard deviation 10 (rnorm(n = 100, mean = 100, sd = 10)) - customer: a sample with size 100 taken with replacement from the first 5 letters of the alphabet in uppercase (sample(x = LETTERS[1:5], size = 100, replace = TRUE))\n\n\nCode\ndf1 &lt;- data.frame(\n  units = sample(x = 1:12, size = 100, replace = TRUE), \n  revenue = rnorm(n = 100, mean = 100, sd = 10),\n  customer = sample(x = LETTERS[1:5], size = 100, replace = TRUE))\n\n\nCan you identify the structure of df1?\n\n\nCode\nstr(df1)\n\n\n'data.frame':   100 obs. of  3 variables:\n $ units   : int  9 2 11 8 8 6 7 11 10 8 ...\n $ revenue : num  97.4 94.3 86.9 93.7 91.1 ...\n $ customer: chr  \"C\" \"C\" \"E\" \"E\" ...\n\n\nShow the first 6 observations from df1 in the console.\n\n\nCode\nhead(df1)\n\n\n  units   revenue customer\n1     9  97.44750        C\n2     2  94.31307        C\n3    11  86.91977        E\n4     8  93.71525        E\n5     8  91.14530        C\n6     6 103.71556        C\n\n\nShow the last 6 observations from df1 in the console\n\n\nCode\ntail(df1)\n\n\n    units   revenue customer\n95      9 107.84707        C\n96      1  94.15832        B\n97      1  92.23498        C\n98      4  99.95981        E\n99      2 105.97902        B\n100    11 112.33348        E\n\n\nExtract the variable revenue from df1 and assign the outcome to a vector vec_rev. How many ways can you identify to perform this operation?\n\n\nCode\nvec_rev &lt;- df1$revenue\n\n# Alternative (data frames are lists)\n# vec_rev &lt;- df1[[2]]\n# vec_rev &lt;- df1[[\"revenue\"]]\n\n\nCalculate the mean of the revenue variable in df1\n\n\nCode\nmean(df1$revenue)\n\n\n[1] 99.69353\n\n\nChange the customer variable in df1 from uppercase to lowercase (make sure to reassign the value).\n\n\nCode\ndf1$customer &lt;- tolower(df1$customer)\n\n\nAdd a variable, price, as the result of the division of revenue and units.\n\n\nCode\ndf1$price &lt;- df1$revenue / df1$units\n\n\nUsing {dplyr}’s mutate() function, would you be able to rewrite the previous code but assign the result to price2?\n\n\nCode\ndf1 &lt;- dplyr::mutate(df1, price2 = revenue/units)\n\n\nCreate a tibble from df1 and assing it to tb1 (note you need to add tibble:: before the function as the tibble package was not loaded in the memory).\n\n\nCode\ntb1 &lt;- tibble::tibble(df1)\n\n\nTry to spot the difference between tb1 and df1. In the console type df1 and compare the output with the case where you type tb1.\nSort the data frame df1 in descending order of price and show the smallest 6 values.\n\n\nCode\nhead(df1[order(df1$price), ])\n\n\n   units  revenue customer    price   price2\n21    12 87.45529        d 7.287941 7.287941\n51    12 89.64255        d 7.470213 7.470213\n20    12 94.30560        b 7.858800 7.858800\n3     11 86.91977        e 7.901797 7.901797\n89    12 97.82058        b 8.151715 8.151715\n40    12 98.97065        b 8.247554 8.247554",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#functions",
    "href": "02_Examples.html#functions",
    "title": "2  A tour of R",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nBase R includes many build in functions. In the previous section, we already used a couple of them, e.g. sample(), rnorm(), mean(), paste(), order(), vector(), matrix(), list(), … . In addition to these build in base R functions, packages include functions that extend the capabilities of R. Examples that we have used so far include dplyr::mutate() or dplyr::arrange(). R further allows you to write your own function. In this section, we’ll introduce functions and show how you can find help for each of them. This section further shows how you can write your own function.\n\n2.3.1 Build in functions: base R and packages\nTo introduce functions, we’ll use the function mean(). To find help for this function you can type use help(mean) or add a question mark ? before the name of the function in the console. This line will open the help pane on the lower right hand side. To find help for the mean() function, you can type\n\n?mean()\nhelp(mean)\n\nin the console. On the right, you’ll see the help file for this function\n\n\n\n\n\nFigure 2.1: Help for the function mean()\n\n\n\n\n\n\n\n\nLet’s review the output in Figure 2.1. There are a couple of sections: Description, Usage, Arguments, Details, Value, References, See Also and Examples. The first section describes what the function does. In this case, it calculates a (potentially trimmed) arithmetic mean. The Usage section shows how you use this function. It shows the arguments of the function. The arguments are included after the function’s name in round brackets ( ) and are separated by a comma. Here, the arguments of the function are x, trim and na.rm. The arguments section includes further information on each of them. x refers to an R object (vector, matrix, …) whose mean you want to calculate. Using trim you can exclude a fraction of the observations (most extreme observations at each end large and small) from the calculation. The third argument tells R what it needs to do in case there is a missing value (NA value). We’ll cover these values more in depth in the next chapter, but for now: a missing value is shown as NA. In case there are missing values, R allows you to calculate the mean after removing these missing values from the object or to keep them. In the latter case, if there are missing values, R will not be able to calculate the mean of the object. You need to include and x as an argument. Else, R doesn’t know which object it needs to use to calculate the mean. The other two arguments of the function, trim and na.rm both have default values. A default value is a value that R will use unless you specify an alternative. In other words, if you don’t specify their values, R will assume that you are satisfied with their default values. The default values are trim = 0 and na.rm = FALSE. In other words, R will use all values to calculate the mean and R will not remove missing values from before it calculates the mean. In case there are missing values, R will return NA as it is unable to calculate a mean if it doesn’t know one of more values.\nThere are a couple of ways to use these arguments. To illustrate them, we’ll use two numeric vectors, one with missing values vec_num and one without missing values (vec_num_nona)\n\nvec_num &lt;- c(0.0001, 0.0005, NA, 1000, 1500, NA, seq(from = 1, to = 4, by = 1))\nvec_num_nona &lt;- c(0.0001, 0.0005, 1000, 1500, seq(from = 1, to = 4, by = 1))\n\nThe first vector has 10 values: there are 4 extreme values (the first four in the vector), two missing values and a sequence of 4 values starting at 1, ending at 4 and increasing in steps of 1. The second equals the first without the missing values\nThe first way to use this function is to name the arguments and add their value. This is also called name matching. Suppose that you want to calculate the mean of vec_num for all observations after removing missing values, you would include the name of the vector after the object argument, x = vec_num, use trim = 0 (use all vaues) and set na.rm = TRUE (eliminate missing values from the calculation):\n\nmean(x = vec_num, trim = 0, na.rm = TRUE)\n\n[1] 313.7501\n\n\nNote that in this case, we didn’t use the assignment operator &lt;- but the equality character =: we don’t want to assign the value of vec_num to a variable x nor do we want to assign the value of 0 to the variable trim. What we want is R to know that the object whose mean we want to calculate is vec_num (x = vec_num), that we don’t want to eliminate extreme observations from the dataset (trim = 0) and that we want to compute the mean after removing missing values (na.rm = TRUE). After R is finished computing the mean, R doesn’t have to remember the value of x, trim or na.rm.\nIf you use name matching, you don’t have to add the arguments in the order in which they appear in the function. In other words, you don’t have to start with the x argument but you can start with na.rm = TRUE, move onto x = vec_num and end with trim = 0:\n\nmean(na.rm = TRUE, trim = 0, x = vec_num)\n\n[1] 313.7501\n\n\nThe second way to use mean() is to list the arguments without their name. This is called position matching: the first argument is x, the second is trim and the third is na.rm. Using this order, you add the values for each argument: vec_num, 0, TRUE:\n\nmean(vec_num, 0, TRUE)\n\n[1] 313.7501\n\n\nR assumes that vec_num is the first value for the first argument x, the second 0 refers to trim and the third to na.rm. Note that in this case, you can not change the order of the arguments. For instance R can not evaluate\n\nmean(0, TRUE, vec_num)\n\nError in `mean.default()`:\n! 'trim' must be numeric of length one\n\n\nThe second argument of the function is trim. R expects a fraction, e.g. 0.20 but not a boolean value TRUE.\nYou don’t need to include all arguments of the function. In case your are satisfied with a default argument, you can leave it out of your code. In In the previous examples, trim was always set to 0. As this is the default value, adding this argument to the function was not necessary and we could have written\n\nmean(x = vec_num, na.rm = TRUE)\n\n[1] 313.7501\n\n\nAs R interprets the first argument in the function as the x argument, we can further shorten this code to\n\nmean(vec_num, na.rm = TRUE)\n\n[1] 313.7501\n\n\nHowever,\n\nmean(vec_num, TRUE)\n\nError in `mean.default()`:\n! 'trim' must be numeric of length one\n\n\nwill not work: R interprets the TRUE as the value for trim.\nIf you are satisfied with the value na.rm = FALSE but would want to trim the extreme values from the calculation, you could shorten the code:\n\nmean(vec_num_nona, 0.20)\n\n[1] 168.3334\n\n\nHere, R interprets vec_num_nona as the value for x and 0.20 as the value for the second argument trim. As the code doesn’t include a third value, R will use the default na.rm = FALSE for the third value.\nIf you are satisfied with the defaults for all optional arguments (you don’t want to exclude extreme values and there are no missing values), you can simply use\n\nmean(vec_num_nona)\n\n[1] 313.7501\n\n\nR executes this line as\n\nmean(x = vec_num_nona, trim = 0, na.rm = FALSE)\n\n[1] 313.7501\n\n\nThe help file further adds details on how the function’s value is computed, references, similar functions and examples.\nPackages include functions. To use these functions, you either load the package or you explicitly refer to the package, add a :: and then call the function, e.g. dplyr::mutate. You can access the help file in a similar way: ?dplyr::mutate(). If {dplyr} is loaded in the memory, you can remove the dplyr:: part. In addition, many packages have separate webpages with additional help and examples. For {dplyr} for instance, I refer to Wickham et al. (2023). For most of the tidyverse packages, you can find the online manuals in the references in Chapter 1.\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nLet’s review the arguments of another function that we have used: sample(). Using ?sample() in the console, you can find the help files for this function. The description shows that this function “takes a sample of the specified size from the elements of x using either with or without replacement”. The information shows two sample functions: sample() and sample.int(). Here, we will focus on the first:\n\nsample(x, size, replace = FALSE, prob = NULL)\n\nIn the arguments list, you can see that x is a vector of one or more elements to sample from, size is a non-negative integer that gives the number of items to choose, replace answers the question is sampling is with or without replacement and prob is a vector of probability weights for the sampled vector. The function shows that by default, the function assumes sampling without replacement (replace = FALSE) and that the every element of x has the same probability of being drawn (there is no vector with probability weights (prob = NULL)). In the details, you can see that there is a third default value: the default value for size is equal to the number of elements in x. In other words, if you don’t include the size argument and x is a vector with 5 elements, the sample() function returns a vector with 5 elements drawn at random from x. Let’s use this function with the first 10 elements of the vector letters: letters[1:10].\nSuppose that you are satisfied with the default arguments: you want a sample of 10 elements from letters[1:10] without replacement where each letter has the same probability of being drawn. Write the function in the shorest possible way.\n\n\nCode\nsample(letters[1:10])\n\n\n [1] \"c\" \"j\" \"d\" \"f\" \"b\" \"g\" \"e\" \"a\" \"h\" \"i\"\n\n\nYou want a random draw of size 10 from letters[1:10] with replacement where each letter had the same probability of being drawn. Use name matching to write the code where you include only those arguments that are necessary.\n\n\nCode\nsample(x = letters[1:10], replace = TRUE)\n\n\n [1] \"f\" \"b\" \"b\" \"d\" \"h\" \"e\" \"a\" \"j\" \"d\" \"f\"\n\n\nCode\n# Name matching: you include the names of the arguments\n# You have two arguments to need to specify: x and replace\n# size by default equals the length of the vector (10)\n\n\nWrite the same function but now use position matching and include only those arguments that are necessary\n\n\nCode\nsample(letters[1:10], 10, TRUE)\n\n\n [1] \"f\" \"h\" \"j\" \"c\" \"i\" \"e\" \"j\" \"f\" \"j\" \"h\"\n\n\nCode\n# You have to add the number of draws (size) even if your number is equal to \n# the default value. As you specify an alternative to the default for \n# replace (TRUE), you hhave to add all arguments (before replace) in your\n# function call. Else, R will treat TRUE as the value of size. \n# This is an invalid argument. \n\n\nCan your predict what this code will do?\n\nsample(size = 15, replace = TRUE, prob = NULL, x = letters[1:10])\n\nWhat would happen is you change replace = TRUE in replace = FALSE?\n\n\nCode\n# The code sample(size = 15, replace = FALSE, prob = NULL, x = letters[1:10]) will \n# return an error. You have a bowl filled with 10 letters and you want to draw 15\n# at random. As you don't replace the onces that were drawn, your bowl will be\n# empty after the first 10 draws and R can not draw the 11th, 12th, ... letter. \n\n\nYou met the seq() function. This function generates a series. Use the help function to identify the arguments of this function as well as their default values.\n\n\nCode\n?seq()\n\nseq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),\n    length.out = NULL, along.with = NULL, ...)\n\n# from (default = 1)\n# to (default = 1)\n# by: number increment of the sequence \n#     (if left empty, sequence starts from 'from' ends at 'to' in steps of 1\n#      see the details (first typical usage))\n# length.out: desired length of a sequence. Default value = NULL\n# along.with: the desired length of the series is taken from the length of this argument\n\n\nLet’s try the various arguments:\n\ngenerate a series starting from 10, ending in 20 in steps of 2 using name matching\n\n\n\nCode\nseq(from = 10, to = 20, by = 2)\n\n\n[1] 10 12 14 16 18 20\n\n\n\ngenerate a series starting at 10, ending at 20 with a desirend length of 25 using name matching\n\n\n\nCode\nseq(from = 10, to = 20, length.out = 25)\n\n\n [1] 10.00000 10.41667 10.83333 11.25000 11.66667 12.08333 12.50000 12.91667\n [9] 13.33333 13.75000 14.16667 14.58333 15.00000 15.41667 15.83333 16.25000\n[17] 16.66667 17.08333 17.50000 17.91667 18.33333 18.75000 19.16667 19.58333\n[25] 20.00000\n\n\n\nusing the same series, the help file shows an abbreviation for length.out: how can you abbreviate this argument?\n\n\n\nCode\nseq(from = 10, to = 20, len = 25)\n\n\n [1] 10.00000 10.41667 10.83333 11.25000 11.66667 12.08333 12.50000 12.91667\n [9] 13.33333 13.75000 14.16667 14.58333 15.00000 15.41667 15.83333 16.25000\n[17] 16.66667 17.08333 17.50000 17.91667 18.33333 18.75000 19.16667 19.58333\n[25] 20.00000\n\n\n\nuse 4 different ways to generate a sequence from 1 to 10 in steps of 1 (one of which is a new function)\n\n\n\nCode\nseq(from = 1, to = 1, by = 1)\n\n\n[1] 1\n\n\nCode\nseq(from = 1, to = 1)\n\n\n[1] 1\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq_len(10)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nTo illustrate along.with, lets use the variable mpg from the mtcars data frame. Recall that this variable includes 32 observations. Using seq() with argument along.with = mtcars$mpg returns a sequence starting at from, ending at to with a desired output of 32 (i.e. the number of observations in the dataset.) seq_along(mtcars$mpg) generate a series starting at 1, ending at 32 in steps of 1. You might wonder why this is useful. Suppose your have to write a for loop (we will cover these loops shortly) where you have to to a calculation for each observation in a dataset. You don’t know how long that data frame is. This is where seq_along() enters: it allows to generate a series starting at 1 and ending at the total number of observations in the data frame in steps of 1.\n\n\nCode\nseq(from = 10, by = 2, along.with = mtcars$mpg)\n\n\n [1] 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58\n[26] 60 62 64 66 68 70 72\n\n\nCode\nseq_along(mtcars$mpg)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32\n\n\n\n\n\n\n\n2.3.2 Creating your own function\nThere are is lot of function available in R and the many packages that have been written in the past and continue to the written today. However, sometimes you want to write your own function. To do so, you can use the following (basic) template:\n\ndo_something &lt;- function(arguments) {\n  \n  body: what the function does with the arguments\n}\n\nThis template creates a function, do_something. This function has arguments. These are included between round brackets after the keyword function. Here, you would include for instance x, y. Using these arguments the function do_something does “something” with the arguments. What that “something” is, is included between the curly brackets { }. To illustrate, let’s define a function to_the_power with two arguments, x and n. This function calculates the n-th power of x: \\(x^n\\).\n\nto_the_power &lt;- function(x, n) {\n  x^n\n}\n\nIn the environment pane, you can see that this function now exits. You can now use that function as you would use any other function:\n\nusing name matching:\n\n\nto_the_power(x = 2, n = 4)\n\n[1] 16\n\nto_the_power(n = 4, x = 2)\n\n[1] 16\n\n\n\nusing position matching:\n\n\nto_the_power(2, 4)\n\n[1] 16\n\n\nHere, we didn’t add any default values. Suppose that you would want to include the square (n = 2) as the default value. To do so, you can add this default value in the definition of the function and write the arguments as (x, n = 2). Doing so, R will use n = 2 if no other variable is given. The following code adds a default value of 2 for the argument n:\n\nto_the_power &lt;- function(x, n = 2) {\n  x^n\n}\n\nThis default value is shown in the environment pane.\nUsing the updated function with x = 5\n\nusing the default value with name matching:\n\n\nto_the_power(x = 5)\n\n[1] 25\n\n\n\nusing name matching with an alternative value for n:\n\n\nto_the_power(x = 5, n = 4)\n\n[1] 625\n\n\n\nusing position matching with default value:\n\n\nto_the_power(5)\n\n[1] 25\n\n\n\nusing position matching with an alternative value for n:\n\n\nto_the_power(5, 4)\n\n[1] 625\n\n\nRecall that R is vectorized: the function to_the_power() will calculate the n-th power of each element of a vector:\n\nto_the_power(x = c(2, 4, 8), n = 3)\n\n[1]   8  64 512\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nYou will create a function future_value. This function calculates how much an investment x in a bond will be worth 1 year from now if the interest rate is equal to i. To determine the future value, you can use the following formula: \\(x * (1 + i)\\): one year from now you receive the money that you invested, x plus the interest rate on your investment \\(i * x\\). In other words, one year from now you will receive \\(x + i * x\\) or \\(x * (1 + i)\\). Write this function.\n\n\nCode\nfuture_value &lt;- function(x, i) {\n  x * (1 + i)\n}\n\n\nWhat is the future value if you invest 200 today and the interest rate is 5% (0.05)? Use your function to determine that value.\n\n\nCode\nfuture_value(x = 200, i = 0.05)\n\n\nAdd a default value for the interest rate equal to 4% (0.04).\n\n\nCode\nfuture_value &lt;- function(x, i = 0.04) {\n  x * (1 + i)\n}\n\n\nWhat is the future value if you invest 600 today and the interest rate is equal to the default interest rate?\n\n\nCode\nfuture_value(600)\n\n\nUsing your updated function, can you calculate the future value if you invest 725 today and the interest rate is equal to 10% (0.10)?\n\n\nCode\nfuture_value(x = 725, i = 0.10)\n\n\nCalculate the future value using the default interest rate of a sequence starting at 100, ending at 500 in steps of 100.\n\n\nCode\nfuture_value(x = seq(100, 500, 100))\n\n\n\n\n\n\n\n2.3.3 Ceci n’est pas une pipe: the pipe operator %&gt;% or |&gt;\nThe pipe operator %&gt;% was introduced in the {magritrr} package (Bache and Wickham (2022)). As the {magrittr} pipe became widely used, the R development team added its functionality in R 4.1 (May 2021). If you want to use the {magrittr} pipe %&gt;%, you have to load the {magrittr} package.\nThe native pipe (the one that is build into R), |&gt; is part of base R and doesn’t require a package. Both pipes share a lot of similarities. There are some differences between both, but for this course, these differences will not cause any major issues. Code written before May 2021 will always use the magrittr %&gt;% pipe. As this pipe operator was widely used, you need to know that it exists as some scripts will include that operator.\nThe pipe operator |&gt; (or %&gt;%) takes whatever is on its left hand side and moves it to the (by default) first position of the function on its right hand side. For example vec_num_nona |&gt; mean() is equivalent to mean(vec_num_nona) and vec_num |&gt; mean(na.rm = TRUE) |&gt; sqrt() is equivalnt to sqrt(mean(x = vec_num, na.rm = TRUE)).\nA pipe operator simplifies the process of writing and reading code quite considerably. Suppose you need to take square root of the natural logarithm of a numeric variable 100. The square root function in R is sqrt() and the natural logarithm (or ln) is log(). There are two options to do this without the pipe operator:\nOption 1\n\na &lt;- 100\nb1 &lt;- log(a)\nc1 &lt;- sqrt(b1)\n\nIn this option, you write three lines of code: in the first you assign the value of 100 to a, in the second you calculate the natural logarithm of a and you assign the result to a variable called b1 and in the third step, you calculate the square root of b1 using sqrt(b1) and you assign the outcome to a variable c1. In other words you need two lines of code to calculate the square of the log. The advantage of this option is that you see what R does: it takes the log (line 2) and then the square root (line 3). In other words, the code is easy to read. There are a couple of disadvantages. First, you code will be quite long. Second, you need to create an intermediary object b1. This object will take up some of the total memory available on your computer. If your datasets are large, this is something you need to avoid.\nOption 2\n\na &lt;- 100\nc2 &lt;- sqrt(log(a))\n\nIn this option, you write two lines of code. In the first is similar to the first line of code for option 1. The second line calculates the outcome by nesting the log function within the square root function. R will first calculate the value of log(a) and then calculate the square root the result. Doing so, R follows the usual order of operations: first the parenthesis and using the outcome of that result, it moves up to the second inner round brackets. This option has the advantage that you don’t need to create intermediary results like b1 in the previous option. However, the code is difficult to read as you need to read the last line from right to left: the second line takes the natural logarithm first and then calculates the square root of the result.\nOption 3: with the pipe operator\n\na &lt;- 100\nc3 &lt;- a |&gt; log() |&gt; sqrt()\n\nIn this third option you use the native pipe operator |&gt;. The first line is the same as in the previous two options. However, you can now read the part after the assignment operator as ” take the value of a, then calculate the log, then calculate the square root”. You can do this because the pipe operator moves what is written on the left-hand side (LHS) of the pipe into the first argument of the function on the right-hand side (RHS) of the pipe. If you look at the code, a is on the left hand side of the first pipe operator. R takes the value of a and moves it to the right hand side where it becomes the first argument in the log() function. R then calculates log(a). This is the left hand side of the second pipe operator. So R moves the value, log(a) from the left hand side to the first argument of the function on the right hand side. That function is the square root. As in option 2, this code does not require you to store intermediary results in an object b1. Unlike option 2, you can read the code from left to right: R first takes the natural logarithm and uses that result to calculate the square root.\nThe first part of the code assigns the outcome to a variable called c3. You can verify that each option has the same end result\n\n\n[1] \"The result for option 1 is 2.14596602628935\"\n\n\n[1] \"The result for option 2 is 2.14596602628935\"\n\n\n[1] \"The result for option 3 is 2.14596602628935\"\n\n\nWe will often need to write code where you apply a function to subsequent results. For instance, given a dataset, first filter observations that dataset to retain only observations that meet a condition, then group these filtered observation per year or per region, then calculate the average of a variable per year or region, then plot this averages in a bar chart. Using the first option, you would write a line of code for each step and save or assign the result to a new object. The second option would require a large number of nested options and the part without the plot would look like ggplot(summarise(group_by(filter(df, condition), group), mean_a = mean(a)), aes() ...). With the pipe operator, you can write df |&gt; filter(condition) |&gt; group_by(group) |&gt; summarise(mean_a = mean(a)) |&gt; ggplot(aes() ...): using the dataset with the name df, first filter the observations that meet the condition condition, second group these filtered observations by a variable group and third calculate the summary statistic mean using the summarise function and last plot the result using ggplot().\nIf you load the {magrittr} package, you can write the third option using the %&gt;% pipe.\n\na &lt;- 100\nc4 &lt;- a %&gt;% log() %&gt;% sqrt()\nprint(paste0(\"The result for the {magrittr} pipe is \", c4))\n\n[1] \"The result for the {magrittr} pipe is 2.14596602628935\"\n\n\nThe functions we used in this example, log()and sqrt() are straightforward as their first argument equals the variable whose natural logarithm of square root you want to calculate. Sometimes this is not the case. Recall that you have used the rnorm() function to create a vector of normal random variables rnorm(n = , mean = , sd = ). The first argument of the rnorm()function is the sample size: how many random variables do you need? In other words, if you would write\n\na &lt;- 10\nvec1 &lt;- a |&gt; rnorm(mean = 2, sd = 4)\n\nR would assume that you want 10 random draws. The pipe |&gt; adds the n = 10 to the rnorm() function as the first argument of that function (which is n). Doing so, it completes rnorm(mean = 2, sd = 4) by adding n = 10 before mean. In other words, the last line in this code is equivalent to rnorm(n = 10, mean = 2, sd = 4. Suppose that you want to use the pipe operator to set the mean, not the number of random draws. The mean is the second argument in the function. To tell R that it needs to use the pipe operator to set the value of an argument other than the first, you need a placeholder. For the native pipe |&gt; that placeholder is an underscore _ and you need to include the name of the argument. The placeholder for the {magrittr} pipe %&gt;% is a dot .. For the {magrittr} pipe, you don’t need to include the argument name.\nThe native pipe |&gt;\n\nm &lt;- 5\nvec1b &lt;- m |&gt; rnorm(n = 1000, mean = _, sd = 10)\n\nprint(paste0(\"The mean of the vector is equal to \", round(mean(vec1b), digits = 4)))\n\n[1] \"The mean of the vector is equal to 4.305\"\n\nprint(paste0(\"The standard deviation of the vector is equal to \", round(sd(vec1b), digits = 4)))\n\n[1] \"The standard deviation of the vector is equal to 10.6771\"\n\n\nThe {magrittr} pipe %&gt;%\nIf we use the name of the argument:\n\nm &lt;- 5\nvec1m1 &lt;- m %&gt;% rnorm(1000, mean = ., sd = 10)\nprint(paste0(\"The mean of the vector is equal to \", round(mean(vec1m1), digits = 4)))\n\n[1] \"The mean of the vector is equal to 5.1481\"\n\nprint(paste0(\"The standard deviation of the vector is equal to \", round(sd(vec1m1), digits = 4)))\n\n[1] \"The standard deviation of the vector is equal to 9.9149\"\n\n\nIf we don’t use the name or the argument, the placeholder will show R where to put the value of m. As the mean is the second argument in the function, R will draw from a normal distribution with mean equal to 5.\n\nm &lt;- 5\nvec1m2 &lt;- m %&gt;% rnorm(1000, ., sd = 10)\nprint(paste0(\"The mean of the vector is equal to \", round(mean(vec1m2), digits = 4)))\n\n[1] \"The mean of the vector is equal to 4.7086\"\n\nprint(paste0(\"The standard deviation of the vector is equal to \", round(sd(vec1m2), digits = 4)))\n\n[1] \"The standard deviation of the vector is equal to 10.1857\"\n\n\nThis code with the native pipe |&gt; fails:\n\nm &lt;- 5\nvec1b2 &lt;- m |&gt; rnorm(1000, _, sd = 10)\n\nError in rnorm(1000, \"_\", sd = 10): pipe placeholder can only be used as a named argument (&lt;input&gt;:2:16)\n\n\nAs you can see from the error, you need a named argument.\nThis is the first difference between the {magrittr} and base R pipe: the first does not need a named argument, the second needs one.\nThere is another difference between both pipes: you can use the placeholder in the base R pipe |&gt; only ones. With the {magrittr} pipe, you can use the placeholder more than one. Suppose that we want to draw 1000 random numbers from a normal distribution with mean 100 and a standard deviation equal to 2 times the mean. With the {magrittr} pipe %&gt;%, you could write\n\nm &lt;- 100\nvec1m3 &lt;- m %&gt;% rnorm(1000, mean = ., sd = 2 * .)\nprint(paste0(\"The mean of the vector is equal to \", round(mean(vec1m3), digits = 4)))\n\n[1] \"The mean of the vector is equal to 102.3394\"\n\nprint(paste0(\"The standard deviation of the vector is equal to \", round(sd(vec1m3), digits = 4)))\n\n[1] \"The standard deviation of the vector is equal to 196.4645\"\n\n\nor even shorter:\n\nm &lt;- 100\nvec1m4 &lt;- m %&gt;% rnorm(1000, ., 2 * .)\nprint(paste0(\"The mean of the vector is equal to \", round(mean(vec1m4), digits = 4)))\n\n[1] \"The mean of the vector is equal to 106.2261\"\n\nprint(paste0(\"The standard deviation of the vector is equal to \", round(sd(vec1m4), digits = 4)))\n\n[1] \"The standard deviation of the vector is equal to 203.6981\"\n\n\nIf you would do the same with the native pipe |&gt; you’ll see that R return an error:\n\nm &lt;- 100\nvec1m3 &lt;- m %&gt;% rnorm(1000, mean = _, sd = 2 * _)\n\nError in vec1m3 &lt;- m %&gt;% rnorm(1000, mean = \"_\", sd = 2 * \"_\"): invalid use of pipe placeholder (&lt;input&gt;:2:0)\n\n\nSo, in most cases, you can treat the {magrittr} pipe %&gt;% and native pipe |&gt; as equal but you must use the . placeholder in the first and _ placeholder in the second case. If you want to use multiple placeholders, you need to use the {magrittr} pipe. For the {magrittr} pipe, you need to run library(magrittr) to load the package.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#pipe-operator",
    "href": "02_Examples.html#pipe-operator",
    "title": "2  A tour of R",
    "section": "2.4 Pipe operator",
    "text": "2.4 Pipe operator",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#plots-and-tables",
    "href": "02_Examples.html#plots-and-tables",
    "title": "2  A tour of R",
    "section": "2.4 Plots and tables",
    "text": "2.4 Plots and tables\nPlots and tables are widely used to communicate the results of an analysis or to review the data. Base R include functions such as plot() or hist() to draw scatter plots or histograms. To illustrate the latter, we’ll create a histogram for 1000 draws from a normal distribution with mean equal to 0 and standard deviation equal to one (rnorm(n = 1000, mean = 0, sd = 1)). As the default values for rnorm() are mean = 0 and sd = 1, we don’t have to add these to the function. This leaves only first argument: the number of draws. Using rnorm(1000) to create a vector with 1000 draws from the standard normal distrubtion and show the result uing a histogram, you can use\n\nhist(rnorm(1000))\n\n\n\n\n\n\n\n\nOn the right hand side, R shows the plot. You can save this histogram by assigning it to e.g. hist1:\n\nhist1 &lt;- hist(rnorm(1000))\n\nIn the environment pane, you can see hist1 in the data section. The reason is that R stores hist1 as a list:\n\ntypeof(hist1)\n\n[1] \"list\"\n\n\n{ggplot2} is a package that is widely used to create graphs in R. Before using the package we first need to load it into the memory:\n\nlibrary(ggplot2)\n\nTo illustrate its use, we’ll develop a scatter plot using the mtcars data frame where we show hp on the x-axis, mpg on the y-axis and change the color of the dots in line with the number of cylinders (cyl).\n\nmtcars |&gt; \n  ggplot(aes(x = hp, y = mpg, colour = cyl)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe first line starts from the data frame: mtcars. Using the pipe operator, that data frame is entered as the first argument of the ggplot() function. This is where this function expects the data frame. This data frame (here mtcars) is where {ggplot2} will look for the variables. Within ggplot() we define what should be on the x-axis, on the y-axis and, if we want to do so, how the color of the dots should change in the aes() or aesthetics part. The following two lines tell R to draw a scatter plot and how it should draw the background an other parts of the layout of the plot. We’ll cover {ggplot2} in depth in Chapter 9, Chapter 10 and Chapter 11.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#essential-programming-structures",
    "href": "02_Examples.html#essential-programming-structures",
    "title": "2  A tour of R",
    "section": "2.5 Essential programming structures",
    "text": "2.5 Essential programming structures\nWe’ll review two essential programming structures: conditional execution and loops. In Chapter 13 we’ll cover these more in depth. We’ll start with conditional execution: a code block is only executed if a condition is met. We’ll then move to loops: repeating the same code a (potentially) large number of times.\n\n2.5.1 Conditional execution: if-statements\nAn if-statement is used of the execution of code depends on a condition being met. In its simplest form, an if-statement is written as\n\nif (condition) {expression}\n\n# Rest of the code\n\nIf the condition in the brackets () is met, then the expression in the curley brackets { } is executed. If that condition is not met, R will continue with the code without executing the expression. A condition is met if the statement is TRUE: examples could be if (a == 2), if (is.numeric(b), if (a &lt; b), … ). If the condition is TRUE, R will continue and execute the lines of code in {expression}. If the condition is FALSE, R will continue with the rest of the code and will not execute the lines of code in {expression}.\nLet’s illustrate this statement using a simple condition: if a &lt; b then R should reassign the value of b to 100. If a is equal to or larger than b, R shouldn’t change the value of b. To check the value of b, we’ll add the line print(paste(\"The value of b is\", b). This line asks R to print the value of b. We’ll first assign a value to a and b\n\na &lt;- 20\nb &lt;- 40\n\nLet’s now write the if-statement:\n\nif (a &lt; b) {b &lt;- 1000}\n\nprint(paste(\"The value of b is\", b))\n\n[1] \"The value of b is 1000\"\n\n\nLet’s see how R evaluates this statement. As the value if a is smaller than the value of b, condition a &lt; b returns TRUE. As a result, R executes the expression between { }. This expression re-assigns the value of b to 100. As it is executed, R reassigns the value of b to 100. The last line prints this value. As you can see, b is indeed 100.\nLet’s now change the value of a from 20 to 2000, re-use b’s original value 40 and rerun the if-statement:\n\na &lt;- 2000\nb &lt;- 40\n\nif (a &lt; b) {b &lt;- 1000}\n\nprint(paste(\"The value of b is\", b))\n\n[1] \"The value of b is 40\"\n\n\nThe condition a &lt; b now returns FALSE and R doesn’t execute the expression b &lt;- 1000. In other words, the value of b remains 40.\nIf the expression includes multiple lines, it is good practice to start these lines on a new line and intend the block (RStudio will do so automatically):\n\nif (condition) {\n  \n  expression line 1\n  expression line 2\n  expression line ...\n  \n}\n\nAn if-else-statment includes an expression in the case the condition returns TRUE as well as an expression in case the condition returns FALSE. An if-else-statement is written as:\n\nif (condition) {\n  expression if TRUE\n} else {\n  expression if FALSE\n}\n\nAs wast the case with the simple if-statement, R evaluates the condition in (condition). If the result is TRUE, R continues with the expression in the first curly brackets and it executes the expression if TRUE. If the condition in (condition) evaluates to FALSE, R executes the code in the second curly brackets after the else statement and executes the expression if FALSE. Note that here, the expressions are usually written on separate lines. The else is included in a separate line. This line starts with the last curly bracket from the first statement and ends with the first curly bracket of the second statement.\nLet’s build on the simple if-statement to illustrate the if-else statement. We already know that R should reassign the value of b to 1000 if a &lt; b. Here we will add what R needs to do in case a is larger than or equal to b. In the simple if-statement, R continued with the rest of the code. Here we will ask R to change the value of b to 5000 if a is not smaller than b. As R completes the statement, it will continue with the rest of the code. Here we will ask R to print the value of b.\n\na &lt;- 20\nb &lt;- 40\n\nif (a &lt; b) {\n  b &lt;- 1000\n} else {\n  b &lt;- 5000\n}\n\nprint(paste(\"The value of b is\", b))\n\n[1] \"The value of b is 1000\"\n\n\nR checks if the condition is met, i.e. is a &lt; b return TRUE. As this is the case, R executes the expression in the first curly brackets: b &lt;- 1000. After executing that expression, R continues with the rest of the code. In other words, it doesn’t execute the code in the second expression. Here, the rest of the code asks R to print the value of b.\nLet’s now change the value of a to 2000, reassign the 40 to b and run the if-else-statement:\n\na &lt;- 2000\nb &lt;- 40\n\nif (a &lt; b) {\n  b &lt;- 1000\n} else {\n  b &lt;- 5000\n}\n\nprint(paste(\"The value of b is\", b))\n\n[1] \"The value of b is 5000\"\n\n\nThe condintion is not met (a &lt; b returns FALSE). R now executes the code in the expression after the else statement and reassign the value of 5000 to b. The first expression is not executed. After it finishes the second expression, R continues with the rest of the code (print()).\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nRecall that R interprets “2.5” as a character variable. Suppose that your dataset includes a variable where all numeric variables are read as character variables. However, somethimes that is not the case. To change these variables from character variables in numeric variables, you first test if a variable is a character variable. If that is the case, you change the variable in numeric format and assign it to the same variable name. If the variable is numeric, you can continue with the code. Let’s use numvar as the variable to test.\nWhat function would you use to test if numvar is a character variable?\n\n\nCode\nis.character(numvar)\n\n\nWhat function would you use to change the type of numvar into numeric in case it is a character variable?\n\n\nCode\n# There are two options: \nas.numeric(numvar)\nas.double(numvar)\n\n\nLet’s assing a value to numvar\n\nnumvar &lt;- \"2.5\"\n\nWrite the if-statement: check if numvar is a character variable. If that is the case, change its type to numeric. The rest of the code should multiply the value of numvar with 10, assign and assign the value to res and print “The value of res is 25” print(paste(\"The value of res is\", res)).\n\n\nCode\nif (is.character(numvar) == TRUE) {numvar &lt;- as.numeric(numvar)}\n\nres &lt;- 10 * numvar\n\nprint(paste(\"The value of res is\", res))\n\n\n[1] \"The value of res is 25\"\n\n\nNote that you can shorten this code:\n\n\nCode\n# the condition \"is.character(numvar)\" evaluates to TRUE or FALSE. \n# if.character(numvar) == TRUE actually equals TRUE == TRUE, which is\n# obviously TRUE, but you can eliminate the \"== TRUE\" part. \n\nnumvar &lt;- \"2.5\"\nif (is.character(numvar)) {numvar &lt;- as.numeric(numvar)}\n\n\nLet’s change the previous excercise: test if numvar is numeric. If that is the case, multiply with 10 and assign the result to res. If numvar is not numeric, change the type to numeric, multiply the result with 100 and assign the result to res. Print the value of res.\nFirst, we set the value for numvar:\n\nnumvar &lt;- \"4.5\"\n\nWrite the if-else-statement.\n\n\nCode\nif (is.numeric(numvar)) {\n  res &lt;- numvar * 10\n} else {\n  res &lt;- as.numeric(numvar) * 100\n}\n\nprint(paste(\"The value of res is\", res))\n\n\n[1] \"The value of res is 450\"\n\n\nUse the vector grades with values c(15, 11, 22, 18, 12). Check if one of the elements of this vector is larger than 20. If that is the case, print “At least one grade is larger than 20”. If that is not the case, print “All grades are less than or equal to 20”.\nHow would you write the condition (recall that R is vectorized and that TRUE is 1 and FALSE is 0)?\n\n\nCode\n# test if a grade is &gt; 20\ngrades &gt; 20\n\n# this statement returns FALSE if an element is &lt;= 20 and 1 if an element is &gt; 20\n# output is e.g. TRUE, FALSE, FALSE, TRUE, FALSE\n# TRUE is 1 and FALSE is 0\n# sum(TRUE, FALSE, FALSE, TRUE, FALSE) = 2\n# if no grades are &gt; 20, the statement evaluates to FALSE for each element\n# and the sum of each of these evaluations is 0\n\n# condition\nsum(grades &gt; 20) == 0\n\n\nWrite the is-else-statement\n\n\nCode\nif (sum(grades &gt; 20) == 0) {\n  print(\"All grades are less than or equal to 20.\")\n} else {\n  print(\"At least one grade is larger than 20.\")\n}\n\n\n[1] \"All grades are less than or equal to 20.\"\n\n\nRerun you previous code with\n\ngrades &lt;- c(10, 12, 15, 18, 8)\n\n\n\n\n\n\n2.5.2 For loop\nThe simpelest for loop is written as\n\nfor (i in sequence) {\n  expression\n}\n\nUsing a for loop, R iterates over a sequence sequence and executes the expression {expression} each iteration. The expression is included in curly brackets. All lines between these two brackets are executed in each iteration. The code for the expression is intended.\nSay the sequence is a sequence starting at 1, ending at 10 in steps of 1 (1:10). As the loop starts, R sets the value of i equal to 1 and executes the code in the expression. After finishing these lines, changes the value of i and sets it equal to the second value in the sequence. This value of 2. Using i = 2 R executes the expression. After finishing the expression for the second time, R returns to the sequence and changes the value of i to 3. This continues until i equals the last value in the sequence, e.g. 10. R executes the expression one last time. After completing the expression, R continues with the rest of the code.\nTo illustrate, we’ll use a sequence 1:5 and ask R to print the value of i.\n\nfor (i in 1:5) {\n  print(paste(\"The value of i is\", i))\n}\n\n[1] \"The value of i is 1\"\n[1] \"The value of i is 2\"\n[1] \"The value of i is 3\"\n[1] \"The value of i is 4\"\n[1] \"The value of i is 5\"\n\nprint(\"R continues with the rest of the code\")\n\n[1] \"R continues with the rest of the code\"\n\n\nIn this loop, the sequence is 1:5. R enters the loop and sets the value of i equal to 1. Using this value, R executes the expression. Here is the expression asks R to print “The value if i is” i. For the first iteration, the result is “The value of i is 1”. After finishing this expression, R returns to the sequence and sets i equal to 2, executes the code (prints “The value of i is 2”), returns to the sequence, … . After the last iteration, R continues with the rest of the code and prints “R continues with the rest of the code”.\nThe sequence doesn’t have to be numeric. In the previous example, we used i, but this doesn’t have to be the case. The next for loop illustrates this using the sequence letters[1:5] and abc as the counter in the loop. Here, abc is first set equal to letters[1]. The code - print(abc) - just asks R to print the value of abc:\n\nfor (abc in letters[1:5]) {\n  print(abc)\n}\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n[1] \"e\"\n\n\nThe sequence can be any other vector. The next loop uses the vector c(\"dog\", \"cat\", \"bird\", \"fish\") and iterates over each element. We just `pet´ as the counter. The expression prints “my pet is a” with the result of the iteration:\n\nfor (pet in c(\"dog\", \"cat\", \"bird\", \"fish\")) {\n  print(paste(\"My pet is a\", pet))\n}\n\n[1] \"My pet is a dog\"\n[1] \"My pet is a cat\"\n[1] \"My pet is a bird\"\n[1] \"My pet is a fish\"\n\n\n\n\n\n\n\n\nCautionYour turn\n\n\n\n\n\nSuppose that you have a vector with the exams that you took called exams with values c(\"Financial statement analysis\", \"Marketing\"). The result on your exams was 14 and 12. These results are stored in res_ex with values c(14, 12). Write a for loop that prints “The result on my xxx exam was yyy” with xxx the exam and yyy the result. You can use print(paste(\"The result for my\", xxx, \"was\", yyy)) where you have to fill in the xxx and yyy.\n\n\nCode\nexams &lt;- c(\"Financial statement analysis\", \"Marketing\")\nres_ex &lt;- c(12, 14)\n\nfor (i in 1:2) {\n  print(paste(\"The result for my\", exams[i], \"was\", res_ex[i]))\n  }\n\n\n[1] \"The result for my Financial statement analysis was 12\"\n[1] \"The result for my Marketing was 14\"\n\n\nA for loop is often used to run simulations. Let’s try to simulate the probability that you pass a multiple choice examen where each question has 4 options, a, b, c, d and there are 48 questions. The result on the exam is equal to the number of correct answers - 8 and rescaled to 20.\nLet’s first set up the parameters of the simulation. The exam has 48 questions with 4 possible answers: a, b, c and d. Use sample() to select 48 random draws from a, b, c, and d and assign the result to the object correct.\n\n\nCode\ncorrect &lt;- sample(letters[1:4], size = 48, replace = TRUE)\n\n\nSecond, let’s set the parameters for this simulation. We’ll run n = 10000 simulations. In each iteration we’ll calculate the score. To store the score you need a vector results. Assign the value of 10000 to n and create an empty numeric vector with of length n.\n\n\nCode\nn &lt;- 10000L\nresults &lt;- vector(mode = \"numeric\", length = n)\n\n\nLet’s now run the simulation. Suppose that a student “gambles” all answers. This is as if the student draws at random letters from a, b, c and d to select to answer. In other words, the answer is also a random draw from a, b, c, d. We’ll change this random draw in each simulation and compare the with correct result. Using answer to store the answer, we can compare answer with the correct answer using answer == correct. The result is TRUE if the ith element in answer is equal to the ith element in correct, else it is FALSE. Using the fact that the TRUE is 1 and FALSE is 0, we can calculate the number of good answers as sum(answer == correct). The final result equals this sum, minus 8 divided by 2. We can calculate that final result after the for loop.\nSet up de for loop: use sample() to select at random 48 values from a, b, c and d and assign the outcome to answer. Calculate the total number of answers where the answer equals the correct answer and store that number in the results vector.\n\n\nCode\nfor (i in 1:n) {\n  answer &lt;- sample(letters[1:4], size = 48, replace = TRUE)\n  results[i] &lt;- sum(answer == correct)\n}\n\n\nNow calculate the final result: subtract 8 from the results and divide by 2\n\n\nCode\nresults &lt;- (results - 8) / 2\n\n\nWhat is the mean, the minimum and maximum score for these 10000 exams? For calculate the minimum, you can use min(); for the maximum you can use max():\n\n\nCode\nmean(results)\nmin(results)\nmax(results)\n\n\nCan you show a histogram if these results?\n\n\nCode\nhist(results)\n\n\nCan you calculate how many exam would have a grade sufficient to tolerate (&gt; 8)?\n\n\nCode\nsum(results &gt; 8)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  },
  {
    "objectID": "02_Examples.html#scripts",
    "href": "02_Examples.html#scripts",
    "title": "2  A tour of R",
    "section": "2.6 Scripts",
    "text": "2.6 Scripts\nWriting code is one thing, remembering why you wrote something the way you did is another. In addition, if you need to share code with others, it is useful is your code is easy to read. Comments and sections allow you to inform others as well as the future your and structure your code.\nIn the editor, write the following lines:\n\n# A comment\n6 + 6\n\n[1] 12\n\n7 + 7\n\n[1] 14\n\n# A second comment\n8 + 8\n\n[1] 16\n\n9 + 9\n\n[1] 18\n\n\nThe console shows the output:\n&gt; # A comment \n&gt; 6 + 6 \n[1] 12 &gt; \n7 + 7 \n[1] 14 \n&gt;  \n&gt; # A second comment \n&gt; 8 + 8 \n[1] 16 \n&gt; 9 + 9 \n[1] 18\nAs you would expect, the console shows all lines of code. However, notice that the lines starting with # are shown in the console but didn’t produce any other result. The # tells R that it doesn’t have to execute that line. If you have multiple lines, you need to add a # before every line. This very useful if you want to add comments to your code.\n\n# The next lines execute code to do something\n# They calculate the difference between two values\n10 - 9\n\n[1] 1\n\n9 - 8\n\n[1] 1\n\n8 - 7\n\n[1] 1\n\n# The next lines execute code to do something else\n# They calculate the sum of two values\n10 + 9\n\n[1] 19\n\n9 + 8\n\n[1] 17\n\n8 + 7\n\n[1] 15\n\n\nComments are extremely important. They make your code readable. If you write code, you will often use a script multiple times and/or you will share it with others. Adding comments to your code helps you to remember what you did, why you did it and how you did it. If you need to re-use the code in the future or if you have to extend the code at some later date, these comments will often be very helpful if you meet lines in your code where you don’t recall why you wrote the code the way you did. If you share your code with others, comments explain your code.\nNote that you don’t need to add comments before a line. You can also add a comment after a line:\n\n11 - 10 #Calculate difference between 11 and 10\n\n[1] 1\n\n\nYou can also use # to comment out code: to temporarily disable lines of code without deleting them. You would usually do this to test parts of the code. In the next example, the first two lines will not be executed:\n\n# 10 + 9\n# 9 + 8\n8 + 7\n\n[1] 15\n\n\nTo comment out various lines of code: select the lines and press Ctrl + shift + C (Command + shift + C). You can do the same if you want to remove the # from these lines of code.\nIf you code is long but you don’t want to split is up in various scripts, you can use # to create sections and subsections in your code. If you add 4 # at the end of a comment line, RStudio creates a section. RStudio would do the same is you end with 4 -’s. To create a subsection, start with ## and end with 4 #. You can add a name between the opening and closing hashtags. Here is an example:\n\n# name of section 1: Importing data ####\n\n# comment line: what is the purpose, ...\n\n2 + 2\n\n[1] 4\n\n## name of subsection: Importing xls files and saving as R datafiles ####\n\n# comment line: what is the purpose and how readr_xls\n\n3 + 3\n\n[1] 6\n\n## name of subsection: Importing csv files ####\n\n# comment line: wat is the purpose and how readr_csv\n\n4 + 4\n\n[1] 8\n\n### name of sub-subsection: unnecessary columns ####\n\n### name of sub-sub section: saving as R datafiles ####\n\n# name of section 2: Data wranging ####\n\n## name of subsection: tidying ####\n\n## name of subsection: transformation ####\n\nNotice that the editor now shows a small triangle next to the line number. If you click on that triangle, the section collapses. If you click on the arrow again, the section expands. These sections help you to add a structure to your code. In addition, they make it easier to scroll through the script in case you need to change specific parts of the code. In R studio, you can see the document outline if you use Ctrl + shift + O (Command + Shift + O) or if you follow Code and Show document outline from the file menu.\nSections are helpful if your code is long but not too long. If your code is long, it is preferable to split your code in various scripts and run these scripts. If you need to run a script, you can use the source() command. To illustrate: open a new file and type\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\na &lt;- pi\nprint(round(a, digits = 2))\n\n[1] 3.14\n\n\nand save as example_script.R you can reuse the script in your workflow. There are two ways to do to: first open the file and run all lines. Second, using the source() function and including the name of the script, R will run your script. For instance, close the script example_script.R, open a new file and type:\n\nsource(\"example_script.R\")\n\nIn the console, you’ll see that R prints “Hello world” and “3.14”.\nThis is very useful is larger workflows as you can distribute various steps of the flow in multiple smaller scripts, e.g. one that you use to import the data, one to clean and transform the data, one where you do some exploratory data plotting, one that includes your analysis and one that includes the plots and tables you will include in your report of paper. Long scripts are often very difficult to navigate. Using a multiple scripts for multiple tasks makes it easier to see where you are in your workflow.\n\n\n\n\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A Forward-Pipe Operator for r.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A tour of R</span>"
    ]
  }
]